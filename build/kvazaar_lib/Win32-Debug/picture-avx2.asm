; Listing generated by Microsoft (R) Optimizing Compiler Version 19.26.28806.0 

	TITLE	F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
	.686P
	.XMM
	include listing.inc
	.model	flat

INCLUDELIB MSVCRTD
INCLUDELIB OLDNAMES

PUBLIC	??_C@_0P@FIMIKDEG@array_checksum@		; `string'
PUBLIC	??_C@_09INHMEIKP@array_md5@			; `string'
PUBLIC	??_C@_07DDJKLCAH@reg_sad@			; `string'
PUBLIC	??_C@_07CCGEEBAL@sad_4x4@			; `string'
PUBLIC	??_C@_07MEAHLBLP@sad_8x8@			; `string'
PUBLIC	??_C@_09CIFGAEGF@sad_16x16@			; `string'
PUBLIC	??_C@_09PHPGBCME@sad_32x32@			; `string'
PUBLIC	??_C@_09HIOKBDLK@sad_64x64@			; `string'
PUBLIC	??_C@_08GDBJPAOD@satd_4x4@			; `string'
PUBLIC	??_C@_08IFHKAAFH@satd_8x8@			; `string'
PUBLIC	??_C@_0L@CJHLPOG@satd_16x16@			; `string'
PUBLIC	??_C@_0L@NNDHKJEH@satd_32x32@			; `string'
PUBLIC	??_C@_0L@FCCLKIDJ@satd_64x64@			; `string'
PUBLIC	??_C@_0O@FHJAMJMP@satd_any_size@		; `string'
PUBLIC	??_C@_0N@PBMCJFCI@sad_4x4_dual@			; `string'
PUBLIC	??_C@_0N@NAFGAMEI@sad_8x8_dual@			; `string'
PUBLIC	??_C@_0P@KGAKCNHP@sad_16x16_dual@		; `string'
PUBLIC	??_C@_0P@EBOHBHKK@sad_32x32_dual@		; `string'
PUBLIC	??_C@_0P@BGJOPAPB@sad_64x64_dual@		; `string'
PUBLIC	??_C@_0O@PJODLFIO@satd_4x4_dual@		; `string'
PUBLIC	??_C@_0O@NIHHCMOO@satd_8x8_dual@		; `string'
PUBLIC	??_C@_0BA@GFDJBOPJ@satd_16x16_dual@		; `string'
PUBLIC	??_C@_0BA@ICNECECM@satd_32x32_dual@		; `string'
PUBLIC	??_C@_0BA@NFKNMDHH@satd_64x64_dual@		; `string'
PUBLIC	??_C@_0BD@EFOGIBKC@satd_any_size_quad@		; `string'
PUBLIC	??_C@_0BA@BPDFDAFM@pixels_calc_ssd@		; `string'
PUBLIC	??_C@_0P@FIKELBGI@bipred_average@		; `string'
PUBLIC	??_C@_0BC@IMILIHON@get_optimized_sad@		; `string'
PUBLIC	??_C@_07OEMLCPNF@ver_sad@			; `string'
PUBLIC	??_C@_07CJIKFDIC@hor_sad@			; `string'
PUBLIC	??_C@_09GDPKBOJB@pixel_var@			; `string'
PUBLIC	??_C@_0BF@BJKPGPEP@fast_forward_dst_4x4@	; `string'
PUBLIC	??_C@_07MAFDGKIL@dct_4x4@			; `string'
PUBLIC	??_C@_07CGDAJKDP@dct_8x8@			; `string'
PUBLIC	??_C@_09LPGJIHFJ@dct_16x16@			; `string'
PUBLIC	??_C@_09GAMJJBPI@dct_32x32@			; `string'
PUBLIC	??_C@_0BF@LGIDMEOF@fast_inverse_dst_4x4@	; `string'
PUBLIC	??_C@_08LDLIJGLD@idct_4x4@			; `string'
PUBLIC	??_C@_08FFNLGGAH@idct_8x8@			; `string'
PUBLIC	??_C@_0L@BMPNPGHO@idct_16x16@			; `string'
PUBLIC	??_C@_0L@MDFNOANP@idct_32x32@			; `string'
PUBLIC	??_C@_0CA@KBMGLOML@filter_hpel_blocks_hor_ver_luma@ ; `string'
PUBLIC	??_C@_0BN@LGIFMJCD@filter_hpel_blocks_diag_luma@ ; `string'
PUBLIC	??_C@_0CA@HAFBJKBM@filter_qpel_blocks_hor_ver_luma@ ; `string'
PUBLIC	??_C@_0BN@LBBJMBKC@filter_qpel_blocks_diag_luma@ ; `string'
PUBLIC	??_C@_0BH@DGLFPIPB@sample_quarterpel_luma@	; `string'
PUBLIC	??_C@_0BF@HOLGGLNK@sample_octpel_chroma@	; `string'
PUBLIC	??_C@_0BK@LCPNGAOF@sample_quarterpel_luma_hi@	; `string'
PUBLIC	??_C@_0BI@EFIAEBMP@sample_octpel_chroma_hi@	; `string'
PUBLIC	??_C@_0BD@MPANGNDO@get_extended_block@		; `string'
PUBLIC	??_C@_05DFPBCFDJ@quant@				; `string'
PUBLIC	??_C@_0BC@MPMPMGJB@quantize_residual@		; `string'
PUBLIC	??_C@_07GMDOJGPA@dequant@			; `string'
PUBLIC	??_C@_0O@ECIHPEON@coeff_abs_sum@		; `string'
PUBLIC	??_C@_0BA@HMNKAKOO@fast_coeff_cost@		; `string'
PUBLIC	??_C@_0N@NMFCEONC@angular_pred@			; `string'
PUBLIC	??_C@_0BC@KPOCLILL@intra_pred_planar@		; `string'
PUBLIC	??_C@_0BH@EAOLFJGL@intra_pred_filtered_dc@	; `string'
PUBLIC	??_C@_0BF@LGILOHOJ@sao_edge_ddistortion@	; `string'
PUBLIC	??_C@_0BC@OCNKGNNI@calc_sao_edge_dir@		; `string'
PUBLIC	??_C@_0BG@IJPOOFFG@sao_reconstruct_color@	; `string'
PUBLIC	??_C@_0BF@HDNLAKN@sao_band_ddistortion@		; `string'
PUBLIC	??_C@_0BB@GDIDDFDB@encode_coeff_nxn@		; `string'
EXTRN	_kvz_reg_sad:DWORD
EXTRN	_kvz_sad_4x4:DWORD
EXTRN	_kvz_sad_8x8:DWORD
EXTRN	_kvz_sad_16x16:DWORD
EXTRN	_kvz_sad_32x32:DWORD
EXTRN	_kvz_sad_64x64:DWORD
EXTRN	_kvz_satd_4x4:DWORD
EXTRN	_kvz_satd_8x8:DWORD
EXTRN	_kvz_satd_16x16:DWORD
EXTRN	_kvz_satd_32x32:DWORD
EXTRN	_kvz_satd_64x64:DWORD
EXTRN	_kvz_satd_any_size:DWORD
EXTRN	_kvz_sad_4x4_dual:DWORD
EXTRN	_kvz_sad_8x8_dual:DWORD
EXTRN	_kvz_sad_16x16_dual:DWORD
EXTRN	_kvz_sad_32x32_dual:DWORD
EXTRN	_kvz_sad_64x64_dual:DWORD
EXTRN	_kvz_satd_4x4_dual:DWORD
EXTRN	_kvz_satd_8x8_dual:DWORD
EXTRN	_kvz_satd_16x16_dual:DWORD
EXTRN	_kvz_satd_32x32_dual:DWORD
EXTRN	_kvz_satd_64x64_dual:DWORD
EXTRN	_kvz_satd_any_size_quad:DWORD
EXTRN	_kvz_pixels_calc_ssd:DWORD
EXTRN	_kvz_bipred_average:DWORD
EXTRN	_kvz_get_optimized_sad:DWORD
EXTRN	_kvz_ver_sad:DWORD
EXTRN	_kvz_hor_sad:DWORD
EXTRN	_kvz_pixel_var:DWORD
EXTRN	_kvz_array_checksum:DWORD
EXTRN	_kvz_array_md5:DWORD
EXTRN	_kvz_fast_forward_dst_4x4:DWORD
EXTRN	_kvz_dct_4x4:DWORD
EXTRN	_kvz_dct_8x8:DWORD
EXTRN	_kvz_dct_16x16:DWORD
EXTRN	_kvz_dct_32x32:DWORD
EXTRN	_kvz_fast_inverse_dst_4x4:DWORD
EXTRN	_kvz_idct_4x4:DWORD
EXTRN	_kvz_idct_8x8:DWORD
EXTRN	_kvz_idct_16x16:DWORD
EXTRN	_kvz_idct_32x32:DWORD
EXTRN	_kvz_filter_hpel_blocks_hor_ver_luma:DWORD
EXTRN	_kvz_filter_hpel_blocks_diag_luma:DWORD
EXTRN	_kvz_filter_qpel_blocks_hor_ver_luma:DWORD
EXTRN	_kvz_filter_qpel_blocks_diag_luma:DWORD
EXTRN	_kvz_get_extended_block:DWORD
EXTRN	_kvz_sample_quarterpel_luma:DWORD
EXTRN	_kvz_sample_octpel_chroma:DWORD
EXTRN	_kvz_sample_quarterpel_luma_hi:DWORD
EXTRN	_kvz_sample_octpel_chroma_hi:DWORD
EXTRN	_kvz_quant:DWORD
EXTRN	_kvz_quantize_residual:DWORD
EXTRN	_kvz_dequant:DWORD
EXTRN	_kvz_coeff_abs_sum:DWORD
EXTRN	_kvz_fast_coeff_cost:DWORD
EXTRN	_kvz_angular_pred:DWORD
EXTRN	_kvz_intra_pred_planar:DWORD
EXTRN	_kvz_intra_pred_filtered_dc:DWORD
EXTRN	_kvz_sao_edge_ddistortion:DWORD
EXTRN	_kvz_calc_sao_edge_dir:DWORD
EXTRN	_kvz_sao_reconstruct_color:DWORD
EXTRN	_kvz_sao_band_ddistortion:DWORD
EXTRN	_kvz_encode_coeff_nxn:DWORD
msvcjmc	SEGMENT
__ED9CC025_corecrt_memcpy_s@h DB 01H
__875914C9_corecrt_wstring@h DB 01H
__731387C4_string@h DB 01H
__1850469A_corecrt_stdio_config@h DB 01H
__01D10305_corecrt_wstdio@h DB 01H
__9FF75F13_stdio@h DB 01H
__614D3496_malloc@h DB 01H
__4DF1518D_xmmintrin@h DB 01H
__9CB4F737_reg_sad_pow2_widths-sse41@h DB 01H
__1E5A298B_reg_sad_pow2_widths-avx2@h DB 01H
__6C65A336_cu@h DB 01H
__E2865EBA_corecrt_math@h DB 01H
__546CF5FC_crypto@h DB 01H
__6BE1C69C_encoderstate@h DB 01H
__5DE43F84_picture-avx2@c DB 01H
msvcjmc	ENDS
;	COMDAT ??_C@_0BB@GDIDDFDB@encode_coeff_nxn@
CONST	SEGMENT
??_C@_0BB@GDIDDFDB@encode_coeff_nxn@ DB 'encode_coeff_nxn', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0BF@HDNLAKN@sao_band_ddistortion@
CONST	SEGMENT
??_C@_0BF@HDNLAKN@sao_band_ddistortion@ DB 'sao_band_ddistortion', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0BG@IJPOOFFG@sao_reconstruct_color@
CONST	SEGMENT
??_C@_0BG@IJPOOFFG@sao_reconstruct_color@ DB 'sao_reconstruct_color', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0BC@OCNKGNNI@calc_sao_edge_dir@
CONST	SEGMENT
??_C@_0BC@OCNKGNNI@calc_sao_edge_dir@ DB 'calc_sao_edge_dir', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0BF@LGILOHOJ@sao_edge_ddistortion@
CONST	SEGMENT
??_C@_0BF@LGILOHOJ@sao_edge_ddistortion@ DB 'sao_edge_ddistortion', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0BH@EAOLFJGL@intra_pred_filtered_dc@
CONST	SEGMENT
??_C@_0BH@EAOLFJGL@intra_pred_filtered_dc@ DB 'intra_pred_filtered_dc', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0BC@KPOCLILL@intra_pred_planar@
CONST	SEGMENT
??_C@_0BC@KPOCLILL@intra_pred_planar@ DB 'intra_pred_planar', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0N@NMFCEONC@angular_pred@
CONST	SEGMENT
??_C@_0N@NMFCEONC@angular_pred@ DB 'angular_pred', 00H	; `string'
CONST	ENDS
;	COMDAT ??_C@_0BA@HMNKAKOO@fast_coeff_cost@
CONST	SEGMENT
??_C@_0BA@HMNKAKOO@fast_coeff_cost@ DB 'fast_coeff_cost', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0O@ECIHPEON@coeff_abs_sum@
CONST	SEGMENT
??_C@_0O@ECIHPEON@coeff_abs_sum@ DB 'coeff_abs_sum', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_07GMDOJGPA@dequant@
CONST	SEGMENT
??_C@_07GMDOJGPA@dequant@ DB 'dequant', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_0BC@MPMPMGJB@quantize_residual@
CONST	SEGMENT
??_C@_0BC@MPMPMGJB@quantize_residual@ DB 'quantize_residual', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_05DFPBCFDJ@quant@
CONST	SEGMENT
??_C@_05DFPBCFDJ@quant@ DB 'quant', 00H			; `string'
CONST	ENDS
;	COMDAT ??_C@_0BD@MPANGNDO@get_extended_block@
CONST	SEGMENT
??_C@_0BD@MPANGNDO@get_extended_block@ DB 'get_extended_block', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0BI@EFIAEBMP@sample_octpel_chroma_hi@
CONST	SEGMENT
??_C@_0BI@EFIAEBMP@sample_octpel_chroma_hi@ DB 'sample_octpel_chroma_hi', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0BK@LCPNGAOF@sample_quarterpel_luma_hi@
CONST	SEGMENT
??_C@_0BK@LCPNGAOF@sample_quarterpel_luma_hi@ DB 'sample_quarterpel_luma_'
	DB	'hi', 00H					; `string'
CONST	ENDS
;	COMDAT ??_C@_0BF@HOLGGLNK@sample_octpel_chroma@
CONST	SEGMENT
??_C@_0BF@HOLGGLNK@sample_octpel_chroma@ DB 'sample_octpel_chroma', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0BH@DGLFPIPB@sample_quarterpel_luma@
CONST	SEGMENT
??_C@_0BH@DGLFPIPB@sample_quarterpel_luma@ DB 'sample_quarterpel_luma', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0BN@LBBJMBKC@filter_qpel_blocks_diag_luma@
CONST	SEGMENT
??_C@_0BN@LBBJMBKC@filter_qpel_blocks_diag_luma@ DB 'filter_qpel_blocks_d'
	DB	'iag_luma', 00H				; `string'
CONST	ENDS
;	COMDAT ??_C@_0CA@HAFBJKBM@filter_qpel_blocks_hor_ver_luma@
CONST	SEGMENT
??_C@_0CA@HAFBJKBM@filter_qpel_blocks_hor_ver_luma@ DB 'filter_qpel_block'
	DB	's_hor_ver_luma', 00H			; `string'
CONST	ENDS
;	COMDAT ??_C@_0BN@LGIFMJCD@filter_hpel_blocks_diag_luma@
CONST	SEGMENT
??_C@_0BN@LGIFMJCD@filter_hpel_blocks_diag_luma@ DB 'filter_hpel_blocks_d'
	DB	'iag_luma', 00H				; `string'
CONST	ENDS
;	COMDAT ??_C@_0CA@KBMGLOML@filter_hpel_blocks_hor_ver_luma@
CONST	SEGMENT
??_C@_0CA@KBMGLOML@filter_hpel_blocks_hor_ver_luma@ DB 'filter_hpel_block'
	DB	's_hor_ver_luma', 00H			; `string'
CONST	ENDS
;	COMDAT ??_C@_0L@MDFNOANP@idct_32x32@
CONST	SEGMENT
??_C@_0L@MDFNOANP@idct_32x32@ DB 'idct_32x32', 00H	; `string'
CONST	ENDS
;	COMDAT ??_C@_0L@BMPNPGHO@idct_16x16@
CONST	SEGMENT
??_C@_0L@BMPNPGHO@idct_16x16@ DB 'idct_16x16', 00H	; `string'
CONST	ENDS
;	COMDAT ??_C@_08FFNLGGAH@idct_8x8@
CONST	SEGMENT
??_C@_08FFNLGGAH@idct_8x8@ DB 'idct_8x8', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_08LDLIJGLD@idct_4x4@
CONST	SEGMENT
??_C@_08LDLIJGLD@idct_4x4@ DB 'idct_4x4', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_0BF@LGIDMEOF@fast_inverse_dst_4x4@
CONST	SEGMENT
??_C@_0BF@LGIDMEOF@fast_inverse_dst_4x4@ DB 'fast_inverse_dst_4x4', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_09GAMJJBPI@dct_32x32@
CONST	SEGMENT
??_C@_09GAMJJBPI@dct_32x32@ DB 'dct_32x32', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_09LPGJIHFJ@dct_16x16@
CONST	SEGMENT
??_C@_09LPGJIHFJ@dct_16x16@ DB 'dct_16x16', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_07CGDAJKDP@dct_8x8@
CONST	SEGMENT
??_C@_07CGDAJKDP@dct_8x8@ DB 'dct_8x8', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_07MAFDGKIL@dct_4x4@
CONST	SEGMENT
??_C@_07MAFDGKIL@dct_4x4@ DB 'dct_4x4', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_0BF@BJKPGPEP@fast_forward_dst_4x4@
CONST	SEGMENT
??_C@_0BF@BJKPGPEP@fast_forward_dst_4x4@ DB 'fast_forward_dst_4x4', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_09GDPKBOJB@pixel_var@
CONST	SEGMENT
??_C@_09GDPKBOJB@pixel_var@ DB 'pixel_var', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_07CJIKFDIC@hor_sad@
CONST	SEGMENT
??_C@_07CJIKFDIC@hor_sad@ DB 'hor_sad', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_07OEMLCPNF@ver_sad@
CONST	SEGMENT
??_C@_07OEMLCPNF@ver_sad@ DB 'ver_sad', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_0BC@IMILIHON@get_optimized_sad@
CONST	SEGMENT
??_C@_0BC@IMILIHON@get_optimized_sad@ DB 'get_optimized_sad', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0P@FIKELBGI@bipred_average@
CONST	SEGMENT
??_C@_0P@FIKELBGI@bipred_average@ DB 'bipred_average', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0BA@BPDFDAFM@pixels_calc_ssd@
CONST	SEGMENT
??_C@_0BA@BPDFDAFM@pixels_calc_ssd@ DB 'pixels_calc_ssd', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0BD@EFOGIBKC@satd_any_size_quad@
CONST	SEGMENT
??_C@_0BD@EFOGIBKC@satd_any_size_quad@ DB 'satd_any_size_quad', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0BA@NFKNMDHH@satd_64x64_dual@
CONST	SEGMENT
??_C@_0BA@NFKNMDHH@satd_64x64_dual@ DB 'satd_64x64_dual', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0BA@ICNECECM@satd_32x32_dual@
CONST	SEGMENT
??_C@_0BA@ICNECECM@satd_32x32_dual@ DB 'satd_32x32_dual', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0BA@GFDJBOPJ@satd_16x16_dual@
CONST	SEGMENT
??_C@_0BA@GFDJBOPJ@satd_16x16_dual@ DB 'satd_16x16_dual', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0O@NIHHCMOO@satd_8x8_dual@
CONST	SEGMENT
??_C@_0O@NIHHCMOO@satd_8x8_dual@ DB 'satd_8x8_dual', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0O@PJODLFIO@satd_4x4_dual@
CONST	SEGMENT
??_C@_0O@PJODLFIO@satd_4x4_dual@ DB 'satd_4x4_dual', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0P@BGJOPAPB@sad_64x64_dual@
CONST	SEGMENT
??_C@_0P@BGJOPAPB@sad_64x64_dual@ DB 'sad_64x64_dual', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0P@EBOHBHKK@sad_32x32_dual@
CONST	SEGMENT
??_C@_0P@EBOHBHKK@sad_32x32_dual@ DB 'sad_32x32_dual', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0P@KGAKCNHP@sad_16x16_dual@
CONST	SEGMENT
??_C@_0P@KGAKCNHP@sad_16x16_dual@ DB 'sad_16x16_dual', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0N@NAFGAMEI@sad_8x8_dual@
CONST	SEGMENT
??_C@_0N@NAFGAMEI@sad_8x8_dual@ DB 'sad_8x8_dual', 00H	; `string'
CONST	ENDS
;	COMDAT ??_C@_0N@PBMCJFCI@sad_4x4_dual@
CONST	SEGMENT
??_C@_0N@PBMCJFCI@sad_4x4_dual@ DB 'sad_4x4_dual', 00H	; `string'
CONST	ENDS
;	COMDAT ??_C@_0O@FHJAMJMP@satd_any_size@
CONST	SEGMENT
??_C@_0O@FHJAMJMP@satd_any_size@ DB 'satd_any_size', 00H ; `string'
CONST	ENDS
;	COMDAT ??_C@_0L@FCCLKIDJ@satd_64x64@
CONST	SEGMENT
??_C@_0L@FCCLKIDJ@satd_64x64@ DB 'satd_64x64', 00H	; `string'
CONST	ENDS
;	COMDAT ??_C@_0L@NNDHKJEH@satd_32x32@
CONST	SEGMENT
??_C@_0L@NNDHKJEH@satd_32x32@ DB 'satd_32x32', 00H	; `string'
CONST	ENDS
;	COMDAT ??_C@_0L@CJHLPOG@satd_16x16@
CONST	SEGMENT
??_C@_0L@CJHLPOG@satd_16x16@ DB 'satd_16x16', 00H	; `string'
CONST	ENDS
;	COMDAT ??_C@_08IFHKAAFH@satd_8x8@
CONST	SEGMENT
??_C@_08IFHKAAFH@satd_8x8@ DB 'satd_8x8', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_08GDBJPAOD@satd_4x4@
CONST	SEGMENT
??_C@_08GDBJPAOD@satd_4x4@ DB 'satd_4x4', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_09HIOKBDLK@sad_64x64@
CONST	SEGMENT
??_C@_09HIOKBDLK@sad_64x64@ DB 'sad_64x64', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_09PHPGBCME@sad_32x32@
CONST	SEGMENT
??_C@_09PHPGBCME@sad_32x32@ DB 'sad_32x32', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_09CIFGAEGF@sad_16x16@
CONST	SEGMENT
??_C@_09CIFGAEGF@sad_16x16@ DB 'sad_16x16', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_07MEAHLBLP@sad_8x8@
CONST	SEGMENT
??_C@_07MEAHLBLP@sad_8x8@ DB 'sad_8x8', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_07CCGEEBAL@sad_4x4@
CONST	SEGMENT
??_C@_07CCGEEBAL@sad_4x4@ DB 'sad_4x4', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_07DDJKLCAH@reg_sad@
CONST	SEGMENT
??_C@_07DDJKLCAH@reg_sad@ DB 'reg_sad', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_09INHMEIKP@array_md5@
CONST	SEGMENT
??_C@_09INHMEIKP@array_md5@ DB 'array_md5', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_0P@FIMIKDEG@array_checksum@
CONST	SEGMENT
??_C@_0P@FIMIKDEG@array_checksum@ DB 'array_checksum', 00H ; `string'
CONST	ENDS
CONST	SEGMENT
_cbf_masks DW	01fH
	DW	0fH
	DW	07H
	DW	03H
	DW	01H
	ORG $+6
_default_fast_coeff_cost_wts DD 03e282e88r	; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e26c094r			; 0.162844
	DD	04081ca43r			; 4.05594
	DD	04064203ar			; 3.56447
	DD	040db915ar			; 6.86149
	DD	03e03d189r			; 0.128729
	DD	04089fbafr			; 4.31197
	DD	0407c5771r			; 3.94284
	DD	040ddeed2r			; 6.9354
	DD	03de33ce6r			; 0.110956
	DD	0408ddcb1r			; 4.43319
	DD	0407c8738r			; 3.94575
	DD	040dc1618r			; 6.8777
	DD	03dc29cfer			; 0.095026
	DD	0408f7938r			; 4.48355
	DD	0408636aar			; 4.19417
	DD	040d90260r			; 6.78154
	DD	03d99b1b8r			; 0.075046
	DD	04094474cr			; 4.6337
	DD	04082b1b6r			; 4.08419
	DD	040d65aeer			; 6.6986
	DD	03d56bca5r			; 0.052426
	DD	0409ef37er			; 4.96722
	DD	04080dee8r			; 4.02721
	DD	040d19306r			; 6.5492
	DD	03d24bcaer			; 0.040219
	DD	040a489car			; 5.14182
	DD	0407ee3bdr			; 3.98265
	DD	040cec513r			; 6.46156
	DD	03d0fba88r			; 0.03509
	DD	040a628e7r			; 5.19249
	DD	040752e49r			; 3.83095
	DD	040cd642ar			; 6.41848
	DD	03cf47d80r			; 0.029845
	DD	040a6c5d0r			; 5.21165
	DD	040743073r			; 3.81546
	DD	040cb0dd8r			; 6.34544
	DD	03cc0b136r			; 0.023522
	DD	040aa4f92r			; 5.32221
	DD	040744224r			; 3.81654
	DD	040cb8aaar			; 6.36068
	DD	03cae87d3r			; 0.021305
	DD	040a73ac3r			; 5.22592
	DD	04075eeccr			; 3.8427
	DD	040ca6cd9r			; 6.32579
	DD	03c821294r			; 0.015878
	DD	040a5dbe0r			; 5.18309
	DD	0407d2f27r			; 3.956
	DD	040ca8cbdr			; 6.32968
	DD	03c2ae297r			; 0.01043
	DD	040a32ce4r			; 5.09923
	DD	04085a85fr			; 4.1768
	DD	040c9c5d6r			; 6.3054
	DD	03c0a2a91r			; 0.008433
	DD	040a0f7der			; 5.03026
	DD	040879a50r			; 4.23759
	DD	040c8a4eer			; 6.27013
	DD	03bd4fdf4r			; 0.0065
	DD	0409f0412r			; 4.96925
	DD	0408adc57r			; 4.3394
	DD	040c6f870r			; 6.21783
	DD	03ba18373r			; 0.004929
	DD	0409d8d50r			; 4.9235
	DD	0408e283fr			; 4.44241
	DD	040c5df6cr			; 6.18352
	DD	03b73775cr			; 0.003715
	DD	0409d4c75r			; 4.91558
	DD	0408dbb1br			; 4.42909
	DD	040c4029fr			; 6.12532
	DD	03b4a70d2r			; 0.003089
	DD	0409c48f7r			; 4.88391
	DD	040920260r			; 4.56279
	DD	040c5019dr			; 6.15645
	DD	03b219c9dr			; 0.002466
	DD	0409c31abr			; 4.88106
	DD	040942800r			; 4.62988
	DD	040c49088r			; 6.14264
	DD	03b0e25c8r			; 0.002169
	DD	0409c3d62r			; 4.88249
	DD	04094ae99r			; 4.64631
	DD	040c415d1r			; 6.12766
	DD	03b26dacbr			; 0.002546
	DD	040996304r			; 4.79334
	DD	0409acc16r			; 4.83741
	DD	040c6606br			; 6.19927
	DD	03aac3a86r			; 0.001314
	DD	04099e220r			; 4.80885
	DD	0409a81bdr			; 4.82834
	DD	040c7ca3cr			; 6.24344
	DD	03a9741d1r			; 0.001154
	DD	0409b9a72r			; 4.8626
	DD	0409b19aar			; 4.84688
	DD	040c693a5r			; 6.20552
	DD	03a80f990r			; 0.000984
	DD	0409bb993r			; 4.8664
	DD	0409b7fa2r			; 4.85933
	DD	040c7b565r			; 6.24089
	DD	03a551f82r			; 0.000813
	DD	0409b698ar			; 4.85663
	DD	0409d95bar			; 4.92453
	DD	040c963a4r			; 6.29341
	DD	03a91c087r			; 0.001112
	DD	04099419er			; 4.78926
	DD	040a050f0r			; 5.00988
	DD	040cddf8fr			; 6.43354
	DD	03a10b418r			; 0.000552
	DD	04098580ar			; 4.76075
	DD	040a2e4f1r			; 5.09045
	DD	040d32e1fr			; 6.59938
	DD	039ccff22r			; 0.000391
	DD	0409ec42cr			; 4.96145
	DD	040a38d95r			; 5.11103
	DD	040d8342fr			; 6.75637
	DD	039ae1049r			; 0.000332
	DD	0409f63f8r			; 4.98095
	DD	040a46b89r			; 5.13813
	DD	040dbc1e8r			; 6.86742
	DD	03952c387r			; 0.000201
	DD	040a5d297r			; 5.18196
	DD	04097af64r			; 4.74016
	DD	040cec07dr			; 6.461
	DD	0397ba882r			; 0.00024
	DD	040a5eeb7r			; 5.18539
	DD	0409bfeb0r			; 4.87484
	DD	040da3603r			; 6.81909
	DD	03908509cr			; 0.00013
	DD	040a8a6b5r			; 5.27035
	DD	040977eacr			; 4.73421
	DD	040da708fr			; 6.82624
	DD	038da1a93r			; 0.000104
	DD	040abe6e8r			; 5.37194
	DD	040930af4r			; 4.59509
	DD	040d5189ar			; 6.65925
	DD	038ae1049r			; 8.3e-05
	DD	040ab9581r			; 5.362
	DD	04093c250r			; 4.61747
	DD	040dacf03r			; 6.83777
	DD	03890b418r			; 6.9e-05
	DD	040a926e3r			; 5.286
	DD	0409828e7r			; 4.75499
	DD	040e516e1r			; 7.15904
	DD	0384d8559r			; 4.9e-05
	DD	040afa18cr			; 5.48847
	DD	0408cace9r			; 4.39611
	DD	040d74682r			; 6.72736
	DD	038734507r			; 5.8e-05
	DD	0409eafa3r			; 4.95894
	DD	040929321r			; 4.58046
	DD	040cf49a5r			; 6.47774
	DD	037eae18br			; 2.8e-05
	DD	040b0ae1br			; 5.52125
	DD	0408e1885r			; 4.44049
	DD	040e68f80r			; 7.20502
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	0379f6230r			; 1.9e-05
	DD	040b9f5d8r			; 5.81126
	DD	0408cc582r			; 4.39911
	DD	040eac30dr			; 7.33631
_g_sig_last_scan_8x8 DD 00H
	DD	02H
	DD	01H
	DD	03H
	DD	00H
	DD	01H
	DD	02H
	DD	03H
	DD	00H
	DD	02H
	DD	01H
	DD	03H
_g_sig_last_scan_16x16 DD 00H
	DD	04H
	DD	01H
	DD	08H
	DD	05H
	DD	02H
	DD	0cH
	DD	09H
	DD	06H
	DD	03H
	DD	0dH
	DD	0aH
	DD	07H
	DD	0eH
	DD	0bH
	DD	0fH
_g_sig_last_scan_32x32 DD 00H
	DD	08H
	DD	01H
	DD	010H
	DD	09H
	DD	02H
	DD	018H
	DD	011H
	DD	0aH
	DD	03H
	DD	020H
	DD	019H
	DD	012H
	DD	0bH
	DD	04H
	DD	028H
	DD	021H
	DD	01aH
	DD	013H
	DD	0cH
	DD	05H
	DD	030H
	DD	029H
	DD	022H
	DD	01bH
	DD	014H
	DD	0dH
	DD	06H
	DD	038H
	DD	031H
	DD	02aH
	DD	023H
	DD	01cH
	DD	015H
	DD	0eH
	DD	07H
	DD	039H
	DD	032H
	DD	02bH
	DD	024H
	DD	01dH
	DD	016H
	DD	0fH
	DD	03aH
	DD	033H
	DD	02cH
	DD	025H
	DD	01eH
	DD	017H
	DD	03bH
	DD	034H
	DD	02dH
	DD	026H
	DD	01fH
	DD	03cH
	DD	035H
	DD	02eH
	DD	027H
	DD	03dH
	DD	036H
	DD	02fH
	DD	03eH
	DD	037H
	DD	03fH
_g_sig_last_scan_cg DD FLAT:_g_sig_last_scan_8x8
	DD	FLAT:_g_sig_last_scan_8x8+16
	DD	FLAT:_g_sig_last_scan_8x8+32
	DD	FLAT:_g_sig_last_scan_8x8
	DD	FLAT:_g_sig_last_scan_8x8+16
	DD	FLAT:_g_sig_last_scan_8x8+32
	DD	FLAT:_g_sig_last_scan_16x16
	DD	00H
	DD	00H
	DD	FLAT:_g_sig_last_scan_32x32
	DD	00H
	DD	00H
_g_group_idx DB	00H
	DB	01H
	DB	02H
	DB	03H
	DB	04H
	DB	04H
	DB	05H
	DB	05H
	DB	06H
	DB	06H
	DB	06H
	DB	06H
	DB	07H
	DB	07H
	DB	07H
	DB	07H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
_g_min_in_group DB 00H
	DB	01H
	DB	02H
	DB	03H
	DB	04H
	DB	06H
	DB	08H
	DB	0cH
	DB	010H
	DB	018H
	ORG $+6
_g_sao_edge_offsets DD 0ffffffffH
	DD	00H
	DD	01H
	DD	00H
	DD	00H
	DD	0ffffffffH
	DD	00H
	DD	01H
	DD	0ffffffffH
	DD	0ffffffffH
	DD	01H
	DD	01H
	DD	01H
	DD	0ffffffffH
	DD	0ffffffffH
	DD	01H
_strategies_to_select DD FLAT:??_C@_0P@FIMIKDEG@array_checksum@
	DD	FLAT:_kvz_array_checksum
	DD	FLAT:??_C@_09INHMEIKP@array_md5@
	DD	FLAT:_kvz_array_md5
	DD	FLAT:??_C@_07DDJKLCAH@reg_sad@
	DD	FLAT:_kvz_reg_sad
	DD	FLAT:??_C@_07CCGEEBAL@sad_4x4@
	DD	FLAT:_kvz_sad_4x4
	DD	FLAT:??_C@_07MEAHLBLP@sad_8x8@
	DD	FLAT:_kvz_sad_8x8
	DD	FLAT:??_C@_09CIFGAEGF@sad_16x16@
	DD	FLAT:_kvz_sad_16x16
	DD	FLAT:??_C@_09PHPGBCME@sad_32x32@
	DD	FLAT:_kvz_sad_32x32
	DD	FLAT:??_C@_09HIOKBDLK@sad_64x64@
	DD	FLAT:_kvz_sad_64x64
	DD	FLAT:??_C@_08GDBJPAOD@satd_4x4@
	DD	FLAT:_kvz_satd_4x4
	DD	FLAT:??_C@_08IFHKAAFH@satd_8x8@
	DD	FLAT:_kvz_satd_8x8
	DD	FLAT:??_C@_0L@CJHLPOG@satd_16x16@
	DD	FLAT:_kvz_satd_16x16
	DD	FLAT:??_C@_0L@NNDHKJEH@satd_32x32@
	DD	FLAT:_kvz_satd_32x32
	DD	FLAT:??_C@_0L@FCCLKIDJ@satd_64x64@
	DD	FLAT:_kvz_satd_64x64
	DD	FLAT:??_C@_0O@FHJAMJMP@satd_any_size@
	DD	FLAT:_kvz_satd_any_size
	DD	FLAT:??_C@_0N@PBMCJFCI@sad_4x4_dual@
	DD	FLAT:_kvz_sad_4x4_dual
	DD	FLAT:??_C@_0N@NAFGAMEI@sad_8x8_dual@
	DD	FLAT:_kvz_sad_8x8_dual
	DD	FLAT:??_C@_0P@KGAKCNHP@sad_16x16_dual@
	DD	FLAT:_kvz_sad_16x16_dual
	DD	FLAT:??_C@_0P@EBOHBHKK@sad_32x32_dual@
	DD	FLAT:_kvz_sad_32x32_dual
	DD	FLAT:??_C@_0P@BGJOPAPB@sad_64x64_dual@
	DD	FLAT:_kvz_sad_64x64_dual
	DD	FLAT:??_C@_0O@PJODLFIO@satd_4x4_dual@
	DD	FLAT:_kvz_satd_4x4_dual
	DD	FLAT:??_C@_0O@NIHHCMOO@satd_8x8_dual@
	DD	FLAT:_kvz_satd_8x8_dual
	DD	FLAT:??_C@_0BA@GFDJBOPJ@satd_16x16_dual@
	DD	FLAT:_kvz_satd_16x16_dual
	DD	FLAT:??_C@_0BA@ICNECECM@satd_32x32_dual@
	DD	FLAT:_kvz_satd_32x32_dual
	DD	FLAT:??_C@_0BA@NFKNMDHH@satd_64x64_dual@
	DD	FLAT:_kvz_satd_64x64_dual
	DD	FLAT:??_C@_0BD@EFOGIBKC@satd_any_size_quad@
	DD	FLAT:_kvz_satd_any_size_quad
	DD	FLAT:??_C@_0BA@BPDFDAFM@pixels_calc_ssd@
	DD	FLAT:_kvz_pixels_calc_ssd
	DD	FLAT:??_C@_0P@FIKELBGI@bipred_average@
	DD	FLAT:_kvz_bipred_average
	DD	FLAT:??_C@_0BC@IMILIHON@get_optimized_sad@
	DD	FLAT:_kvz_get_optimized_sad
	DD	FLAT:??_C@_07OEMLCPNF@ver_sad@
	DD	FLAT:_kvz_ver_sad
	DD	FLAT:??_C@_07CJIKFDIC@hor_sad@
	DD	FLAT:_kvz_hor_sad
	DD	FLAT:??_C@_09GDPKBOJB@pixel_var@
	DD	FLAT:_kvz_pixel_var
	DD	FLAT:??_C@_0BF@BJKPGPEP@fast_forward_dst_4x4@
	DD	FLAT:_kvz_fast_forward_dst_4x4
	DD	FLAT:??_C@_07MAFDGKIL@dct_4x4@
	DD	FLAT:_kvz_dct_4x4
	DD	FLAT:??_C@_07CGDAJKDP@dct_8x8@
	DD	FLAT:_kvz_dct_8x8
	DD	FLAT:??_C@_09LPGJIHFJ@dct_16x16@
	DD	FLAT:_kvz_dct_16x16
	DD	FLAT:??_C@_09GAMJJBPI@dct_32x32@
	DD	FLAT:_kvz_dct_32x32
	DD	FLAT:??_C@_0BF@LGIDMEOF@fast_inverse_dst_4x4@
	DD	FLAT:_kvz_fast_inverse_dst_4x4
	DD	FLAT:??_C@_08LDLIJGLD@idct_4x4@
	DD	FLAT:_kvz_idct_4x4
	DD	FLAT:??_C@_08FFNLGGAH@idct_8x8@
	DD	FLAT:_kvz_idct_8x8
	DD	FLAT:??_C@_0L@BMPNPGHO@idct_16x16@
	DD	FLAT:_kvz_idct_16x16
	DD	FLAT:??_C@_0L@MDFNOANP@idct_32x32@
	DD	FLAT:_kvz_idct_32x32
	DD	FLAT:??_C@_0CA@KBMGLOML@filter_hpel_blocks_hor_ver_luma@
	DD	FLAT:_kvz_filter_hpel_blocks_hor_ver_luma
	DD	FLAT:??_C@_0BN@LGIFMJCD@filter_hpel_blocks_diag_luma@
	DD	FLAT:_kvz_filter_hpel_blocks_diag_luma
	DD	FLAT:??_C@_0CA@HAFBJKBM@filter_qpel_blocks_hor_ver_luma@
	DD	FLAT:_kvz_filter_qpel_blocks_hor_ver_luma
	DD	FLAT:??_C@_0BN@LBBJMBKC@filter_qpel_blocks_diag_luma@
	DD	FLAT:_kvz_filter_qpel_blocks_diag_luma
	DD	FLAT:??_C@_0BH@DGLFPIPB@sample_quarterpel_luma@
	DD	FLAT:_kvz_sample_quarterpel_luma
	DD	FLAT:??_C@_0BF@HOLGGLNK@sample_octpel_chroma@
	DD	FLAT:_kvz_sample_octpel_chroma
	DD	FLAT:??_C@_0BK@LCPNGAOF@sample_quarterpel_luma_hi@
	DD	FLAT:_kvz_sample_quarterpel_luma_hi
	DD	FLAT:??_C@_0BI@EFIAEBMP@sample_octpel_chroma_hi@
	DD	FLAT:_kvz_sample_octpel_chroma_hi
	DD	FLAT:??_C@_0BD@MPANGNDO@get_extended_block@
	DD	FLAT:_kvz_get_extended_block
	DD	FLAT:??_C@_05DFPBCFDJ@quant@
	DD	FLAT:_kvz_quant
	DD	FLAT:??_C@_0BC@MPMPMGJB@quantize_residual@
	DD	FLAT:_kvz_quantize_residual
	DD	FLAT:??_C@_07GMDOJGPA@dequant@
	DD	FLAT:_kvz_dequant
	DD	FLAT:??_C@_0O@ECIHPEON@coeff_abs_sum@
	DD	FLAT:_kvz_coeff_abs_sum
	DD	FLAT:??_C@_0BA@HMNKAKOO@fast_coeff_cost@
	DD	FLAT:_kvz_fast_coeff_cost
	DD	FLAT:??_C@_0N@NMFCEONC@angular_pred@
	DD	FLAT:_kvz_angular_pred
	DD	FLAT:??_C@_0BC@KPOCLILL@intra_pred_planar@
	DD	FLAT:_kvz_intra_pred_planar
	DD	FLAT:??_C@_0BH@EAOLFJGL@intra_pred_filtered_dc@
	DD	FLAT:_kvz_intra_pred_filtered_dc
	DD	FLAT:??_C@_0BF@LGILOHOJ@sao_edge_ddistortion@
	DD	FLAT:_kvz_sao_edge_ddistortion
	DD	FLAT:??_C@_0BC@OCNKGNNI@calc_sao_edge_dir@
	DD	FLAT:_kvz_calc_sao_edge_dir
	DD	FLAT:??_C@_0BG@IJPOOFFG@sao_reconstruct_color@
	DD	FLAT:_kvz_sao_reconstruct_color
	DD	FLAT:??_C@_0BF@HDNLAKN@sao_band_ddistortion@
	DD	FLAT:_kvz_sao_band_ddistortion
	DD	FLAT:??_C@_0BB@GDIDDFDB@encode_coeff_nxn@
	DD	FLAT:_kvz_encode_coeff_nxn
	DD	00H
	DD	00H
CONST	ENDS
PUBLIC	_kvz_strategy_register_picture_avx2
PUBLIC	_kvz_reg_sad_avx2
PUBLIC	__JustMyCode_Default
PUBLIC	?__LINE__Var@?0??bipred_average_px_px_template_avx2@@9@9 ; `bipred_average_px_px_template_avx2'::`1'::__LINE__Var
PUBLIC	??_C@_0CE@OJCKHEGB@Branch?5for?54x4?5not?5yet?5implemen@ ; `string'
PUBLIC	??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@ ; `string'
PUBLIC	??_C@_1IG@PFICKALD@?$AA?$CB?$AA?$CI?$AAp?$AAu?$AA_?$AAw?$AA?5?$AA?$DN?$AA?$DN?$AA?5?$AA4?$AA?5?$AA?$CG?$AA?$CG?$AA?5@ ; `string'
PUBLIC	??_C@_0CE@ICBEPKHC@Branch?5for?52x8?5not?5yet?5implemen@ ; `string'
PUBLIC	??_C@_1IG@BCNOOMBC@?$AA?$CB?$AA?$CI?$AAp?$AAu?$AA_?$AAw?$AA?5?$AA?$DN?$AA?$DN?$AA?5?$AA2?$AA?5?$AA?$CG?$AA?$CG?$AA?5@ ; `string'
PUBLIC	??_C@_1DO@PGNEGAJ@?$AA0?$AA?5?$AA?$CG?$AA?$CG?$AA?5?$AA?$CC?$AAU?$AAn?$AAe?$AAx?$AAp?$AAe?$AAc?$AAt?$AAe@ ; `string'
PUBLIC	?__LINE__Var@?0??bipred_average_px_px_avx2@@9@9	; `bipred_average_px_px_avx2'::`1'::__LINE__Var
PUBLIC	?__LINE__Var@?0??bipred_average_im_im_template_avx2@@9@9 ; `bipred_average_im_im_template_avx2'::`1'::__LINE__Var
PUBLIC	?__LINE__Var@?0??bipred_average_im_im_avx2@@9@9	; `bipred_average_im_im_avx2'::`1'::__LINE__Var
PUBLIC	?__LINE__Var@?0??bipred_average_px_im_template_avx2@@9@9 ; `bipred_average_px_im_template_avx2'::`1'::__LINE__Var
PUBLIC	?__LINE__Var@?0??bipred_average_px_im_avx2@@9@9	; `bipred_average_px_im_avx2'::`1'::__LINE__Var
PUBLIC	??_C@_04GEEJMEMG@avx2@				; `string'
PUBLIC	__xmm@00000000ffffffffffffffffffffffff
PUBLIC	__xmm@00010001000100010001000100010001
PUBLIC	__xmm@08080808080808080000000000000000
PUBLIC	__xmm@0c0c0c0c080808080404040400000000
PUBLIC	__xmm@0f0e0d0c0b0a09080706050403020100
PUBLIC	__xmm@41f00000000000000000000000000000
PUBLIC	__xmm@ffffffffffffffffffffffffffffffff
PUBLIC	__ymm@0000000000000000000000000000000000000000ffffffffffffffffffffffff
PUBLIC	__ymm@0000000000000000ffffffffffffffffffffffff000000000000000000000000
PUBLIC	__ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
PUBLIC	__ymm@0001000100010001000100010001000100010001000100010001000100010001
PUBLIC	__ymm@1f1e1d1c1b1a191817161514131211100f0e0d0c0b0a09080706050403020100
PUBLIC	__ymm@7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f
PUBLIC	__ymm@ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
EXTRN	__imp___wassert:PROC
EXTRN	_kvz_strategyselector_register:PROC
EXTRN	_kvz_fast_clip_32bit_to_pixel:PROC
EXTRN	_kvz_satd_4x4_subblock_generic:PROC
EXTRN	_kvz_satd_4x4_subblock_quad_generic:PROC
EXTRN	@_RTC_CheckStackVars@8:PROC
EXTRN	@__CheckForDebuggerJustMyCode@4:PROC
EXTRN	@__security_check_cookie@4:PROC
EXTRN	__RTC_CheckEsp:PROC
EXTRN	__RTC_InitBase:PROC
EXTRN	__RTC_Shutdown:PROC
EXTRN	__chkstk:PROC
EXTRN	__ltod3:PROC
EXTRN	___security_cookie:DWORD
EXTRN	__fltused:DWORD
;	COMDAT __ymm@ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
CONST	SEGMENT
__ymm@ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff DB 0ffH
	DB	0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH
	DB	0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH
	DB	0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH
CONST	ENDS
;	COMDAT __ymm@7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f
CONST	SEGMENT
__ymm@7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f DB 07fH
	DB	07fH, 07fH, 07fH, 07fH, 07fH, 07fH, 07fH, 07fH, 07fH, 07fH, 07fH
	DB	07fH, 07fH, 07fH, 07fH, 07fH, 07fH, 07fH, 07fH, 07fH, 07fH, 07fH
	DB	07fH, 07fH, 07fH, 07fH, 07fH, 07fH, 07fH, 07fH, 07fH
CONST	ENDS
;	COMDAT __ymm@1f1e1d1c1b1a191817161514131211100f0e0d0c0b0a09080706050403020100
CONST	SEGMENT
__ymm@1f1e1d1c1b1a191817161514131211100f0e0d0c0b0a09080706050403020100 DB 00H
	DB	01H, 02H, 03H, 04H, 05H, 06H, 07H, 08H, 09H, 0aH, 0bH, 0cH, 0dH
	DB	0eH, 0fH, 010H, 011H, 012H, 013H, 014H, 015H, 016H, 017H, 018H
	DB	019H, 01aH, 01bH, 01cH, 01dH, 01eH, 01fH
CONST	ENDS
;	COMDAT __ymm@0001000100010001000100010001000100010001000100010001000100010001
CONST	SEGMENT
__ymm@0001000100010001000100010001000100010001000100010001000100010001 DB 01H
	DB	00H, 01H, 00H, 01H, 00H, 01H, 00H, 01H, 00H, 01H, 00H, 01H, 00H
	DB	01H, 00H, 01H, 00H, 01H, 00H, 01H, 00H, 01H, 00H, 01H, 00H, 01H
	DB	00H, 01H, 00H, 01H, 00H
CONST	ENDS
;	COMDAT __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
CONST	SEGMENT
__ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff DB 0ffH
	DB	0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH
	DB	0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH
	DB	0ffH, 00H, 00H, 00H, 00H, 00H, 00H, 00H, 00H
CONST	ENDS
;	COMDAT __ymm@0000000000000000ffffffffffffffffffffffff000000000000000000000000
CONST	SEGMENT
__ymm@0000000000000000ffffffffffffffffffffffff000000000000000000000000 DB 00H
	DB	00H, 00H, 00H, 00H, 00H, 00H, 00H, 00H, 00H, 00H, 00H, 0ffH, 0ffH
	DB	0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 00H
	DB	00H, 00H, 00H, 00H, 00H, 00H, 00H
CONST	ENDS
;	COMDAT __ymm@0000000000000000000000000000000000000000ffffffffffffffffffffffff
CONST	SEGMENT
__ymm@0000000000000000000000000000000000000000ffffffffffffffffffffffff DB 0ffH
	DB	0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH
	DB	00H, 00H, 00H, 00H, 00H, 00H, 00H, 00H, 00H, 00H, 00H, 00H, 00H
	DB	00H, 00H, 00H, 00H, 00H, 00H, 00H
CONST	ENDS
;	COMDAT __xmm@ffffffffffffffffffffffffffffffff
CONST	SEGMENT
__xmm@ffffffffffffffffffffffffffffffff DB 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH
	DB	0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH
CONST	ENDS
;	COMDAT __xmm@41f00000000000000000000000000000
CONST	SEGMENT
__xmm@41f00000000000000000000000000000 DB 00H, 00H, 00H, 00H, 00H, 00H, 00H
	DB	00H, 00H, 00H, 00H, 00H, 00H, 00H, 0f0H, 'A'
CONST	ENDS
;	COMDAT __xmm@0f0e0d0c0b0a09080706050403020100
CONST	SEGMENT
__xmm@0f0e0d0c0b0a09080706050403020100 DB 00H, 01H, 02H, 03H, 04H, 05H, 06H
	DB	07H, 08H, 09H, 0aH, 0bH, 0cH, 0dH, 0eH, 0fH
CONST	ENDS
;	COMDAT __xmm@0c0c0c0c080808080404040400000000
CONST	SEGMENT
__xmm@0c0c0c0c080808080404040400000000 DB 00H, 00H, 00H, 00H, 04H, 04H, 04H
	DB	04H, 08H, 08H, 08H, 08H, 0cH, 0cH, 0cH, 0cH
CONST	ENDS
;	COMDAT __xmm@08080808080808080000000000000000
CONST	SEGMENT
__xmm@08080808080808080000000000000000 DB 00H, 00H, 00H, 00H, 00H, 00H, 00H
	DB	00H, 08H, 08H, 08H, 08H, 08H, 08H, 08H, 08H
CONST	ENDS
;	COMDAT __xmm@00010001000100010001000100010001
CONST	SEGMENT
__xmm@00010001000100010001000100010001 DB 01H, 00H, 01H, 00H, 01H, 00H, 01H
	DB	00H, 01H, 00H, 01H, 00H, 01H, 00H, 01H, 00H
CONST	ENDS
;	COMDAT __xmm@00000000ffffffffffffffffffffffff
CONST	SEGMENT
__xmm@00000000ffffffffffffffffffffffff DB 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH
	DB	0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 0ffH, 00H, 00H, 00H, 00H
CONST	ENDS
;	COMDAT rtc$TMZ
rtc$TMZ	SEGMENT
__RTC_Shutdown.rtc$TMZ DD FLAT:__RTC_Shutdown
rtc$TMZ	ENDS
;	COMDAT rtc$IMZ
rtc$IMZ	SEGMENT
__RTC_InitBase.rtc$IMZ DD FLAT:__RTC_InitBase
rtc$IMZ	ENDS
;	COMDAT ??_C@_04GEEJMEMG@avx2@
CONST	SEGMENT
??_C@_04GEEJMEMG@avx2@ DB 'avx2', 00H			; `string'
CONST	ENDS
;	COMDAT ?__LINE__Var@?0??bipred_average_px_im_avx2@@9@9
_DATA	SEGMENT
?__LINE__Var@?0??bipred_average_px_im_avx2@@9@9 DD 0569H ; `bipred_average_px_im_avx2'::`1'::__LINE__Var
_DATA	ENDS
;	COMDAT ?__LINE__Var@?0??bipred_average_px_im_template_avx2@@9@9
_DATA	SEGMENT
?__LINE__Var@?0??bipred_average_px_im_template_avx2@@9@9 DD 04a0H ; `bipred_average_px_im_template_avx2'::`1'::__LINE__Var
_DATA	ENDS
;	COMDAT ?__LINE__Var@?0??bipred_average_im_im_avx2@@9@9
_DATA	SEGMENT
?__LINE__Var@?0??bipred_average_im_im_avx2@@9@9 DD 0478H ; `bipred_average_im_im_avx2'::`1'::__LINE__Var
_DATA	ENDS
;	COMDAT ?__LINE__Var@?0??bipred_average_im_im_template_avx2@@9@9
_DATA	SEGMENT
?__LINE__Var@?0??bipred_average_im_im_template_avx2@@9@9 DD 03bcH ; `bipred_average_im_im_template_avx2'::`1'::__LINE__Var
_DATA	ENDS
;	COMDAT ?__LINE__Var@?0??bipred_average_px_px_avx2@@9@9
_DATA	SEGMENT
?__LINE__Var@?0??bipred_average_px_px_avx2@@9@9 DD 0394H ; `bipred_average_px_px_avx2'::`1'::__LINE__Var
_DATA	ENDS
;	COMDAT ??_C@_1DO@PGNEGAJ@?$AA0?$AA?5?$AA?$CG?$AA?$CG?$AA?5?$AA?$CC?$AAU?$AAn?$AAe?$AAx?$AAp?$AAe?$AAc?$AAt?$AAe@
CONST	SEGMENT
??_C@_1DO@PGNEGAJ@?$AA0?$AA?5?$AA?$CG?$AA?$CG?$AA?5?$AA?$CC?$AAU?$AAn?$AAe?$AAx?$AAp?$AAe?$AAc?$AAt?$AAe@ DB '0'
	DB	00H, ' ', 00H, '&', 00H, '&', 00H, ' ', 00H, '"', 00H, 'U', 00H
	DB	'n', 00H, 'e', 00H, 'x', 00H, 'p', 00H, 'e', 00H, 'c', 00H, 't'
	DB	00H, 'e', 00H, 'd', 00H, ' ', 00H, 'b', 00H, 'l', 00H, 'o', 00H
	DB	'c', 00H, 'k', 00H, ' ', 00H, 'w', 00H, 'i', 00H, 'd', 00H, 't'
	DB	00H, 'h', 00H, '.', 00H, '"', 00H, 00H, 00H	; `string'
CONST	ENDS
;	COMDAT ??_C@_1IG@BCNOOMBC@?$AA?$CB?$AA?$CI?$AAp?$AAu?$AA_?$AAw?$AA?5?$AA?$DN?$AA?$DN?$AA?5?$AA2?$AA?5?$AA?$CG?$AA?$CG?$AA?5@
CONST	SEGMENT
??_C@_1IG@BCNOOMBC@?$AA?$CB?$AA?$CI?$AAp?$AAu?$AA_?$AAw?$AA?5?$AA?$DN?$AA?$DN?$AA?5?$AA2?$AA?5?$AA?$CG?$AA?$CG?$AA?5@ DB '!'
	DB	00H, '(', 00H, 'p', 00H, 'u', 00H, '_', 00H, 'w', 00H, ' ', 00H
	DB	'=', 00H, '=', 00H, ' ', 00H, '2', 00H, ' ', 00H, '&', 00H, '&'
	DB	00H, ' ', 00H, 'p', 00H, 'u', 00H, '_', 00H, 'h', 00H, ' ', 00H
	DB	'=', 00H, '=', 00H, ' ', 00H, '8', 00H, ')', 00H, ' ', 00H, '&'
	DB	00H, '&', 00H, ' ', 00H, '"', 00H, 'B', 00H, 'r', 00H, 'a', 00H
	DB	'n', 00H, 'c', 00H, 'h', 00H, ' ', 00H, 'f', 00H, 'o', 00H, 'r'
	DB	00H, ' ', 00H, '2', 00H, 'x', 00H, '8', 00H, ' ', 00H, 'n', 00H
	DB	'o', 00H, 't', 00H, ' ', 00H, 'y', 00H, 'e', 00H, 't', 00H, ' '
	DB	00H, 'i', 00H, 'm', 00H, 'p', 00H, 'l', 00H, 'e', 00H, 'm', 00H
	DB	'e', 00H, 'n', 00H, 't', 00H, 'e', 00H, 'd', 00H, '.', 00H, '"'
	DB	00H, 00H, 00H				; `string'
CONST	ENDS
;	COMDAT ??_C@_0CE@ICBEPKHC@Branch?5for?52x8?5not?5yet?5implemen@
CONST	SEGMENT
??_C@_0CE@ICBEPKHC@Branch?5for?52x8?5not?5yet?5implemen@ DB 'Branch for 2'
	DB	'x8 not yet implemented.', 00H		; `string'
CONST	ENDS
;	COMDAT ??_C@_1IG@PFICKALD@?$AA?$CB?$AA?$CI?$AAp?$AAu?$AA_?$AAw?$AA?5?$AA?$DN?$AA?$DN?$AA?5?$AA4?$AA?5?$AA?$CG?$AA?$CG?$AA?5@
CONST	SEGMENT
??_C@_1IG@PFICKALD@?$AA?$CB?$AA?$CI?$AAp?$AAu?$AA_?$AAw?$AA?5?$AA?$DN?$AA?$DN?$AA?5?$AA4?$AA?5?$AA?$CG?$AA?$CG?$AA?5@ DB '!'
	DB	00H, '(', 00H, 'p', 00H, 'u', 00H, '_', 00H, 'w', 00H, ' ', 00H
	DB	'=', 00H, '=', 00H, ' ', 00H, '4', 00H, ' ', 00H, '&', 00H, '&'
	DB	00H, ' ', 00H, 'p', 00H, 'u', 00H, '_', 00H, 'h', 00H, ' ', 00H
	DB	'=', 00H, '=', 00H, ' ', 00H, '4', 00H, ')', 00H, ' ', 00H, '&'
	DB	00H, '&', 00H, ' ', 00H, '"', 00H, 'B', 00H, 'r', 00H, 'a', 00H
	DB	'n', 00H, 'c', 00H, 'h', 00H, ' ', 00H, 'f', 00H, 'o', 00H, 'r'
	DB	00H, ' ', 00H, '4', 00H, 'x', 00H, '4', 00H, ' ', 00H, 'n', 00H
	DB	'o', 00H, 't', 00H, ' ', 00H, 'y', 00H, 'e', 00H, 't', 00H, ' '
	DB	00H, 'i', 00H, 'm', 00H, 'p', 00H, 'l', 00H, 'e', 00H, 'm', 00H
	DB	'e', 00H, 'n', 00H, 't', 00H, 'e', 00H, 'd', 00H, '.', 00H, '"'
	DB	00H, 00H, 00H				; `string'
CONST	ENDS
;	COMDAT ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
CONST	SEGMENT
??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@ DB 'F'
	DB	00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H, 'n', 00H
	DB	'_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c', 00H, '_'
	DB	00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H, '_', 00H
	DB	'2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k', 00H, 'v'
	DB	00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H, '-', 00H
	DB	'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r', 00H, '\'
	DB	00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H, 't', 00H
	DB	'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i', 00H, 'e'
	DB	00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H, '2', 00H
	DB	'\', 00H, 'p', 00H, 'i', 00H, 'c', 00H, 't', 00H, 'u', 00H, 'r'
	DB	00H, 'e', 00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H, '2', 00H
	DB	'.', 00H, 'c', 00H, 00H, 00H			; `string'
CONST	ENDS
;	COMDAT ??_C@_0CE@OJCKHEGB@Branch?5for?54x4?5not?5yet?5implemen@
CONST	SEGMENT
??_C@_0CE@OJCKHEGB@Branch?5for?54x4?5not?5yet?5implemen@ DB 'Branch for 4'
	DB	'x4 not yet implemented.', 00H		; `string'
CONST	ENDS
;	COMDAT ?__LINE__Var@?0??bipred_average_px_px_template_avx2@@9@9
_DATA	SEGMENT
?__LINE__Var@?0??bipred_average_px_px_template_avx2@@9@9 DD 0333H ; `bipred_average_px_px_template_avx2'::`1'::__LINE__Var
_DATA	ENDS
; Function compile flags: /Odt
;	COMDAT __JustMyCode_Default
_TEXT	SEGMENT
__JustMyCode_Default PROC				; COMDAT
	push	ebp
	mov	ebp, esp
	pop	ebp
	ret	0
__JustMyCode_Default ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _pixel_var_avx2
_TEXT	SEGMENT
_buf$ = 8						; size = 4
_len$ = 12						; size = 4
_pixel_var_avx2 PROC					; COMDAT

; 1706 : {

	push	ebp
	mov	ebp, esp
	sub	esp, 192				; 000000c0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-192]
	mov	ecx, 48					; 00000030H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 1707 :   return pixel_var_avx2_largebuf(buf, len);

	mov	eax, DWORD PTR _len$[ebp]
	push	eax
	mov	ecx, DWORD PTR _buf$[ebp]
	push	ecx
	call	_pixel_var_avx2_largebuf
	add	esp, 8

; 1708 : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 192				; 000000c0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_pixel_var_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _pixel_var_avx2_largebuf
_TEXT	SEGMENT
tv274 = -3472						; size = 4
tv293 = -3464						; size = 8
tv331 = -3456						; size = 4
tv289 = -3456						; size = 4
_diff$1 = -2488						; size = 4
_var_sum$ = -2476					; size = 4
_accum8$ = -2464					; size = 16
_accum7$ = -2432					; size = 32
_accum6$ = -2368					; size = 32
_accum5$ = -2304					; size = 32
_accum4$ = -2240					; size = 32
_accum3$ = -2176					; size = 32
_accum2$ = -2112					; size = 32
_accum2_d$ = -2048					; size = 32
_accum_d$ = -1984					; size = 32
_curr$2 = -1920						; size = 32
_curr23$3 = -1856					; size = 32
_curr01$4 = -1792					; size = 32
_curr3_v$5 = -1728					; size = 32
_curr2_v$6 = -1664					; size = 32
_curr1_v$7 = -1600					; size = 32
_curr0_v$8 = -1536					; size = 32
_curr3_sd$9 = -1472					; size = 32
_curr2_sd$10 = -1408					; size = 32
_curr1_sd$11 = -1344					; size = 32
_curr0_sd$12 = -1280					; size = 32
_curr3_f$13 = -1216					; size = 32
_curr2_f$14 = -1152					; size = 32
_curr1_f$15 = -1088					; size = 32
_curr0_f$16 = -1024					; size = 32
_curr3_32$17 = -960					; size = 32
_curr2_32$18 = -896					; size = 32
_curr1_32$19 = -832					; size = 32
_curr0_32$20 = -768					; size = 32
_curr3$21 = -704					; size = 16
_curr2$22 = -672					; size = 16
_curr1$23 = -640					; size = 16
_curr0$24 = -608					; size = 16
_accum$ = -576						; size = 32
_mean$ = -512						; size = 32
_mean_f$ = -460						; size = 4
_sum_5$ = -448						; size = 16
_sum_4$ = -416						; size = 16
_sum_3$ = -384						; size = 16
_sum_hi$ = -352						; size = 16
_sum_lo$ = -320						; size = 16
_curr_sum$25 = -288					; size = 32
_curr$26 = -224						; size = 32
_sums$ = -160						; size = 32
_i$ = -92						; size = 4
_sum$ = -80						; size = 8
_zero$ = -64						; size = 32
_len_f$ = -8						; size = 4
_buf$ = 8						; size = 4
_len$ = 12						; size = 4
_pixel_var_avx2_largebuf PROC				; COMDAT

; 1534 : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 3480				; 00000d98H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-3480]
	mov	ecx, 870				; 00000366H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 1535 :   const float len_f  = (float)len;

	mov	eax, DWORD PTR _len$[ebx]
	mov	DWORD PTR tv289[ebp], eax
	vcvtsi2sd xmm0, xmm0, DWORD PTR tv289[ebp]
	mov	ecx, DWORD PTR tv289[ebp]
	shr	ecx, 31					; 0000001fH
	vaddsd	xmm0, xmm0, QWORD PTR __xmm@41f00000000000000000000000000000[ecx*8]
	vmovsd	QWORD PTR tv293[ebp], xmm0
	vcvtsd2ss xmm0, xmm0, QWORD PTR tv293[ebp]
	vmovss	DWORD PTR _len_f$[ebp], xmm0

; 1536 :   const __m256i zero = _mm256_setzero_si256();

	vpxor	xmm0, xmm0, xmm0
	vmovdqu	YMMWORD PTR _zero$[ebp], ymm0

; 1537 : 
; 1538 :   int64_t sum;
; 1539 :   size_t i;
; 1540 :   __m256i sums = zero;

	vmovdqu	ymm0, YMMWORD PTR _zero$[ebp]
	vmovdqu	YMMWORD PTR _sums$[ebp], ymm0

; 1541 :   for (i = 0; i + 31 < len; i += 32) {

	mov	DWORD PTR _i$[ebp], 0
	jmp	SHORT $LN4@pixel_var_
$LN2@pixel_var_:
	mov	eax, DWORD PTR _i$[ebp]
	add	eax, 32					; 00000020H
	mov	DWORD PTR _i$[ebp], eax
$LN4@pixel_var_:
	mov	eax, DWORD PTR _i$[ebp]
	add	eax, 31					; 0000001fH
	cmp	eax, DWORD PTR _len$[ebx]
	jae	SHORT $LN3@pixel_var_

; 1542 :     __m256i curr = _mm256_loadu_si256((const __m256i *)(buf + i));

	mov	eax, DWORD PTR _buf$[ebx]
	add	eax, DWORD PTR _i$[ebp]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _curr$26[ebp], ymm0

; 1543 :     __m256i curr_sum = _mm256_sad_epu8(curr, zero);

	vmovdqu	ymm0, YMMWORD PTR _curr$26[ebp]
	vpsadbw	ymm0, ymm0, YMMWORD PTR _zero$[ebp]
	vmovdqu	YMMWORD PTR _curr_sum$25[ebp], ymm0

; 1544 :             sums = _mm256_add_epi64(sums, curr_sum);

	vmovdqu	ymm0, YMMWORD PTR _sums$[ebp]
	vpaddq	ymm0, ymm0, YMMWORD PTR _curr_sum$25[ebp]
	vmovdqu	YMMWORD PTR _sums$[ebp], ymm0

; 1545 :   }

	jmp	SHORT $LN2@pixel_var_
$LN3@pixel_var_:

; 1546 :   __m128i sum_lo = _mm256_castsi256_si128  (sums);

	vmovdqu	xmm0, XMMWORD PTR _sums$[ebp]
	vmovdqa	XMMWORD PTR _sum_lo$[ebp], xmm0

; 1547 :   __m128i sum_hi = _mm256_extracti128_si256(sums,   1);

	vmovdqu	ymm0, YMMWORD PTR _sums$[ebp]
	vextracti128 xmm0, ymm0, 1
	vmovdqa	XMMWORD PTR _sum_hi$[ebp], xmm0

; 1548 :   __m128i sum_3  = _mm_add_epi64           (sum_lo, sum_hi);

	vmovdqa	xmm0, XMMWORD PTR _sum_lo$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sum_hi$[ebp]
	vmovdqa	XMMWORD PTR _sum_3$[ebp], xmm0

; 1549 :   __m128i sum_4  = _mm_shuffle_epi32       (sum_3,  _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sum_3$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sum_4$[ebp], xmm0

; 1550 :   __m128i sum_5  = _mm_add_epi64           (sum_3,  sum_4);

	vmovdqa	xmm0, XMMWORD PTR _sum_3$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sum_4$[ebp]
	vmovdqa	XMMWORD PTR _sum_5$[ebp], xmm0

; 1551 : 
; 1552 :   _mm_storel_epi64((__m128i *)&sum, sum_5);

	vmovdqa	xmm0, XMMWORD PTR _sum_5$[ebp]
	vmovq	QWORD PTR _sum$[ebp], xmm0

; 1553 : 
; 1554 :   // Remaining len mod 32 pixels
; 1555 :   for (; i < len; ++i) {

	jmp	SHORT $LN7@pixel_var_
$LN5@pixel_var_:
	mov	eax, DWORD PTR _i$[ebp]
	add	eax, 1
	mov	DWORD PTR _i$[ebp], eax
$LN7@pixel_var_:
	mov	eax, DWORD PTR _i$[ebp]
	cmp	eax, DWORD PTR _len$[ebx]
	jae	SHORT $LN6@pixel_var_

; 1556 :     sum += buf[i];

	mov	eax, DWORD PTR _buf$[ebx]
	add	eax, DWORD PTR _i$[ebp]
	movzx	eax, BYTE PTR [eax]
	cdq
	add	eax, DWORD PTR _sum$[ebp]
	adc	edx, DWORD PTR _sum$[ebp+4]
	mov	DWORD PTR _sum$[ebp], eax
	mov	DWORD PTR _sum$[ebp+4], edx

; 1557 :   }

	jmp	SHORT $LN5@pixel_var_
$LN6@pixel_var_:

; 1558 : 
; 1559 :   float   mean_f = (float)sum / len_f;

	mov	edx, DWORD PTR _sum$[ebp+4]
	mov	ecx, DWORD PTR _sum$[ebp]
	call	__ltod3
	vcvtsd2ss xmm0, xmm0, xmm0
	vdivss	xmm0, xmm0, DWORD PTR _len_f$[ebp]
	vmovss	DWORD PTR _mean_f$[ebp], xmm0

; 1560 :   __m256  mean   = _mm256_set1_ps(mean_f);

	vmovss	xmm0, DWORD PTR _mean_f$[ebp]
	vbroadcastss ymm0, xmm0
	vmovups	YMMWORD PTR _mean$[ebp], ymm0

; 1561 :   __m256  accum  = _mm256_setzero_ps();

	vxorps	xmm0, xmm0, xmm0
	vmovups	YMMWORD PTR _accum$[ebp], ymm0

; 1562 : 
; 1563 :   for (i = 0; i + 31 < len; i += 32) {

	mov	DWORD PTR _i$[ebp], 0
	jmp	SHORT $LN10@pixel_var_
$LN8@pixel_var_:
	mov	eax, DWORD PTR _i$[ebp]
	add	eax, 32					; 00000020H
	mov	DWORD PTR _i$[ebp], eax
$LN10@pixel_var_:
	mov	eax, DWORD PTR _i$[ebp]
	add	eax, 31					; 0000001fH
	cmp	eax, DWORD PTR _len$[ebx]
	jae	$LN9@pixel_var_

; 1564 :     __m128i curr0    = _mm_loadl_epi64((const __m128i *)(buf + i +  0));

	mov	eax, DWORD PTR _buf$[ebx]
	add	eax, DWORD PTR _i$[ebp]
	vmovq	xmm0, QWORD PTR [eax]
	vmovdqa	XMMWORD PTR _curr0$24[ebp], xmm0

; 1565 :     __m128i curr1    = _mm_loadl_epi64((const __m128i *)(buf + i +  8));

	mov	eax, DWORD PTR _buf$[ebx]
	add	eax, DWORD PTR _i$[ebp]
	vmovq	xmm0, QWORD PTR [eax+8]
	vmovdqa	XMMWORD PTR _curr1$23[ebp], xmm0

; 1566 :     __m128i curr2    = _mm_loadl_epi64((const __m128i *)(buf + i + 16));

	mov	eax, DWORD PTR _buf$[ebx]
	add	eax, DWORD PTR _i$[ebp]
	vmovq	xmm0, QWORD PTR [eax+16]
	vmovdqa	XMMWORD PTR _curr2$22[ebp], xmm0

; 1567 :     __m128i curr3    = _mm_loadl_epi64((const __m128i *)(buf + i + 24));

	mov	eax, DWORD PTR _buf$[ebx]
	add	eax, DWORD PTR _i$[ebp]
	vmovq	xmm0, QWORD PTR [eax+24]
	vmovdqa	XMMWORD PTR _curr3$21[ebp], xmm0

; 1568 : 
; 1569 :     __m256i curr0_32 = _mm256_cvtepu8_epi32(curr0);

	vpmovzxbd ymm0, XMMWORD PTR _curr0$24[ebp]
	vmovdqu	YMMWORD PTR _curr0_32$20[ebp], ymm0

; 1570 :     __m256i curr1_32 = _mm256_cvtepu8_epi32(curr1);

	vpmovzxbd ymm0, XMMWORD PTR _curr1$23[ebp]
	vmovdqu	YMMWORD PTR _curr1_32$19[ebp], ymm0

; 1571 :     __m256i curr2_32 = _mm256_cvtepu8_epi32(curr2);

	vpmovzxbd ymm0, XMMWORD PTR _curr2$22[ebp]
	vmovdqu	YMMWORD PTR _curr2_32$18[ebp], ymm0

; 1572 :     __m256i curr3_32 = _mm256_cvtepu8_epi32(curr3);

	vpmovzxbd ymm0, XMMWORD PTR _curr3$21[ebp]
	vmovdqu	YMMWORD PTR _curr3_32$17[ebp], ymm0

; 1573 : 
; 1574 :     __m256  curr0_f  = _mm256_cvtepi32_ps  (curr0_32);

	vmovdqu	ymm0, YMMWORD PTR _curr0_32$20[ebp]
	vcvtdq2ps ymm0, ymm0
	vmovups	YMMWORD PTR _curr0_f$16[ebp], ymm0

; 1575 :     __m256  curr1_f  = _mm256_cvtepi32_ps  (curr1_32);

	vmovdqu	ymm0, YMMWORD PTR _curr1_32$19[ebp]
	vcvtdq2ps ymm0, ymm0
	vmovups	YMMWORD PTR _curr1_f$15[ebp], ymm0

; 1576 :     __m256  curr2_f  = _mm256_cvtepi32_ps  (curr2_32);

	vmovdqu	ymm0, YMMWORD PTR _curr2_32$18[ebp]
	vcvtdq2ps ymm0, ymm0
	vmovups	YMMWORD PTR _curr2_f$14[ebp], ymm0

; 1577 :     __m256  curr3_f  = _mm256_cvtepi32_ps  (curr3_32);

	vmovdqu	ymm0, YMMWORD PTR _curr3_32$17[ebp]
	vcvtdq2ps ymm0, ymm0
	vmovups	YMMWORD PTR _curr3_f$13[ebp], ymm0

; 1578 : 
; 1579 :     __m256  curr0_sd = _mm256_sub_ps       (curr0_f,  mean);

	vmovups	ymm0, YMMWORD PTR _curr0_f$16[ebp]
	vsubps	ymm0, ymm0, YMMWORD PTR _mean$[ebp]
	vmovups	YMMWORD PTR _curr0_sd$12[ebp], ymm0

; 1580 :     __m256  curr1_sd = _mm256_sub_ps       (curr1_f,  mean);

	vmovups	ymm0, YMMWORD PTR _curr1_f$15[ebp]
	vsubps	ymm0, ymm0, YMMWORD PTR _mean$[ebp]
	vmovups	YMMWORD PTR _curr1_sd$11[ebp], ymm0

; 1581 :     __m256  curr2_sd = _mm256_sub_ps       (curr2_f,  mean);

	vmovups	ymm0, YMMWORD PTR _curr2_f$14[ebp]
	vsubps	ymm0, ymm0, YMMWORD PTR _mean$[ebp]
	vmovups	YMMWORD PTR _curr2_sd$10[ebp], ymm0

; 1582 :     __m256  curr3_sd = _mm256_sub_ps       (curr3_f,  mean);

	vmovups	ymm0, YMMWORD PTR _curr3_f$13[ebp]
	vsubps	ymm0, ymm0, YMMWORD PTR _mean$[ebp]
	vmovups	YMMWORD PTR _curr3_sd$9[ebp], ymm0

; 1583 : 
; 1584 :     __m256  curr0_v  = _mm256_mul_ps       (curr0_sd, curr0_sd);

	vmovups	ymm0, YMMWORD PTR _curr0_sd$12[ebp]
	vmulps	ymm0, ymm0, YMMWORD PTR _curr0_sd$12[ebp]
	vmovups	YMMWORD PTR _curr0_v$8[ebp], ymm0

; 1585 :     __m256  curr1_v  = _mm256_mul_ps       (curr1_sd, curr1_sd);

	vmovups	ymm0, YMMWORD PTR _curr1_sd$11[ebp]
	vmulps	ymm0, ymm0, YMMWORD PTR _curr1_sd$11[ebp]
	vmovups	YMMWORD PTR _curr1_v$7[ebp], ymm0

; 1586 :     __m256  curr2_v  = _mm256_mul_ps       (curr2_sd, curr2_sd);

	vmovups	ymm0, YMMWORD PTR _curr2_sd$10[ebp]
	vmulps	ymm0, ymm0, YMMWORD PTR _curr2_sd$10[ebp]
	vmovups	YMMWORD PTR _curr2_v$6[ebp], ymm0

; 1587 :     __m256  curr3_v  = _mm256_mul_ps       (curr3_sd, curr3_sd);

	vmovups	ymm0, YMMWORD PTR _curr3_sd$9[ebp]
	vmulps	ymm0, ymm0, YMMWORD PTR _curr3_sd$9[ebp]
	vmovups	YMMWORD PTR _curr3_v$5[ebp], ymm0

; 1588 : 
; 1589 :     __m256  curr01   = _mm256_add_ps       (curr0_v,  curr1_v);

	vmovups	ymm0, YMMWORD PTR _curr0_v$8[ebp]
	vaddps	ymm0, ymm0, YMMWORD PTR _curr1_v$7[ebp]
	vmovups	YMMWORD PTR _curr01$4[ebp], ymm0

; 1590 :     __m256  curr23   = _mm256_add_ps       (curr2_v,  curr3_v);

	vmovups	ymm0, YMMWORD PTR _curr2_v$6[ebp]
	vaddps	ymm0, ymm0, YMMWORD PTR _curr3_v$5[ebp]
	vmovups	YMMWORD PTR _curr23$3[ebp], ymm0

; 1591 :     __m256  curr     = _mm256_add_ps       (curr01,   curr23);

	vmovups	ymm0, YMMWORD PTR _curr01$4[ebp]
	vaddps	ymm0, ymm0, YMMWORD PTR _curr23$3[ebp]
	vmovups	YMMWORD PTR _curr$2[ebp], ymm0

; 1592 :             accum    = _mm256_add_ps       (accum,    curr);

	vmovups	ymm0, YMMWORD PTR _accum$[ebp]
	vaddps	ymm0, ymm0, YMMWORD PTR _curr$2[ebp]
	vmovups	YMMWORD PTR _accum$[ebp], ymm0

; 1593 :   }

	jmp	$LN8@pixel_var_
$LN9@pixel_var_:

; 1594 :   __m256d accum_d  = _mm256_castps_pd     (accum);

	vmovupd	ymm0, YMMWORD PTR _accum$[ebp]
	vmovupd	YMMWORD PTR _accum_d$[ebp], ymm0

; 1595 :   __m256d accum2_d = _mm256_permute4x64_pd(accum_d, _MM_SHUFFLE(1, 0, 3, 2));

	vpermpd	ymm0, YMMWORD PTR _accum_d$[ebp], 78	; 0000004eH
	vmovupd	YMMWORD PTR _accum2_d$[ebp], ymm0

; 1596 :   __m256  accum2   = _mm256_castpd_ps     (accum2_d);

	vmovups	ymm0, YMMWORD PTR _accum2_d$[ebp]
	vmovups	YMMWORD PTR _accum2$[ebp], ymm0

; 1597 : 
; 1598 :   __m256  accum3   = _mm256_add_ps        (accum,  accum2);

	vmovups	ymm0, YMMWORD PTR _accum$[ebp]
	vaddps	ymm0, ymm0, YMMWORD PTR _accum2$[ebp]
	vmovups	YMMWORD PTR _accum3$[ebp], ymm0

; 1599 :   __m256  accum4   = _mm256_permute_ps    (accum3, _MM_SHUFFLE(1, 0, 3, 2));

	vpermilps ymm0, YMMWORD PTR _accum3$[ebp], 78	; 0000004eH
	vmovups	YMMWORD PTR _accum4$[ebp], ymm0

; 1600 :   __m256  accum5   = _mm256_add_ps        (accum3, accum4);

	vmovups	ymm0, YMMWORD PTR _accum3$[ebp]
	vaddps	ymm0, ymm0, YMMWORD PTR _accum4$[ebp]
	vmovups	YMMWORD PTR _accum5$[ebp], ymm0

; 1601 :   __m256  accum6   = _mm256_permute_ps    (accum5, _MM_SHUFFLE(2, 3, 0, 1));

	vpermilps ymm0, YMMWORD PTR _accum5$[ebp], 177	; 000000b1H
	vmovups	YMMWORD PTR _accum6$[ebp], ymm0

; 1602 :   __m256  accum7   = _mm256_add_ps        (accum5, accum6);

	vmovups	ymm0, YMMWORD PTR _accum5$[ebp]
	vaddps	ymm0, ymm0, YMMWORD PTR _accum6$[ebp]
	vmovups	YMMWORD PTR _accum7$[ebp], ymm0

; 1603 : 
; 1604 :   __m128  accum8   = _mm256_castps256_ps128(accum7);

	vmovups	xmm0, XMMWORD PTR _accum7$[ebp]
	vmovaps	XMMWORD PTR _accum8$[ebp], xmm0

; 1605 :   float   var_sum  = _mm_cvtss_f32         (accum8);

	vmovaps	xmm0, XMMWORD PTR _accum8$[ebp]
	vmovss	DWORD PTR tv274[ebp], xmm0
	vmovss	xmm0, DWORD PTR tv274[ebp]
	vmovss	DWORD PTR _var_sum$[ebp], xmm0

; 1606 : 
; 1607 :   // Remaining len mod 32 pixels
; 1608 :   for (; i < len; ++i) {

	jmp	SHORT $LN13@pixel_var_
$LN11@pixel_var_:
	mov	eax, DWORD PTR _i$[ebp]
	add	eax, 1
	mov	DWORD PTR _i$[ebp], eax
$LN13@pixel_var_:
	mov	eax, DWORD PTR _i$[ebp]
	cmp	eax, DWORD PTR _len$[ebx]
	jae	SHORT $LN12@pixel_var_

; 1609 :     float diff = buf[i] - mean_f;

	mov	eax, DWORD PTR _buf$[ebx]
	add	eax, DWORD PTR _i$[ebp]
	movzx	ecx, BYTE PTR [eax]
	vcvtsi2ss xmm0, xmm0, ecx
	vsubss	xmm0, xmm0, DWORD PTR _mean_f$[ebp]
	vmovss	DWORD PTR _diff$1[ebp], xmm0

; 1610 :     var_sum += diff * diff;

	vmovss	xmm0, DWORD PTR _diff$1[ebp]
	vmulss	xmm0, xmm0, DWORD PTR _diff$1[ebp]
	vmovss	xmm1, DWORD PTR _var_sum$[ebp]
	vaddss	xmm0, xmm1, xmm0
	vmovss	DWORD PTR _var_sum$[ebp], xmm0

; 1611 :   }

	jmp	SHORT $LN11@pixel_var_
$LN12@pixel_var_:

; 1612 : 
; 1613 :   return  var_sum / len_f;

	vmovss	xmm0, DWORD PTR _var_sum$[ebp]
	vdivss	xmm0, xmm0, DWORD PTR _len_f$[ebp]
	vmovss	DWORD PTR tv331[ebp], xmm0
	fld	DWORD PTR tv331[ebp]

; 1614 : }

	push	edx
	mov	ecx, ebp
	push	eax
	lea	edx, DWORD PTR $LN17@pixel_var_
	call	@_RTC_CheckStackVars@8
	pop	eax
	pop	edx
	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
$LN17@pixel_var_:
	DD	1
	DD	$LN16@pixel_var_
$LN16@pixel_var_:
	DD	-80					; ffffffb0H
	DD	8
	DD	$LN15@pixel_var_
$LN15@pixel_var_:
	DB	115					; 00000073H
	DB	117					; 00000075H
	DB	109					; 0000006dH
	DB	0
_pixel_var_avx2_largebuf ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _hor_sad_avx2
_TEXT	SEGMENT
_pic_data$ = 8						; size = 4
_ref_data$ = 12						; size = 4
_width$ = 16						; size = 4
_height$ = 20						; size = 4
_pic_stride$ = 24					; size = 4
_ref_stride$ = 28					; size = 4
_left$ = 32						; size = 4
_right$ = 36						; size = 4
_hor_sad_avx2 PROC					; COMDAT

; 1515 : {

	push	ebp
	mov	ebp, esp
	sub	esp, 192				; 000000c0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-192]
	mov	ecx, 48					; 00000030H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 1516 :   if (width == 4)

	cmp	DWORD PTR _width$[ebp], 4
	jne	SHORT $LN2@hor_sad_av

; 1517 :     return hor_sad_sse41_w4(pic_data, ref_data, height,

	mov	eax, DWORD PTR _right$[ebp]
	push	eax
	mov	ecx, DWORD PTR _left$[ebp]
	push	ecx
	mov	edx, DWORD PTR _ref_stride$[ebp]
	push	edx
	mov	eax, DWORD PTR _pic_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _height$[ebp]
	push	ecx
	mov	edx, DWORD PTR _ref_data$[ebp]
	push	edx
	mov	eax, DWORD PTR _pic_data$[ebp]
	push	eax
	call	_hor_sad_sse41_w4
	add	esp, 28					; 0000001cH
	jmp	$LN1@hor_sad_av
$LN2@hor_sad_av:

; 1518 :                             pic_stride, ref_stride, left, right);
; 1519 :   if (width == 8)

	cmp	DWORD PTR _width$[ebp], 8
	jne	SHORT $LN3@hor_sad_av

; 1520 :     return hor_sad_sse41_w8(pic_data, ref_data, height,

	mov	eax, DWORD PTR _right$[ebp]
	push	eax
	mov	ecx, DWORD PTR _left$[ebp]
	push	ecx
	mov	edx, DWORD PTR _ref_stride$[ebp]
	push	edx
	mov	eax, DWORD PTR _pic_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _height$[ebp]
	push	ecx
	mov	edx, DWORD PTR _ref_data$[ebp]
	push	edx
	mov	eax, DWORD PTR _pic_data$[ebp]
	push	eax
	call	_hor_sad_sse41_w8
	add	esp, 28					; 0000001cH
	jmp	$LN1@hor_sad_av
$LN3@hor_sad_av:

; 1521 :                             pic_stride, ref_stride, left, right);
; 1522 :   if (width == 16)

	cmp	DWORD PTR _width$[ebp], 16		; 00000010H
	jne	SHORT $LN4@hor_sad_av

; 1523 :     return hor_sad_sse41_w16(pic_data, ref_data, height,

	mov	eax, DWORD PTR _right$[ebp]
	push	eax
	mov	ecx, DWORD PTR _left$[ebp]
	push	ecx
	mov	edx, DWORD PTR _ref_stride$[ebp]
	push	edx
	mov	eax, DWORD PTR _pic_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _height$[ebp]
	push	ecx
	mov	edx, DWORD PTR _ref_data$[ebp]
	push	edx
	mov	eax, DWORD PTR _pic_data$[ebp]
	push	eax
	call	_hor_sad_sse41_w16
	add	esp, 28					; 0000001cH
	jmp	SHORT $LN1@hor_sad_av
$LN4@hor_sad_av:

; 1524 :                              pic_stride, ref_stride, left, right);
; 1525 :   if (width == 32)

	cmp	DWORD PTR _width$[ebp], 32		; 00000020H
	jne	SHORT $LN5@hor_sad_av

; 1526 :     return hor_sad_avx2_w32 (pic_data, ref_data, height,

	mov	eax, DWORD PTR _right$[ebp]
	push	eax
	mov	ecx, DWORD PTR _left$[ebp]
	push	ecx
	mov	edx, DWORD PTR _ref_stride$[ebp]
	push	edx
	mov	eax, DWORD PTR _pic_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _height$[ebp]
	push	ecx
	mov	edx, DWORD PTR _ref_data$[ebp]
	push	edx
	mov	eax, DWORD PTR _pic_data$[ebp]
	push	eax
	call	_hor_sad_avx2_w32
	add	esp, 28					; 0000001cH
	jmp	SHORT $LN1@hor_sad_av
	jmp	SHORT $LN1@hor_sad_av
$LN5@hor_sad_av:

; 1527 :                              pic_stride, ref_stride, left, right);
; 1528 :   else
; 1529 :     return hor_sad_sse41_arbitrary(pic_data, ref_data, width, height,

	mov	eax, DWORD PTR _right$[ebp]
	push	eax
	mov	ecx, DWORD PTR _left$[ebp]
	push	ecx
	mov	edx, DWORD PTR _ref_stride$[ebp]
	push	edx
	mov	eax, DWORD PTR _pic_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _height$[ebp]
	push	ecx
	mov	edx, DWORD PTR _width$[ebp]
	push	edx
	mov	eax, DWORD PTR _ref_data$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pic_data$[ebp]
	push	ecx
	call	_hor_sad_sse41_arbitrary
	add	esp, 32					; 00000020H
$LN1@hor_sad_av:

; 1530 :                                    pic_stride, ref_stride, left, right);
; 1531 : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 192				; 000000c0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_hor_sad_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _ver_sad_avx2
_TEXT	SEGMENT
_pic_data$ = 8						; size = 4
_ref_data$ = 12						; size = 4
_width$ = 16						; size = 4
_height$ = 20						; size = 4
_stride$ = 24						; size = 4
_ver_sad_avx2 PROC					; COMDAT

; 1497 : {

	push	ebp
	mov	ebp, esp
	sub	esp, 192				; 000000c0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-192]
	mov	ecx, 48					; 00000030H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 1498 :   if (width == 0)

	cmp	DWORD PTR _width$[ebp], 0
	jne	SHORT $LN2@ver_sad_av

; 1499 :     return 0;

	xor	eax, eax
	jmp	$LN1@ver_sad_av
$LN2@ver_sad_av:

; 1500 :   if (width == 4)

	cmp	DWORD PTR _width$[ebp], 4
	jne	SHORT $LN3@ver_sad_av

; 1501 :     return ver_sad_w4(pic_data, ref_data, height, stride);

	mov	eax, DWORD PTR _stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _height$[ebp]
	push	ecx
	mov	edx, DWORD PTR _ref_data$[ebp]
	push	edx
	mov	eax, DWORD PTR _pic_data$[ebp]
	push	eax
	call	_ver_sad_w4
	add	esp, 16					; 00000010H
	jmp	SHORT $LN1@ver_sad_av
$LN3@ver_sad_av:

; 1502 :   if (width == 8)

	cmp	DWORD PTR _width$[ebp], 8
	jne	SHORT $LN4@ver_sad_av

; 1503 :     return ver_sad_w8(pic_data, ref_data, height, stride);

	mov	eax, DWORD PTR _stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _height$[ebp]
	push	ecx
	mov	edx, DWORD PTR _ref_data$[ebp]
	push	edx
	mov	eax, DWORD PTR _pic_data$[ebp]
	push	eax
	call	_ver_sad_w8
	add	esp, 16					; 00000010H
	jmp	SHORT $LN1@ver_sad_av
$LN4@ver_sad_av:

; 1504 :   if (width == 12)

	cmp	DWORD PTR _width$[ebp], 12		; 0000000cH
	jne	SHORT $LN5@ver_sad_av

; 1505 :     return ver_sad_w12(pic_data, ref_data, height, stride);

	mov	eax, DWORD PTR _stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _height$[ebp]
	push	ecx
	mov	edx, DWORD PTR _ref_data$[ebp]
	push	edx
	mov	eax, DWORD PTR _pic_data$[ebp]
	push	eax
	call	_ver_sad_w12
	add	esp, 16					; 00000010H
	jmp	SHORT $LN1@ver_sad_av
$LN5@ver_sad_av:

; 1506 :   if (width == 16)

	cmp	DWORD PTR _width$[ebp], 16		; 00000010H
	jne	SHORT $LN6@ver_sad_av

; 1507 :     return ver_sad_w16(pic_data, ref_data, height, stride);

	mov	eax, DWORD PTR _stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _height$[ebp]
	push	ecx
	mov	edx, DWORD PTR _ref_data$[ebp]
	push	edx
	mov	eax, DWORD PTR _pic_data$[ebp]
	push	eax
	call	_ver_sad_w16
	add	esp, 16					; 00000010H
	jmp	SHORT $LN1@ver_sad_av
	jmp	SHORT $LN1@ver_sad_av
$LN6@ver_sad_av:

; 1508 :   else
; 1509 :     return ver_sad_arbitrary(pic_data, ref_data, width, height, stride);

	mov	eax, DWORD PTR _stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _height$[ebp]
	push	ecx
	mov	edx, DWORD PTR _width$[ebp]
	push	edx
	mov	eax, DWORD PTR _ref_data$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pic_data$[ebp]
	push	ecx
	call	_ver_sad_arbitrary
	add	esp, 20					; 00000014H
$LN1@ver_sad_av:

; 1510 : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 192				; 000000c0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_ver_sad_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _get_optimized_sad_avx2
_TEXT	SEGMENT
_width$ = 8						; size = 4
_get_optimized_sad_avx2 PROC				; COMDAT

; 1474 : {

	push	ebp
	mov	ebp, esp
	sub	esp, 192				; 000000c0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-192]
	mov	ecx, 48					; 00000030H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 1475 :   if (width == 0)

	cmp	DWORD PTR _width$[ebp], 0
	jne	SHORT $LN2@get_optimi

; 1476 :     return reg_sad_w0;

	mov	eax, OFFSET _reg_sad_w0
	jmp	SHORT $LN1@get_optimi
$LN2@get_optimi:

; 1477 :   if (width == 4)

	cmp	DWORD PTR _width$[ebp], 4
	jne	SHORT $LN3@get_optimi

; 1478 :     return reg_sad_w4;

	mov	eax, OFFSET _reg_sad_w4
	jmp	SHORT $LN1@get_optimi
$LN3@get_optimi:

; 1479 :   if (width == 8)

	cmp	DWORD PTR _width$[ebp], 8
	jne	SHORT $LN4@get_optimi

; 1480 :     return reg_sad_w8;

	mov	eax, OFFSET _reg_sad_w8
	jmp	SHORT $LN1@get_optimi
$LN4@get_optimi:

; 1481 :   if (width == 12)

	cmp	DWORD PTR _width$[ebp], 12		; 0000000cH
	jne	SHORT $LN5@get_optimi

; 1482 :     return reg_sad_w12;

	mov	eax, OFFSET _reg_sad_w12
	jmp	SHORT $LN1@get_optimi
$LN5@get_optimi:

; 1483 :   if (width == 16)

	cmp	DWORD PTR _width$[ebp], 16		; 00000010H
	jne	SHORT $LN6@get_optimi

; 1484 :     return reg_sad_w16;

	mov	eax, OFFSET _reg_sad_w16
	jmp	SHORT $LN1@get_optimi
$LN6@get_optimi:

; 1485 :   if (width == 24)

	cmp	DWORD PTR _width$[ebp], 24		; 00000018H
	jne	SHORT $LN7@get_optimi

; 1486 :     return reg_sad_w24;

	mov	eax, OFFSET _reg_sad_w24
	jmp	SHORT $LN1@get_optimi
$LN7@get_optimi:

; 1487 :   if (width == 32)

	cmp	DWORD PTR _width$[ebp], 32		; 00000020H
	jne	SHORT $LN8@get_optimi

; 1488 :     return reg_sad_w32;

	mov	eax, OFFSET _reg_sad_w32
	jmp	SHORT $LN1@get_optimi
$LN8@get_optimi:

; 1489 :   if (width == 64)

	cmp	DWORD PTR _width$[ebp], 64		; 00000040H
	jne	SHORT $LN9@get_optimi

; 1490 :     return reg_sad_w64;

	mov	eax, OFFSET _reg_sad_w64
	jmp	SHORT $LN1@get_optimi
	jmp	SHORT $LN1@get_optimi
$LN9@get_optimi:

; 1491 :   else
; 1492 :     return NULL;

	xor	eax, eax
$LN1@get_optimi:

; 1493 : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 192				; 000000c0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_get_optimized_sad_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _bipred_average_avx2
_TEXT	SEGMENT
tv225 = -316						; size = 4
tv220 = -316						; size = 4
tv215 = -316						; size = 4
tv210 = -316						; size = 4
tv139 = -316						; size = 4
tv134 = -316						; size = 4
_src_im_v$1 = -116					; size = 4
_src_px_v$2 = -104					; size = 4
_src_im_u$3 = -92					; size = 4
_src_px_u$4 = -80					; size = 4
_pb_h$5 = -68						; size = 4
_pb_w$6 = -56						; size = 4
_pb_offset$7 = -44					; size = 4
_src_im$8 = -32						; size = 4
_src_px$9 = -20						; size = 4
_pb_offset$10 = -8					; size = 4
_lcu$ = 8						; size = 4
_px_L0$ = 12						; size = 4
_px_L1$ = 16						; size = 4
_im_L0$ = 20						; size = 4
_im_L1$ = 24						; size = 4
_pu_x$ = 28						; size = 4
_pu_y$ = 32						; size = 4
_pu_w$ = 36						; size = 4
_pu_h$ = 40						; size = 4
_im_flags_L0$ = 44					; size = 4
_im_flags_L1$ = 48					; size = 4
_predict_luma$ = 52					; size = 1
_predict_chroma$ = 56					; size = 1
_bipred_average_avx2 PROC				; COMDAT

; 1431 :   const bool predict_chroma) {

	push	ebp
	mov	ebp, esp
	sub	esp, 316				; 0000013cH
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-316]
	mov	ecx, 79					; 0000004fH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 1432 : 
; 1433 :   //After reconstruction, merge the predictors by taking an average of each pixel
; 1434 :   if (predict_luma) {

	movzx	eax, BYTE PTR _predict_luma$[ebp]
	test	eax, eax
	je	$LN2@bipred_ave

; 1435 :     unsigned pb_offset = SUB_SCU(pu_y) * LCU_WIDTH + SUB_SCU(pu_x);

	mov	eax, DWORD PTR _pu_y$[ebp]
	and	eax, 63					; 0000003fH
	shl	eax, 6
	mov	ecx, DWORD PTR _pu_x$[ebp]
	and	ecx, 63					; 0000003fH
	add	eax, ecx
	mov	DWORD PTR _pb_offset$10[ebp], eax

; 1436 : 
; 1437 :     if (!(im_flags_L0 & 1) && !(im_flags_L1 & 1)) {

	mov	eax, DWORD PTR _im_flags_L0$[ebp]
	and	eax, 1
	jne	SHORT $LN3@bipred_ave
	mov	eax, DWORD PTR _im_flags_L1$[ebp]
	and	eax, 1
	jne	SHORT $LN3@bipred_ave

; 1438 :       bipred_average_px_px_avx2(lcu->rec.y + pb_offset, px_L0->y, px_L1->y, pu_w, pu_h, LCU_WIDTH);

	push	64					; 00000040H
	mov	eax, DWORD PTR _pu_h$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_w$[ebp]
	push	ecx
	mov	edx, DWORD PTR _px_L1$[ebp]
	mov	eax, DWORD PTR [edx+4]
	push	eax
	mov	ecx, DWORD PTR _px_L0$[ebp]
	mov	edx, DWORD PTR [ecx+4]
	push	edx
	mov	eax, DWORD PTR _pb_offset$10[ebp]
	mov	ecx, DWORD PTR _lcu$[ebp]
	lea	edx, DWORD PTR [ecx+eax+6540]
	push	edx
	call	_bipred_average_px_px_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN3@bipred_ave:

; 1439 : 
; 1440 :     } else if ((im_flags_L0 & 1) && (im_flags_L1 & 1)) {

	mov	eax, DWORD PTR _im_flags_L0$[ebp]
	and	eax, 1
	je	SHORT $LN5@bipred_ave
	mov	eax, DWORD PTR _im_flags_L1$[ebp]
	and	eax, 1
	je	SHORT $LN5@bipred_ave

; 1441 :       bipred_average_im_im_avx2(lcu->rec.y + pb_offset, im_L0->y, im_L1->y, pu_w, pu_h, LCU_WIDTH);

	push	64					; 00000040H
	mov	eax, DWORD PTR _pu_h$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_w$[ebp]
	push	ecx
	mov	edx, DWORD PTR _im_L1$[ebp]
	mov	eax, DWORD PTR [edx+4]
	push	eax
	mov	ecx, DWORD PTR _im_L0$[ebp]
	mov	edx, DWORD PTR [ecx+4]
	push	edx
	mov	eax, DWORD PTR _pb_offset$10[ebp]
	mov	ecx, DWORD PTR _lcu$[ebp]
	lea	edx, DWORD PTR [ecx+eax+6540]
	push	edx
	call	_bipred_average_im_im_avx2
	add	esp, 24					; 00000018H

; 1442 : 
; 1443 :     } else {

	jmp	SHORT $LN2@bipred_ave
$LN5@bipred_ave:

; 1444 :       kvz_pixel *src_px    = (im_flags_L0 & 1) ? px_L1->y : px_L0->y;

	mov	eax, DWORD PTR _im_flags_L0$[ebp]
	and	eax, 1
	je	SHORT $LN13@bipred_ave
	mov	ecx, DWORD PTR _px_L1$[ebp]
	mov	edx, DWORD PTR [ecx+4]
	mov	DWORD PTR tv134[ebp], edx
	jmp	SHORT $LN14@bipred_ave
$LN13@bipred_ave:
	mov	eax, DWORD PTR _px_L0$[ebp]
	mov	ecx, DWORD PTR [eax+4]
	mov	DWORD PTR tv134[ebp], ecx
$LN14@bipred_ave:
	mov	edx, DWORD PTR tv134[ebp]
	mov	DWORD PTR _src_px$9[ebp], edx

; 1445 :       kvz_pixel_im *src_im = (im_flags_L0 & 1) ? im_L0->y : im_L1->y;

	mov	eax, DWORD PTR _im_flags_L0$[ebp]
	and	eax, 1
	je	SHORT $LN15@bipred_ave
	mov	ecx, DWORD PTR _im_L0$[ebp]
	mov	edx, DWORD PTR [ecx+4]
	mov	DWORD PTR tv139[ebp], edx
	jmp	SHORT $LN16@bipred_ave
$LN15@bipred_ave:
	mov	eax, DWORD PTR _im_L1$[ebp]
	mov	ecx, DWORD PTR [eax+4]
	mov	DWORD PTR tv139[ebp], ecx
$LN16@bipred_ave:
	mov	edx, DWORD PTR tv139[ebp]
	mov	DWORD PTR _src_im$8[ebp], edx

; 1446 :       bipred_average_px_im_avx2(lcu->rec.y + pb_offset, src_px, src_im, pu_w, pu_h, LCU_WIDTH);

	push	64					; 00000040H
	mov	eax, DWORD PTR _pu_h$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_w$[ebp]
	push	ecx
	mov	edx, DWORD PTR _src_im$8[ebp]
	push	edx
	mov	eax, DWORD PTR _src_px$9[ebp]
	push	eax
	mov	ecx, DWORD PTR _pb_offset$10[ebp]
	mov	edx, DWORD PTR _lcu$[ebp]
	lea	eax, DWORD PTR [edx+ecx+6540]
	push	eax
	call	_bipred_average_px_im_avx2
	add	esp, 24					; 00000018H
$LN2@bipred_ave:

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	movzx	eax, BYTE PTR _predict_chroma$[ebp]
	test	eax, eax
	je	$LN1@bipred_ave

; 1450 :     unsigned pb_offset = SUB_SCU(pu_y) / 2 * LCU_WIDTH_C + SUB_SCU(pu_x) / 2;

	mov	eax, DWORD PTR _pu_y$[ebp]
	and	eax, 63					; 0000003fH
	shr	eax, 1
	shl	eax, 5
	mov	ecx, DWORD PTR _pu_x$[ebp]
	and	ecx, 63					; 0000003fH
	shr	ecx, 1
	add	eax, ecx
	mov	DWORD PTR _pb_offset$7[ebp], eax

; 1451 :     unsigned pb_w = pu_w / 2;

	mov	eax, DWORD PTR _pu_w$[ebp]
	shr	eax, 1
	mov	DWORD PTR _pb_w$6[ebp], eax

; 1452 :     unsigned pb_h = pu_h / 2;

	mov	eax, DWORD PTR _pu_h$[ebp]
	shr	eax, 1
	mov	DWORD PTR _pb_h$5[ebp], eax

; 1453 : 
; 1454 :     if (!(im_flags_L0 & 2) && !(im_flags_L1 & 2)) {

	mov	eax, DWORD PTR _im_flags_L0$[ebp]
	and	eax, 2
	jne	SHORT $LN8@bipred_ave
	mov	eax, DWORD PTR _im_flags_L1$[ebp]
	and	eax, 2
	jne	SHORT $LN8@bipred_ave

; 1455 :       bipred_average_px_px_avx2(lcu->rec.u + pb_offset, px_L0->u, px_L1->u, pb_w, pb_h, LCU_WIDTH_C);

	push	32					; 00000020H
	mov	eax, DWORD PTR _pb_h$5[ebp]
	push	eax
	mov	ecx, DWORD PTR _pb_w$6[ebp]
	push	ecx
	mov	edx, DWORD PTR _px_L1$[ebp]
	mov	eax, DWORD PTR [edx+8]
	push	eax
	mov	ecx, DWORD PTR _px_L0$[ebp]
	mov	edx, DWORD PTR [ecx+8]
	push	edx
	mov	eax, DWORD PTR _pb_offset$7[ebp]
	mov	ecx, DWORD PTR _lcu$[ebp]
	lea	edx, DWORD PTR [ecx+eax+10636]
	push	edx
	call	_bipred_average_px_px_avx2
	add	esp, 24					; 00000018H

; 1456 :       bipred_average_px_px_avx2(lcu->rec.v + pb_offset, px_L0->v, px_L1->v, pb_w, pb_h, LCU_WIDTH_C);

	push	32					; 00000020H
	mov	eax, DWORD PTR _pb_h$5[ebp]
	push	eax
	mov	ecx, DWORD PTR _pb_w$6[ebp]
	push	ecx
	mov	edx, DWORD PTR _px_L1$[ebp]
	mov	eax, DWORD PTR [edx+12]
	push	eax
	mov	ecx, DWORD PTR _px_L0$[ebp]
	mov	edx, DWORD PTR [ecx+12]
	push	edx
	mov	eax, DWORD PTR _pb_offset$7[ebp]
	mov	ecx, DWORD PTR _lcu$[ebp]
	lea	edx, DWORD PTR [ecx+eax+11660]
	push	edx
	call	_bipred_average_px_px_avx2
	add	esp, 24					; 00000018H
	jmp	$LN1@bipred_ave
$LN8@bipred_ave:

; 1457 : 
; 1458 :     } else if ((im_flags_L0 & 2) && (im_flags_L1 & 2)) {

	mov	eax, DWORD PTR _im_flags_L0$[ebp]
	and	eax, 2
	je	SHORT $LN10@bipred_ave
	mov	eax, DWORD PTR _im_flags_L1$[ebp]
	and	eax, 2
	je	SHORT $LN10@bipred_ave

; 1459 :       bipred_average_im_im_avx2(lcu->rec.u + pb_offset, im_L0->u, im_L1->u, pb_w, pb_h, LCU_WIDTH_C);

	push	32					; 00000020H
	mov	eax, DWORD PTR _pb_h$5[ebp]
	push	eax
	mov	ecx, DWORD PTR _pb_w$6[ebp]
	push	ecx
	mov	edx, DWORD PTR _im_L1$[ebp]
	mov	eax, DWORD PTR [edx+8]
	push	eax
	mov	ecx, DWORD PTR _im_L0$[ebp]
	mov	edx, DWORD PTR [ecx+8]
	push	edx
	mov	eax, DWORD PTR _pb_offset$7[ebp]
	mov	ecx, DWORD PTR _lcu$[ebp]
	lea	edx, DWORD PTR [ecx+eax+10636]
	push	edx
	call	_bipred_average_im_im_avx2
	add	esp, 24					; 00000018H

; 1460 :       bipred_average_im_im_avx2(lcu->rec.v + pb_offset, im_L0->v, im_L1->v, pb_w, pb_h, LCU_WIDTH_C);

	push	32					; 00000020H
	mov	eax, DWORD PTR _pb_h$5[ebp]
	push	eax
	mov	ecx, DWORD PTR _pb_w$6[ebp]
	push	ecx
	mov	edx, DWORD PTR _im_L1$[ebp]
	mov	eax, DWORD PTR [edx+12]
	push	eax
	mov	ecx, DWORD PTR _im_L0$[ebp]
	mov	edx, DWORD PTR [ecx+12]
	push	edx
	mov	eax, DWORD PTR _pb_offset$7[ebp]
	mov	ecx, DWORD PTR _lcu$[ebp]
	lea	edx, DWORD PTR [ecx+eax+11660]
	push	edx
	call	_bipred_average_im_im_avx2
	add	esp, 24					; 00000018H

; 1461 : 
; 1462 :     } else {

	jmp	$LN1@bipred_ave
$LN10@bipred_ave:

; 1463 :       kvz_pixel    *src_px_u = (im_flags_L0 & 2) ? px_L1->u : px_L0->u;

	mov	eax, DWORD PTR _im_flags_L0$[ebp]
	and	eax, 2
	je	SHORT $LN17@bipred_ave
	mov	ecx, DWORD PTR _px_L1$[ebp]
	mov	edx, DWORD PTR [ecx+8]
	mov	DWORD PTR tv210[ebp], edx
	jmp	SHORT $LN18@bipred_ave
$LN17@bipred_ave:
	mov	eax, DWORD PTR _px_L0$[ebp]
	mov	ecx, DWORD PTR [eax+8]
	mov	DWORD PTR tv210[ebp], ecx
$LN18@bipred_ave:
	mov	edx, DWORD PTR tv210[ebp]
	mov	DWORD PTR _src_px_u$4[ebp], edx

; 1464 :       kvz_pixel_im *src_im_u = (im_flags_L0 & 2) ? im_L0->u : im_L1->u;

	mov	eax, DWORD PTR _im_flags_L0$[ebp]
	and	eax, 2
	je	SHORT $LN19@bipred_ave
	mov	ecx, DWORD PTR _im_L0$[ebp]
	mov	edx, DWORD PTR [ecx+8]
	mov	DWORD PTR tv215[ebp], edx
	jmp	SHORT $LN20@bipred_ave
$LN19@bipred_ave:
	mov	eax, DWORD PTR _im_L1$[ebp]
	mov	ecx, DWORD PTR [eax+8]
	mov	DWORD PTR tv215[ebp], ecx
$LN20@bipred_ave:
	mov	edx, DWORD PTR tv215[ebp]
	mov	DWORD PTR _src_im_u$3[ebp], edx

; 1465 :       kvz_pixel    *src_px_v = (im_flags_L0 & 2) ? px_L1->v : px_L0->v;

	mov	eax, DWORD PTR _im_flags_L0$[ebp]
	and	eax, 2
	je	SHORT $LN21@bipred_ave
	mov	ecx, DWORD PTR _px_L1$[ebp]
	mov	edx, DWORD PTR [ecx+12]
	mov	DWORD PTR tv220[ebp], edx
	jmp	SHORT $LN22@bipred_ave
$LN21@bipred_ave:
	mov	eax, DWORD PTR _px_L0$[ebp]
	mov	ecx, DWORD PTR [eax+12]
	mov	DWORD PTR tv220[ebp], ecx
$LN22@bipred_ave:
	mov	edx, DWORD PTR tv220[ebp]
	mov	DWORD PTR _src_px_v$2[ebp], edx

; 1466 :       kvz_pixel_im *src_im_v = (im_flags_L0 & 2) ? im_L0->v : im_L1->v;

	mov	eax, DWORD PTR _im_flags_L0$[ebp]
	and	eax, 2
	je	SHORT $LN23@bipred_ave
	mov	ecx, DWORD PTR _im_L0$[ebp]
	mov	edx, DWORD PTR [ecx+12]
	mov	DWORD PTR tv225[ebp], edx
	jmp	SHORT $LN24@bipred_ave
$LN23@bipred_ave:
	mov	eax, DWORD PTR _im_L1$[ebp]
	mov	ecx, DWORD PTR [eax+12]
	mov	DWORD PTR tv225[ebp], ecx
$LN24@bipred_ave:
	mov	edx, DWORD PTR tv225[ebp]
	mov	DWORD PTR _src_im_v$1[ebp], edx

; 1467 :       bipred_average_px_im_avx2(lcu->rec.u + pb_offset, src_px_u, src_im_u, pb_w, pb_h, LCU_WIDTH_C);

	push	32					; 00000020H
	mov	eax, DWORD PTR _pb_h$5[ebp]
	push	eax
	mov	ecx, DWORD PTR _pb_w$6[ebp]
	push	ecx
	mov	edx, DWORD PTR _src_im_u$3[ebp]
	push	edx
	mov	eax, DWORD PTR _src_px_u$4[ebp]
	push	eax
	mov	ecx, DWORD PTR _pb_offset$7[ebp]
	mov	edx, DWORD PTR _lcu$[ebp]
	lea	eax, DWORD PTR [edx+ecx+10636]
	push	eax
	call	_bipred_average_px_im_avx2
	add	esp, 24					; 00000018H

; 1468 :       bipred_average_px_im_avx2(lcu->rec.v + pb_offset, src_px_v, src_im_v, pb_w, pb_h, LCU_WIDTH_C);

	push	32					; 00000020H
	mov	eax, DWORD PTR _pb_h$5[ebp]
	push	eax
	mov	ecx, DWORD PTR _pb_w$6[ebp]
	push	ecx
	mov	edx, DWORD PTR _src_im_v$1[ebp]
	push	edx
	mov	eax, DWORD PTR _src_px_v$2[ebp]
	push	eax
	mov	ecx, DWORD PTR _pb_offset$7[ebp]
	mov	edx, DWORD PTR _lcu$[ebp]
	lea	eax, DWORD PTR [edx+ecx+11660]
	push	eax
	call	_bipred_average_px_im_avx2
	add	esp, 24					; 00000018H
$LN1@bipred_ave:

; 1469 :     }
; 1470 :   }
; 1471 : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 316				; 0000013cH
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_bipred_average_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _bipred_average_px_im_avx2
_TEXT	SEGMENT
tv67 = -292						; size = 4
_rounded$1 = -92					; size = 4
_sample_im$2 = -80					; size = 2
_sample_px$3 = -68					; size = 2
_x$4 = -56						; size = 4
_y$5 = -44						; size = 4
_i$6 = -32						; size = 4
_offset$7 = -20						; size = 4
_shift$8 = -8						; size = 4
_dst$ = 8						; size = 4
_px$ = 12						; size = 4
_im$ = 16						; size = 4
_pu_w$ = 20						; size = 4
_pu_h$ = 24						; size = 4
_dst_stride$ = 28					; size = 4
_bipred_average_px_im_avx2 PROC				; COMDAT

; 1385 : {

	push	ebp
	mov	ebp, esp
	sub	esp, 292				; 00000124H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-292]
	mov	ecx, 73					; 00000049H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 1386 :   // Use scalar code for yet unoptimized block sizes (4x4, 2x8)
; 1387 :   if (!(pu_w == 4 && pu_h == 4) && pu_w > 2) {

	cmp	DWORD PTR _pu_w$[ebp], 4
	jne	SHORT $LN9@bipred_ave
	cmp	DWORD PTR _pu_h$[ebp], 4
	je	$LN7@bipred_ave
$LN9@bipred_ave:
	cmp	DWORD PTR _pu_w$[ebp], 2
	jbe	$LN7@bipred_ave

; 1388 :     switch (pu_w) {

	mov	eax, DWORD PTR _pu_w$[ebp]
	mov	DWORD PTR tv67[ebp], eax
	mov	ecx, DWORD PTR tv67[ebp]
	sub	ecx, 4
	mov	DWORD PTR tv67[ebp], ecx
	cmp	DWORD PTR tv67[ebp], 60			; 0000003cH
	ja	$LN19@bipred_ave
	mov	edx, DWORD PTR tv67[ebp]
	movzx	eax, BYTE PTR $LN22@bipred_ave[edx]
	jmp	DWORD PTR $LN23@bipred_ave[eax*4]
$LN10@bipred_ave:

; 1389 :       case  4: bipred_average_px_im_template_avx2(dst, px, im,  4, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	4
	mov	edx, DWORD PTR _im$[ebp]
	push	edx
	mov	eax, DWORD PTR _px$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_px_im_template_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN11@bipred_ave:

; 1390 :       case  8: bipred_average_px_im_template_avx2(dst, px, im,  8, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	8
	mov	edx, DWORD PTR _im$[ebp]
	push	edx
	mov	eax, DWORD PTR _px$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_px_im_template_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN12@bipred_ave:

; 1391 :       case 16: bipred_average_px_im_template_avx2(dst, px, im, 16, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	16					; 00000010H
	mov	edx, DWORD PTR _im$[ebp]
	push	edx
	mov	eax, DWORD PTR _px$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_px_im_template_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN13@bipred_ave:

; 1392 :       case 32: bipred_average_px_im_template_avx2(dst, px, im, 32, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	32					; 00000020H
	mov	edx, DWORD PTR _im$[ebp]
	push	edx
	mov	eax, DWORD PTR _px$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_px_im_template_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN14@bipred_ave:

; 1393 :       case 64: bipred_average_px_im_template_avx2(dst, px, im, 64, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	64					; 00000040H
	mov	edx, DWORD PTR _im$[ebp]
	push	edx
	mov	eax, DWORD PTR _px$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_px_im_template_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN15@bipred_ave:

; 1394 : 
; 1395 :       case  6: bipred_average_px_im_template_avx2(dst, px, im,  6, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	6
	mov	edx, DWORD PTR _im$[ebp]
	push	edx
	mov	eax, DWORD PTR _px$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_px_im_template_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN16@bipred_ave:

; 1396 :       case 12: bipred_average_px_im_template_avx2(dst, px, im, 12, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	12					; 0000000cH
	mov	edx, DWORD PTR _im$[ebp]
	push	edx
	mov	eax, DWORD PTR _px$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_px_im_template_avx2
	add	esp, 24					; 00000018H
	jmp	SHORT $LN2@bipred_ave
$LN17@bipred_ave:

; 1397 :       case 24: bipred_average_px_im_template_avx2(dst, px, im, 24, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	24					; 00000018H
	mov	edx, DWORD PTR _im$[ebp]
	push	edx
	mov	eax, DWORD PTR _px$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_px_im_template_avx2
	add	esp, 24					; 00000018H
	jmp	SHORT $LN2@bipred_ave
$LN18@bipred_ave:

; 1398 :       case 48: bipred_average_px_im_template_avx2(dst, px, im, 48, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	48					; 00000030H
	mov	edx, DWORD PTR _im$[ebp]
	push	edx
	mov	eax, DWORD PTR _px$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_px_im_template_avx2
	add	esp, 24					; 00000018H
	jmp	SHORT $LN2@bipred_ave
$LN19@bipred_ave:

; 1399 :       default:
; 1400 :         assert(0 && "Unexpected block width.");

	xor	eax, eax
	jne	SHORT $LN2@bipred_ave
	mov	ecx, DWORD PTR ?__LINE__Var@?0??bipred_average_px_im_avx2@@9@9
	add	ecx, 15					; 0000000fH
	mov	esi, esp
	push	ecx
	push	OFFSET ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
	push	OFFSET ??_C@_1DO@PGNEGAJ@?$AA0?$AA?5?$AA?$CG?$AA?$CG?$AA?5?$AA?$CC?$AAU?$AAn?$AAe?$AAx?$AAp?$AAe?$AAc?$AAt?$AAe@
	call	DWORD PTR __imp___wassert
	add	esp, 12					; 0000000cH
	cmp	esi, esp
	call	__RTC_CheckEsp
$LN2@bipred_ave:

; 1401 :         break;
; 1402 :     }
; 1403 :   } else {

	jmp	$LN1@bipred_ave
$LN7@bipred_ave:

; 1404 :     int32_t shift = 15 - KVZ_BIT_DEPTH; // TODO: defines

	mov	DWORD PTR _shift$8[ebp], 7

; 1405 :     int32_t offset = 1 << (shift - 1);

	mov	eax, DWORD PTR _shift$8[ebp]
	sub	eax, 1
	mov	ecx, 1
	shlx	edx, ecx, eax
	mov	DWORD PTR _offset$7[ebp], edx

; 1406 : 
; 1407 :     for (int i = 0; i < pu_w * pu_h; ++i)

	mov	DWORD PTR _i$6[ebp], 0
	jmp	SHORT $LN6@bipred_ave
$LN4@bipred_ave:
	mov	eax, DWORD PTR _i$6[ebp]
	add	eax, 1
	mov	DWORD PTR _i$6[ebp], eax
$LN6@bipred_ave:
	mov	eax, DWORD PTR _pu_w$[ebp]
	imul	eax, DWORD PTR _pu_h$[ebp]
	cmp	DWORD PTR _i$6[ebp], eax
	jae	SHORT $LN1@bipred_ave

; 1408 :     {
; 1409 :       int y = i / pu_w;

	mov	eax, DWORD PTR _i$6[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebp]
	mov	DWORD PTR _y$5[ebp], eax

; 1410 :       int x = i % pu_w;

	mov	eax, DWORD PTR _i$6[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebp]
	mov	DWORD PTR _x$4[ebp], edx

; 1411 :       int16_t sample_px = px[i] << (14 - KVZ_BIT_DEPTH);

	mov	eax, DWORD PTR _px$[ebp]
	add	eax, DWORD PTR _i$6[ebp]
	movzx	ecx, BYTE PTR [eax]
	shl	ecx, 6
	mov	WORD PTR _sample_px$3[ebp], cx

; 1412 :       int16_t sample_im = im[i];

	mov	eax, DWORD PTR _i$6[ebp]
	mov	ecx, DWORD PTR _im$[ebp]
	mov	dx, WORD PTR [ecx+eax*2]
	mov	WORD PTR _sample_im$2[ebp], dx

; 1413 :       int32_t rounded = (sample_px + sample_im + offset) >> shift;

	movsx	eax, WORD PTR _sample_px$3[ebp]
	movsx	ecx, WORD PTR _sample_im$2[ebp]
	add	eax, DWORD PTR _offset$7[ebp]
	add	ecx, eax
	mov	edx, DWORD PTR _shift$8[ebp]
	sarx	eax, ecx, edx
	mov	DWORD PTR _rounded$1[ebp], eax

; 1414 :       dst[y * dst_stride + x] = kvz_fast_clip_32bit_to_pixel(rounded);

	mov	eax, DWORD PTR _rounded$1[ebp]
	push	eax
	call	_kvz_fast_clip_32bit_to_pixel
	add	esp, 4
	mov	ecx, DWORD PTR _y$5[ebp]
	imul	ecx, DWORD PTR _dst_stride$[ebp]
	add	ecx, DWORD PTR _x$4[ebp]
	mov	edx, DWORD PTR _dst$[ebp]
	mov	BYTE PTR [edx+ecx], al

; 1415 :     }

	jmp	SHORT $LN4@bipred_ave
$LN1@bipred_ave:

; 1416 :   }
; 1417 : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 292				; 00000124H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
	npad	2
$LN23@bipred_ave:
	DD	$LN10@bipred_ave
	DD	$LN15@bipred_ave
	DD	$LN11@bipred_ave
	DD	$LN16@bipred_ave
	DD	$LN12@bipred_ave
	DD	$LN17@bipred_ave
	DD	$LN13@bipred_ave
	DD	$LN18@bipred_ave
	DD	$LN14@bipred_ave
	DD	$LN19@bipred_ave
$LN22@bipred_ave:
	DB	0
	DB	9
	DB	1
	DB	9
	DB	2
	DB	9
	DB	9
	DB	9
	DB	3
	DB	9
	DB	9
	DB	9
	DB	4
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	5
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	6
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	7
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	8
_bipred_average_px_im_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _bipred_average_px_im_template_avx2
_TEXT	SEGMENT
tv73 = -5308						; size = 4
tv425 = -5304						; size = 4
tv401 = -5304						; size = 4
tv227 = -5304						; size = 4
tv78 = -5304						; size = 4
tv71 = -5304						; size = 4
_elements_89ab$1 = -3760				; size = 4
_elements_67$2 = -3748					; size = 2
_elements_45$3 = -3736					; size = 2
_elements_0123$4 = -3724				; size = 4
_avg$5 = -3712						; size = 16
_avg256$6 = -3680					; size = 32
_avg_hi$7 = -3616					; size = 32
_avg_lo$8 = -3552					; size = 32
_all_ones$9 = -3488					; size = 32
_sample_px_im_hi$10 = -3424				; size = 32
_sample_px_im_lo$11 = -3360				; size = 32
_sample_im_16bit$12 = -3296				; size = 32
_sample_px_16bit$13 = -3232				; size = 32
_mask$14 = -3168					; size = 32
_sample_px_8bit$15 = -3120				; size = 16
_mask128$16 = -3088					; size = 16
_y$17 = -3064						; size = 4
_i$18 = -3052						; size = 4
_avg$19 = -3040						; size = 16
_avg256$20 = -3008					; size = 32
_avg_hi$21 = -2944					; size = 32
_avg_lo$22 = -2880					; size = 32
_all_ones$23 = -2816					; size = 32
_sample_px_im_hi$24 = -2752				; size = 32
_sample_px_im_lo$25 = -2688				; size = 32
_sample_im_16bit$26 = -2624				; size = 32
_sample_px_16bit$27 = -2560				; size = 32
_sample_px_8bit$28 = -2512				; size = 16
_y$29 = -2488						; size = 4
_i$30 = -2476						; size = 4
_avg$31 = -2464						; size = 32
_avg0213$32 = -2400					; size = 32
_avg_23$33 = -2336					; size = 32
_avg_01$34 = -2272					; size = 32
_avg_b_hi$35 = -2208					; size = 32
_avg_b_lo$36 = -2144					; size = 32
_avg_a_hi$37 = -2080					; size = 32
_avg_a_lo$38 = -2016					; size = 32
_all_ones$39 = -1952					; size = 32
_sample_px_im_b_hi$40 = -1888				; size = 32
_sample_px_im_b_lo$41 = -1824				; size = 32
_sample_px_im_a_hi$42 = -1760				; size = 32
_sample_px_im_a_lo$43 = -1696				; size = 32
_sample_im_b_16bit$44 = -1632				; size = 32
_sample_im_a_16bit$45 = -1568				; size = 32
_sample_px_b_16bit$46 = -1504				; size = 32
_sample_px_a_16bit$47 = -1440				; size = 32
_sample_px_b_8bit$48 = -1376				; size = 16
_sample_px_a_8bit$49 = -1344				; size = 16
_mask$50 = -1312					; size = 32
_x$51 = -1252						; size = 4
_y$52 = -1240						; size = 4
_i$53 = -1228						; size = 4
_avg$54 = -1216						; size = 32
_avg0213$55 = -1152					; size = 32
_avg_23$56 = -1088					; size = 32
_avg_01$57 = -1024					; size = 32
_avg_b_hi$58 = -960					; size = 32
_avg_b_lo$59 = -896					; size = 32
_avg_a_hi$60 = -832					; size = 32
_avg_a_lo$61 = -768					; size = 32
_all_ones$62 = -704					; size = 32
_sample_px_im_b_hi$63 = -640				; size = 32
_sample_px_im_b_lo$64 = -576				; size = 32
_sample_px_im_a_hi$65 = -512				; size = 32
_sample_px_im_a_lo$66 = -448				; size = 32
_sample_im_b_16bit$67 = -384				; size = 32
_sample_im_a_16bit$68 = -320				; size = 32
_sample_px_b_16bit$69 = -256				; size = 32
_sample_px_a_16bit$70 = -192				; size = 32
_x$71 = -124						; size = 4
_y$72 = -112						; size = 4
_i$73 = -100						; size = 4
_area_mod_32$ = -85					; size = 1
_has_pow2_width$ = -73					; size = 1
_offset$ = -64						; size = 32
_scalar_offset$ = -20					; size = 4
_shift$ = -8						; size = 4
_dst$ = 8						; size = 4
_px$ = 12						; size = 4
_im$ = 16						; size = 4
_pu_w$ = 20						; size = 4
_pu_h$ = 24						; size = 4
_dst_stride$ = 28					; size = 4
_bipred_average_px_im_template_avx2 PROC		; COMDAT

; 1184 : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	mov	eax, 5336				; 000014d8H
	call	__chkstk
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-5336]
	mov	ecx, 1334				; 00000536H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 1185 :   int32_t shift = 15 - KVZ_BIT_DEPTH; // TODO: defines

	mov	DWORD PTR _shift$[ebp], 7

; 1186 :   int32_t scalar_offset = 1 << (shift - 1);

	mov	eax, DWORD PTR _shift$[ebp]
	sub	eax, 1
	mov	ecx, 1
	shlx	edx, ecx, eax
	mov	DWORD PTR _scalar_offset$[ebp], edx

; 1187 :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovd	xmm0, DWORD PTR _scalar_offset$[ebp]
	vpbroadcastd ymm0, xmm0
	vmovdqu	YMMWORD PTR _offset$[ebp], ymm0

; 1188 : 
; 1189 :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	popcnt	eax, DWORD PTR _pu_w$[ebx]
	cmp	eax, 1
	jne	SHORT $LN36@bipred_ave
	mov	DWORD PTR tv71[ebp], 1
	jmp	SHORT $LN37@bipred_ave
$LN36@bipred_ave:
	mov	DWORD PTR tv71[ebp], 0
$LN37@bipred_ave:
	cmp	DWORD PTR tv71[ebp], 0
	jne	SHORT $LN38@bipred_ave
	mov	DWORD PTR tv73[ebp], 0
	jmp	SHORT $LN39@bipred_ave
$LN38@bipred_ave:
	mov	DWORD PTR tv73[ebp], 1
$LN39@bipred_ave:
	mov	cl, BYTE PTR tv73[ebp]
	mov	BYTE PTR _has_pow2_width$[ebp], cl

; 1190 :   bool area_mod_32 = (pu_w * pu_h) % 32;

	mov	eax, DWORD PTR _pu_w$[ebx]
	imul	eax, DWORD PTR _pu_h$[ebx]
	xor	edx, edx
	mov	ecx, 32					; 00000020H
	div	ecx
	test	edx, edx
	jne	SHORT $LN40@bipred_ave
	mov	DWORD PTR tv78[ebp], 0
	jmp	SHORT $LN41@bipred_ave
$LN40@bipred_ave:
	mov	DWORD PTR tv78[ebp], 1
$LN41@bipred_ave:
	mov	dl, BYTE PTR tv78[ebp]
	mov	BYTE PTR _area_mod_32$[ebp], dl

; 1191 :   assert(!(pu_w == 4 && pu_h == 4) && "Branch for 4x4 not yet implemented.");

	cmp	DWORD PTR _pu_w$[ebx], 4
	jne	SHORT $LN42@bipred_ave
	cmp	DWORD PTR _pu_h$[ebx], 4
	je	SHORT $LN43@bipred_ave
$LN42@bipred_ave:
	mov	eax, OFFSET ??_C@_0CE@OJCKHEGB@Branch?5for?54x4?5not?5yet?5implemen@
	test	eax, eax
	jne	SHORT $LN44@bipred_ave
$LN43@bipred_ave:
	mov	ecx, DWORD PTR ?__LINE__Var@?0??bipred_average_px_im_template_avx2@@9@9
	add	ecx, 7
	mov	esi, esp
	push	ecx
	push	OFFSET ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
	push	OFFSET ??_C@_1IG@PFICKALD@?$AA?$CB?$AA?$CI?$AAp?$AAu?$AA_?$AAw?$AA?5?$AA?$DN?$AA?$DN?$AA?5?$AA4?$AA?5?$AA?$CG?$AA?$CG?$AA?5@
	call	DWORD PTR __imp___wassert
	add	esp, 12					; 0000000cH
	cmp	esi, esp
	call	__RTC_CheckEsp
$LN44@bipred_ave:

; 1192 :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");

	cmp	DWORD PTR _pu_w$[ebx], 2
	jne	SHORT $LN45@bipred_ave
	cmp	DWORD PTR _pu_h$[ebx], 8
	je	SHORT $LN46@bipred_ave
$LN45@bipred_ave:
	mov	eax, OFFSET ??_C@_0CE@ICBEPKHC@Branch?5for?52x8?5not?5yet?5implemen@
	test	eax, eax
	jne	SHORT $LN47@bipred_ave
$LN46@bipred_ave:
	mov	ecx, DWORD PTR ?__LINE__Var@?0??bipred_average_px_im_template_avx2@@9@9
	add	ecx, 8
	mov	esi, esp
	push	ecx
	push	OFFSET ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
	push	OFFSET ??_C@_1IG@BCNOOMBC@?$AA?$CB?$AA?$CI?$AAp?$AAu?$AA_?$AAw?$AA?5?$AA?$DN?$AA?$DN?$AA?5?$AA2?$AA?5?$AA?$CG?$AA?$CG?$AA?5@
	call	DWORD PTR __imp___wassert
	add	esp, 12					; 0000000cH
	cmp	esi, esp
	call	__RTC_CheckEsp
$LN47@bipred_ave:

; 1193 : 
; 1194 :   if (has_pow2_width && area_mod_32 == 0) {

	movzx	eax, BYTE PTR _has_pow2_width$[ebp]
	test	eax, eax
	je	$LN20@bipred_ave
	movzx	eax, BYTE PTR _area_mod_32$[ebp]
	test	eax, eax
	jne	$LN20@bipred_ave

; 1195 :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	mov	DWORD PTR _i$73[ebp], 0
	jmp	SHORT $LN4@bipred_ave
$LN2@bipred_ave:
	mov	eax, DWORD PTR _i$73[ebp]
	add	eax, 32					; 00000020H
	mov	DWORD PTR _i$73[ebp], eax
$LN4@bipred_ave:
	mov	eax, DWORD PTR _pu_w$[ebx]
	imul	eax, DWORD PTR _pu_h$[ebx]
	cmp	DWORD PTR _i$73[ebp], eax
	jae	$LN3@bipred_ave

; 1196 : 
; 1197 :       int y = i / pu_w;

	mov	eax, DWORD PTR _i$73[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR _y$72[ebp], eax

; 1198 :       int x = i % pu_w;

	mov	eax, DWORD PTR _i$73[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR _x$71[ebp], edx

; 1199 : 
; 1200 :       __m256i sample_px_a_16bit = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&px[i]));

	mov	eax, DWORD PTR _px$[ebx]
	add	eax, DWORD PTR _i$73[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vpmovzxbw ymm0, xmm0
	vmovdqu	YMMWORD PTR _sample_px_a_16bit$70[ebp], ymm0

; 1201 :       __m256i sample_px_b_16bit = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&px[i + 16]));

	mov	eax, DWORD PTR _i$73[ebp]
	mov	ecx, DWORD PTR _px$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [ecx+eax+16]
	vpmovzxbw ymm0, xmm0
	vmovdqu	YMMWORD PTR _sample_px_b_16bit$69[ebp], ymm0

; 1202 :       sample_px_a_16bit         = _mm256_slli_epi16(sample_px_a_16bit, 14 - KVZ_BIT_DEPTH);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_a_16bit$70[ebp]
	vpsllw	ymm0, ymm0, 6
	vmovdqu	YMMWORD PTR _sample_px_a_16bit$70[ebp], ymm0

; 1203 :       sample_px_b_16bit         = _mm256_slli_epi16(sample_px_b_16bit, 14 - KVZ_BIT_DEPTH);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_b_16bit$69[ebp]
	vpsllw	ymm0, ymm0, 6
	vmovdqu	YMMWORD PTR _sample_px_b_16bit$69[ebp], ymm0

; 1204 :       __m256i sample_im_a_16bit = _mm256_loadu_si256((__m256i*)&im[i]);

	mov	eax, DWORD PTR _i$73[ebp]
	mov	ecx, DWORD PTR _im$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx+eax*2]
	vmovdqu	YMMWORD PTR _sample_im_a_16bit$68[ebp], ymm0

; 1205 :       __m256i sample_im_b_16bit = _mm256_loadu_si256((__m256i*)&im[i + 16]);

	mov	eax, DWORD PTR _i$73[ebp]
	mov	ecx, DWORD PTR _im$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx+eax*2+32]
	vmovdqu	YMMWORD PTR _sample_im_b_16bit$67[ebp], ymm0

; 1206 : 
; 1207 :       __m256i sample_px_im_a_lo = _mm256_unpacklo_epi16(sample_px_a_16bit, sample_im_a_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_a_16bit$70[ebp]
	vpunpcklwd ymm0, ymm0, YMMWORD PTR _sample_im_a_16bit$68[ebp]
	vmovdqu	YMMWORD PTR _sample_px_im_a_lo$66[ebp], ymm0

; 1208 :       __m256i sample_px_im_a_hi = _mm256_unpackhi_epi16(sample_px_a_16bit, sample_im_a_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_a_16bit$70[ebp]
	vpunpckhwd ymm0, ymm0, YMMWORD PTR _sample_im_a_16bit$68[ebp]
	vmovdqu	YMMWORD PTR _sample_px_im_a_hi$65[ebp], ymm0

; 1209 :       __m256i sample_px_im_b_lo = _mm256_unpacklo_epi16(sample_px_b_16bit, sample_im_b_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_b_16bit$69[ebp]
	vpunpcklwd ymm0, ymm0, YMMWORD PTR _sample_im_b_16bit$67[ebp]
	vmovdqu	YMMWORD PTR _sample_px_im_b_lo$64[ebp], ymm0

; 1210 :       __m256i sample_px_im_b_hi = _mm256_unpackhi_epi16(sample_px_b_16bit, sample_im_b_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_b_16bit$69[ebp]
	vpunpckhwd ymm0, ymm0, YMMWORD PTR _sample_im_b_16bit$67[ebp]
	vmovdqu	YMMWORD PTR _sample_px_im_b_hi$63[ebp], ymm0

; 1211 : 
; 1212 :       __m256i all_ones  = _mm256_set1_epi16(1);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vmovdqu	YMMWORD PTR _all_ones$62[ebp], ymm0

; 1213 :       __m256i avg_a_lo = _mm256_madd_epi16(sample_px_im_a_lo, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_im_a_lo$66[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$62[ebp]
	vmovdqu	YMMWORD PTR _avg_a_lo$61[ebp], ymm0

; 1214 :       __m256i avg_a_hi = _mm256_madd_epi16(sample_px_im_a_hi, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_im_a_hi$65[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$62[ebp]
	vmovdqu	YMMWORD PTR _avg_a_hi$60[ebp], ymm0

; 1215 :       __m256i avg_b_lo = _mm256_madd_epi16(sample_px_im_b_lo, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_im_b_lo$64[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$62[ebp]
	vmovdqu	YMMWORD PTR _avg_b_lo$59[ebp], ymm0

; 1216 :       __m256i avg_b_hi = _mm256_madd_epi16(sample_px_im_b_hi, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_im_b_hi$63[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$62[ebp]
	vmovdqu	YMMWORD PTR _avg_b_hi$58[ebp], ymm0

; 1217 : 
; 1218 :       avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_lo$61[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_a_lo$61[ebp], ymm0

; 1219 :       avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_hi$60[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_a_hi$60[ebp], ymm0

; 1220 :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_lo$59[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_b_lo$59[ebp], ymm0

; 1221 :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_hi$58[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_b_hi$58[ebp], ymm0

; 1222 : 
; 1223 :       avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_lo$61[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_a_lo$61[ebp], ymm0

; 1224 :       avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_hi$60[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_a_hi$60[ebp], ymm0

; 1225 :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_lo$59[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_b_lo$59[ebp], ymm0

; 1226 :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_hi$58[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_b_hi$58[ebp], ymm0

; 1227 : 
; 1228 :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_lo$61[ebp]
	vpackusdw ymm0, ymm0, YMMWORD PTR _avg_a_hi$60[ebp]
	vmovdqu	YMMWORD PTR _avg_01$57[ebp], ymm0

; 1229 :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_lo$59[ebp]
	vpackusdw ymm0, ymm0, YMMWORD PTR _avg_b_hi$58[ebp]
	vmovdqu	YMMWORD PTR _avg_23$56[ebp], ymm0

; 1230 :       __m256i avg0213 = _mm256_packus_epi16(avg_01, avg_23);

	vmovdqu	ymm0, YMMWORD PTR _avg_01$57[ebp]
	vpackuswb ymm0, ymm0, YMMWORD PTR _avg_23$56[ebp]
	vmovdqu	YMMWORD PTR _avg0213$55[ebp], ymm0

; 1231 :       __m256i avg     = _mm256_permute4x64_epi64(avg0213, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm0, YMMWORD PTR _avg0213$55[ebp], 216	; 000000d8H
	vmovdqu	YMMWORD PTR _avg$54[ebp], ymm0

; 1232 : 
; 1233 :       switch (pu_w) {

	mov	eax, DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR tv227[ebp], eax
	mov	ecx, DWORD PTR tv227[ebp]
	sub	ecx, 4
	mov	DWORD PTR tv227[ebp], ecx
	cmp	DWORD PTR tv227[ebp], 60		; 0000003cH
	ja	$LN26@bipred_ave
	mov	edx, DWORD PTR tv227[ebp]
	movzx	eax, BYTE PTR $LN51@bipred_ave[edx]
	jmp	DWORD PTR $LN52@bipred_ave[eax*4]
$LN22@bipred_ave:

; 1234 :         case  4: scatter_ymm_4x8_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebx]
	push	eax
	vmovdqu	ymm0, YMMWORD PTR _avg$54[ebp]
	mov	ecx, DWORD PTR _y$72[ebp]
	imul	ecx, DWORD PTR _dst_stride$[ebx]
	add	ecx, DWORD PTR _x$71[ebp]
	add	ecx, DWORD PTR _dst$[ebx]
	push	ecx
	call	_scatter_ymm_4x8_8bit
	add	esp, 8
	jmp	$LN5@bipred_ave
$LN23@bipred_ave:

; 1235 :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebx]
	push	eax
	vmovdqu	ymm0, YMMWORD PTR _avg$54[ebp]
	mov	ecx, DWORD PTR _y$72[ebp]
	imul	ecx, DWORD PTR _dst_stride$[ebx]
	add	ecx, DWORD PTR _x$71[ebp]
	add	ecx, DWORD PTR _dst$[ebx]
	push	ecx
	call	_scatter_ymm_8x4_8bit
	add	esp, 8
	jmp	SHORT $LN5@bipred_ave
$LN24@bipred_ave:

; 1236 :         case 16: scatter_ymm_16x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebx]
	push	eax
	vmovdqu	ymm0, YMMWORD PTR _avg$54[ebp]
	mov	ecx, DWORD PTR _y$72[ebp]
	imul	ecx, DWORD PTR _dst_stride$[ebx]
	add	ecx, DWORD PTR _x$71[ebp]
	add	ecx, DWORD PTR _dst$[ebx]
	push	ecx
	call	_scatter_ymm_16x2_8bit
	add	esp, 8
	jmp	SHORT $LN5@bipred_ave
$LN25@bipred_ave:

; 1237 :         case 32: // Same as case 64
; 1238 :         case 64: _mm256_storeu_si256((__m256i*)&dst[y * dst_stride + x], avg); break;

	vmovdqu	ymm0, YMMWORD PTR _avg$54[ebp]
	mov	eax, DWORD PTR _y$72[ebp]
	imul	eax, DWORD PTR _dst_stride$[ebx]
	add	eax, DWORD PTR _x$71[ebp]
	add	eax, DWORD PTR _dst$[ebx]
	vmovdqu	YMMWORD PTR [eax], ymm0
	jmp	SHORT $LN5@bipred_ave
$LN26@bipred_ave:

; 1239 :         default:
; 1240 :           assert(0 && "Unexpected block width.");

	xor	eax, eax
	jne	SHORT $LN5@bipred_ave
	mov	ecx, DWORD PTR ?__LINE__Var@?0??bipred_average_px_im_template_avx2@@9@9
	add	ecx, 56					; 00000038H
	mov	esi, esp
	push	ecx
	push	OFFSET ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
	push	OFFSET ??_C@_1DO@PGNEGAJ@?$AA0?$AA?5?$AA?$CG?$AA?$CG?$AA?5?$AA?$CC?$AAU?$AAn?$AAe?$AAx?$AAp?$AAe?$AAc?$AAt?$AAe@
	call	DWORD PTR __imp___wassert
	add	esp, 12					; 0000000cH
	cmp	esi, esp
	call	__RTC_CheckEsp
$LN5@bipred_ave:

; 1241 :           break;
; 1242 :       }
; 1243 :     }

	jmp	$LN2@bipred_ave
$LN3@bipred_ave:

; 1244 :   } else if (area_mod_32 == 0) {

	jmp	$LN1@bipred_ave
$LN20@bipred_ave:
	movzx	eax, BYTE PTR _area_mod_32$[ebp]
	test	eax, eax
	jne	$LN27@bipred_ave

; 1245 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	mov	DWORD PTR _i$53[ebp], 0
	jmp	SHORT $LN9@bipred_ave
$LN7@bipred_ave:
	mov	eax, DWORD PTR _i$53[ebp]
	add	eax, 24					; 00000018H
	mov	DWORD PTR _i$53[ebp], eax
$LN9@bipred_ave:
	mov	eax, DWORD PTR _pu_w$[ebx]
	imul	eax, DWORD PTR _pu_h$[ebx]
	cmp	DWORD PTR _i$53[ebp], eax
	jae	$LN8@bipred_ave

; 1246 : 
; 1247 :       int y = i / pu_w;

	mov	eax, DWORD PTR _i$53[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR _y$52[ebp], eax

; 1248 :       int x = i % pu_w;

	mov	eax, DWORD PTR _i$53[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR _x$51[ebp], edx

; 1249 : 
; 1250 :       // Last 64 bits of the 256 / 32 bits of the 128 are not used to simplify the loop
; 1251 :       __m256i mask              = _mm256_setr_epi64x(-1, -1, -1, 0);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	YMMWORD PTR _mask$50[ebp], ymm0

; 1252 :       __m128i sample_px_a_8bit  = _mm_loadu_si128((__m128i*)&px[i]);

	mov	eax, DWORD PTR _px$[ebx]
	add	eax, DWORD PTR _i$53[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _sample_px_a_8bit$49[ebp], xmm0

; 1253 :       __m128i sample_px_b_8bit  = _mm_loadl_epi64((__m128i*)&px[i + 16]);

	mov	eax, DWORD PTR _px$[ebx]
	add	eax, DWORD PTR _i$53[ebp]
	vmovq	xmm0, QWORD PTR [eax+16]
	vmovdqa	XMMWORD PTR _sample_px_b_8bit$48[ebp], xmm0

; 1254 :       __m256i sample_px_a_16bit = _mm256_cvtepu8_epi16(sample_px_a_8bit);

	vpmovzxbw ymm0, XMMWORD PTR _sample_px_a_8bit$49[ebp]
	vmovdqu	YMMWORD PTR _sample_px_a_16bit$47[ebp], ymm0

; 1255 :       __m256i sample_px_b_16bit = _mm256_cvtepu8_epi16(sample_px_b_8bit);

	vpmovzxbw ymm0, XMMWORD PTR _sample_px_b_8bit$48[ebp]
	vmovdqu	YMMWORD PTR _sample_px_b_16bit$46[ebp], ymm0

; 1256 :       sample_px_a_16bit         = _mm256_slli_epi16(sample_px_a_16bit, 14 - KVZ_BIT_DEPTH);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_a_16bit$47[ebp]
	vpsllw	ymm0, ymm0, 6
	vmovdqu	YMMWORD PTR _sample_px_a_16bit$47[ebp], ymm0

; 1257 :       sample_px_b_16bit         = _mm256_slli_epi16(sample_px_b_16bit, 14 - KVZ_BIT_DEPTH);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_b_16bit$46[ebp]
	vpsllw	ymm0, ymm0, 6
	vmovdqu	YMMWORD PTR _sample_px_b_16bit$46[ebp], ymm0

; 1258 :       __m256i sample_im_a_16bit = _mm256_loadu_si256((__m256i*)&im[i]);

	mov	eax, DWORD PTR _i$53[ebp]
	mov	ecx, DWORD PTR _im$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx+eax*2]
	vmovdqu	YMMWORD PTR _sample_im_a_16bit$45[ebp], ymm0

; 1259 :       __m256i sample_im_b_16bit = _mm256_castsi128_si256(_mm_loadu_si128((__m128i*)&im[i + 16]));

	mov	eax, DWORD PTR _i$53[ebp]
	mov	ecx, DWORD PTR _im$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [ecx+eax*2+32]
	vmovdqu	YMMWORD PTR _sample_im_b_16bit$44[ebp], ymm0

; 1260 : 
; 1261 :       __m256i sample_px_im_a_lo = _mm256_unpacklo_epi16(sample_px_a_16bit, sample_im_a_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_a_16bit$47[ebp]
	vpunpcklwd ymm0, ymm0, YMMWORD PTR _sample_im_a_16bit$45[ebp]
	vmovdqu	YMMWORD PTR _sample_px_im_a_lo$43[ebp], ymm0

; 1262 :       __m256i sample_px_im_a_hi = _mm256_unpackhi_epi16(sample_px_a_16bit, sample_im_a_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_a_16bit$47[ebp]
	vpunpckhwd ymm0, ymm0, YMMWORD PTR _sample_im_a_16bit$45[ebp]
	vmovdqu	YMMWORD PTR _sample_px_im_a_hi$42[ebp], ymm0

; 1263 :       __m256i sample_px_im_b_lo = _mm256_unpacklo_epi16(sample_px_b_16bit, sample_im_b_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_b_16bit$46[ebp]
	vpunpcklwd ymm0, ymm0, YMMWORD PTR _sample_im_b_16bit$44[ebp]
	vmovdqu	YMMWORD PTR _sample_px_im_b_lo$41[ebp], ymm0

; 1264 :       __m256i sample_px_im_b_hi = _mm256_unpackhi_epi16(sample_px_b_16bit, sample_im_b_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_b_16bit$46[ebp]
	vpunpckhwd ymm0, ymm0, YMMWORD PTR _sample_im_b_16bit$44[ebp]
	vmovdqu	YMMWORD PTR _sample_px_im_b_hi$40[ebp], ymm0

; 1265 : 
; 1266 :       __m256i all_ones = _mm256_set1_epi16(1);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vmovdqu	YMMWORD PTR _all_ones$39[ebp], ymm0

; 1267 :       __m256i avg_a_lo = _mm256_madd_epi16(sample_px_im_a_lo, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_im_a_lo$43[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$39[ebp]
	vmovdqu	YMMWORD PTR _avg_a_lo$38[ebp], ymm0

; 1268 :       __m256i avg_a_hi = _mm256_madd_epi16(sample_px_im_a_hi, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_im_a_hi$42[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$39[ebp]
	vmovdqu	YMMWORD PTR _avg_a_hi$37[ebp], ymm0

; 1269 :       __m256i avg_b_lo = _mm256_madd_epi16(sample_px_im_b_lo, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_im_b_lo$41[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$39[ebp]
	vmovdqu	YMMWORD PTR _avg_b_lo$36[ebp], ymm0

; 1270 :       __m256i avg_b_hi = _mm256_madd_epi16(sample_px_im_b_hi, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_im_b_hi$40[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$39[ebp]
	vmovdqu	YMMWORD PTR _avg_b_hi$35[ebp], ymm0

; 1271 : 
; 1272 :       avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_lo$38[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_a_lo$38[ebp], ymm0

; 1273 :       avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_hi$37[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_a_hi$37[ebp], ymm0

; 1274 :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_lo$36[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_b_lo$36[ebp], ymm0

; 1275 :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_hi$35[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_b_hi$35[ebp], ymm0

; 1276 : 
; 1277 :       avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_lo$38[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_a_lo$38[ebp], ymm0

; 1278 :       avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_hi$37[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_a_hi$37[ebp], ymm0

; 1279 :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_lo$36[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_b_lo$36[ebp], ymm0

; 1280 :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_hi$35[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_b_hi$35[ebp], ymm0

; 1281 : 
; 1282 :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_lo$38[ebp]
	vpackusdw ymm0, ymm0, YMMWORD PTR _avg_a_hi$37[ebp]
	vmovdqu	YMMWORD PTR _avg_01$34[ebp], ymm0

; 1283 :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_lo$36[ebp]
	vpackusdw ymm0, ymm0, YMMWORD PTR _avg_b_hi$35[ebp]
	vmovdqu	YMMWORD PTR _avg_23$33[ebp], ymm0

; 1284 :       __m256i avg0213 = _mm256_packus_epi16(avg_01, avg_23);

	vmovdqu	ymm0, YMMWORD PTR _avg_01$34[ebp]
	vpackuswb ymm0, ymm0, YMMWORD PTR _avg_23$33[ebp]
	vmovdqu	YMMWORD PTR _avg0213$32[ebp], ymm0

; 1285 :       __m256i avg     = _mm256_permute4x64_epi64(avg0213, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm0, YMMWORD PTR _avg0213$32[ebp], 216	; 000000d8H
	vmovdqu	YMMWORD PTR _avg$31[ebp], ymm0

; 1286 : 
; 1287 :       switch (pu_w) {

	mov	eax, DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR tv401[ebp], eax
	cmp	DWORD PTR tv401[ebp], 12		; 0000000cH
	je	SHORT $LN29@bipred_ave
	cmp	DWORD PTR tv401[ebp], 24		; 00000018H
	je	SHORT $LN30@bipred_ave
	cmp	DWORD PTR tv401[ebp], 48		; 00000030H
	je	SHORT $LN30@bipred_ave
	jmp	SHORT $LN31@bipred_ave
$LN29@bipred_ave:

; 1288 :         case 12: scatter_ymm_12x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebx]
	push	eax
	vmovdqu	ymm0, YMMWORD PTR _avg$31[ebp]
	mov	ecx, DWORD PTR _y$52[ebp]
	imul	ecx, DWORD PTR _dst_stride$[ebx]
	add	ecx, DWORD PTR _x$51[ebp]
	add	ecx, DWORD PTR _dst$[ebx]
	push	ecx
	call	_scatter_ymm_12x2_8bit
	add	esp, 8
	jmp	SHORT $LN10@bipred_ave
$LN30@bipred_ave:

; 1289 :         case 24: // Same as case 48
; 1290 :         case 48: _mm256_maskstore_epi64((long long*)&dst[y * dst_stride + x], mask, avg); break;

	vmovdqu	ymm0, YMMWORD PTR _avg$31[ebp]
	vmovdqu	ymm1, YMMWORD PTR _mask$50[ebp]
	mov	eax, DWORD PTR _y$52[ebp]
	imul	eax, DWORD PTR _dst_stride$[ebx]
	add	eax, DWORD PTR _x$51[ebp]
	add	eax, DWORD PTR _dst$[ebx]
	vpmaskmovq YMMWORD PTR [eax], ymm1, ymm0
	jmp	SHORT $LN10@bipred_ave
$LN31@bipred_ave:

; 1291 :         default:
; 1292 :           assert(0 && "Unexpected block width.");

	xor	eax, eax
	jne	SHORT $LN10@bipred_ave
	mov	ecx, DWORD PTR ?__LINE__Var@?0??bipred_average_px_im_template_avx2@@9@9
	add	ecx, 108				; 0000006cH
	mov	esi, esp
	push	ecx
	push	OFFSET ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
	push	OFFSET ??_C@_1DO@PGNEGAJ@?$AA0?$AA?5?$AA?$CG?$AA?$CG?$AA?5?$AA?$CC?$AAU?$AAn?$AAe?$AAx?$AAp?$AAe?$AAc?$AAt?$AAe@
	call	DWORD PTR __imp___wassert
	add	esp, 12					; 0000000cH
	cmp	esi, esp
	call	__RTC_CheckEsp
$LN10@bipred_ave:

; 1293 :           break;
; 1294 :       }
; 1295 :     }

	jmp	$LN7@bipred_ave
$LN8@bipred_ave:

; 1296 :   } else {

	jmp	$LN1@bipred_ave
$LN27@bipred_ave:

; 1297 :     // 8x2, 8x6, 6x8 blocks (and maybe 2x8 in the future)
; 1298 :     switch (pu_w) {

	mov	eax, DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR tv425[ebp], eax
	cmp	DWORD PTR tv425[ebp], 6
	je	$LN33@bipred_ave
	cmp	DWORD PTR tv425[ebp], 8
	je	SHORT $LN32@bipred_ave
	jmp	$LN34@bipred_ave
$LN32@bipred_ave:

; 1299 :       case 8: // 8x2, 8x6
; 1300 :         for (int i = 0; i < pu_w * pu_h; i += 16) {

	mov	DWORD PTR _i$30[ebp], 0
	jmp	SHORT $LN16@bipred_ave
$LN14@bipred_ave:
	mov	eax, DWORD PTR _i$30[ebp]
	add	eax, 16					; 00000010H
	mov	DWORD PTR _i$30[ebp], eax
$LN16@bipred_ave:
	mov	eax, DWORD PTR _pu_w$[ebx]
	imul	eax, DWORD PTR _pu_h$[ebx]
	cmp	DWORD PTR _i$30[ebp], eax
	jae	$LN15@bipred_ave

; 1301 : 
; 1302 :           int y = i / pu_w;

	mov	eax, DWORD PTR _i$30[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR _y$29[ebp], eax

; 1303 : 
; 1304 :           __m128i sample_px_8bit  = _mm_loadu_si128((__m128i*)&px[i]);

	mov	eax, DWORD PTR _px$[ebx]
	add	eax, DWORD PTR _i$30[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _sample_px_8bit$28[ebp], xmm0

; 1305 :           __m256i sample_px_16bit = _mm256_cvtepu8_epi16(sample_px_8bit);

	vpmovzxbw ymm0, XMMWORD PTR _sample_px_8bit$28[ebp]
	vmovdqu	YMMWORD PTR _sample_px_16bit$27[ebp], ymm0

; 1306 :           sample_px_16bit         = _mm256_slli_epi16(sample_px_16bit, 14 - KVZ_BIT_DEPTH);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_16bit$27[ebp]
	vpsllw	ymm0, ymm0, 6
	vmovdqu	YMMWORD PTR _sample_px_16bit$27[ebp], ymm0

; 1307 :           __m256i sample_im_16bit = _mm256_loadu_si256((__m256i*)&im[i]);

	mov	eax, DWORD PTR _i$30[ebp]
	mov	ecx, DWORD PTR _im$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx+eax*2]
	vmovdqu	YMMWORD PTR _sample_im_16bit$26[ebp], ymm0

; 1308 : 
; 1309 :           __m256i sample_px_im_lo = _mm256_unpacklo_epi16(sample_px_16bit, sample_im_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_16bit$27[ebp]
	vpunpcklwd ymm0, ymm0, YMMWORD PTR _sample_im_16bit$26[ebp]
	vmovdqu	YMMWORD PTR _sample_px_im_lo$25[ebp], ymm0

; 1310 :           __m256i sample_px_im_hi = _mm256_unpackhi_epi16(sample_px_16bit, sample_im_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_16bit$27[ebp]
	vpunpckhwd ymm0, ymm0, YMMWORD PTR _sample_im_16bit$26[ebp]
	vmovdqu	YMMWORD PTR _sample_px_im_hi$24[ebp], ymm0

; 1311 : 
; 1312 :           __m256i all_ones = _mm256_set1_epi16(1);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vmovdqu	YMMWORD PTR _all_ones$23[ebp], ymm0

; 1313 :           __m256i avg_lo   = _mm256_madd_epi16(sample_px_im_lo, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_im_lo$25[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$23[ebp]
	vmovdqu	YMMWORD PTR _avg_lo$22[ebp], ymm0

; 1314 :           __m256i avg_hi   = _mm256_madd_epi16(sample_px_im_hi, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_im_hi$24[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$23[ebp]
	vmovdqu	YMMWORD PTR _avg_hi$21[ebp], ymm0

; 1315 : 
; 1316 :           avg_lo = _mm256_add_epi32(avg_lo, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_lo$22[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_lo$22[ebp], ymm0

; 1317 :           avg_hi = _mm256_add_epi32(avg_hi, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_hi$21[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_hi$21[ebp], ymm0

; 1318 : 
; 1319 :           avg_lo = _mm256_srai_epi32(avg_lo, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_lo$22[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_lo$22[ebp], ymm0

; 1320 :           avg_hi = _mm256_srai_epi32(avg_hi, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_hi$21[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_hi$21[ebp], ymm0

; 1321 : 
; 1322 :           __m256i avg256 = _mm256_packus_epi32(avg_lo, avg_hi);

	vmovdqu	ymm0, YMMWORD PTR _avg_lo$22[ebp]
	vpackusdw ymm0, ymm0, YMMWORD PTR _avg_hi$21[ebp]
	vmovdqu	YMMWORD PTR _avg256$20[ebp], ymm0

; 1323 :           avg256         = _mm256_packus_epi16(avg256, avg256);

	vmovdqu	ymm0, YMMWORD PTR _avg256$20[ebp]
	vpackuswb ymm0, ymm0, YMMWORD PTR _avg256$20[ebp]
	vmovdqu	YMMWORD PTR _avg256$20[ebp], ymm0

; 1324 :           avg256         = _mm256_permute4x64_epi64(avg256, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm0, YMMWORD PTR _avg256$20[ebp], 216	; 000000d8H
	vmovdqu	YMMWORD PTR _avg256$20[ebp], ymm0

; 1325 :           __m128i avg    = _mm256_castsi256_si128(avg256);

	vmovdqu	xmm0, XMMWORD PTR _avg256$20[ebp]
	vmovdqa	XMMWORD PTR _avg$19[ebp], xmm0

; 1326 : 
; 1327 :           _mm_storel_epi64((__m128i*)&dst[y * dst_stride], avg);

	vmovdqa	xmm0, XMMWORD PTR _avg$19[ebp]
	mov	eax, DWORD PTR _y$29[ebp]
	imul	eax, DWORD PTR _dst_stride$[ebx]
	add	eax, DWORD PTR _dst$[ebx]
	vmovq	QWORD PTR [eax], xmm0

; 1328 :           _mm_storeh_pd((double*)&dst[(y + 1) * dst_stride], _mm_castsi128_pd(avg));

	vmovdqa	xmm0, XMMWORD PTR _avg$19[ebp]
	mov	eax, DWORD PTR _y$29[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _dst_stride$[ebx]
	add	eax, DWORD PTR _dst$[ebx]
	vmovhpd	QWORD PTR [eax], xmm0

; 1329 :         }

	jmp	$LN14@bipred_ave
$LN15@bipred_ave:

; 1330 :         break;

	jmp	$LN1@bipred_ave
$LN33@bipred_ave:

; 1331 :       case 6: // 6x8
; 1332 :         for (int i = 0; i < pu_w * pu_h; i += 12) {

	mov	DWORD PTR _i$18[ebp], 0
	jmp	SHORT $LN19@bipred_ave
$LN17@bipred_ave:
	mov	eax, DWORD PTR _i$18[ebp]
	add	eax, 12					; 0000000cH
	mov	DWORD PTR _i$18[ebp], eax
$LN19@bipred_ave:
	mov	eax, DWORD PTR _pu_w$[ebx]
	imul	eax, DWORD PTR _pu_h$[ebx]
	cmp	DWORD PTR _i$18[ebp], eax
	jae	$LN18@bipred_ave

; 1333 : 
; 1334 :           int y = i / pu_w;

	mov	eax, DWORD PTR _i$18[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR _y$17[ebp], eax

; 1335 : 
; 1336 :           __m128i mask128         = _mm_setr_epi32(-1, -1, -1, 0);

	vmovdqa	xmm0, XMMWORD PTR __xmm@00000000ffffffffffffffffffffffff
	vmovdqa	XMMWORD PTR _mask128$16[ebp], xmm0

; 1337 :           __m128i sample_px_8bit  = _mm_maskload_epi32((const int*)(&px[i]), mask128);

	vmovdqa	xmm0, XMMWORD PTR _mask128$16[ebp]
	mov	eax, DWORD PTR _px$[ebx]
	add	eax, DWORD PTR _i$18[ebp]
	vpmaskmovd xmm0, xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _sample_px_8bit$15[ebp], xmm0

; 1338 : 
; 1339 :           __m256i mask            = _mm256_setr_epi64x(-1, -1, -1, 0);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	YMMWORD PTR _mask$14[ebp], ymm0

; 1340 :           __m256i sample_px_16bit = _mm256_cvtepu8_epi16(sample_px_8bit);

	vpmovzxbw ymm0, XMMWORD PTR _sample_px_8bit$15[ebp]
	vmovdqu	YMMWORD PTR _sample_px_16bit$13[ebp], ymm0

; 1341 :           sample_px_16bit         = _mm256_slli_epi16(sample_px_16bit, 14 - KVZ_BIT_DEPTH);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_16bit$13[ebp]
	vpsllw	ymm0, ymm0, 6
	vmovdqu	YMMWORD PTR _sample_px_16bit$13[ebp], ymm0

; 1342 :           __m256i sample_im_16bit = _mm256_maskload_epi64((const long long*)(&im[i]), mask);

	vmovdqu	ymm0, YMMWORD PTR _mask$14[ebp]
	mov	eax, DWORD PTR _i$18[ebp]
	mov	ecx, DWORD PTR _im$[ebx]
	vpmaskmovq ymm0, ymm0, YMMWORD PTR [ecx+eax*2]
	vmovdqu	YMMWORD PTR _sample_im_16bit$12[ebp], ymm0

; 1343 : 
; 1344 :           __m256i sample_px_im_lo = _mm256_unpacklo_epi16(sample_px_16bit, sample_im_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_16bit$13[ebp]
	vpunpcklwd ymm0, ymm0, YMMWORD PTR _sample_im_16bit$12[ebp]
	vmovdqu	YMMWORD PTR _sample_px_im_lo$11[ebp], ymm0

; 1345 :           __m256i sample_px_im_hi = _mm256_unpackhi_epi16(sample_px_16bit, sample_im_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_16bit$13[ebp]
	vpunpckhwd ymm0, ymm0, YMMWORD PTR _sample_im_16bit$12[ebp]
	vmovdqu	YMMWORD PTR _sample_px_im_hi$10[ebp], ymm0

; 1346 : 
; 1347 :           __m256i all_ones = _mm256_set1_epi16(1);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vmovdqu	YMMWORD PTR _all_ones$9[ebp], ymm0

; 1348 :           __m256i avg_lo   = _mm256_madd_epi16(sample_px_im_lo, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_im_lo$11[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$9[ebp]
	vmovdqu	YMMWORD PTR _avg_lo$8[ebp], ymm0

; 1349 :           __m256i avg_hi   = _mm256_madd_epi16(sample_px_im_hi, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_px_im_hi$10[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$9[ebp]
	vmovdqu	YMMWORD PTR _avg_hi$7[ebp], ymm0

; 1350 : 
; 1351 :           avg_lo = _mm256_add_epi32(avg_lo, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_lo$8[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_lo$8[ebp], ymm0

; 1352 :           avg_hi = _mm256_add_epi32(avg_hi, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_hi$7[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_hi$7[ebp], ymm0

; 1353 : 
; 1354 :           avg_lo = _mm256_srai_epi32(avg_lo, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_lo$8[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_lo$8[ebp], ymm0

; 1355 :           avg_hi = _mm256_srai_epi32(avg_hi, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_hi$7[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_hi$7[ebp], ymm0

; 1356 : 
; 1357 :           __m256i avg256 = _mm256_packus_epi32(avg_lo, avg_hi);

	vmovdqu	ymm0, YMMWORD PTR _avg_lo$8[ebp]
	vpackusdw ymm0, ymm0, YMMWORD PTR _avg_hi$7[ebp]
	vmovdqu	YMMWORD PTR _avg256$6[ebp], ymm0

; 1358 :           avg256         = _mm256_packus_epi16(avg256, avg256);

	vmovdqu	ymm0, YMMWORD PTR _avg256$6[ebp]
	vpackuswb ymm0, ymm0, YMMWORD PTR _avg256$6[ebp]
	vmovdqu	YMMWORD PTR _avg256$6[ebp], ymm0

; 1359 :           avg256         = _mm256_permute4x64_epi64(avg256, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm0, YMMWORD PTR _avg256$6[ebp], 216	; 000000d8H
	vmovdqu	YMMWORD PTR _avg256$6[ebp], ymm0

; 1360 :           __m128i avg    = _mm256_castsi256_si128(avg256);

	vmovdqu	xmm0, XMMWORD PTR _avg256$6[ebp]
	vmovdqa	XMMWORD PTR _avg$5[ebp], xmm0

; 1361 : 
; 1362 :           uint32_t elements_0123 = _mm_cvtsi128_si32(avg);

	vmovdqa	xmm0, XMMWORD PTR _avg$5[ebp]
	vmovd	eax, xmm0
	mov	DWORD PTR _elements_0123$4[ebp], eax

; 1363 :           uint16_t elements_45   = _mm_extract_epi16(avg, 2);

	vmovdqa	xmm0, XMMWORD PTR _avg$5[ebp]
	vpextrw	eax, xmm0, 2
	mov	WORD PTR _elements_45$3[ebp], ax

; 1364 :           uint16_t elements_67   = _mm_extract_epi16(avg, 3);

	vmovdqa	xmm0, XMMWORD PTR _avg$5[ebp]
	vpextrw	eax, xmm0, 3
	mov	WORD PTR _elements_67$2[ebp], ax

; 1365 :           uint32_t elements_89ab = _mm_extract_epi32(avg, 2);

	vmovdqa	xmm0, XMMWORD PTR _avg$5[ebp]
	vpextrd	eax, xmm0, 2
	mov	DWORD PTR _elements_89ab$1[ebp], eax

; 1366 :           *(uint32_t*)&dst[(y + 0) * dst_stride + 0] = elements_0123;

	mov	eax, DWORD PTR _y$17[ebp]
	imul	eax, DWORD PTR _dst_stride$[ebx]
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	edx, DWORD PTR _elements_0123$4[ebp]
	mov	DWORD PTR [ecx+eax], edx

; 1367 :           *(uint16_t*)&dst[(y + 0) * dst_stride + 4] = elements_45;

	mov	eax, DWORD PTR _y$17[ebp]
	imul	eax, DWORD PTR _dst_stride$[ebx]
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	dx, WORD PTR _elements_45$3[ebp]
	mov	WORD PTR [ecx+eax+4], dx

; 1368 :           *(uint16_t*)&dst[(y + 1) * dst_stride + 0] = elements_67;

	mov	eax, DWORD PTR _y$17[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _dst_stride$[ebx]
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	dx, WORD PTR _elements_67$2[ebp]
	mov	WORD PTR [ecx+eax], dx

; 1369 :           *(uint32_t*)&dst[(y + 1) * dst_stride + 2] = elements_89ab;

	mov	eax, DWORD PTR _y$17[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _dst_stride$[ebx]
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	edx, DWORD PTR _elements_89ab$1[ebp]
	mov	DWORD PTR [ecx+eax+2], edx

; 1370 :         }

	jmp	$LN17@bipred_ave
$LN18@bipred_ave:

; 1371 :         break;

	jmp	SHORT $LN1@bipred_ave
$LN34@bipred_ave:

; 1372 :       default:
; 1373 :         assert(0 && "Unexpected block width.");

	xor	eax, eax
	jne	SHORT $LN1@bipred_ave
	mov	ecx, DWORD PTR ?__LINE__Var@?0??bipred_average_px_im_template_avx2@@9@9
	add	ecx, 189				; 000000bdH
	mov	esi, esp
	push	ecx
	push	OFFSET ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
	push	OFFSET ??_C@_1DO@PGNEGAJ@?$AA0?$AA?5?$AA?$CG?$AA?$CG?$AA?5?$AA?$CC?$AAU?$AAn?$AAe?$AAx?$AAp?$AAe?$AAc?$AAt?$AAe@
	call	DWORD PTR __imp___wassert
	add	esp, 12					; 0000000cH
	cmp	esi, esp
	call	__RTC_CheckEsp
$LN1@bipred_ave:

; 1374 :         break;
; 1375 :     }
; 1376 :   }
; 1377 : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
$LN52@bipred_ave:
	DD	$LN22@bipred_ave
	DD	$LN23@bipred_ave
	DD	$LN24@bipred_ave
	DD	$LN25@bipred_ave
	DD	$LN26@bipred_ave
$LN51@bipred_ave:
	DB	0
	DB	4
	DB	4
	DB	4
	DB	1
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	2
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	3
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	3
_bipred_average_px_im_template_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _bipred_average_im_im_avx2
_TEXT	SEGMENT
tv67 = -292						; size = 4
_rounded$1 = -92					; size = 4
_sample_L1$2 = -80					; size = 2
_sample_L0$3 = -68					; size = 2
_x$4 = -56						; size = 4
_y$5 = -44						; size = 4
_i$6 = -32						; size = 4
_offset$7 = -20						; size = 4
_shift$8 = -8						; size = 4
_dst$ = 8						; size = 4
_im_L0$ = 12						; size = 4
_im_L1$ = 16						; size = 4
_pu_w$ = 20						; size = 4
_pu_h$ = 24						; size = 4
_dst_stride$ = 28					; size = 4
_bipred_average_im_im_avx2 PROC				; COMDAT

; 1144 : {

	push	ebp
	mov	ebp, esp
	sub	esp, 292				; 00000124H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-292]
	mov	ecx, 73					; 00000049H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 1145 :   // Use scalar code for yet unoptimized block sizes (4x4, 2x8)
; 1146 :   if (!(pu_w == 4 && pu_h == 4) && pu_w > 2) {

	cmp	DWORD PTR _pu_w$[ebp], 4
	jne	SHORT $LN9@bipred_ave
	cmp	DWORD PTR _pu_h$[ebp], 4
	je	$LN7@bipred_ave
$LN9@bipred_ave:
	cmp	DWORD PTR _pu_w$[ebp], 2
	jbe	$LN7@bipred_ave

; 1147 :     switch (pu_w) {

	mov	eax, DWORD PTR _pu_w$[ebp]
	mov	DWORD PTR tv67[ebp], eax
	mov	ecx, DWORD PTR tv67[ebp]
	sub	ecx, 4
	mov	DWORD PTR tv67[ebp], ecx
	cmp	DWORD PTR tv67[ebp], 60			; 0000003cH
	ja	$LN19@bipred_ave
	mov	edx, DWORD PTR tv67[ebp]
	movzx	eax, BYTE PTR $LN22@bipred_ave[edx]
	jmp	DWORD PTR $LN23@bipred_ave[eax*4]
$LN10@bipred_ave:

; 1148 :       case  4: bipred_average_im_im_template_avx2(dst, im_L0, im_L1,  4, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	4
	mov	edx, DWORD PTR _im_L1$[ebp]
	push	edx
	mov	eax, DWORD PTR _im_L0$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_im_im_template_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN11@bipred_ave:

; 1149 :       case  8: bipred_average_im_im_template_avx2(dst, im_L0, im_L1,  8, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	8
	mov	edx, DWORD PTR _im_L1$[ebp]
	push	edx
	mov	eax, DWORD PTR _im_L0$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_im_im_template_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN12@bipred_ave:

; 1150 :       case 16: bipred_average_im_im_template_avx2(dst, im_L0, im_L1, 16, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	16					; 00000010H
	mov	edx, DWORD PTR _im_L1$[ebp]
	push	edx
	mov	eax, DWORD PTR _im_L0$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_im_im_template_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN13@bipred_ave:

; 1151 :       case 32: bipred_average_im_im_template_avx2(dst, im_L0, im_L1, 32, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	32					; 00000020H
	mov	edx, DWORD PTR _im_L1$[ebp]
	push	edx
	mov	eax, DWORD PTR _im_L0$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_im_im_template_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN14@bipred_ave:

; 1152 :       case 64: bipred_average_im_im_template_avx2(dst, im_L0, im_L1, 64, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	64					; 00000040H
	mov	edx, DWORD PTR _im_L1$[ebp]
	push	edx
	mov	eax, DWORD PTR _im_L0$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_im_im_template_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN15@bipred_ave:

; 1153 : 
; 1154 :       case  6: bipred_average_im_im_template_avx2(dst, im_L0, im_L1,  6, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	6
	mov	edx, DWORD PTR _im_L1$[ebp]
	push	edx
	mov	eax, DWORD PTR _im_L0$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_im_im_template_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN16@bipred_ave:

; 1155 :       case 12: bipred_average_im_im_template_avx2(dst, im_L0, im_L1, 12, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	12					; 0000000cH
	mov	edx, DWORD PTR _im_L1$[ebp]
	push	edx
	mov	eax, DWORD PTR _im_L0$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_im_im_template_avx2
	add	esp, 24					; 00000018H
	jmp	SHORT $LN2@bipred_ave
$LN17@bipred_ave:

; 1156 :       case 24: bipred_average_im_im_template_avx2(dst, im_L0, im_L1, 24, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	24					; 00000018H
	mov	edx, DWORD PTR _im_L1$[ebp]
	push	edx
	mov	eax, DWORD PTR _im_L0$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_im_im_template_avx2
	add	esp, 24					; 00000018H
	jmp	SHORT $LN2@bipred_ave
$LN18@bipred_ave:

; 1157 :       case 48: bipred_average_im_im_template_avx2(dst, im_L0, im_L1, 48, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	48					; 00000030H
	mov	edx, DWORD PTR _im_L1$[ebp]
	push	edx
	mov	eax, DWORD PTR _im_L0$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_im_im_template_avx2
	add	esp, 24					; 00000018H
	jmp	SHORT $LN2@bipred_ave
$LN19@bipred_ave:

; 1158 :       default:
; 1159 :         assert(0 && "Unexpected block width.");

	xor	eax, eax
	jne	SHORT $LN2@bipred_ave
	mov	ecx, DWORD PTR ?__LINE__Var@?0??bipred_average_im_im_avx2@@9@9
	add	ecx, 15					; 0000000fH
	mov	esi, esp
	push	ecx
	push	OFFSET ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
	push	OFFSET ??_C@_1DO@PGNEGAJ@?$AA0?$AA?5?$AA?$CG?$AA?$CG?$AA?5?$AA?$CC?$AAU?$AAn?$AAe?$AAx?$AAp?$AAe?$AAc?$AAt?$AAe@
	call	DWORD PTR __imp___wassert
	add	esp, 12					; 0000000cH
	cmp	esi, esp
	call	__RTC_CheckEsp
$LN2@bipred_ave:

; 1160 :         break;
; 1161 :     }
; 1162 :   } else {

	jmp	$LN1@bipred_ave
$LN7@bipred_ave:

; 1163 :     int32_t shift = 15 - KVZ_BIT_DEPTH; // TODO: defines

	mov	DWORD PTR _shift$8[ebp], 7

; 1164 :     int32_t offset = 1 << (shift - 1);

	mov	eax, DWORD PTR _shift$8[ebp]
	sub	eax, 1
	mov	ecx, 1
	shlx	edx, ecx, eax
	mov	DWORD PTR _offset$7[ebp], edx

; 1165 : 
; 1166 :     for (int i = 0; i < pu_w * pu_h; ++i)

	mov	DWORD PTR _i$6[ebp], 0
	jmp	SHORT $LN6@bipred_ave
$LN4@bipred_ave:
	mov	eax, DWORD PTR _i$6[ebp]
	add	eax, 1
	mov	DWORD PTR _i$6[ebp], eax
$LN6@bipred_ave:
	mov	eax, DWORD PTR _pu_w$[ebp]
	imul	eax, DWORD PTR _pu_h$[ebp]
	cmp	DWORD PTR _i$6[ebp], eax
	jae	SHORT $LN1@bipred_ave

; 1167 :     {
; 1168 :       int y = i / pu_w;

	mov	eax, DWORD PTR _i$6[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebp]
	mov	DWORD PTR _y$5[ebp], eax

; 1169 :       int x = i % pu_w;

	mov	eax, DWORD PTR _i$6[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebp]
	mov	DWORD PTR _x$4[ebp], edx

; 1170 :       int16_t sample_L0 = im_L0[i];

	mov	eax, DWORD PTR _i$6[ebp]
	mov	ecx, DWORD PTR _im_L0$[ebp]
	mov	dx, WORD PTR [ecx+eax*2]
	mov	WORD PTR _sample_L0$3[ebp], dx

; 1171 :       int16_t sample_L1 = im_L1[i];

	mov	eax, DWORD PTR _i$6[ebp]
	mov	ecx, DWORD PTR _im_L1$[ebp]
	mov	dx, WORD PTR [ecx+eax*2]
	mov	WORD PTR _sample_L1$2[ebp], dx

; 1172 :       int32_t rounded = (sample_L0 + sample_L1 + offset) >> shift;

	movsx	eax, WORD PTR _sample_L0$3[ebp]
	movsx	ecx, WORD PTR _sample_L1$2[ebp]
	add	eax, DWORD PTR _offset$7[ebp]
	add	ecx, eax
	mov	edx, DWORD PTR _shift$8[ebp]
	sarx	eax, ecx, edx
	mov	DWORD PTR _rounded$1[ebp], eax

; 1173 :       dst[y * dst_stride + x] = kvz_fast_clip_32bit_to_pixel(rounded);

	mov	eax, DWORD PTR _rounded$1[ebp]
	push	eax
	call	_kvz_fast_clip_32bit_to_pixel
	add	esp, 4
	mov	ecx, DWORD PTR _y$5[ebp]
	imul	ecx, DWORD PTR _dst_stride$[ebp]
	add	ecx, DWORD PTR _x$4[ebp]
	mov	edx, DWORD PTR _dst$[ebp]
	mov	BYTE PTR [edx+ecx], al

; 1174 :     }

	jmp	SHORT $LN4@bipred_ave
$LN1@bipred_ave:

; 1175 :   }
; 1176 : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 292				; 00000124H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
$LN23@bipred_ave:
	DD	$LN10@bipred_ave
	DD	$LN15@bipred_ave
	DD	$LN11@bipred_ave
	DD	$LN16@bipred_ave
	DD	$LN12@bipred_ave
	DD	$LN17@bipred_ave
	DD	$LN13@bipred_ave
	DD	$LN18@bipred_ave
	DD	$LN14@bipred_ave
	DD	$LN19@bipred_ave
$LN22@bipred_ave:
	DB	0
	DB	9
	DB	1
	DB	9
	DB	2
	DB	9
	DB	9
	DB	9
	DB	3
	DB	9
	DB	9
	DB	9
	DB	4
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	5
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	6
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	7
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	8
_bipred_average_im_im_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _bipred_average_im_im_template_avx2
_TEXT	SEGMENT
tv73 = -5148						; size = 4
tv407 = -5144						; size = 4
tv383 = -5144						; size = 4
tv217 = -5144						; size = 4
tv78 = -5144						; size = 4
tv71 = -5144						; size = 4
_elements_89ab$1 = -3600				; size = 4
_elements_67$2 = -3588					; size = 2
_elements_45$3 = -3576					; size = 2
_elements_0123$4 = -3564				; size = 4
_avg$5 = -3552						; size = 16
_avg256$6 = -3520					; size = 32
_avg_a_hi$7 = -3456					; size = 32
_avg_a_lo$8 = -3392					; size = 32
_all_ones$9 = -3328					; size = 32
_sample_L0_L1_hi$10 = -3264				; size = 32
_sample_L0_L1_lo$11 = -3200				; size = 32
_sample_L1_16bit$12 = -3136				; size = 32
_sample_L0_16bit$13 = -3072				; size = 32
_mask$14 = -3008					; size = 32
_y$15 = -2968						; size = 4
_i$16 = -2956						; size = 4
_avg$17 = -2944						; size = 16
_avg256$18 = -2912					; size = 32
_avg_hi$19 = -2848					; size = 32
_avg_lo$20 = -2784					; size = 32
_all_ones$21 = -2720					; size = 32
_sample_L0_L1_hi$22 = -2656				; size = 32
_sample_L0_L1_lo$23 = -2592				; size = 32
_sample_L1_16bit$24 = -2528				; size = 32
_sample_L0_16bit$25 = -2464				; size = 32
_y$26 = -2424						; size = 4
_i$27 = -2412						; size = 4
_avg$28 = -2400						; size = 32
_avg0213$29 = -2336					; size = 32
_avg_23$30 = -2272					; size = 32
_avg_01$31 = -2208					; size = 32
_avg_b_hi$32 = -2144					; size = 32
_avg_b_lo$33 = -2080					; size = 32
_avg_a_hi$34 = -2016					; size = 32
_avg_a_lo$35 = -1952					; size = 32
_all_ones$36 = -1888					; size = 32
_sample_L0_L1_b_hi$37 = -1824				; size = 32
_sample_L0_L1_b_lo$38 = -1760				; size = 32
_sample_L0_L1_a_hi$39 = -1696				; size = 32
_sample_L0_L1_a_lo$40 = -1632				; size = 32
_sample_L1_b_16bit$41 = -1568				; size = 32
_sample_L0_b_16bit$42 = -1504				; size = 32
_sample_L1_a_16bit$43 = -1440				; size = 32
_sample_L0_a_16bit$44 = -1376				; size = 32
_mask$45 = -1312					; size = 32
_x$46 = -1252						; size = 4
_y$47 = -1240						; size = 4
_i$48 = -1228						; size = 4
_avg$49 = -1216						; size = 32
_avg0213$50 = -1152					; size = 32
_avg_23$51 = -1088					; size = 32
_avg_01$52 = -1024					; size = 32
_avg_b_hi$53 = -960					; size = 32
_avg_b_lo$54 = -896					; size = 32
_avg_a_hi$55 = -832					; size = 32
_avg_a_lo$56 = -768					; size = 32
_all_ones$57 = -704					; size = 32
_sample_L0_L1_b_hi$58 = -640				; size = 32
_sample_L0_L1_b_lo$59 = -576				; size = 32
_sample_L0_L1_a_hi$60 = -512				; size = 32
_sample_L0_L1_a_lo$61 = -448				; size = 32
_sample_L1_b_16bit$62 = -384				; size = 32
_sample_L0_b_16bit$63 = -320				; size = 32
_sample_L1_a_16bit$64 = -256				; size = 32
_sample_L0_a_16bit$65 = -192				; size = 32
_x$66 = -124						; size = 4
_y$67 = -112						; size = 4
_i$68 = -100						; size = 4
_area_mod_32$ = -85					; size = 1
_has_pow2_width$ = -73					; size = 1
_offset$ = -64						; size = 32
_scalar_offset$ = -20					; size = 4
_shift$ = -8						; size = 4
_dst$ = 8						; size = 4
_im_L0$ = 12						; size = 4
_im_L1$ = 16						; size = 4
_pu_w$ = 20						; size = 4
_pu_h$ = 24						; size = 4
_dst_stride$ = 28					; size = 4
_bipred_average_im_im_template_avx2 PROC		; COMDAT

; 956  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	mov	eax, 5176				; 00001438H
	call	__chkstk
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-5176]
	mov	ecx, 1294				; 0000050eH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 957  :   int32_t shift = 15 - KVZ_BIT_DEPTH; // TODO: defines

	mov	DWORD PTR _shift$[ebp], 7

; 958  :   int32_t scalar_offset = 1 << (shift - 1);

	mov	eax, DWORD PTR _shift$[ebp]
	sub	eax, 1
	mov	ecx, 1
	shlx	edx, ecx, eax
	mov	DWORD PTR _scalar_offset$[ebp], edx

; 959  :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovd	xmm0, DWORD PTR _scalar_offset$[ebp]
	vpbroadcastd ymm0, xmm0
	vmovdqu	YMMWORD PTR _offset$[ebp], ymm0

; 960  : 
; 961  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	popcnt	eax, DWORD PTR _pu_w$[ebx]
	cmp	eax, 1
	jne	SHORT $LN36@bipred_ave
	mov	DWORD PTR tv71[ebp], 1
	jmp	SHORT $LN37@bipred_ave
$LN36@bipred_ave:
	mov	DWORD PTR tv71[ebp], 0
$LN37@bipred_ave:
	cmp	DWORD PTR tv71[ebp], 0
	jne	SHORT $LN38@bipred_ave
	mov	DWORD PTR tv73[ebp], 0
	jmp	SHORT $LN39@bipred_ave
$LN38@bipred_ave:
	mov	DWORD PTR tv73[ebp], 1
$LN39@bipred_ave:
	mov	cl, BYTE PTR tv73[ebp]
	mov	BYTE PTR _has_pow2_width$[ebp], cl

; 962  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	mov	eax, DWORD PTR _pu_w$[ebx]
	imul	eax, DWORD PTR _pu_h$[ebx]
	xor	edx, edx
	mov	ecx, 32					; 00000020H
	div	ecx
	test	edx, edx
	jne	SHORT $LN40@bipred_ave
	mov	DWORD PTR tv78[ebp], 0
	jmp	SHORT $LN41@bipred_ave
$LN40@bipred_ave:
	mov	DWORD PTR tv78[ebp], 1
$LN41@bipred_ave:
	mov	dl, BYTE PTR tv78[ebp]
	mov	BYTE PTR _area_mod_32$[ebp], dl

; 963  :   assert(!(pu_w == 4 && pu_h == 4) && "Branch for 4x4 not yet implemented.");

	cmp	DWORD PTR _pu_w$[ebx], 4
	jne	SHORT $LN42@bipred_ave
	cmp	DWORD PTR _pu_h$[ebx], 4
	je	SHORT $LN43@bipred_ave
$LN42@bipred_ave:
	mov	eax, OFFSET ??_C@_0CE@OJCKHEGB@Branch?5for?54x4?5not?5yet?5implemen@
	test	eax, eax
	jne	SHORT $LN44@bipred_ave
$LN43@bipred_ave:
	mov	ecx, DWORD PTR ?__LINE__Var@?0??bipred_average_im_im_template_avx2@@9@9
	add	ecx, 7
	mov	esi, esp
	push	ecx
	push	OFFSET ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
	push	OFFSET ??_C@_1IG@PFICKALD@?$AA?$CB?$AA?$CI?$AAp?$AAu?$AA_?$AAw?$AA?5?$AA?$DN?$AA?$DN?$AA?5?$AA4?$AA?5?$AA?$CG?$AA?$CG?$AA?5@
	call	DWORD PTR __imp___wassert
	add	esp, 12					; 0000000cH
	cmp	esi, esp
	call	__RTC_CheckEsp
$LN44@bipred_ave:

; 964  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");

	cmp	DWORD PTR _pu_w$[ebx], 2
	jne	SHORT $LN45@bipred_ave
	cmp	DWORD PTR _pu_h$[ebx], 8
	je	SHORT $LN46@bipred_ave
$LN45@bipred_ave:
	mov	eax, OFFSET ??_C@_0CE@ICBEPKHC@Branch?5for?52x8?5not?5yet?5implemen@
	test	eax, eax
	jne	SHORT $LN47@bipred_ave
$LN46@bipred_ave:
	mov	ecx, DWORD PTR ?__LINE__Var@?0??bipred_average_im_im_template_avx2@@9@9
	add	ecx, 8
	mov	esi, esp
	push	ecx
	push	OFFSET ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
	push	OFFSET ??_C@_1IG@BCNOOMBC@?$AA?$CB?$AA?$CI?$AAp?$AAu?$AA_?$AAw?$AA?5?$AA?$DN?$AA?$DN?$AA?5?$AA2?$AA?5?$AA?$CG?$AA?$CG?$AA?5@
	call	DWORD PTR __imp___wassert
	add	esp, 12					; 0000000cH
	cmp	esi, esp
	call	__RTC_CheckEsp
$LN47@bipred_ave:

; 965  : 
; 966  :   if (has_pow2_width && area_mod_32 == 0) {

	movzx	eax, BYTE PTR _has_pow2_width$[ebp]
	test	eax, eax
	je	$LN20@bipred_ave
	movzx	eax, BYTE PTR _area_mod_32$[ebp]
	test	eax, eax
	jne	$LN20@bipred_ave

; 967  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	mov	DWORD PTR _i$68[ebp], 0
	jmp	SHORT $LN4@bipred_ave
$LN2@bipred_ave:
	mov	eax, DWORD PTR _i$68[ebp]
	add	eax, 32					; 00000020H
	mov	DWORD PTR _i$68[ebp], eax
$LN4@bipred_ave:
	mov	eax, DWORD PTR _pu_w$[ebx]
	imul	eax, DWORD PTR _pu_h$[ebx]
	cmp	DWORD PTR _i$68[ebp], eax
	jae	$LN3@bipred_ave

; 968  :       int y = i / pu_w;

	mov	eax, DWORD PTR _i$68[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR _y$67[ebp], eax

; 969  :       int x = i % pu_w;

	mov	eax, DWORD PTR _i$68[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR _x$66[ebp], edx

; 970  : 
; 971  :       __m256i sample_L0_a_16bit = _mm256_loadu_si256((__m256i*)&im_L0[i]);

	mov	eax, DWORD PTR _i$68[ebp]
	mov	ecx, DWORD PTR _im_L0$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx+eax*2]
	vmovdqu	YMMWORD PTR _sample_L0_a_16bit$65[ebp], ymm0

; 972  :       __m256i sample_L1_a_16bit = _mm256_loadu_si256((__m256i*)&im_L1[i]);

	mov	eax, DWORD PTR _i$68[ebp]
	mov	ecx, DWORD PTR _im_L1$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx+eax*2]
	vmovdqu	YMMWORD PTR _sample_L1_a_16bit$64[ebp], ymm0

; 973  :       __m256i sample_L0_b_16bit = _mm256_loadu_si256((__m256i*)&im_L0[i + 16]);

	mov	eax, DWORD PTR _i$68[ebp]
	mov	ecx, DWORD PTR _im_L0$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx+eax*2+32]
	vmovdqu	YMMWORD PTR _sample_L0_b_16bit$63[ebp], ymm0

; 974  :       __m256i sample_L1_b_16bit = _mm256_loadu_si256((__m256i*)&im_L1[i + 16]);

	mov	eax, DWORD PTR _i$68[ebp]
	mov	ecx, DWORD PTR _im_L1$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx+eax*2+32]
	vmovdqu	YMMWORD PTR _sample_L1_b_16bit$62[ebp], ymm0

; 975  : 
; 976  :       __m256i sample_L0_L1_a_lo = _mm256_unpacklo_epi16(sample_L0_a_16bit, sample_L1_a_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_a_16bit$65[ebp]
	vpunpcklwd ymm0, ymm0, YMMWORD PTR _sample_L1_a_16bit$64[ebp]
	vmovdqu	YMMWORD PTR _sample_L0_L1_a_lo$61[ebp], ymm0

; 977  :       __m256i sample_L0_L1_a_hi = _mm256_unpackhi_epi16(sample_L0_a_16bit, sample_L1_a_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_a_16bit$65[ebp]
	vpunpckhwd ymm0, ymm0, YMMWORD PTR _sample_L1_a_16bit$64[ebp]
	vmovdqu	YMMWORD PTR _sample_L0_L1_a_hi$60[ebp], ymm0

; 978  :       __m256i sample_L0_L1_b_lo = _mm256_unpacklo_epi16(sample_L0_b_16bit, sample_L1_b_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_b_16bit$63[ebp]
	vpunpcklwd ymm0, ymm0, YMMWORD PTR _sample_L1_b_16bit$62[ebp]
	vmovdqu	YMMWORD PTR _sample_L0_L1_b_lo$59[ebp], ymm0

; 979  :       __m256i sample_L0_L1_b_hi = _mm256_unpackhi_epi16(sample_L0_b_16bit, sample_L1_b_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_b_16bit$63[ebp]
	vpunpckhwd ymm0, ymm0, YMMWORD PTR _sample_L1_b_16bit$62[ebp]
	vmovdqu	YMMWORD PTR _sample_L0_L1_b_hi$58[ebp], ymm0

; 980  : 
; 981  :       __m256i all_ones = _mm256_set1_epi16(1);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vmovdqu	YMMWORD PTR _all_ones$57[ebp], ymm0

; 982  :       __m256i avg_a_lo = _mm256_madd_epi16(sample_L0_L1_a_lo, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_L1_a_lo$61[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$57[ebp]
	vmovdqu	YMMWORD PTR _avg_a_lo$56[ebp], ymm0

; 983  :       __m256i avg_a_hi = _mm256_madd_epi16(sample_L0_L1_a_hi, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_L1_a_hi$60[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$57[ebp]
	vmovdqu	YMMWORD PTR _avg_a_hi$55[ebp], ymm0

; 984  :       __m256i avg_b_lo = _mm256_madd_epi16(sample_L0_L1_b_lo, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_L1_b_lo$59[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$57[ebp]
	vmovdqu	YMMWORD PTR _avg_b_lo$54[ebp], ymm0

; 985  :       __m256i avg_b_hi = _mm256_madd_epi16(sample_L0_L1_b_hi, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_L1_b_hi$58[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$57[ebp]
	vmovdqu	YMMWORD PTR _avg_b_hi$53[ebp], ymm0

; 986  : 
; 987  :       avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_lo$56[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_a_lo$56[ebp], ymm0

; 988  :       avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_hi$55[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_a_hi$55[ebp], ymm0

; 989  :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_lo$54[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_b_lo$54[ebp], ymm0

; 990  :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_hi$53[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_b_hi$53[ebp], ymm0

; 991  : 
; 992  :       avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_lo$56[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_a_lo$56[ebp], ymm0

; 993  :       avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_hi$55[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_a_hi$55[ebp], ymm0

; 994  :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_lo$54[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_b_lo$54[ebp], ymm0

; 995  :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_hi$53[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_b_hi$53[ebp], ymm0

; 996  : 
; 997  :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_lo$56[ebp]
	vpackusdw ymm0, ymm0, YMMWORD PTR _avg_a_hi$55[ebp]
	vmovdqu	YMMWORD PTR _avg_01$52[ebp], ymm0

; 998  :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_lo$54[ebp]
	vpackusdw ymm0, ymm0, YMMWORD PTR _avg_b_hi$53[ebp]
	vmovdqu	YMMWORD PTR _avg_23$51[ebp], ymm0

; 999  :       __m256i avg0213 = _mm256_packus_epi16(avg_01, avg_23);

	vmovdqu	ymm0, YMMWORD PTR _avg_01$52[ebp]
	vpackuswb ymm0, ymm0, YMMWORD PTR _avg_23$51[ebp]
	vmovdqu	YMMWORD PTR _avg0213$50[ebp], ymm0

; 1000 :       __m256i avg     = _mm256_permute4x64_epi64(avg0213, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm0, YMMWORD PTR _avg0213$50[ebp], 216	; 000000d8H
	vmovdqu	YMMWORD PTR _avg$49[ebp], ymm0

; 1001 : 
; 1002 :       switch (pu_w) {

	mov	eax, DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR tv217[ebp], eax
	mov	ecx, DWORD PTR tv217[ebp]
	sub	ecx, 4
	mov	DWORD PTR tv217[ebp], ecx
	cmp	DWORD PTR tv217[ebp], 60		; 0000003cH
	ja	$LN26@bipred_ave
	mov	edx, DWORD PTR tv217[ebp]
	movzx	eax, BYTE PTR $LN51@bipred_ave[edx]
	jmp	DWORD PTR $LN52@bipred_ave[eax*4]
$LN22@bipred_ave:

; 1003 :         case  4: scatter_ymm_4x8_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebx]
	push	eax
	vmovdqu	ymm0, YMMWORD PTR _avg$49[ebp]
	mov	ecx, DWORD PTR _y$67[ebp]
	imul	ecx, DWORD PTR _dst_stride$[ebx]
	add	ecx, DWORD PTR _x$66[ebp]
	add	ecx, DWORD PTR _dst$[ebx]
	push	ecx
	call	_scatter_ymm_4x8_8bit
	add	esp, 8
	jmp	$LN5@bipred_ave
$LN23@bipred_ave:

; 1004 :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebx]
	push	eax
	vmovdqu	ymm0, YMMWORD PTR _avg$49[ebp]
	mov	ecx, DWORD PTR _y$67[ebp]
	imul	ecx, DWORD PTR _dst_stride$[ebx]
	add	ecx, DWORD PTR _x$66[ebp]
	add	ecx, DWORD PTR _dst$[ebx]
	push	ecx
	call	_scatter_ymm_8x4_8bit
	add	esp, 8
	jmp	SHORT $LN5@bipred_ave
$LN24@bipred_ave:

; 1005 :         case 16: scatter_ymm_16x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebx]
	push	eax
	vmovdqu	ymm0, YMMWORD PTR _avg$49[ebp]
	mov	ecx, DWORD PTR _y$67[ebp]
	imul	ecx, DWORD PTR _dst_stride$[ebx]
	add	ecx, DWORD PTR _x$66[ebp]
	add	ecx, DWORD PTR _dst$[ebx]
	push	ecx
	call	_scatter_ymm_16x2_8bit
	add	esp, 8
	jmp	SHORT $LN5@bipred_ave
$LN25@bipred_ave:

; 1006 :         case 32: // Same as case 64
; 1007 :         case 64: _mm256_storeu_si256((__m256i*)&dst[y * dst_stride + x], avg); break;

	vmovdqu	ymm0, YMMWORD PTR _avg$49[ebp]
	mov	eax, DWORD PTR _y$67[ebp]
	imul	eax, DWORD PTR _dst_stride$[ebx]
	add	eax, DWORD PTR _x$66[ebp]
	add	eax, DWORD PTR _dst$[ebx]
	vmovdqu	YMMWORD PTR [eax], ymm0
	jmp	SHORT $LN5@bipred_ave
$LN26@bipred_ave:

; 1008 :         default:
; 1009 :           assert(0 && "Unexpected block width.");

	xor	eax, eax
	jne	SHORT $LN5@bipred_ave
	mov	ecx, DWORD PTR ?__LINE__Var@?0??bipred_average_im_im_template_avx2@@9@9
	add	ecx, 53					; 00000035H
	mov	esi, esp
	push	ecx
	push	OFFSET ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
	push	OFFSET ??_C@_1DO@PGNEGAJ@?$AA0?$AA?5?$AA?$CG?$AA?$CG?$AA?5?$AA?$CC?$AAU?$AAn?$AAe?$AAx?$AAp?$AAe?$AAc?$AAt?$AAe@
	call	DWORD PTR __imp___wassert
	add	esp, 12					; 0000000cH
	cmp	esi, esp
	call	__RTC_CheckEsp
$LN5@bipred_ave:

; 1010 :           break;
; 1011 :       }
; 1012 :     }

	jmp	$LN2@bipred_ave
$LN3@bipred_ave:

; 1013 :   } else if (area_mod_32 == 0) {

	jmp	$LN1@bipred_ave
$LN20@bipred_ave:
	movzx	eax, BYTE PTR _area_mod_32$[ebp]
	test	eax, eax
	jne	$LN27@bipred_ave

; 1014 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	mov	DWORD PTR _i$48[ebp], 0
	jmp	SHORT $LN9@bipred_ave
$LN7@bipred_ave:
	mov	eax, DWORD PTR _i$48[ebp]
	add	eax, 24					; 00000018H
	mov	DWORD PTR _i$48[ebp], eax
$LN9@bipred_ave:
	mov	eax, DWORD PTR _pu_w$[ebx]
	imul	eax, DWORD PTR _pu_h$[ebx]
	cmp	DWORD PTR _i$48[ebp], eax
	jae	$LN8@bipred_ave

; 1015 : 
; 1016 :       int y = i / pu_w;

	mov	eax, DWORD PTR _i$48[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR _y$47[ebp], eax

; 1017 :       int x = i % pu_w;

	mov	eax, DWORD PTR _i$48[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR _x$46[ebp], edx

; 1018 : 
; 1019 :       // Last 64 bits of the 256 are not used to simplify the loop
; 1020 :       __m256i mask              = _mm256_setr_epi64x(-1, -1, -1, 0);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	YMMWORD PTR _mask$45[ebp], ymm0

; 1021 :       __m256i sample_L0_a_16bit = _mm256_loadu_si256((__m256i*)&im_L0[i]);

	mov	eax, DWORD PTR _i$48[ebp]
	mov	ecx, DWORD PTR _im_L0$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx+eax*2]
	vmovdqu	YMMWORD PTR _sample_L0_a_16bit$44[ebp], ymm0

; 1022 :       __m256i sample_L1_a_16bit = _mm256_loadu_si256((__m256i*)&im_L1[i]);

	mov	eax, DWORD PTR _i$48[ebp]
	mov	ecx, DWORD PTR _im_L1$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx+eax*2]
	vmovdqu	YMMWORD PTR _sample_L1_a_16bit$43[ebp], ymm0

; 1023 :       __m256i sample_L0_b_16bit = _mm256_castsi128_si256(_mm_loadu_si128((__m128i*)&im_L0[i + 16]));

	mov	eax, DWORD PTR _i$48[ebp]
	mov	ecx, DWORD PTR _im_L0$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [ecx+eax*2+32]
	vmovdqu	YMMWORD PTR _sample_L0_b_16bit$42[ebp], ymm0

; 1024 :       __m256i sample_L1_b_16bit = _mm256_castsi128_si256(_mm_loadu_si128((__m128i*)&im_L1[i + 16]));

	mov	eax, DWORD PTR _i$48[ebp]
	mov	ecx, DWORD PTR _im_L1$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [ecx+eax*2+32]
	vmovdqu	YMMWORD PTR _sample_L1_b_16bit$41[ebp], ymm0

; 1025 : 
; 1026 :       __m256i sample_L0_L1_a_lo = _mm256_unpacklo_epi16(sample_L0_a_16bit, sample_L1_a_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_a_16bit$44[ebp]
	vpunpcklwd ymm0, ymm0, YMMWORD PTR _sample_L1_a_16bit$43[ebp]
	vmovdqu	YMMWORD PTR _sample_L0_L1_a_lo$40[ebp], ymm0

; 1027 :       __m256i sample_L0_L1_a_hi = _mm256_unpackhi_epi16(sample_L0_a_16bit, sample_L1_a_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_a_16bit$44[ebp]
	vpunpckhwd ymm0, ymm0, YMMWORD PTR _sample_L1_a_16bit$43[ebp]
	vmovdqu	YMMWORD PTR _sample_L0_L1_a_hi$39[ebp], ymm0

; 1028 :       __m256i sample_L0_L1_b_lo = _mm256_unpacklo_epi16(sample_L0_b_16bit, sample_L1_b_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_b_16bit$42[ebp]
	vpunpcklwd ymm0, ymm0, YMMWORD PTR _sample_L1_b_16bit$41[ebp]
	vmovdqu	YMMWORD PTR _sample_L0_L1_b_lo$38[ebp], ymm0

; 1029 :       __m256i sample_L0_L1_b_hi = _mm256_unpackhi_epi16(sample_L0_b_16bit, sample_L1_b_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_b_16bit$42[ebp]
	vpunpckhwd ymm0, ymm0, YMMWORD PTR _sample_L1_b_16bit$41[ebp]
	vmovdqu	YMMWORD PTR _sample_L0_L1_b_hi$37[ebp], ymm0

; 1030 : 
; 1031 :       __m256i all_ones = _mm256_set1_epi16(1);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vmovdqu	YMMWORD PTR _all_ones$36[ebp], ymm0

; 1032 :       __m256i avg_a_lo = _mm256_madd_epi16(sample_L0_L1_a_lo, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_L1_a_lo$40[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$36[ebp]
	vmovdqu	YMMWORD PTR _avg_a_lo$35[ebp], ymm0

; 1033 :       __m256i avg_a_hi = _mm256_madd_epi16(sample_L0_L1_a_hi, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_L1_a_hi$39[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$36[ebp]
	vmovdqu	YMMWORD PTR _avg_a_hi$34[ebp], ymm0

; 1034 :       __m256i avg_b_lo = _mm256_madd_epi16(sample_L0_L1_b_lo, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_L1_b_lo$38[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$36[ebp]
	vmovdqu	YMMWORD PTR _avg_b_lo$33[ebp], ymm0

; 1035 :       __m256i avg_b_hi = _mm256_madd_epi16(sample_L0_L1_b_hi, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_L1_b_hi$37[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$36[ebp]
	vmovdqu	YMMWORD PTR _avg_b_hi$32[ebp], ymm0

; 1036 : 
; 1037 :       avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_lo$35[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_a_lo$35[ebp], ymm0

; 1038 :       avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_hi$34[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_a_hi$34[ebp], ymm0

; 1039 :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_lo$33[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_b_lo$33[ebp], ymm0

; 1040 :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_hi$32[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_b_hi$32[ebp], ymm0

; 1041 : 
; 1042 :       avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_lo$35[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_a_lo$35[ebp], ymm0

; 1043 :       avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_hi$34[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_a_hi$34[ebp], ymm0

; 1044 :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_lo$33[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_b_lo$33[ebp], ymm0

; 1045 :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_hi$32[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_b_hi$32[ebp], ymm0

; 1046 : 
; 1047 :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_lo$35[ebp]
	vpackusdw ymm0, ymm0, YMMWORD PTR _avg_a_hi$34[ebp]
	vmovdqu	YMMWORD PTR _avg_01$31[ebp], ymm0

; 1048 :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vmovdqu	ymm0, YMMWORD PTR _avg_b_lo$33[ebp]
	vpackusdw ymm0, ymm0, YMMWORD PTR _avg_b_hi$32[ebp]
	vmovdqu	YMMWORD PTR _avg_23$30[ebp], ymm0

; 1049 :       __m256i avg0213 = _mm256_packus_epi16(avg_01, avg_23);

	vmovdqu	ymm0, YMMWORD PTR _avg_01$31[ebp]
	vpackuswb ymm0, ymm0, YMMWORD PTR _avg_23$30[ebp]
	vmovdqu	YMMWORD PTR _avg0213$29[ebp], ymm0

; 1050 :       __m256i avg     = _mm256_permute4x64_epi64(avg0213, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm0, YMMWORD PTR _avg0213$29[ebp], 216	; 000000d8H
	vmovdqu	YMMWORD PTR _avg$28[ebp], ymm0

; 1051 : 
; 1052 :       switch (pu_w) {

	mov	eax, DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR tv383[ebp], eax
	cmp	DWORD PTR tv383[ebp], 12		; 0000000cH
	je	SHORT $LN29@bipred_ave
	cmp	DWORD PTR tv383[ebp], 24		; 00000018H
	je	SHORT $LN30@bipred_ave
	cmp	DWORD PTR tv383[ebp], 48		; 00000030H
	je	SHORT $LN30@bipred_ave
	jmp	SHORT $LN31@bipred_ave
$LN29@bipred_ave:

; 1053 :         case 12: scatter_ymm_12x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebx]
	push	eax
	vmovdqu	ymm0, YMMWORD PTR _avg$28[ebp]
	mov	ecx, DWORD PTR _y$47[ebp]
	imul	ecx, DWORD PTR _dst_stride$[ebx]
	add	ecx, DWORD PTR _x$46[ebp]
	add	ecx, DWORD PTR _dst$[ebx]
	push	ecx
	call	_scatter_ymm_12x2_8bit
	add	esp, 8
	jmp	SHORT $LN10@bipred_ave
$LN30@bipred_ave:

; 1054 :         case 24: // Same as case 48
; 1055 :         case 48: _mm256_maskstore_epi64((long long*)&dst[y * dst_stride + x], mask, avg); break;

	vmovdqu	ymm0, YMMWORD PTR _avg$28[ebp]
	vmovdqu	ymm1, YMMWORD PTR _mask$45[ebp]
	mov	eax, DWORD PTR _y$47[ebp]
	imul	eax, DWORD PTR _dst_stride$[ebx]
	add	eax, DWORD PTR _x$46[ebp]
	add	eax, DWORD PTR _dst$[ebx]
	vpmaskmovq YMMWORD PTR [eax], ymm1, ymm0
	jmp	SHORT $LN10@bipred_ave
$LN31@bipred_ave:

; 1056 :         default:
; 1057 :           assert(0 && "Unexpected block width.");

	xor	eax, eax
	jne	SHORT $LN10@bipred_ave
	mov	ecx, DWORD PTR ?__LINE__Var@?0??bipred_average_im_im_template_avx2@@9@9
	add	ecx, 101				; 00000065H
	mov	esi, esp
	push	ecx
	push	OFFSET ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
	push	OFFSET ??_C@_1DO@PGNEGAJ@?$AA0?$AA?5?$AA?$CG?$AA?$CG?$AA?5?$AA?$CC?$AAU?$AAn?$AAe?$AAx?$AAp?$AAe?$AAc?$AAt?$AAe@
	call	DWORD PTR __imp___wassert
	add	esp, 12					; 0000000cH
	cmp	esi, esp
	call	__RTC_CheckEsp
$LN10@bipred_ave:

; 1058 :           break;
; 1059 :       }
; 1060 :     }

	jmp	$LN7@bipred_ave
$LN8@bipred_ave:

; 1061 :   } else {

	jmp	$LN1@bipred_ave
$LN27@bipred_ave:

; 1062 :     // 8x2, 8x6, 6x8 blocks (and maybe 2x8 in the future)
; 1063 :     switch (pu_w) {

	mov	eax, DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR tv407[ebp], eax
	cmp	DWORD PTR tv407[ebp], 6
	je	$LN33@bipred_ave
	cmp	DWORD PTR tv407[ebp], 8
	je	SHORT $LN32@bipred_ave
	jmp	$LN34@bipred_ave
$LN32@bipred_ave:

; 1064 :       case 8: // 8x2, 8x6
; 1065 :         for (int i = 0; i < pu_w * pu_h; i += 16) {

	mov	DWORD PTR _i$27[ebp], 0
	jmp	SHORT $LN16@bipred_ave
$LN14@bipred_ave:
	mov	eax, DWORD PTR _i$27[ebp]
	add	eax, 16					; 00000010H
	mov	DWORD PTR _i$27[ebp], eax
$LN16@bipred_ave:
	mov	eax, DWORD PTR _pu_w$[ebx]
	imul	eax, DWORD PTR _pu_h$[ebx]
	cmp	DWORD PTR _i$27[ebp], eax
	jae	$LN15@bipred_ave

; 1066 : 
; 1067 :           int y = i / pu_w;

	mov	eax, DWORD PTR _i$27[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR _y$26[ebp], eax

; 1068 : 
; 1069 :           __m256i sample_L0_16bit = _mm256_loadu_si256((__m256i*)&im_L0[i]);

	mov	eax, DWORD PTR _i$27[ebp]
	mov	ecx, DWORD PTR _im_L0$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx+eax*2]
	vmovdqu	YMMWORD PTR _sample_L0_16bit$25[ebp], ymm0

; 1070 :           __m256i sample_L1_16bit = _mm256_loadu_si256((__m256i*)&im_L1[i]);

	mov	eax, DWORD PTR _i$27[ebp]
	mov	ecx, DWORD PTR _im_L1$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx+eax*2]
	vmovdqu	YMMWORD PTR _sample_L1_16bit$24[ebp], ymm0

; 1071 : 
; 1072 :           __m256i sample_L0_L1_lo = _mm256_unpacklo_epi16(sample_L0_16bit, sample_L1_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_16bit$25[ebp]
	vpunpcklwd ymm0, ymm0, YMMWORD PTR _sample_L1_16bit$24[ebp]
	vmovdqu	YMMWORD PTR _sample_L0_L1_lo$23[ebp], ymm0

; 1073 :           __m256i sample_L0_L1_hi = _mm256_unpackhi_epi16(sample_L0_16bit, sample_L1_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_16bit$25[ebp]
	vpunpckhwd ymm0, ymm0, YMMWORD PTR _sample_L1_16bit$24[ebp]
	vmovdqu	YMMWORD PTR _sample_L0_L1_hi$22[ebp], ymm0

; 1074 : 
; 1075 :           __m256i all_ones = _mm256_set1_epi16(1);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vmovdqu	YMMWORD PTR _all_ones$21[ebp], ymm0

; 1076 :           __m256i avg_lo   = _mm256_madd_epi16(sample_L0_L1_lo, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_L1_lo$23[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$21[ebp]
	vmovdqu	YMMWORD PTR _avg_lo$20[ebp], ymm0

; 1077 :           __m256i avg_hi   = _mm256_madd_epi16(sample_L0_L1_hi, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_L1_hi$22[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$21[ebp]
	vmovdqu	YMMWORD PTR _avg_hi$19[ebp], ymm0

; 1078 : 
; 1079 :           avg_lo = _mm256_add_epi32(avg_lo, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_lo$20[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_lo$20[ebp], ymm0

; 1080 :           avg_hi = _mm256_add_epi32(avg_hi, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_hi$19[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_hi$19[ebp], ymm0

; 1081 : 
; 1082 :           avg_lo = _mm256_srai_epi32(avg_lo, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_lo$20[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_lo$20[ebp], ymm0

; 1083 :           avg_hi = _mm256_srai_epi32(avg_hi, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_hi$19[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_hi$19[ebp], ymm0

; 1084 : 
; 1085 :           __m256i avg256 = _mm256_packus_epi32(avg_lo, avg_hi);

	vmovdqu	ymm0, YMMWORD PTR _avg_lo$20[ebp]
	vpackusdw ymm0, ymm0, YMMWORD PTR _avg_hi$19[ebp]
	vmovdqu	YMMWORD PTR _avg256$18[ebp], ymm0

; 1086 :           avg256         = _mm256_packus_epi16(avg256, avg256);

	vmovdqu	ymm0, YMMWORD PTR _avg256$18[ebp]
	vpackuswb ymm0, ymm0, YMMWORD PTR _avg256$18[ebp]
	vmovdqu	YMMWORD PTR _avg256$18[ebp], ymm0

; 1087 :           avg256         = _mm256_permute4x64_epi64(avg256, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm0, YMMWORD PTR _avg256$18[ebp], 216	; 000000d8H
	vmovdqu	YMMWORD PTR _avg256$18[ebp], ymm0

; 1088 :           __m128i avg    = _mm256_castsi256_si128(avg256);

	vmovdqu	xmm0, XMMWORD PTR _avg256$18[ebp]
	vmovdqa	XMMWORD PTR _avg$17[ebp], xmm0

; 1089 : 
; 1090 :           _mm_storel_epi64((__m128i*)&dst[y * dst_stride], avg);

	vmovdqa	xmm0, XMMWORD PTR _avg$17[ebp]
	mov	eax, DWORD PTR _y$26[ebp]
	imul	eax, DWORD PTR _dst_stride$[ebx]
	add	eax, DWORD PTR _dst$[ebx]
	vmovq	QWORD PTR [eax], xmm0

; 1091 :           _mm_storeh_pd((double*)&dst[(y + 1) * dst_stride], _mm_castsi128_pd(avg));

	vmovdqa	xmm0, XMMWORD PTR _avg$17[ebp]
	mov	eax, DWORD PTR _y$26[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _dst_stride$[ebx]
	add	eax, DWORD PTR _dst$[ebx]
	vmovhpd	QWORD PTR [eax], xmm0

; 1092 :         }

	jmp	$LN14@bipred_ave
$LN15@bipred_ave:

; 1093 :         break;

	jmp	$LN1@bipred_ave
$LN33@bipred_ave:

; 1094 :       case 6: // 6x8
; 1095 :         for (int i = 0; i < pu_w * pu_h; i += 12) {

	mov	DWORD PTR _i$16[ebp], 0
	jmp	SHORT $LN19@bipred_ave
$LN17@bipred_ave:
	mov	eax, DWORD PTR _i$16[ebp]
	add	eax, 12					; 0000000cH
	mov	DWORD PTR _i$16[ebp], eax
$LN19@bipred_ave:
	mov	eax, DWORD PTR _pu_w$[ebx]
	imul	eax, DWORD PTR _pu_h$[ebx]
	cmp	DWORD PTR _i$16[ebp], eax
	jae	$LN18@bipred_ave

; 1096 : 
; 1097 :           int y = i / pu_w;

	mov	eax, DWORD PTR _i$16[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR _y$15[ebp], eax

; 1098 : 
; 1099 :           __m256i mask            = _mm256_setr_epi64x(-1, -1, -1, 0);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	YMMWORD PTR _mask$14[ebp], ymm0

; 1100 :           __m256i sample_L0_16bit = _mm256_maskload_epi64((const long long*)(&im_L0[i]), mask);

	vmovdqu	ymm0, YMMWORD PTR _mask$14[ebp]
	mov	eax, DWORD PTR _i$16[ebp]
	mov	ecx, DWORD PTR _im_L0$[ebx]
	vpmaskmovq ymm0, ymm0, YMMWORD PTR [ecx+eax*2]
	vmovdqu	YMMWORD PTR _sample_L0_16bit$13[ebp], ymm0

; 1101 :           __m256i sample_L1_16bit = _mm256_maskload_epi64((const long long*)(&im_L1[i]), mask);

	vmovdqu	ymm0, YMMWORD PTR _mask$14[ebp]
	mov	eax, DWORD PTR _i$16[ebp]
	mov	ecx, DWORD PTR _im_L1$[ebx]
	vpmaskmovq ymm0, ymm0, YMMWORD PTR [ecx+eax*2]
	vmovdqu	YMMWORD PTR _sample_L1_16bit$12[ebp], ymm0

; 1102 : 
; 1103 :           __m256i sample_L0_L1_lo = _mm256_unpacklo_epi16(sample_L0_16bit, sample_L1_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_16bit$13[ebp]
	vpunpcklwd ymm0, ymm0, YMMWORD PTR _sample_L1_16bit$12[ebp]
	vmovdqu	YMMWORD PTR _sample_L0_L1_lo$11[ebp], ymm0

; 1104 :           __m256i sample_L0_L1_hi = _mm256_unpackhi_epi16(sample_L0_16bit, sample_L1_16bit);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_16bit$13[ebp]
	vpunpckhwd ymm0, ymm0, YMMWORD PTR _sample_L1_16bit$12[ebp]
	vmovdqu	YMMWORD PTR _sample_L0_L1_hi$10[ebp], ymm0

; 1105 : 
; 1106 :           __m256i all_ones = _mm256_set1_epi16(1);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vmovdqu	YMMWORD PTR _all_ones$9[ebp], ymm0

; 1107 :           __m256i avg_a_lo = _mm256_madd_epi16(sample_L0_L1_lo, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_L1_lo$11[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$9[ebp]
	vmovdqu	YMMWORD PTR _avg_a_lo$8[ebp], ymm0

; 1108 :           __m256i avg_a_hi = _mm256_madd_epi16(sample_L0_L1_hi, all_ones);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0_L1_hi$10[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _all_ones$9[ebp]
	vmovdqu	YMMWORD PTR _avg_a_hi$7[ebp], ymm0

; 1109 : 
; 1110 :           avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_lo$8[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_a_lo$8[ebp], ymm0

; 1111 :           avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_hi$7[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _offset$[ebp]
	vmovdqu	YMMWORD PTR _avg_a_hi$7[ebp], ymm0

; 1112 : 
; 1113 :           avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_lo$8[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_a_lo$8[ebp], ymm0

; 1114 :           avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_hi$7[ebp]
	vmovd	xmm1, DWORD PTR _shift$[ebp]
	vpsrad	ymm0, ymm0, xmm1
	vmovdqu	YMMWORD PTR _avg_a_hi$7[ebp], ymm0

; 1115 : 
; 1116 :           __m256i avg256 = _mm256_packus_epi32(avg_a_lo, avg_a_hi);

	vmovdqu	ymm0, YMMWORD PTR _avg_a_lo$8[ebp]
	vpackusdw ymm0, ymm0, YMMWORD PTR _avg_a_hi$7[ebp]
	vmovdqu	YMMWORD PTR _avg256$6[ebp], ymm0

; 1117 :           avg256         = _mm256_packus_epi16(avg256, avg256);

	vmovdqu	ymm0, YMMWORD PTR _avg256$6[ebp]
	vpackuswb ymm0, ymm0, YMMWORD PTR _avg256$6[ebp]
	vmovdqu	YMMWORD PTR _avg256$6[ebp], ymm0

; 1118 :           avg256         = _mm256_permute4x64_epi64(avg256, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm0, YMMWORD PTR _avg256$6[ebp], 216	; 000000d8H
	vmovdqu	YMMWORD PTR _avg256$6[ebp], ymm0

; 1119 :           __m128i avg    = _mm256_castsi256_si128(avg256);

	vmovdqu	xmm0, XMMWORD PTR _avg256$6[ebp]
	vmovdqa	XMMWORD PTR _avg$5[ebp], xmm0

; 1120 : 
; 1121 :           uint32_t elements_0123 = _mm_cvtsi128_si32(avg);

	vmovdqa	xmm0, XMMWORD PTR _avg$5[ebp]
	vmovd	eax, xmm0
	mov	DWORD PTR _elements_0123$4[ebp], eax

; 1122 :           uint16_t elements_45   = _mm_extract_epi16(avg, 2);

	vmovdqa	xmm0, XMMWORD PTR _avg$5[ebp]
	vpextrw	eax, xmm0, 2
	mov	WORD PTR _elements_45$3[ebp], ax

; 1123 :           uint16_t elements_67   = _mm_extract_epi16(avg, 3);

	vmovdqa	xmm0, XMMWORD PTR _avg$5[ebp]
	vpextrw	eax, xmm0, 3
	mov	WORD PTR _elements_67$2[ebp], ax

; 1124 :           uint32_t elements_89ab = _mm_extract_epi32(avg, 2);

	vmovdqa	xmm0, XMMWORD PTR _avg$5[ebp]
	vpextrd	eax, xmm0, 2
	mov	DWORD PTR _elements_89ab$1[ebp], eax

; 1125 :           *(uint32_t*)&dst[(y + 0) * dst_stride + 0] = elements_0123;

	mov	eax, DWORD PTR _y$15[ebp]
	imul	eax, DWORD PTR _dst_stride$[ebx]
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	edx, DWORD PTR _elements_0123$4[ebp]
	mov	DWORD PTR [ecx+eax], edx

; 1126 :           *(uint16_t*)&dst[(y + 0) * dst_stride + 4] = elements_45;

	mov	eax, DWORD PTR _y$15[ebp]
	imul	eax, DWORD PTR _dst_stride$[ebx]
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	dx, WORD PTR _elements_45$3[ebp]
	mov	WORD PTR [ecx+eax+4], dx

; 1127 :           *(uint16_t*)&dst[(y + 1) * dst_stride + 0] = elements_67;

	mov	eax, DWORD PTR _y$15[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _dst_stride$[ebx]
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	dx, WORD PTR _elements_67$2[ebp]
	mov	WORD PTR [ecx+eax], dx

; 1128 :           *(uint32_t*)&dst[(y + 1) * dst_stride + 2] = elements_89ab;

	mov	eax, DWORD PTR _y$15[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _dst_stride$[ebx]
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	edx, DWORD PTR _elements_89ab$1[ebp]
	mov	DWORD PTR [ecx+eax+2], edx

; 1129 :         }

	jmp	$LN17@bipred_ave
$LN18@bipred_ave:

; 1130 :         break;

	jmp	SHORT $LN1@bipred_ave
$LN34@bipred_ave:

; 1131 :       default:
; 1132 :         assert(0 && "Unexpected block width.");

	xor	eax, eax
	jne	SHORT $LN1@bipred_ave
	mov	ecx, DWORD PTR ?__LINE__Var@?0??bipred_average_im_im_template_avx2@@9@9
	add	ecx, 176				; 000000b0H
	mov	esi, esp
	push	ecx
	push	OFFSET ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
	push	OFFSET ??_C@_1DO@PGNEGAJ@?$AA0?$AA?5?$AA?$CG?$AA?$CG?$AA?5?$AA?$CC?$AAU?$AAn?$AAe?$AAx?$AAp?$AAe?$AAc?$AAt?$AAe@
	call	DWORD PTR __imp___wassert
	add	esp, 12					; 0000000cH
	cmp	esi, esp
	call	__RTC_CheckEsp
$LN1@bipred_ave:

; 1133 :         break;
; 1134 :     }
; 1135 :   }
; 1136 : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
	npad	3
$LN52@bipred_ave:
	DD	$LN22@bipred_ave
	DD	$LN23@bipred_ave
	DD	$LN24@bipred_ave
	DD	$LN25@bipred_ave
	DD	$LN26@bipred_ave
$LN51@bipred_ave:
	DB	0
	DB	4
	DB	4
	DB	4
	DB	1
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	2
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	3
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	3
_bipred_average_im_im_template_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _bipred_average_px_px_avx2
_TEXT	SEGMENT
tv67 = -292						; size = 4
_rounded$1 = -92					; size = 4
_sample_L1$2 = -80					; size = 2
_sample_L0$3 = -68					; size = 2
_x$4 = -56						; size = 4
_y$5 = -44						; size = 4
_i$6 = -32						; size = 4
_offset$7 = -20						; size = 4
_shift$8 = -8						; size = 4
_dst$ = 8						; size = 4
_px_L0$ = 12						; size = 4
_px_L1$ = 16						; size = 4
_pu_w$ = 20						; size = 4
_pu_h$ = 24						; size = 4
_dst_stride$ = 28					; size = 4
_bipred_average_px_px_avx2 PROC				; COMDAT

; 916  : {

	push	ebp
	mov	ebp, esp
	sub	esp, 292				; 00000124H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-292]
	mov	ecx, 73					; 00000049H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 917  :   // Use scalar code for yet unoptimized block sizes (4x4, 2x8)
; 918  :   if (!(pu_w == 4 && pu_h == 4) && pu_w > 2) {

	cmp	DWORD PTR _pu_w$[ebp], 4
	jne	SHORT $LN9@bipred_ave
	cmp	DWORD PTR _pu_h$[ebp], 4
	je	$LN7@bipred_ave
$LN9@bipred_ave:
	cmp	DWORD PTR _pu_w$[ebp], 2
	jbe	$LN7@bipred_ave

; 919  :     switch (pu_w) {

	mov	eax, DWORD PTR _pu_w$[ebp]
	mov	DWORD PTR tv67[ebp], eax
	mov	ecx, DWORD PTR tv67[ebp]
	sub	ecx, 4
	mov	DWORD PTR tv67[ebp], ecx
	cmp	DWORD PTR tv67[ebp], 60			; 0000003cH
	ja	$LN19@bipred_ave
	mov	edx, DWORD PTR tv67[ebp]
	movzx	eax, BYTE PTR $LN22@bipred_ave[edx]
	jmp	DWORD PTR $LN23@bipred_ave[eax*4]
$LN10@bipred_ave:

; 920  :       case  4: bipred_average_px_px_template_avx2(dst, px_L0, px_L1,  4, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	4
	mov	edx, DWORD PTR _px_L1$[ebp]
	push	edx
	mov	eax, DWORD PTR _px_L0$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_px_px_template_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN11@bipred_ave:

; 921  :       case  8: bipred_average_px_px_template_avx2(dst, px_L0, px_L1,  8, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	8
	mov	edx, DWORD PTR _px_L1$[ebp]
	push	edx
	mov	eax, DWORD PTR _px_L0$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_px_px_template_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN12@bipred_ave:

; 922  :       case 16: bipred_average_px_px_template_avx2(dst, px_L0, px_L1, 16, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	16					; 00000010H
	mov	edx, DWORD PTR _px_L1$[ebp]
	push	edx
	mov	eax, DWORD PTR _px_L0$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_px_px_template_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN13@bipred_ave:

; 923  :       case 32: bipred_average_px_px_template_avx2(dst, px_L0, px_L1, 32, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	32					; 00000020H
	mov	edx, DWORD PTR _px_L1$[ebp]
	push	edx
	mov	eax, DWORD PTR _px_L0$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_px_px_template_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN14@bipred_ave:

; 924  :       case 64: bipred_average_px_px_template_avx2(dst, px_L0, px_L1, 64, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	64					; 00000040H
	mov	edx, DWORD PTR _px_L1$[ebp]
	push	edx
	mov	eax, DWORD PTR _px_L0$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_px_px_template_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN15@bipred_ave:

; 925  : 
; 926  :       case  6: bipred_average_px_px_template_avx2(dst, px_L0, px_L1,  6, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	6
	mov	edx, DWORD PTR _px_L1$[ebp]
	push	edx
	mov	eax, DWORD PTR _px_L0$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_px_px_template_avx2
	add	esp, 24					; 00000018H
	jmp	$LN2@bipred_ave
$LN16@bipred_ave:

; 927  :       case 12: bipred_average_px_px_template_avx2(dst, px_L0, px_L1, 12, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	12					; 0000000cH
	mov	edx, DWORD PTR _px_L1$[ebp]
	push	edx
	mov	eax, DWORD PTR _px_L0$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_px_px_template_avx2
	add	esp, 24					; 00000018H
	jmp	SHORT $LN2@bipred_ave
$LN17@bipred_ave:

; 928  :       case 24: bipred_average_px_px_template_avx2(dst, px_L0, px_L1, 24, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	24					; 00000018H
	mov	edx, DWORD PTR _px_L1$[ebp]
	push	edx
	mov	eax, DWORD PTR _px_L0$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_px_px_template_avx2
	add	esp, 24					; 00000018H
	jmp	SHORT $LN2@bipred_ave
$LN18@bipred_ave:

; 929  :       case 48: bipred_average_px_px_template_avx2(dst, px_L0, px_L1, 48, pu_h, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _pu_h$[ebp]
	push	ecx
	push	48					; 00000030H
	mov	edx, DWORD PTR _px_L1$[ebp]
	push	edx
	mov	eax, DWORD PTR _px_L0$[ebp]
	push	eax
	mov	ecx, DWORD PTR _dst$[ebp]
	push	ecx
	call	_bipred_average_px_px_template_avx2
	add	esp, 24					; 00000018H
	jmp	SHORT $LN2@bipred_ave
$LN19@bipred_ave:

; 930  :       default:
; 931  :         assert(0 && "Unexpected block width.");

	xor	eax, eax
	jne	SHORT $LN2@bipred_ave
	mov	ecx, DWORD PTR ?__LINE__Var@?0??bipred_average_px_px_avx2@@9@9
	add	ecx, 15					; 0000000fH
	mov	esi, esp
	push	ecx
	push	OFFSET ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
	push	OFFSET ??_C@_1DO@PGNEGAJ@?$AA0?$AA?5?$AA?$CG?$AA?$CG?$AA?5?$AA?$CC?$AAU?$AAn?$AAe?$AAx?$AAp?$AAe?$AAc?$AAt?$AAe@
	call	DWORD PTR __imp___wassert
	add	esp, 12					; 0000000cH
	cmp	esi, esp
	call	__RTC_CheckEsp
$LN2@bipred_ave:

; 932  :         break;
; 933  :     }
; 934  :   } else {

	jmp	$LN1@bipred_ave
$LN7@bipred_ave:

; 935  :     int32_t shift = 15 - KVZ_BIT_DEPTH; // TODO: defines

	mov	DWORD PTR _shift$8[ebp], 7

; 936  :     int32_t offset = 1 << (shift - 1);

	mov	eax, DWORD PTR _shift$8[ebp]
	sub	eax, 1
	mov	ecx, 1
	shlx	edx, ecx, eax
	mov	DWORD PTR _offset$7[ebp], edx

; 937  : 
; 938  :     for (int i = 0; i < pu_w * pu_h; ++i)

	mov	DWORD PTR _i$6[ebp], 0
	jmp	SHORT $LN6@bipred_ave
$LN4@bipred_ave:
	mov	eax, DWORD PTR _i$6[ebp]
	add	eax, 1
	mov	DWORD PTR _i$6[ebp], eax
$LN6@bipred_ave:
	mov	eax, DWORD PTR _pu_w$[ebp]
	imul	eax, DWORD PTR _pu_h$[ebp]
	cmp	DWORD PTR _i$6[ebp], eax
	jae	SHORT $LN1@bipred_ave

; 939  :     {
; 940  :       int y = i / pu_w;

	mov	eax, DWORD PTR _i$6[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebp]
	mov	DWORD PTR _y$5[ebp], eax

; 941  :       int x = i % pu_w;

	mov	eax, DWORD PTR _i$6[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebp]
	mov	DWORD PTR _x$4[ebp], edx

; 942  :       int16_t sample_L0 = px_L0[i] << (14 - KVZ_BIT_DEPTH);

	mov	eax, DWORD PTR _px_L0$[ebp]
	add	eax, DWORD PTR _i$6[ebp]
	movzx	ecx, BYTE PTR [eax]
	shl	ecx, 6
	mov	WORD PTR _sample_L0$3[ebp], cx

; 943  :       int16_t sample_L1 = px_L1[i] << (14 - KVZ_BIT_DEPTH);

	mov	eax, DWORD PTR _px_L1$[ebp]
	add	eax, DWORD PTR _i$6[ebp]
	movzx	ecx, BYTE PTR [eax]
	shl	ecx, 6
	mov	WORD PTR _sample_L1$2[ebp], cx

; 944  :       int32_t rounded = (sample_L0 + sample_L1 + offset) >> shift;

	movsx	eax, WORD PTR _sample_L0$3[ebp]
	movsx	ecx, WORD PTR _sample_L1$2[ebp]
	add	eax, DWORD PTR _offset$7[ebp]
	add	ecx, eax
	mov	edx, DWORD PTR _shift$8[ebp]
	sarx	eax, ecx, edx
	mov	DWORD PTR _rounded$1[ebp], eax

; 945  :       dst[y * dst_stride + x] = kvz_fast_clip_32bit_to_pixel(rounded);

	mov	eax, DWORD PTR _rounded$1[ebp]
	push	eax
	call	_kvz_fast_clip_32bit_to_pixel
	add	esp, 4
	mov	ecx, DWORD PTR _y$5[ebp]
	imul	ecx, DWORD PTR _dst_stride$[ebp]
	add	ecx, DWORD PTR _x$4[ebp]
	mov	edx, DWORD PTR _dst$[ebp]
	mov	BYTE PTR [edx+ecx], al

; 946  :     }

	jmp	$LN4@bipred_ave
$LN1@bipred_ave:

; 947  :   }
; 948  : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 292				; 00000124H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
	npad	1
$LN23@bipred_ave:
	DD	$LN10@bipred_ave
	DD	$LN15@bipred_ave
	DD	$LN11@bipred_ave
	DD	$LN16@bipred_ave
	DD	$LN12@bipred_ave
	DD	$LN17@bipred_ave
	DD	$LN13@bipred_ave
	DD	$LN18@bipred_ave
	DD	$LN14@bipred_ave
	DD	$LN19@bipred_ave
$LN22@bipred_ave:
	DB	0
	DB	9
	DB	1
	DB	9
	DB	2
	DB	9
	DB	9
	DB	9
	DB	3
	DB	9
	DB	9
	DB	9
	DB	4
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	5
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	6
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	7
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	8
_bipred_average_px_px_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _bipred_average_px_px_template_avx2
_TEXT	SEGMENT
tv69 = -1228						; size = 4
tv229 = -1224						; size = 4
tv205 = -1224						; size = 4
tv144 = -1224						; size = 4
tv74 = -1224						; size = 4
tv67 = -1224						; size = 4
_elements_89ab$1 = -832					; size = 4
_elements_67$2 = -820					; size = 2
_elements_45$3 = -808					; size = 2
_elements_0123$4 = -796					; size = 4
_avg$5 = -784						; size = 16
_sample_L1$6 = -752					; size = 16
_sample_L0$7 = -720					; size = 16
_mask$8 = -688						; size = 16
_y$9 = -656						; size = 4
_i$10 = -644						; size = 4
_y$11 = -632						; size = 4
_i$12 = -620						; size = 4
_avg$13 = -608						; size = 16
_sample_L1$14 = -576					; size = 16
_sample_L0$15 = -544					; size = 16
_avg$16 = -512						; size = 32
_sample_L1$17 = -448					; size = 32
_sample_L0$18 = -384					; size = 32
_mask$19 = -320						; size = 32
_x$20 = -260						; size = 4
_y$21 = -248						; size = 4
_i$22 = -236						; size = 4
_avg$23 = -224						; size = 32
_sample_L1$24 = -160					; size = 32
_sample_L0$25 = -96					; size = 32
_x$26 = -56						; size = 4
_y$27 = -44						; size = 4
_i$28 = -32						; size = 4
_area_mod_32$ = -17					; size = 1
_has_pow2_width$ = -5					; size = 1
_dst$ = 8						; size = 4
_px_L0$ = 12						; size = 4
_px_L1$ = 16						; size = 4
_pu_w$ = 20						; size = 4
_pu_h$ = 24						; size = 4
_dst_stride$ = 28					; size = 4
_bipred_average_px_px_template_avx2 PROC		; COMDAT

; 819  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 1240				; 000004d8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-1240]
	mov	ecx, 310				; 00000136H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	popcnt	eax, DWORD PTR _pu_w$[ebx]
	cmp	eax, 1
	jne	SHORT $LN36@bipred_ave
	mov	DWORD PTR tv67[ebp], 1
	jmp	SHORT $LN37@bipred_ave
$LN36@bipred_ave:
	mov	DWORD PTR tv67[ebp], 0
$LN37@bipred_ave:
	cmp	DWORD PTR tv67[ebp], 0
	jne	SHORT $LN38@bipred_ave
	mov	DWORD PTR tv69[ebp], 0
	jmp	SHORT $LN39@bipred_ave
$LN38@bipred_ave:
	mov	DWORD PTR tv69[ebp], 1
$LN39@bipred_ave:
	mov	cl, BYTE PTR tv69[ebp]
	mov	BYTE PTR _has_pow2_width$[ebp], cl

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	mov	eax, DWORD PTR _pu_w$[ebx]
	imul	eax, DWORD PTR _pu_h$[ebx]
	xor	edx, edx
	mov	ecx, 32					; 00000020H
	div	ecx
	test	edx, edx
	jne	SHORT $LN40@bipred_ave
	mov	DWORD PTR tv74[ebp], 0
	jmp	SHORT $LN41@bipred_ave
$LN40@bipred_ave:
	mov	DWORD PTR tv74[ebp], 1
$LN41@bipred_ave:
	mov	dl, BYTE PTR tv74[ebp]
	mov	BYTE PTR _area_mod_32$[ebp], dl

; 822  :   assert(!(pu_w == 4 && pu_h == 4) && "Branch for 4x4 not yet implemented.");

	cmp	DWORD PTR _pu_w$[ebx], 4
	jne	SHORT $LN42@bipred_ave
	cmp	DWORD PTR _pu_h$[ebx], 4
	je	SHORT $LN43@bipred_ave
$LN42@bipred_ave:
	mov	eax, OFFSET ??_C@_0CE@OJCKHEGB@Branch?5for?54x4?5not?5yet?5implemen@
	test	eax, eax
	jne	SHORT $LN44@bipred_ave
$LN43@bipred_ave:
	mov	ecx, DWORD PTR ?__LINE__Var@?0??bipred_average_px_px_template_avx2@@9@9
	add	ecx, 3
	mov	esi, esp
	push	ecx
	push	OFFSET ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
	push	OFFSET ??_C@_1IG@PFICKALD@?$AA?$CB?$AA?$CI?$AAp?$AAu?$AA_?$AAw?$AA?5?$AA?$DN?$AA?$DN?$AA?5?$AA4?$AA?5?$AA?$CG?$AA?$CG?$AA?5@
	call	DWORD PTR __imp___wassert
	add	esp, 12					; 0000000cH
	cmp	esi, esp
	call	__RTC_CheckEsp
$LN44@bipred_ave:

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");

	cmp	DWORD PTR _pu_w$[ebx], 2
	jne	SHORT $LN45@bipred_ave
	cmp	DWORD PTR _pu_h$[ebx], 8
	je	SHORT $LN46@bipred_ave
$LN45@bipred_ave:
	mov	eax, OFFSET ??_C@_0CE@ICBEPKHC@Branch?5for?52x8?5not?5yet?5implemen@
	test	eax, eax
	jne	SHORT $LN47@bipred_ave
$LN46@bipred_ave:
	mov	ecx, DWORD PTR ?__LINE__Var@?0??bipred_average_px_px_template_avx2@@9@9
	add	ecx, 4
	mov	esi, esp
	push	ecx
	push	OFFSET ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
	push	OFFSET ??_C@_1IG@BCNOOMBC@?$AA?$CB?$AA?$CI?$AAp?$AAu?$AA_?$AAw?$AA?5?$AA?$DN?$AA?$DN?$AA?5?$AA2?$AA?5?$AA?$CG?$AA?$CG?$AA?5@
	call	DWORD PTR __imp___wassert
	add	esp, 12					; 0000000cH
	cmp	esi, esp
	call	__RTC_CheckEsp
$LN47@bipred_ave:

; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	movzx	eax, BYTE PTR _has_pow2_width$[ebp]
	test	eax, eax
	je	$LN20@bipred_ave
	movzx	eax, BYTE PTR _area_mod_32$[ebp]
	test	eax, eax
	jne	$LN20@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	mov	DWORD PTR _i$28[ebp], 0
	jmp	SHORT $LN4@bipred_ave
$LN2@bipred_ave:
	mov	eax, DWORD PTR _i$28[ebp]
	add	eax, 32					; 00000020H
	mov	DWORD PTR _i$28[ebp], eax
$LN4@bipred_ave:
	mov	eax, DWORD PTR _pu_w$[ebx]
	imul	eax, DWORD PTR _pu_h$[ebx]
	cmp	DWORD PTR _i$28[ebp], eax
	jae	$LN3@bipred_ave

; 827  : 
; 828  :       int y = i / pu_w;

	mov	eax, DWORD PTR _i$28[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR _y$27[ebp], eax

; 829  :       int x = i % pu_w;

	mov	eax, DWORD PTR _i$28[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR _x$26[ebp], edx

; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);

	mov	eax, DWORD PTR _px_L0$[ebx]
	add	eax, DWORD PTR _i$28[ebp]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _sample_L0$25[ebp], ymm0

; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);

	mov	eax, DWORD PTR _px_L1$[ebx]
	add	eax, DWORD PTR _i$28[ebp]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _sample_L1$24[ebp], ymm0

; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0$25[ebp]
	vpavgb	ymm0, ymm0, YMMWORD PTR _sample_L1$24[ebp]
	vmovdqu	YMMWORD PTR _avg$23[ebp], ymm0

; 834  : 
; 835  :       switch (pu_w) {

	mov	eax, DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR tv144[ebp], eax
	mov	ecx, DWORD PTR tv144[ebp]
	sub	ecx, 4
	mov	DWORD PTR tv144[ebp], ecx
	cmp	DWORD PTR tv144[ebp], 60		; 0000003cH
	ja	$LN26@bipred_ave
	mov	edx, DWORD PTR tv144[ebp]
	movzx	eax, BYTE PTR $LN51@bipred_ave[edx]
	jmp	DWORD PTR $LN52@bipred_ave[eax*4]
$LN22@bipred_ave:

; 836  :         case  4: scatter_ymm_4x8_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebx]
	push	eax
	vmovdqu	ymm0, YMMWORD PTR _avg$23[ebp]
	mov	ecx, DWORD PTR _y$27[ebp]
	imul	ecx, DWORD PTR _dst_stride$[ebx]
	add	ecx, DWORD PTR _x$26[ebp]
	add	ecx, DWORD PTR _dst$[ebx]
	push	ecx
	call	_scatter_ymm_4x8_8bit
	add	esp, 8
	jmp	$LN5@bipred_ave
$LN23@bipred_ave:

; 837  :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebx]
	push	eax
	vmovdqu	ymm0, YMMWORD PTR _avg$23[ebp]
	mov	ecx, DWORD PTR _y$27[ebp]
	imul	ecx, DWORD PTR _dst_stride$[ebx]
	add	ecx, DWORD PTR _x$26[ebp]
	add	ecx, DWORD PTR _dst$[ebx]
	push	ecx
	call	_scatter_ymm_8x4_8bit
	add	esp, 8
	jmp	SHORT $LN5@bipred_ave
$LN24@bipred_ave:

; 838  :         case 16: scatter_ymm_16x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebx]
	push	eax
	vmovdqu	ymm0, YMMWORD PTR _avg$23[ebp]
	mov	ecx, DWORD PTR _y$27[ebp]
	imul	ecx, DWORD PTR _dst_stride$[ebx]
	add	ecx, DWORD PTR _x$26[ebp]
	add	ecx, DWORD PTR _dst$[ebx]
	push	ecx
	call	_scatter_ymm_16x2_8bit
	add	esp, 8
	jmp	SHORT $LN5@bipred_ave
$LN25@bipred_ave:

; 839  :         case 32: // Same as case 64
; 840  :         case 64: _mm256_storeu_si256((__m256i *)&dst[y * dst_stride + x], avg); break;

	vmovdqu	ymm0, YMMWORD PTR _avg$23[ebp]
	mov	eax, DWORD PTR _y$27[ebp]
	imul	eax, DWORD PTR _dst_stride$[ebx]
	add	eax, DWORD PTR _x$26[ebp]
	add	eax, DWORD PTR _dst$[ebx]
	vmovdqu	YMMWORD PTR [eax], ymm0
	jmp	SHORT $LN5@bipred_ave
$LN26@bipred_ave:

; 841  :         default:
; 842  :           assert(0 && "Unexpected block width.");

	xor	eax, eax
	jne	SHORT $LN5@bipred_ave
	mov	ecx, DWORD PTR ?__LINE__Var@?0??bipred_average_px_px_template_avx2@@9@9
	add	ecx, 23					; 00000017H
	mov	esi, esp
	push	ecx
	push	OFFSET ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
	push	OFFSET ??_C@_1DO@PGNEGAJ@?$AA0?$AA?5?$AA?$CG?$AA?$CG?$AA?5?$AA?$CC?$AAU?$AAn?$AAe?$AAx?$AAp?$AAe?$AAc?$AAt?$AAe@
	call	DWORD PTR __imp___wassert
	add	esp, 12					; 0000000cH
	cmp	esi, esp
	call	__RTC_CheckEsp
$LN5@bipred_ave:

; 843  :           break;
; 844  :       }
; 845  :     }

	jmp	$LN2@bipred_ave
$LN3@bipred_ave:

; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LN1@bipred_ave
$LN20@bipred_ave:
	movzx	eax, BYTE PTR _area_mod_32$[ebp]
	test	eax, eax
	jne	$LN27@bipred_ave

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	mov	DWORD PTR _i$22[ebp], 0
	jmp	SHORT $LN9@bipred_ave
$LN7@bipred_ave:
	mov	eax, DWORD PTR _i$22[ebp]
	add	eax, 24					; 00000018H
	mov	DWORD PTR _i$22[ebp], eax
$LN9@bipred_ave:
	mov	eax, DWORD PTR _pu_w$[ebx]
	imul	eax, DWORD PTR _pu_h$[ebx]
	cmp	DWORD PTR _i$22[ebp], eax
	jae	$LN8@bipred_ave

; 848  : 
; 849  :       int y = i / pu_w;

	mov	eax, DWORD PTR _i$22[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR _y$21[ebp], eax

; 850  :       int x = i % pu_w;

	mov	eax, DWORD PTR _i$22[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR _x$20[ebp], edx

; 851  : 
; 852  :       // Last 64 bits of the 256 are not used to simplify the loop
; 853  :       __m256i mask      = _mm256_setr_epi64x(-1, -1, -1, 0);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	YMMWORD PTR _mask$19[ebp], ymm0

; 854  :       __m256i sample_L0 = _mm256_maskload_epi64((const long long*)&px_L0[i], mask);

	vmovdqu	ymm0, YMMWORD PTR _mask$19[ebp]
	mov	eax, DWORD PTR _px_L0$[ebx]
	add	eax, DWORD PTR _i$22[ebp]
	vpmaskmovq ymm0, ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _sample_L0$18[ebp], ymm0

; 855  :       __m256i sample_L1 = _mm256_maskload_epi64((const long long*)&px_L1[i], mask);

	vmovdqu	ymm0, YMMWORD PTR _mask$19[ebp]
	mov	eax, DWORD PTR _px_L1$[ebx]
	add	eax, DWORD PTR _i$22[ebp]
	vpmaskmovq ymm0, ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _sample_L1$17[ebp], ymm0

; 856  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR _sample_L0$18[ebp]
	vpavgb	ymm0, ymm0, YMMWORD PTR _sample_L1$17[ebp]
	vmovdqu	YMMWORD PTR _avg$16[ebp], ymm0

; 857  : 
; 858  :       switch (pu_w) {

	mov	eax, DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR tv205[ebp], eax
	cmp	DWORD PTR tv205[ebp], 12		; 0000000cH
	je	SHORT $LN29@bipred_ave
	cmp	DWORD PTR tv205[ebp], 24		; 00000018H
	je	SHORT $LN30@bipred_ave
	cmp	DWORD PTR tv205[ebp], 48		; 00000030H
	je	SHORT $LN30@bipred_ave
	jmp	SHORT $LN31@bipred_ave
$LN29@bipred_ave:

; 859  :         case 12: scatter_ymm_12x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	mov	eax, DWORD PTR _dst_stride$[ebx]
	push	eax
	vmovdqu	ymm0, YMMWORD PTR _avg$16[ebp]
	mov	ecx, DWORD PTR _y$21[ebp]
	imul	ecx, DWORD PTR _dst_stride$[ebx]
	add	ecx, DWORD PTR _x$20[ebp]
	add	ecx, DWORD PTR _dst$[ebx]
	push	ecx
	call	_scatter_ymm_12x2_8bit
	add	esp, 8
	jmp	SHORT $LN10@bipred_ave
$LN30@bipred_ave:

; 860  :         case 24: // Same as case 48
; 861  :         case 48: _mm256_maskstore_epi64((long long*)&dst[y * dst_stride + x], mask, avg); break;

	vmovdqu	ymm0, YMMWORD PTR _avg$16[ebp]
	vmovdqu	ymm1, YMMWORD PTR _mask$19[ebp]
	mov	eax, DWORD PTR _y$21[ebp]
	imul	eax, DWORD PTR _dst_stride$[ebx]
	add	eax, DWORD PTR _x$20[ebp]
	add	eax, DWORD PTR _dst$[ebx]
	vpmaskmovq YMMWORD PTR [eax], ymm1, ymm0
	jmp	SHORT $LN10@bipred_ave
$LN31@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	xor	eax, eax
	jne	SHORT $LN10@bipred_ave
	mov	ecx, DWORD PTR ?__LINE__Var@?0??bipred_average_px_px_template_avx2@@9@9
	add	ecx, 44					; 0000002cH
	mov	esi, esp
	push	ecx
	push	OFFSET ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
	push	OFFSET ??_C@_1DO@PGNEGAJ@?$AA0?$AA?5?$AA?$CG?$AA?$CG?$AA?5?$AA?$CC?$AAU?$AAn?$AAe?$AAx?$AAp?$AAe?$AAc?$AAt?$AAe@
	call	DWORD PTR __imp___wassert
	add	esp, 12					; 0000000cH
	cmp	esi, esp
	call	__RTC_CheckEsp
$LN10@bipred_ave:

; 864  :           break;
; 865  :       }
; 866  :     }

	jmp	$LN7@bipred_ave
$LN8@bipred_ave:

; 867  :   } else {

	jmp	$LN1@bipred_ave
$LN27@bipred_ave:

; 868  :     // 8x2, 8x6, 6x8 blocks (and maybe 2x8 in the future)
; 869  :     switch (pu_w) {

	mov	eax, DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR tv229[ebp], eax
	cmp	DWORD PTR tv229[ebp], 6
	je	$LN33@bipred_ave
	cmp	DWORD PTR tv229[ebp], 8
	je	SHORT $LN32@bipred_ave
	jmp	$LN34@bipred_ave
$LN32@bipred_ave:

; 870  :       __m128i sample_L0, sample_L1, avg;
; 871  :       case 8: // 8x2, 8x6
; 872  :         for (int i = 0; i < pu_w * pu_h; i += 16) {

	mov	DWORD PTR _i$12[ebp], 0
	jmp	SHORT $LN16@bipred_ave
$LN14@bipred_ave:
	mov	eax, DWORD PTR _i$12[ebp]
	add	eax, 16					; 00000010H
	mov	DWORD PTR _i$12[ebp], eax
$LN16@bipred_ave:
	mov	eax, DWORD PTR _pu_w$[ebx]
	imul	eax, DWORD PTR _pu_h$[ebx]
	cmp	DWORD PTR _i$12[ebp], eax
	jae	$LN15@bipred_ave

; 873  : 
; 874  :           int y = i / pu_w;

	mov	eax, DWORD PTR _i$12[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR _y$11[ebp], eax

; 875  : 
; 876  :           sample_L0 = _mm_loadu_si128((__m128i*)&px_L0[i]);

	mov	eax, DWORD PTR _px_L0$[ebx]
	add	eax, DWORD PTR _i$12[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _sample_L0$15[ebp], xmm0

; 877  :           sample_L1 = _mm_loadu_si128((__m128i*)&px_L1[i]);

	mov	eax, DWORD PTR _px_L1$[ebx]
	add	eax, DWORD PTR _i$12[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _sample_L1$14[ebp], xmm0

; 878  :           avg       = _mm_avg_epu8(sample_L0, sample_L1);

	vmovdqa	xmm0, XMMWORD PTR _sample_L0$15[ebp]
	vpavgb	xmm0, xmm0, XMMWORD PTR _sample_L1$14[ebp]
	vmovdqa	XMMWORD PTR _avg$13[ebp], xmm0

; 879  :           _mm_storel_epi64((__m128i*)&dst[y * dst_stride], avg);

	vmovdqa	xmm0, XMMWORD PTR _avg$13[ebp]
	mov	eax, DWORD PTR _y$11[ebp]
	imul	eax, DWORD PTR _dst_stride$[ebx]
	add	eax, DWORD PTR _dst$[ebx]
	vmovq	QWORD PTR [eax], xmm0

; 880  :           _mm_storeh_pd((double*)&dst[(y + 1) * dst_stride], _mm_castsi128_pd(avg));

	vmovdqa	xmm0, XMMWORD PTR _avg$13[ebp]
	mov	eax, DWORD PTR _y$11[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _dst_stride$[ebx]
	add	eax, DWORD PTR _dst$[ebx]
	vmovhpd	QWORD PTR [eax], xmm0

; 881  :         }

	jmp	$LN14@bipred_ave
$LN15@bipred_ave:

; 882  :         break;

	jmp	$LN1@bipred_ave
$LN33@bipred_ave:

; 883  :       case 6: // 6x8
; 884  :         for (int i = 0; i < pu_w * pu_h; i += 12) {

	mov	DWORD PTR _i$10[ebp], 0
	jmp	SHORT $LN19@bipred_ave
$LN17@bipred_ave:
	mov	eax, DWORD PTR _i$10[ebp]
	add	eax, 12					; 0000000cH
	mov	DWORD PTR _i$10[ebp], eax
$LN19@bipred_ave:
	mov	eax, DWORD PTR _pu_w$[ebx]
	imul	eax, DWORD PTR _pu_h$[ebx]
	cmp	DWORD PTR _i$10[ebp], eax
	jae	$LN18@bipred_ave

; 885  : 
; 886  :           int y = i / pu_w;

	mov	eax, DWORD PTR _i$10[ebp]
	xor	edx, edx
	div	DWORD PTR _pu_w$[ebx]
	mov	DWORD PTR _y$9[ebp], eax

; 887  : 
; 888  :           __m128i mask      = _mm_setr_epi32(-1, -1, -1, 0);

	vmovdqa	xmm0, XMMWORD PTR __xmm@00000000ffffffffffffffffffffffff
	vmovdqa	XMMWORD PTR _mask$8[ebp], xmm0

; 889  :           __m128i sample_L0 = _mm_maskload_epi32((const int*)(&px_L0[i]), mask);

	vmovdqa	xmm0, XMMWORD PTR _mask$8[ebp]
	mov	eax, DWORD PTR _px_L0$[ebx]
	add	eax, DWORD PTR _i$10[ebp]
	vpmaskmovd xmm0, xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _sample_L0$7[ebp], xmm0

; 890  :           __m128i sample_L1 = _mm_maskload_epi32((const int*)(&px_L1[i]), mask);

	vmovdqa	xmm0, XMMWORD PTR _mask$8[ebp]
	mov	eax, DWORD PTR _px_L1$[ebx]
	add	eax, DWORD PTR _i$10[ebp]
	vpmaskmovd xmm0, xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _sample_L1$6[ebp], xmm0

; 891  :           __m128i avg       = _mm_avg_epu8(sample_L0, sample_L1);

	vmovdqa	xmm0, XMMWORD PTR _sample_L0$7[ebp]
	vpavgb	xmm0, xmm0, XMMWORD PTR _sample_L1$6[ebp]
	vmovdqa	XMMWORD PTR _avg$5[ebp], xmm0

; 892  : 
; 893  :           uint32_t elements_0123 = _mm_cvtsi128_si32(avg);

	vmovdqa	xmm0, XMMWORD PTR _avg$5[ebp]
	vmovd	eax, xmm0
	mov	DWORD PTR _elements_0123$4[ebp], eax

; 894  :           uint16_t elements_45   = _mm_extract_epi16(avg, 2);

	vmovdqa	xmm0, XMMWORD PTR _avg$5[ebp]
	vpextrw	eax, xmm0, 2
	mov	WORD PTR _elements_45$3[ebp], ax

; 895  :           uint16_t elements_67   = _mm_extract_epi16(avg, 3);

	vmovdqa	xmm0, XMMWORD PTR _avg$5[ebp]
	vpextrw	eax, xmm0, 3
	mov	WORD PTR _elements_67$2[ebp], ax

; 896  :           uint32_t elements_89ab = _mm_extract_epi32(avg, 2);

	vmovdqa	xmm0, XMMWORD PTR _avg$5[ebp]
	vpextrd	eax, xmm0, 2
	mov	DWORD PTR _elements_89ab$1[ebp], eax

; 897  :           *(uint32_t*)&dst[(y + 0) * dst_stride + 0] = elements_0123;

	mov	eax, DWORD PTR _y$9[ebp]
	imul	eax, DWORD PTR _dst_stride$[ebx]
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	edx, DWORD PTR _elements_0123$4[ebp]
	mov	DWORD PTR [ecx+eax], edx

; 898  :           *(uint16_t*)&dst[(y + 0) * dst_stride + 4] = elements_45;

	mov	eax, DWORD PTR _y$9[ebp]
	imul	eax, DWORD PTR _dst_stride$[ebx]
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	dx, WORD PTR _elements_45$3[ebp]
	mov	WORD PTR [ecx+eax+4], dx

; 899  :           *(uint16_t*)&dst[(y + 1) * dst_stride + 0] = elements_67;

	mov	eax, DWORD PTR _y$9[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _dst_stride$[ebx]
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	dx, WORD PTR _elements_67$2[ebp]
	mov	WORD PTR [ecx+eax], dx

; 900  :           *(uint32_t*)&dst[(y + 1) * dst_stride + 2] = elements_89ab;

	mov	eax, DWORD PTR _y$9[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _dst_stride$[ebx]
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	edx, DWORD PTR _elements_89ab$1[ebp]
	mov	DWORD PTR [ecx+eax+2], edx

; 901  :         }

	jmp	$LN17@bipred_ave
$LN18@bipred_ave:

; 902  :         break;

	jmp	SHORT $LN1@bipred_ave
$LN34@bipred_ave:

; 903  :       default:
; 904  :         assert(0 && "Unexpected block width.");

	xor	eax, eax
	jne	SHORT $LN1@bipred_ave
	mov	ecx, DWORD PTR ?__LINE__Var@?0??bipred_average_px_px_template_avx2@@9@9
	add	ecx, 85					; 00000055H
	mov	esi, esp
	push	ecx
	push	OFFSET ??_C@_1JG@HCGLAOEM@?$AAF?$AA?3?$AA?2?$AAo?$AAp?$AAe?$AAn?$AA_?$AAc?$AAo?$AAd?$AAe?$AAc?$AA_?$AAl@
	push	OFFSET ??_C@_1DO@PGNEGAJ@?$AA0?$AA?5?$AA?$CG?$AA?$CG?$AA?5?$AA?$CC?$AAU?$AAn?$AAe?$AAx?$AAp?$AAe?$AAc?$AAt?$AAe@
	call	DWORD PTR __imp___wassert
	add	esp, 12					; 0000000cH
	cmp	esi, esp
	call	__RTC_CheckEsp
$LN1@bipred_ave:

; 905  :         break;
; 906  :     }
; 907  :   }
; 908  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
	npad	3
$LN52@bipred_ave:
	DD	$LN22@bipred_ave
	DD	$LN23@bipred_ave
	DD	$LN24@bipred_ave
	DD	$LN25@bipred_ave
	DD	$LN26@bipred_ave
$LN51@bipred_ave:
	DB	0
	DB	4
	DB	4
	DB	4
	DB	1
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	2
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	3
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	4
	DB	3
_bipred_average_px_px_template_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _scatter_ymm_12x2_8bit
_TEXT	SEGMENT
_mask_b$ = -192						; size = 32
_mask_a$ = -128						; size = 32
_ymm$ = -64						; size = 32
_dst$ = 8						; size = 4
_dst_stride$ = 12					; size = 4
_scatter_ymm_12x2_8bit PROC				; COMDAT
; _ymm$ = ymm0

; 806  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 408				; 00000198H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-408]
	mov	ecx, 102				; 00000066H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	vmovups	YMMWORD PTR _ymm$[ebp], ymm0
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 807  :   __m256i mask_a = _mm256_setr_epi32(-1, -1, -1, 0, 0, 0, 0, 0);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000000000000000000000000000000000000ffffffffffffffffffffffff
	vmovdqu	YMMWORD PTR _mask_a$[ebp], ymm0

; 808  :   __m256i mask_b = _mm256_setr_epi32(0, 0, 0, -1, -1, -1, 0, 0);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffff000000000000000000000000
	vmovdqu	YMMWORD PTR _mask_b$[ebp], ymm0

; 809  :   _mm256_maskstore_epi32((int32_t*)dst, mask_a, ymm); dst += dst_stride - 3 * 4;

	vmovdqu	ymm0, YMMWORD PTR _ymm$[ebp]
	vmovdqu	ymm1, YMMWORD PTR _mask_a$[ebp]
	mov	eax, DWORD PTR _dst$[ebx]
	vpmaskmovd YMMWORD PTR [eax], ymm1, ymm0
	mov	eax, DWORD PTR _dst_stride$[ebx]
	mov	ecx, DWORD PTR _dst$[ebx]
	lea	edx, DWORD PTR [ecx+eax-12]
	mov	DWORD PTR _dst$[ebx], edx

; 810  :   _mm256_maskstore_epi32((int32_t*)dst, mask_b, ymm);

	vmovdqu	ymm0, YMMWORD PTR _ymm$[ebp]
	vmovdqu	ymm1, YMMWORD PTR _mask_b$[ebp]
	mov	eax, DWORD PTR _dst$[ebx]
	vpmaskmovd YMMWORD PTR [eax], ymm1, ymm0

; 811  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_scatter_ymm_12x2_8bit ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _scatter_ymm_16x2_8bit
_TEXT	SEGMENT
_ymm_hi$ = -128						; size = 16
_ymm_lo$ = -96						; size = 16
_ymm$ = -64						; size = 32
_dst$ = 8						; size = 4
_dst_stride$ = 12					; size = 4
_scatter_ymm_16x2_8bit PROC				; COMDAT
; _ymm$ = ymm0

; 798  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 344				; 00000158H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-344]
	mov	ecx, 86					; 00000056H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	vmovups	YMMWORD PTR _ymm$[ebp], ymm0
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 799  :   __m128i ymm_lo = _mm256_castsi256_si128(ymm);

	vmovdqu	xmm0, XMMWORD PTR _ymm$[ebp]
	vmovdqa	XMMWORD PTR _ymm_lo$[ebp], xmm0

; 800  :   __m128i ymm_hi = _mm256_extracti128_si256(ymm, 1);

	vmovdqu	ymm0, YMMWORD PTR _ymm$[ebp]
	vextracti128 xmm0, ymm0, 1
	vmovdqa	XMMWORD PTR _ymm_hi$[ebp], xmm0

; 801  :   _mm_storeu_si128((__m128i *)dst, ymm_lo); dst += dst_stride;

	vmovdqu	xmm0, XMMWORD PTR _ymm_lo$[ebp]
	mov	eax, DWORD PTR _dst$[ebx]
	vmovdqu	XMMWORD PTR [eax], xmm0
	mov	eax, DWORD PTR _dst$[ebx]
	add	eax, DWORD PTR _dst_stride$[ebx]
	mov	DWORD PTR _dst$[ebx], eax

; 802  :   _mm_storeu_si128((__m128i *)dst, ymm_hi);

	vmovdqu	xmm0, XMMWORD PTR _ymm_hi$[ebp]
	mov	eax, DWORD PTR _dst$[ebx]
	vmovdqu	XMMWORD PTR [eax], xmm0

; 803  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_scatter_ymm_16x2_8bit ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _scatter_ymm_8x4_8bit
_TEXT	SEGMENT
_ymm_hi$ = -192						; size = 16
_ymm_lo$ = -160						; size = 16
_ymm_as_m256d$ = -128					; size = 32
_ymm$ = -64						; size = 32
_dst$ = 8						; size = 4
_dst_stride$ = 12					; size = 4
_scatter_ymm_8x4_8bit PROC				; COMDAT
; _ymm$ = ymm0

; 787  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 408				; 00000198H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-408]
	mov	ecx, 102				; 00000066H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	vmovups	YMMWORD PTR _ymm$[ebp], ymm0
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 788  :   __m256d ymm_as_m256d = _mm256_castsi256_pd(ymm);

	vmovupd	ymm0, YMMWORD PTR _ymm$[ebp]
	vmovupd	YMMWORD PTR _ymm_as_m256d$[ebp], ymm0

; 789  :   __m128d ymm_lo = _mm256_castpd256_pd128(ymm_as_m256d);

	vmovupd	xmm0, XMMWORD PTR _ymm_as_m256d$[ebp]
	vmovaps	XMMWORD PTR _ymm_lo$[ebp], xmm0

; 790  :   __m128d ymm_hi = _mm256_extractf128_pd(ymm_as_m256d, 1);

	vmovupd	ymm0, YMMWORD PTR _ymm_as_m256d$[ebp]
	vextractf128 xmm0, ymm0, 1
	vmovaps	XMMWORD PTR _ymm_hi$[ebp], xmm0

; 791  :   _mm_storel_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovaps	xmm0, XMMWORD PTR _ymm_lo$[ebp]
	mov	eax, DWORD PTR _dst$[ebx]
	vmovlpd	QWORD PTR [eax], xmm0
	mov	eax, DWORD PTR _dst$[ebx]
	add	eax, DWORD PTR _dst_stride$[ebx]
	mov	DWORD PTR _dst$[ebx], eax

; 792  :   _mm_storeh_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovaps	xmm0, XMMWORD PTR _ymm_lo$[ebp]
	mov	eax, DWORD PTR _dst$[ebx]
	vmovhpd	QWORD PTR [eax], xmm0
	mov	eax, DWORD PTR _dst$[ebx]
	add	eax, DWORD PTR _dst_stride$[ebx]
	mov	DWORD PTR _dst$[ebx], eax

; 793  :   _mm_storel_pd((double*)dst, ymm_hi); dst += dst_stride;

	vmovaps	xmm0, XMMWORD PTR _ymm_hi$[ebp]
	mov	eax, DWORD PTR _dst$[ebx]
	vmovlpd	QWORD PTR [eax], xmm0
	mov	eax, DWORD PTR _dst$[ebx]
	add	eax, DWORD PTR _dst_stride$[ebx]
	mov	DWORD PTR _dst$[ebx], eax

; 794  :   _mm_storeh_pd((double*)dst, ymm_hi);

	vmovaps	xmm0, XMMWORD PTR _ymm_hi$[ebp]
	mov	eax, DWORD PTR _dst$[ebx]
	vmovhpd	QWORD PTR [eax], xmm0

; 795  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_scatter_ymm_8x4_8bit ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _scatter_ymm_4x8_8bit
_TEXT	SEGMENT
_ymm_hi$ = -128						; size = 16
_ymm_lo$ = -96						; size = 16
_ymm$ = -64						; size = 32
_dst$ = 8						; size = 4
_dst_stride$ = 12					; size = 4
_scatter_ymm_4x8_8bit PROC				; COMDAT
; _ymm$ = ymm0

; 773  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 344				; 00000158H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-344]
	mov	ecx, 86					; 00000056H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	vmovups	YMMWORD PTR _ymm$[ebp], ymm0
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 774  :   __m128i ymm_lo = _mm256_castsi256_si128(ymm);

	vmovdqu	xmm0, XMMWORD PTR _ymm$[ebp]
	vmovdqa	XMMWORD PTR _ymm_lo$[ebp], xmm0

; 775  :   __m128i ymm_hi = _mm256_extracti128_si256(ymm, 1);

	vmovdqu	ymm0, YMMWORD PTR _ymm$[ebp]
	vextracti128 xmm0, ymm0, 1
	vmovdqa	XMMWORD PTR _ymm_hi$[ebp], xmm0

; 776  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_lo); dst += dst_stride;

	vmovdqa	xmm0, XMMWORD PTR _ymm_lo$[ebp]
	vmovd	eax, xmm0
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	DWORD PTR [ecx], eax
	mov	eax, DWORD PTR _dst$[ebx]
	add	eax, DWORD PTR _dst_stride$[ebx]
	mov	DWORD PTR _dst$[ebx], eax

; 777  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 1); dst += dst_stride;

	vmovdqa	xmm0, XMMWORD PTR _ymm_lo$[ebp]
	vpextrd	eax, xmm0, 1
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	DWORD PTR [ecx], eax
	mov	eax, DWORD PTR _dst$[ebx]
	add	eax, DWORD PTR _dst_stride$[ebx]
	mov	DWORD PTR _dst$[ebx], eax

; 778  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 2); dst += dst_stride;

	vmovdqa	xmm0, XMMWORD PTR _ymm_lo$[ebp]
	vpextrd	eax, xmm0, 2
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	DWORD PTR [ecx], eax
	mov	eax, DWORD PTR _dst$[ebx]
	add	eax, DWORD PTR _dst_stride$[ebx]
	mov	DWORD PTR _dst$[ebx], eax

; 779  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 3); dst += dst_stride;

	vmovdqa	xmm0, XMMWORD PTR _ymm_lo$[ebp]
	vpextrd	eax, xmm0, 3
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	DWORD PTR [ecx], eax
	mov	eax, DWORD PTR _dst$[ebx]
	add	eax, DWORD PTR _dst_stride$[ebx]
	mov	DWORD PTR _dst$[ebx], eax

; 780  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_hi); dst += dst_stride;

	vmovdqa	xmm0, XMMWORD PTR _ymm_hi$[ebp]
	vmovd	eax, xmm0
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	DWORD PTR [ecx], eax
	mov	eax, DWORD PTR _dst$[ebx]
	add	eax, DWORD PTR _dst_stride$[ebx]
	mov	DWORD PTR _dst$[ebx], eax

; 781  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 1); dst += dst_stride;

	vmovdqa	xmm0, XMMWORD PTR _ymm_hi$[ebp]
	vpextrd	eax, xmm0, 1
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	DWORD PTR [ecx], eax
	mov	eax, DWORD PTR _dst$[ebx]
	add	eax, DWORD PTR _dst_stride$[ebx]
	mov	DWORD PTR _dst$[ebx], eax

; 782  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 2); dst += dst_stride;

	vmovdqa	xmm0, XMMWORD PTR _ymm_hi$[ebp]
	vpextrd	eax, xmm0, 2
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	DWORD PTR [ecx], eax
	mov	eax, DWORD PTR _dst$[ebx]
	add	eax, DWORD PTR _dst_stride$[ebx]
	mov	DWORD PTR _dst$[ebx], eax

; 783  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 3);

	vmovdqa	xmm0, XMMWORD PTR _ymm_hi$[ebp]
	vpextrd	eax, xmm0, 3
	mov	ecx, DWORD PTR _dst$[ebx]
	mov	DWORD PTR [ecx], eax

; 784  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_scatter_ymm_4x8_8bit ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _pixels_calc_ssd_avx2
_TEXT	SEGMENT
tv65 = -984						; size = 4
_i$1 = -592						; size = 4
_x$2 = -580						; size = 4
_y$3 = -568						; size = 4
_ssd$ = -556						; size = 4
_rec_row3$ = -544					; size = 16
_rec_row2$ = -512					; size = 16
_rec_row1$ = -480					; size = 16
_rec_row0$ = -448					; size = 16
_ref_row3$ = -416					; size = 16
_ref_row2$ = -384					; size = 16
_ref_row1$ = -352					; size = 16
_ref_row0$ = -320					; size = 16
_rec_epi16$ = -288					; size = 32
_ref_epi16$ = -224					; size = 32
_sum$ = -160						; size = 16
_diff$ = -128						; size = 32
_ssd_part$ = -64					; size = 32
_ref$ = 8						; size = 4
_rec$ = 12						; size = 4
_ref_stride$ = 16					; size = 4
_rec_stride$ = 20					; size = 4
_width$ = 24						; size = 4
_pixels_calc_ssd_avx2 PROC				; COMDAT

; 700  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 984				; 000003d8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-984]
	mov	ecx, 246				; 000000f6H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 701  :   __m256i ssd_part;
; 702  :   __m256i diff = _mm256_setzero_si256();

	vpxor	xmm0, xmm0, xmm0
	vmovdqu	YMMWORD PTR _diff$[ebp], ymm0

; 703  :   __m128i sum;
; 704  : 
; 705  :   __m256i ref_epi16;
; 706  :   __m256i rec_epi16;
; 707  : 
; 708  :   __m128i ref_row0, ref_row1, ref_row2, ref_row3;
; 709  :   __m128i rec_row0, rec_row1, rec_row2, rec_row3;
; 710  : 
; 711  :   int ssd;
; 712  : 
; 713  :   switch (width) {

	mov	eax, DWORD PTR _width$[ebx]
	mov	DWORD PTR tv65[ebp], eax
	cmp	DWORD PTR tv65[ebp], 4
	je	SHORT $LN13@pixels_cal
	jmp	$LN14@pixels_cal
$LN13@pixels_cal:

; 714  : 
; 715  :   case 4:
; 716  : 
; 717  :     ref_row0 = _mm_cvtsi32_si128(*(int32_t*)&(ref[0 * ref_stride]));

	imul	eax, DWORD PTR _ref_stride$[ebx], 0
	mov	ecx, DWORD PTR _ref$[ebx]
	vmovd	xmm0, DWORD PTR [ecx+eax]
	vmovdqa	XMMWORD PTR _ref_row0$[ebp], xmm0

; 718  :     ref_row1 = _mm_cvtsi32_si128(*(int32_t*)&(ref[1 * ref_stride]));

	mov	eax, DWORD PTR _ref$[ebx]
	add	eax, DWORD PTR _ref_stride$[ebx]
	vmovd	xmm0, DWORD PTR [eax]
	vmovdqa	XMMWORD PTR _ref_row1$[ebp], xmm0

; 719  :     ref_row2 = _mm_cvtsi32_si128(*(int32_t*)&(ref[2 * ref_stride]));

	mov	eax, DWORD PTR _ref_stride$[ebx]
	mov	ecx, DWORD PTR _ref$[ebx]
	vmovd	xmm0, DWORD PTR [ecx+eax*2]
	vmovdqa	XMMWORD PTR _ref_row2$[ebp], xmm0

; 720  :     ref_row3 = _mm_cvtsi32_si128(*(int32_t*)&(ref[3 * ref_stride]));

	imul	eax, DWORD PTR _ref_stride$[ebx], 3
	mov	ecx, DWORD PTR _ref$[ebx]
	vmovd	xmm0, DWORD PTR [ecx+eax]
	vmovdqa	XMMWORD PTR _ref_row3$[ebp], xmm0

; 721  : 
; 722  :     ref_row0 = _mm_unpacklo_epi32(ref_row0, ref_row1);

	vmovdqa	xmm0, XMMWORD PTR _ref_row0$[ebp]
	vpunpckldq xmm0, xmm0, XMMWORD PTR _ref_row1$[ebp]
	vmovdqa	XMMWORD PTR _ref_row0$[ebp], xmm0

; 723  :     ref_row1 = _mm_unpacklo_epi32(ref_row2, ref_row3);

	vmovdqa	xmm0, XMMWORD PTR _ref_row2$[ebp]
	vpunpckldq xmm0, xmm0, XMMWORD PTR _ref_row3$[ebp]
	vmovdqa	XMMWORD PTR _ref_row1$[ebp], xmm0

; 724  :     ref_epi16 = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(ref_row0, ref_row1) );

	vmovdqa	xmm0, XMMWORD PTR _ref_row0$[ebp]
	vpunpcklqdq xmm0, xmm0, XMMWORD PTR _ref_row1$[ebp]
	vpmovzxbw ymm0, xmm0
	vmovdqu	YMMWORD PTR _ref_epi16$[ebp], ymm0

; 725  : 
; 726  :     rec_row0 = _mm_cvtsi32_si128(*(int32_t*)&(rec[0 * rec_stride]));

	imul	eax, DWORD PTR _rec_stride$[ebx], 0
	mov	ecx, DWORD PTR _rec$[ebx]
	vmovd	xmm0, DWORD PTR [ecx+eax]
	vmovdqa	XMMWORD PTR _rec_row0$[ebp], xmm0

; 727  :     rec_row1 = _mm_cvtsi32_si128(*(int32_t*)&(rec[1 * rec_stride]));

	mov	eax, DWORD PTR _rec$[ebx]
	add	eax, DWORD PTR _rec_stride$[ebx]
	vmovd	xmm0, DWORD PTR [eax]
	vmovdqa	XMMWORD PTR _rec_row1$[ebp], xmm0

; 728  :     rec_row2 = _mm_cvtsi32_si128(*(int32_t*)&(rec[2 * rec_stride]));

	mov	eax, DWORD PTR _rec_stride$[ebx]
	mov	ecx, DWORD PTR _rec$[ebx]
	vmovd	xmm0, DWORD PTR [ecx+eax*2]
	vmovdqa	XMMWORD PTR _rec_row2$[ebp], xmm0

; 729  :     rec_row3 = _mm_cvtsi32_si128(*(int32_t*)&(rec[3 * rec_stride]));

	imul	eax, DWORD PTR _rec_stride$[ebx], 3
	mov	ecx, DWORD PTR _rec$[ebx]
	vmovd	xmm0, DWORD PTR [ecx+eax]
	vmovdqa	XMMWORD PTR _rec_row3$[ebp], xmm0

; 730  : 
; 731  :     rec_row0 = _mm_unpacklo_epi32(rec_row0, rec_row1);

	vmovdqa	xmm0, XMMWORD PTR _rec_row0$[ebp]
	vpunpckldq xmm0, xmm0, XMMWORD PTR _rec_row1$[ebp]
	vmovdqa	XMMWORD PTR _rec_row0$[ebp], xmm0

; 732  :     rec_row1 = _mm_unpacklo_epi32(rec_row2, rec_row3);

	vmovdqa	xmm0, XMMWORD PTR _rec_row2$[ebp]
	vpunpckldq xmm0, xmm0, XMMWORD PTR _rec_row3$[ebp]
	vmovdqa	XMMWORD PTR _rec_row1$[ebp], xmm0

; 733  :     rec_epi16 = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(rec_row0, rec_row1) );

	vmovdqa	xmm0, XMMWORD PTR _rec_row0$[ebp]
	vpunpcklqdq xmm0, xmm0, XMMWORD PTR _rec_row1$[ebp]
	vpmovzxbw ymm0, xmm0
	vmovdqu	YMMWORD PTR _rec_epi16$[ebp], ymm0

; 734  : 
; 735  :     diff = _mm256_sub_epi16(ref_epi16, rec_epi16);

	vmovdqu	ymm0, YMMWORD PTR _ref_epi16$[ebp]
	vpsubw	ymm0, ymm0, YMMWORD PTR _rec_epi16$[ebp]
	vmovdqu	YMMWORD PTR _diff$[ebp], ymm0

; 736  :     ssd_part =  _mm256_madd_epi16(diff, diff);

	vmovdqu	ymm0, YMMWORD PTR _diff$[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _diff$[ebp]
	vmovdqu	YMMWORD PTR _ssd_part$[ebp], ymm0

; 737  : 
; 738  :     sum = _mm_add_epi32(_mm256_castsi256_si128(ssd_part), _mm256_extracti128_si256(ssd_part, 1));

	vmovdqu	ymm0, YMMWORD PTR _ssd_part$[ebp]
	vextracti128 xmm0, ymm0, 1
	vmovdqa	xmm1, XMMWORD PTR _ssd_part$[ebp]
	vpaddd	xmm0, xmm1, xmm0
	vmovdqa	XMMWORD PTR _sum$[ebp], xmm0

; 739  :     sum = _mm_add_epi32(sum, _mm_shuffle_epi32(sum, _MM_SHUFFLE(1, 0, 3, 2)));

	vpshufd	xmm0, XMMWORD PTR _sum$[ebp], 78	; 0000004eH
	vmovdqa	xmm1, XMMWORD PTR _sum$[ebp]
	vpaddd	xmm0, xmm1, xmm0
	vmovdqa	XMMWORD PTR _sum$[ebp], xmm0

; 740  :     sum = _mm_add_epi32(sum, _mm_shuffle_epi32(sum, _MM_SHUFFLE(0, 1, 0, 1)));

	vpshufd	xmm0, XMMWORD PTR _sum$[ebp], 17	; 00000011H
	vmovdqa	xmm1, XMMWORD PTR _sum$[ebp]
	vpaddd	xmm0, xmm1, xmm0
	vmovdqa	XMMWORD PTR _sum$[ebp], xmm0

; 741  : 
; 742  :     ssd = _mm_cvtsi128_si32(sum);

	vmovdqa	xmm0, XMMWORD PTR _sum$[ebp]
	vmovd	eax, xmm0
	mov	DWORD PTR _ssd$[ebp], eax

; 743  : 
; 744  :     return ssd >> (2*(KVZ_BIT_DEPTH-8));

	mov	eax, DWORD PTR _ssd$[ebp]
	jmp	$LN1@pixels_cal

; 745  :     break;

	jmp	$LN1@pixels_cal
$LN14@pixels_cal:

; 746  : 
; 747  :   default:
; 748  : 
; 749  :     ssd_part = _mm256_setzero_si256();

	vpxor	xmm0, xmm0, xmm0
	vmovdqu	YMMWORD PTR _ssd_part$[ebp], ymm0

; 750  :     for (int y = 0; y < width; y += 8) {

	mov	DWORD PTR _y$3[ebp], 0
	jmp	SHORT $LN6@pixels_cal
$LN4@pixels_cal:
	mov	eax, DWORD PTR _y$3[ebp]
	add	eax, 8
	mov	DWORD PTR _y$3[ebp], eax
$LN6@pixels_cal:
	mov	eax, DWORD PTR _y$3[ebp]
	cmp	eax, DWORD PTR _width$[ebx]
	jge	$LN5@pixels_cal

; 751  :       for (int x = 0; x < width; x += 8) {

	mov	DWORD PTR _x$2[ebp], 0
	jmp	SHORT $LN9@pixels_cal
$LN7@pixels_cal:
	mov	eax, DWORD PTR _x$2[ebp]
	add	eax, 8
	mov	DWORD PTR _x$2[ebp], eax
$LN9@pixels_cal:
	mov	eax, DWORD PTR _x$2[ebp]
	cmp	eax, DWORD PTR _width$[ebx]
	jge	$LN8@pixels_cal

; 752  :         for (int i = 0; i < 8; i += 2) {

	mov	DWORD PTR _i$1[ebp], 0
	jmp	SHORT $LN12@pixels_cal
$LN10@pixels_cal:
	mov	eax, DWORD PTR _i$1[ebp]
	add	eax, 2
	mov	DWORD PTR _i$1[ebp], eax
$LN12@pixels_cal:
	cmp	DWORD PTR _i$1[ebp], 8
	jge	$LN11@pixels_cal

; 753  :           ref_epi16 = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(_mm_loadl_epi64((__m128i*)&(ref[x + (y + i) * ref_stride])), _mm_loadl_epi64((__m128i*)&(ref[x + (y + i + 1) * ref_stride]))));

	mov	eax, DWORD PTR _i$1[ebp]
	mov	ecx, DWORD PTR _y$3[ebp]
	lea	edx, DWORD PTR [ecx+eax+1]
	imul	edx, DWORD PTR _ref_stride$[ebx]
	add	edx, DWORD PTR _x$2[ebp]
	mov	eax, DWORD PTR _ref$[ebx]
	vmovq	xmm0, QWORD PTR [eax+edx]
	mov	ecx, DWORD PTR _y$3[ebp]
	add	ecx, DWORD PTR _i$1[ebp]
	imul	ecx, DWORD PTR _ref_stride$[ebx]
	add	ecx, DWORD PTR _x$2[ebp]
	mov	edx, DWORD PTR _ref$[ebx]
	vmovq	xmm1, QWORD PTR [edx+ecx]
	vpunpcklqdq xmm0, xmm1, xmm0
	vpmovzxbw ymm0, xmm0
	vmovdqu	YMMWORD PTR _ref_epi16$[ebp], ymm0

; 754  :           rec_epi16 = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(_mm_loadl_epi64((__m128i*)&(rec[x + (y + i) * rec_stride])), _mm_loadl_epi64((__m128i*)&(rec[x + (y + i + 1) * rec_stride]))));

	mov	eax, DWORD PTR _i$1[ebp]
	mov	ecx, DWORD PTR _y$3[ebp]
	lea	edx, DWORD PTR [ecx+eax+1]
	imul	edx, DWORD PTR _rec_stride$[ebx]
	add	edx, DWORD PTR _x$2[ebp]
	mov	eax, DWORD PTR _rec$[ebx]
	vmovq	xmm0, QWORD PTR [eax+edx]
	mov	ecx, DWORD PTR _y$3[ebp]
	add	ecx, DWORD PTR _i$1[ebp]
	imul	ecx, DWORD PTR _rec_stride$[ebx]
	add	ecx, DWORD PTR _x$2[ebp]
	mov	edx, DWORD PTR _rec$[ebx]
	vmovq	xmm1, QWORD PTR [edx+ecx]
	vpunpcklqdq xmm0, xmm1, xmm0
	vpmovzxbw ymm0, xmm0
	vmovdqu	YMMWORD PTR _rec_epi16$[ebp], ymm0

; 755  :           diff = _mm256_sub_epi16(ref_epi16, rec_epi16);

	vmovdqu	ymm0, YMMWORD PTR _ref_epi16$[ebp]
	vpsubw	ymm0, ymm0, YMMWORD PTR _rec_epi16$[ebp]
	vmovdqu	YMMWORD PTR _diff$[ebp], ymm0

; 756  :           ssd_part = _mm256_add_epi32(ssd_part, _mm256_madd_epi16(diff, diff));

	vmovdqu	ymm0, YMMWORD PTR _diff$[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR _diff$[ebp]
	vmovdqu	ymm1, YMMWORD PTR _ssd_part$[ebp]
	vpaddd	ymm0, ymm1, ymm0
	vmovdqu	YMMWORD PTR _ssd_part$[ebp], ymm0

; 757  :         }

	jmp	$LN10@pixels_cal
$LN11@pixels_cal:

; 758  :       }

	jmp	$LN7@pixels_cal
$LN8@pixels_cal:

; 759  :     }

	jmp	$LN4@pixels_cal
$LN5@pixels_cal:

; 760  : 
; 761  :     sum = _mm_add_epi32(_mm256_castsi256_si128(ssd_part), _mm256_extracti128_si256(ssd_part, 1));

	vmovdqu	ymm0, YMMWORD PTR _ssd_part$[ebp]
	vextracti128 xmm0, ymm0, 1
	vmovdqa	xmm1, XMMWORD PTR _ssd_part$[ebp]
	vpaddd	xmm0, xmm1, xmm0
	vmovdqa	XMMWORD PTR _sum$[ebp], xmm0

; 762  :     sum = _mm_add_epi32(sum, _mm_shuffle_epi32(sum, _MM_SHUFFLE(1, 0, 3, 2)));

	vpshufd	xmm0, XMMWORD PTR _sum$[ebp], 78	; 0000004eH
	vmovdqa	xmm1, XMMWORD PTR _sum$[ebp]
	vpaddd	xmm0, xmm1, xmm0
	vmovdqa	XMMWORD PTR _sum$[ebp], xmm0

; 763  :     sum = _mm_add_epi32(sum, _mm_shuffle_epi32(sum, _MM_SHUFFLE(0, 1, 0, 1)));

	vpshufd	xmm0, XMMWORD PTR _sum$[ebp], 17	; 00000011H
	vmovdqa	xmm1, XMMWORD PTR _sum$[ebp]
	vpaddd	xmm0, xmm1, xmm0
	vmovdqa	XMMWORD PTR _sum$[ebp], xmm0

; 764  : 
; 765  :     ssd = _mm_cvtsi128_si32(sum);

	vmovdqa	xmm0, XMMWORD PTR _sum$[ebp]
	vmovd	eax, xmm0
	mov	DWORD PTR _ssd$[ebp], eax

; 766  : 
; 767  :     return ssd >> (2*(KVZ_BIT_DEPTH-8));

	mov	eax, DWORD PTR _ssd$[ebp]
$LN1@pixels_cal:

; 768  :     break;
; 769  :   }
; 770  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_pixels_calc_ssd_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _satd_any_size_quad_avx2
_TEXT	SEGMENT
_i$1 = -144						; size = 4
_x$2 = -132						; size = 4
_y$3 = -120						; size = 4
_blk$4 = -108						; size = 4
_x$5 = -96						; size = 4
_blk$6 = -84						; size = 4
_y$7 = -72						; size = 4
_orig_ptr$ = -60					; size = 4
_pred_ptrs$ = -48					; size = 16
_sums$ = -24						; size = 16
__$ArrayPad$ = -4					; size = 4
_width$ = 8						; size = 4
_height$ = 12						; size = 4
_preds$ = 16						; size = 4
_stride$ = 20						; size = 4
_orig$ = 24						; size = 4
_orig_stride$ = 28					; size = 4
_num_modes$ = 32					; size = 4
_costs_out$ = 36					; size = 4
_valid$ = 40						; size = 4
_satd_any_size_quad_avx2 PROC				; COMDAT

; 694  : SATD_ANY_SIZE_MULTI_AVX2(quad_avx2, 4)

	push	ebp
	mov	ebp, esp
	sub	esp, 340				; 00000154H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-340]
	mov	ecx, 85					; 00000055H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	eax, DWORD PTR ___security_cookie
	xor	eax, ebp
	mov	DWORD PTR __$ArrayPad$[ebp], eax
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4
	xor	eax, eax
	mov	DWORD PTR _sums$[ebp], eax
	mov	DWORD PTR _sums$[ebp+4], eax
	mov	DWORD PTR _sums$[ebp+8], eax
	mov	DWORD PTR _sums$[ebp+12], eax
	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, DWORD PTR _preds$[ebp]
	mov	eax, DWORD PTR [edx+ecx]
	mov	DWORD PTR _pred_ptrs$[ebp], eax
	mov	eax, 4
	shl	eax, 0
	mov	ecx, DWORD PTR _preds$[ebp]
	mov	edx, DWORD PTR [ecx+eax]
	mov	DWORD PTR _pred_ptrs$[ebp+4], edx
	mov	eax, 4
	shl	eax, 1
	mov	ecx, DWORD PTR _preds$[ebp]
	mov	edx, DWORD PTR [ecx+eax]
	mov	DWORD PTR _pred_ptrs$[ebp+8], edx
	mov	eax, 4
	imul	ecx, eax, 3
	mov	edx, DWORD PTR _preds$[ebp]
	mov	eax, DWORD PTR [edx+ecx]
	mov	DWORD PTR _pred_ptrs$[ebp+12], eax
	mov	eax, DWORD PTR _orig$[ebp]
	mov	DWORD PTR _orig_ptr$[ebp], eax
	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, DWORD PTR _costs_out$[ebp]
	mov	DWORD PTR [edx+ecx], 0
	mov	eax, 4
	shl	eax, 0
	mov	ecx, DWORD PTR _costs_out$[ebp]
	mov	DWORD PTR [ecx+eax], 0
	mov	eax, 4
	shl	eax, 1
	mov	ecx, DWORD PTR _costs_out$[ebp]
	mov	DWORD PTR [ecx+eax], 0
	mov	eax, 4
	imul	ecx, eax, 3
	mov	edx, DWORD PTR _costs_out$[ebp]
	mov	DWORD PTR [edx+ecx], 0
	mov	eax, DWORD PTR _width$[ebp]
	and	eax, -2147483641			; 80000007H
	jns	SHORT $LN30@satd_any_s
	dec	eax
	or	eax, -8					; fffffff8H
	inc	eax
$LN30@satd_any_s:
	test	eax, eax
	je	SHORT $LN23@satd_any_s
	mov	DWORD PTR _y$7[ebp], 0
	jmp	SHORT $LN4@satd_any_s
$LN2@satd_any_s:
	mov	eax, DWORD PTR _y$7[ebp]
	add	eax, 4
	mov	DWORD PTR _y$7[ebp], eax
$LN4@satd_any_s:
	mov	eax, DWORD PTR _y$7[ebp]
	cmp	eax, DWORD PTR _height$[ebp]
	jge	SHORT $LN3@satd_any_s
	lea	eax, DWORD PTR _sums$[ebp]
	push	eax
	mov	ecx, DWORD PTR _orig_stride$[ebp]
	push	ecx
	mov	edx, DWORD PTR _orig$[ebp]
	push	edx
	mov	eax, DWORD PTR _stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _preds$[ebp]
	push	ecx
	call	_kvz_satd_4x4_subblock_quad_avx2
	add	esp, 20					; 00000014H
	jmp	SHORT $LN2@satd_any_s
$LN3@satd_any_s:
	mov	eax, DWORD PTR _orig_ptr$[ebp]
	add	eax, 4
	mov	DWORD PTR _orig_ptr$[ebp], eax
	mov	DWORD PTR _blk$6[ebp], 0
	jmp	SHORT $LN7@satd_any_s
$LN5@satd_any_s:
	mov	eax, DWORD PTR _blk$6[ebp]
	add	eax, 1
	mov	DWORD PTR _blk$6[ebp], eax
$LN7@satd_any_s:
	cmp	DWORD PTR _blk$6[ebp], 4
	jge	SHORT $LN6@satd_any_s
	mov	eax, DWORD PTR _blk$6[ebp]
	mov	ecx, DWORD PTR _pred_ptrs$[ebp+eax*4]
	add	ecx, 4
	mov	edx, DWORD PTR _blk$6[ebp]
	mov	DWORD PTR _pred_ptrs$[ebp+edx*4], ecx
	jmp	SHORT $LN5@satd_any_s
$LN6@satd_any_s:
	mov	eax, DWORD PTR _width$[ebp]
	sub	eax, 4
	mov	DWORD PTR _width$[ebp], eax
$LN23@satd_any_s:
	mov	eax, DWORD PTR _height$[ebp]
	and	eax, -2147483641			; 80000007H
	jns	SHORT $LN31@satd_any_s
	dec	eax
	or	eax, -8					; fffffff8H
	inc	eax
$LN31@satd_any_s:
	test	eax, eax
	je	SHORT $LN24@satd_any_s
	mov	DWORD PTR _x$5[ebp], 0
	jmp	SHORT $LN10@satd_any_s
$LN8@satd_any_s:
	mov	eax, DWORD PTR _x$5[ebp]
	add	eax, 4
	mov	DWORD PTR _x$5[ebp], eax
$LN10@satd_any_s:
	mov	eax, DWORD PTR _x$5[ebp]
	cmp	eax, DWORD PTR _width$[ebp]
	jge	SHORT $LN9@satd_any_s
	lea	eax, DWORD PTR _sums$[ebp]
	push	eax
	mov	ecx, DWORD PTR _orig_stride$[ebp]
	push	ecx
	mov	edx, DWORD PTR _orig_ptr$[ebp]
	push	edx
	mov	eax, DWORD PTR _stride$[ebp]
	push	eax
	lea	ecx, DWORD PTR _pred_ptrs$[ebp]
	push	ecx
	call	_kvz_satd_4x4_subblock_quad_avx2
	add	esp, 20					; 00000014H
	jmp	SHORT $LN8@satd_any_s
$LN9@satd_any_s:
	mov	eax, DWORD PTR _orig_stride$[ebp]
	mov	ecx, DWORD PTR _orig_ptr$[ebp]
	lea	edx, DWORD PTR [ecx+eax*4]
	mov	DWORD PTR _orig_ptr$[ebp], edx
	mov	DWORD PTR _blk$4[ebp], 0
	jmp	SHORT $LN13@satd_any_s
$LN11@satd_any_s:
	mov	eax, DWORD PTR _blk$4[ebp]
	add	eax, 1
	mov	DWORD PTR _blk$4[ebp], eax
$LN13@satd_any_s:
	cmp	DWORD PTR _blk$4[ebp], 4
	jge	SHORT $LN12@satd_any_s
	mov	eax, DWORD PTR _blk$4[ebp]
	mov	ecx, DWORD PTR _pred_ptrs$[ebp+eax*4]
	mov	edx, DWORD PTR _stride$[ebp]
	lea	eax, DWORD PTR [ecx+edx*4]
	mov	ecx, DWORD PTR _blk$4[ebp]
	mov	DWORD PTR _pred_ptrs$[ebp+ecx*4], eax
	jmp	SHORT $LN11@satd_any_s
$LN12@satd_any_s:
	mov	eax, DWORD PTR _height$[ebp]
	sub	eax, 4
	mov	DWORD PTR _height$[ebp], eax
$LN24@satd_any_s:
	mov	DWORD PTR _y$3[ebp], 0
	jmp	SHORT $LN16@satd_any_s
$LN14@satd_any_s:
	mov	eax, DWORD PTR _y$3[ebp]
	add	eax, 8
	mov	DWORD PTR _y$3[ebp], eax
$LN16@satd_any_s:
	mov	eax, DWORD PTR _y$3[ebp]
	cmp	eax, DWORD PTR _height$[ebp]
	jge	$LN15@satd_any_s
	mov	eax, DWORD PTR _y$3[ebp]
	imul	eax, DWORD PTR _orig_stride$[ebp]
	add	eax, DWORD PTR _orig$[ebp]
	mov	DWORD PTR _orig_ptr$[ebp], eax
	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, DWORD PTR _y$3[ebp]
	imul	edx, DWORD PTR _stride$[ebp]
	mov	eax, DWORD PTR _preds$[ebp]
	mov	ecx, DWORD PTR [eax+ecx]
	add	ecx, edx
	mov	edx, 4
	imul	eax, edx, 0
	mov	DWORD PTR _pred_ptrs$[ebp+eax], ecx
	mov	eax, 4
	shl	eax, 0
	mov	ecx, DWORD PTR _y$3[ebp]
	imul	ecx, DWORD PTR _stride$[ebp]
	mov	edx, DWORD PTR _preds$[ebp]
	mov	eax, DWORD PTR [edx+eax]
	add	eax, ecx
	mov	ecx, 4
	shl	ecx, 0
	mov	DWORD PTR _pred_ptrs$[ebp+ecx], eax
	mov	eax, 4
	shl	eax, 1
	mov	ecx, DWORD PTR _y$3[ebp]
	imul	ecx, DWORD PTR _stride$[ebp]
	mov	edx, DWORD PTR _preds$[ebp]
	mov	eax, DWORD PTR [edx+eax]
	add	eax, ecx
	mov	ecx, 4
	shl	ecx, 1
	mov	DWORD PTR _pred_ptrs$[ebp+ecx], eax
	mov	eax, 4
	imul	ecx, eax, 3
	mov	edx, DWORD PTR _y$3[ebp]
	imul	edx, DWORD PTR _stride$[ebp]
	mov	eax, DWORD PTR _preds$[ebp]
	mov	ecx, DWORD PTR [eax+ecx]
	add	ecx, edx
	mov	edx, 4
	imul	eax, edx, 3
	mov	DWORD PTR _pred_ptrs$[ebp+eax], ecx
	mov	DWORD PTR _x$2[ebp], 0
	jmp	SHORT $LN19@satd_any_s
$LN17@satd_any_s:
	mov	eax, DWORD PTR _x$2[ebp]
	add	eax, 8
	mov	DWORD PTR _x$2[ebp], eax
$LN19@satd_any_s:
	mov	eax, DWORD PTR _x$2[ebp]
	cmp	eax, DWORD PTR _width$[ebp]
	jge	$LN18@satd_any_s
	lea	eax, DWORD PTR _sums$[ebp]
	push	eax
	mov	ecx, DWORD PTR _orig_stride$[ebp]
	push	ecx
	mov	edx, DWORD PTR _orig_ptr$[ebp]
	push	edx
	mov	eax, DWORD PTR _stride$[ebp]
	push	eax
	lea	ecx, DWORD PTR _pred_ptrs$[ebp]
	push	ecx
	call	_satd_8x8_subblock_quad_avx2
	add	esp, 20					; 00000014H
	mov	eax, DWORD PTR _orig_ptr$[ebp]
	add	eax, 8
	mov	DWORD PTR _orig_ptr$[ebp], eax
	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, DWORD PTR _pred_ptrs$[ebp+ecx]
	add	edx, 8
	mov	eax, 4
	imul	ecx, eax, 0
	mov	DWORD PTR _pred_ptrs$[ebp+ecx], edx
	mov	eax, 4
	shl	eax, 0
	mov	ecx, DWORD PTR _pred_ptrs$[ebp+eax]
	add	ecx, 8
	mov	edx, 4
	shl	edx, 0
	mov	DWORD PTR _pred_ptrs$[ebp+edx], ecx
	mov	eax, 4
	shl	eax, 1
	mov	ecx, DWORD PTR _pred_ptrs$[ebp+eax]
	add	ecx, 8
	mov	edx, 4
	shl	edx, 1
	mov	DWORD PTR _pred_ptrs$[ebp+edx], ecx
	mov	eax, 4
	imul	ecx, eax, 3
	mov	edx, DWORD PTR _pred_ptrs$[ebp+ecx]
	add	edx, 8
	mov	eax, 4
	imul	ecx, eax, 3
	mov	DWORD PTR _pred_ptrs$[ebp+ecx], edx
	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, 4
	imul	eax, edx, 0
	mov	edx, DWORD PTR _costs_out$[ebp]
	mov	ecx, DWORD PTR [edx+ecx]
	add	ecx, DWORD PTR _sums$[ebp+eax]
	mov	edx, 4
	imul	eax, edx, 0
	mov	edx, DWORD PTR _costs_out$[ebp]
	mov	DWORD PTR [edx+eax], ecx
	mov	eax, 4
	shl	eax, 0
	mov	ecx, 4
	shl	ecx, 0
	mov	edx, DWORD PTR _costs_out$[ebp]
	mov	eax, DWORD PTR [edx+eax]
	add	eax, DWORD PTR _sums$[ebp+ecx]
	mov	ecx, 4
	shl	ecx, 0
	mov	edx, DWORD PTR _costs_out$[ebp]
	mov	DWORD PTR [edx+ecx], eax
	mov	eax, 4
	shl	eax, 1
	mov	ecx, 4
	shl	ecx, 1
	mov	edx, DWORD PTR _costs_out$[ebp]
	mov	eax, DWORD PTR [edx+eax]
	add	eax, DWORD PTR _sums$[ebp+ecx]
	mov	ecx, 4
	shl	ecx, 1
	mov	edx, DWORD PTR _costs_out$[ebp]
	mov	DWORD PTR [edx+ecx], eax
	mov	eax, 4
	imul	ecx, eax, 3
	mov	edx, 4
	imul	eax, edx, 3
	mov	edx, DWORD PTR _costs_out$[ebp]
	mov	ecx, DWORD PTR [edx+ecx]
	add	ecx, DWORD PTR _sums$[ebp+eax]
	mov	edx, 4
	imul	eax, edx, 3
	mov	edx, DWORD PTR _costs_out$[ebp]
	mov	DWORD PTR [edx+eax], ecx
	jmp	$LN17@satd_any_s
$LN18@satd_any_s:
	jmp	$LN14@satd_any_s
$LN15@satd_any_s:
	mov	DWORD PTR _i$1[ebp], 0
	jmp	SHORT $LN22@satd_any_s
$LN20@satd_any_s:
	mov	eax, DWORD PTR _i$1[ebp]
	add	eax, 1
	mov	DWORD PTR _i$1[ebp], eax
$LN22@satd_any_s:
	cmp	DWORD PTR _i$1[ebp], 4
	jge	SHORT $LN1@satd_any_s
	mov	eax, DWORD PTR _i$1[ebp]
	mov	ecx, DWORD PTR _costs_out$[ebp]
	mov	edx, DWORD PTR _i$1[ebp]
	mov	esi, DWORD PTR _costs_out$[ebp]
	mov	edx, DWORD PTR [esi+edx*4]
	mov	DWORD PTR [ecx+eax*4], edx
	jmp	SHORT $LN20@satd_any_s
$LN1@satd_any_s:
	push	edx
	mov	ecx, ebp
	push	eax
	lea	edx, DWORD PTR $LN29@satd_any_s
	call	@_RTC_CheckStackVars@8
	pop	eax
	pop	edx
	pop	edi
	pop	esi
	pop	ebx
	mov	ecx, DWORD PTR __$ArrayPad$[ebp]
	xor	ecx, ebp
	call	@__security_check_cookie@4
	add	esp, 340				; 00000154H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
$LN29@satd_any_s:
	DD	2
	DD	$LN28@satd_any_s
$LN28@satd_any_s:
	DD	-24					; ffffffe8H
	DD	16					; 00000010H
	DD	$LN26@satd_any_s
	DD	-48					; ffffffd0H
	DD	16					; 00000010H
	DD	$LN27@satd_any_s
$LN27@satd_any_s:
	DB	112					; 00000070H
	DB	114					; 00000072H
	DB	101					; 00000065H
	DB	100					; 00000064H
	DB	95					; 0000005fH
	DB	112					; 00000070H
	DB	116					; 00000074H
	DB	114					; 00000072H
	DB	115					; 00000073H
	DB	0
$LN26@satd_any_s:
	DB	115					; 00000073H
	DB	117					; 00000075H
	DB	109					; 0000006dH
	DB	115					; 00000073H
	DB	0
_satd_any_size_quad_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _satd_8bit_64x64_dual_avx2
_TEXT	SEGMENT
_row$1 = -56						; size = 4
_sum2$ = -44						; size = 4
_sum1$ = -32						; size = 4
_y$ = -20						; size = 4
_x$ = -8						; size = 4
_preds$ = 8						; size = 4
_orig$ = 12						; size = 4
_num_modes$ = 16					; size = 4
_satds_out$ = 20					; size = 4
_satd_8bit_64x64_dual_avx2 PROC				; COMDAT

; 628  : SATD_NXN_DUAL_AVX2(64)

	push	ebp
	mov	ebp, esp
	sub	esp, 252				; 000000fcH
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-252]
	mov	ecx, 63					; 0000003fH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4
	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, DWORD PTR _satds_out$[ebp]
	mov	DWORD PTR [edx+ecx], 0
	mov	eax, 4
	shl	eax, 0
	mov	ecx, DWORD PTR _satds_out$[ebp]
	mov	DWORD PTR [ecx+eax], 0
	mov	DWORD PTR _sum1$[ebp], 0
	mov	DWORD PTR _sum2$[ebp], 0
	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@satd_8bit_
$LN2@satd_8bit_:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 8
	mov	DWORD PTR _y$[ebp], eax
$LN4@satd_8bit_:
	cmp	DWORD PTR _y$[ebp], 64			; 00000040H
	jae	$LN3@satd_8bit_
	mov	eax, DWORD PTR _y$[ebp]
	shl	eax, 6
	mov	DWORD PTR _row$1[ebp], eax
	mov	DWORD PTR _x$[ebp], 0
	jmp	SHORT $LN7@satd_8bit_
$LN5@satd_8bit_:
	mov	eax, DWORD PTR _x$[ebp]
	add	eax, 8
	mov	DWORD PTR _x$[ebp], eax
$LN7@satd_8bit_:
	cmp	DWORD PTR _x$[ebp], 64			; 00000040H
	jae	$LN6@satd_8bit_
	lea	eax, DWORD PTR _sum2$[ebp]
	push	eax
	lea	ecx, DWORD PTR _sum1$[ebp]
	push	ecx
	push	64					; 00000040H
	mov	edx, DWORD PTR _row$1[ebp]
	add	edx, DWORD PTR _x$[ebp]
	add	edx, DWORD PTR _orig$[ebp]
	push	edx
	push	64					; 00000040H
	mov	eax, 1024				; 00000400H
	shl	eax, 0
	add	eax, DWORD PTR _preds$[ebp]
	mov	ecx, DWORD PTR _row$1[ebp]
	add	ecx, DWORD PTR _x$[ebp]
	add	eax, ecx
	push	eax
	push	64					; 00000040H
	mov	edx, 1024				; 00000400H
	imul	eax, edx, 0
	add	eax, DWORD PTR _preds$[ebp]
	mov	ecx, DWORD PTR _row$1[ebp]
	add	ecx, DWORD PTR _x$[ebp]
	add	eax, ecx
	push	eax
	call	_kvz_satd_8bit_8x8_general_dual_avx2
	add	esp, 32					; 00000020H
	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, DWORD PTR _satds_out$[ebp]
	mov	eax, DWORD PTR [edx+ecx]
	add	eax, DWORD PTR _sum1$[ebp]
	mov	ecx, 4
	imul	edx, ecx, 0
	mov	ecx, DWORD PTR _satds_out$[ebp]
	mov	DWORD PTR [ecx+edx], eax
	mov	eax, 4
	shl	eax, 0
	mov	ecx, DWORD PTR _satds_out$[ebp]
	mov	edx, DWORD PTR [ecx+eax]
	add	edx, DWORD PTR _sum2$[ebp]
	mov	eax, 4
	shl	eax, 0
	mov	ecx, DWORD PTR _satds_out$[ebp]
	mov	DWORD PTR [ecx+eax], edx
	jmp	$LN5@satd_8bit_
$LN6@satd_8bit_:
	jmp	$LN2@satd_8bit_
$LN3@satd_8bit_:
	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, 4
	imul	eax, edx, 0
	mov	edx, DWORD PTR _satds_out$[ebp]
	mov	esi, DWORD PTR _satds_out$[ebp]
	mov	ecx, DWORD PTR [esi+ecx]
	mov	DWORD PTR [edx+eax], ecx
	mov	eax, 4
	shl	eax, 0
	mov	ecx, 4
	shl	ecx, 0
	mov	edx, DWORD PTR _satds_out$[ebp]
	mov	esi, DWORD PTR _satds_out$[ebp]
	mov	eax, DWORD PTR [esi+eax]
	mov	DWORD PTR [edx+ecx], eax
	push	edx
	mov	ecx, ebp
	push	eax
	lea	edx, DWORD PTR $LN12@satd_8bit_
	call	@_RTC_CheckStackVars@8
	pop	eax
	pop	edx
	pop	edi
	pop	esi
	pop	ebx
	add	esp, 252				; 000000fcH
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
$LN12@satd_8bit_:
	DD	2
	DD	$LN11@satd_8bit_
$LN11@satd_8bit_:
	DD	-32					; ffffffe0H
	DD	4
	DD	$LN9@satd_8bit_
	DD	-44					; ffffffd4H
	DD	4
	DD	$LN10@satd_8bit_
$LN10@satd_8bit_:
	DB	115					; 00000073H
	DB	117					; 00000075H
	DB	109					; 0000006dH
	DB	50					; 00000032H
	DB	0
$LN9@satd_8bit_:
	DB	115					; 00000073H
	DB	117					; 00000075H
	DB	109					; 0000006dH
	DB	49					; 00000031H
	DB	0
_satd_8bit_64x64_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _satd_8bit_32x32_dual_avx2
_TEXT	SEGMENT
_row$1 = -56						; size = 4
_sum2$ = -44						; size = 4
_sum1$ = -32						; size = 4
_y$ = -20						; size = 4
_x$ = -8						; size = 4
_preds$ = 8						; size = 4
_orig$ = 12						; size = 4
_num_modes$ = 16					; size = 4
_satds_out$ = 20					; size = 4
_satd_8bit_32x32_dual_avx2 PROC				; COMDAT

; 627  : SATD_NXN_DUAL_AVX2(32)

	push	ebp
	mov	ebp, esp
	sub	esp, 252				; 000000fcH
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-252]
	mov	ecx, 63					; 0000003fH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4
	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, DWORD PTR _satds_out$[ebp]
	mov	DWORD PTR [edx+ecx], 0
	mov	eax, 4
	shl	eax, 0
	mov	ecx, DWORD PTR _satds_out$[ebp]
	mov	DWORD PTR [ecx+eax], 0
	mov	DWORD PTR _sum1$[ebp], 0
	mov	DWORD PTR _sum2$[ebp], 0
	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@satd_8bit_
$LN2@satd_8bit_:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 8
	mov	DWORD PTR _y$[ebp], eax
$LN4@satd_8bit_:
	cmp	DWORD PTR _y$[ebp], 32			; 00000020H
	jae	$LN3@satd_8bit_
	mov	eax, DWORD PTR _y$[ebp]
	shl	eax, 5
	mov	DWORD PTR _row$1[ebp], eax
	mov	DWORD PTR _x$[ebp], 0
	jmp	SHORT $LN7@satd_8bit_
$LN5@satd_8bit_:
	mov	eax, DWORD PTR _x$[ebp]
	add	eax, 8
	mov	DWORD PTR _x$[ebp], eax
$LN7@satd_8bit_:
	cmp	DWORD PTR _x$[ebp], 32			; 00000020H
	jae	$LN6@satd_8bit_
	lea	eax, DWORD PTR _sum2$[ebp]
	push	eax
	lea	ecx, DWORD PTR _sum1$[ebp]
	push	ecx
	push	32					; 00000020H
	mov	edx, DWORD PTR _row$1[ebp]
	add	edx, DWORD PTR _x$[ebp]
	add	edx, DWORD PTR _orig$[ebp]
	push	edx
	push	32					; 00000020H
	mov	eax, 1024				; 00000400H
	shl	eax, 0
	add	eax, DWORD PTR _preds$[ebp]
	mov	ecx, DWORD PTR _row$1[ebp]
	add	ecx, DWORD PTR _x$[ebp]
	add	eax, ecx
	push	eax
	push	32					; 00000020H
	mov	edx, 1024				; 00000400H
	imul	eax, edx, 0
	add	eax, DWORD PTR _preds$[ebp]
	mov	ecx, DWORD PTR _row$1[ebp]
	add	ecx, DWORD PTR _x$[ebp]
	add	eax, ecx
	push	eax
	call	_kvz_satd_8bit_8x8_general_dual_avx2
	add	esp, 32					; 00000020H
	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, DWORD PTR _satds_out$[ebp]
	mov	eax, DWORD PTR [edx+ecx]
	add	eax, DWORD PTR _sum1$[ebp]
	mov	ecx, 4
	imul	edx, ecx, 0
	mov	ecx, DWORD PTR _satds_out$[ebp]
	mov	DWORD PTR [ecx+edx], eax
	mov	eax, 4
	shl	eax, 0
	mov	ecx, DWORD PTR _satds_out$[ebp]
	mov	edx, DWORD PTR [ecx+eax]
	add	edx, DWORD PTR _sum2$[ebp]
	mov	eax, 4
	shl	eax, 0
	mov	ecx, DWORD PTR _satds_out$[ebp]
	mov	DWORD PTR [ecx+eax], edx
	jmp	$LN5@satd_8bit_
$LN6@satd_8bit_:
	jmp	$LN2@satd_8bit_
$LN3@satd_8bit_:
	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, 4
	imul	eax, edx, 0
	mov	edx, DWORD PTR _satds_out$[ebp]
	mov	esi, DWORD PTR _satds_out$[ebp]
	mov	ecx, DWORD PTR [esi+ecx]
	mov	DWORD PTR [edx+eax], ecx
	mov	eax, 4
	shl	eax, 0
	mov	ecx, 4
	shl	ecx, 0
	mov	edx, DWORD PTR _satds_out$[ebp]
	mov	esi, DWORD PTR _satds_out$[ebp]
	mov	eax, DWORD PTR [esi+eax]
	mov	DWORD PTR [edx+ecx], eax
	push	edx
	mov	ecx, ebp
	push	eax
	lea	edx, DWORD PTR $LN12@satd_8bit_
	call	@_RTC_CheckStackVars@8
	pop	eax
	pop	edx
	pop	edi
	pop	esi
	pop	ebx
	add	esp, 252				; 000000fcH
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
$LN12@satd_8bit_:
	DD	2
	DD	$LN11@satd_8bit_
$LN11@satd_8bit_:
	DD	-32					; ffffffe0H
	DD	4
	DD	$LN9@satd_8bit_
	DD	-44					; ffffffd4H
	DD	4
	DD	$LN10@satd_8bit_
$LN10@satd_8bit_:
	DB	115					; 00000073H
	DB	117					; 00000075H
	DB	109					; 0000006dH
	DB	50					; 00000032H
	DB	0
$LN9@satd_8bit_:
	DB	115					; 00000073H
	DB	117					; 00000075H
	DB	109					; 0000006dH
	DB	49					; 00000031H
	DB	0
_satd_8bit_32x32_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _satd_8bit_16x16_dual_avx2
_TEXT	SEGMENT
_row$1 = -56						; size = 4
_sum2$ = -44						; size = 4
_sum1$ = -32						; size = 4
_y$ = -20						; size = 4
_x$ = -8						; size = 4
_preds$ = 8						; size = 4
_orig$ = 12						; size = 4
_num_modes$ = 16					; size = 4
_satds_out$ = 20					; size = 4
_satd_8bit_16x16_dual_avx2 PROC				; COMDAT

; 626  : SATD_NXN_DUAL_AVX2(16)

	push	ebp
	mov	ebp, esp
	sub	esp, 252				; 000000fcH
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-252]
	mov	ecx, 63					; 0000003fH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4
	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, DWORD PTR _satds_out$[ebp]
	mov	DWORD PTR [edx+ecx], 0
	mov	eax, 4
	shl	eax, 0
	mov	ecx, DWORD PTR _satds_out$[ebp]
	mov	DWORD PTR [ecx+eax], 0
	mov	DWORD PTR _sum1$[ebp], 0
	mov	DWORD PTR _sum2$[ebp], 0
	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@satd_8bit_
$LN2@satd_8bit_:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 8
	mov	DWORD PTR _y$[ebp], eax
$LN4@satd_8bit_:
	cmp	DWORD PTR _y$[ebp], 16			; 00000010H
	jae	$LN3@satd_8bit_
	mov	eax, DWORD PTR _y$[ebp]
	shl	eax, 4
	mov	DWORD PTR _row$1[ebp], eax
	mov	DWORD PTR _x$[ebp], 0
	jmp	SHORT $LN7@satd_8bit_
$LN5@satd_8bit_:
	mov	eax, DWORD PTR _x$[ebp]
	add	eax, 8
	mov	DWORD PTR _x$[ebp], eax
$LN7@satd_8bit_:
	cmp	DWORD PTR _x$[ebp], 16			; 00000010H
	jae	$LN6@satd_8bit_
	lea	eax, DWORD PTR _sum2$[ebp]
	push	eax
	lea	ecx, DWORD PTR _sum1$[ebp]
	push	ecx
	push	16					; 00000010H
	mov	edx, DWORD PTR _row$1[ebp]
	add	edx, DWORD PTR _x$[ebp]
	add	edx, DWORD PTR _orig$[ebp]
	push	edx
	push	16					; 00000010H
	mov	eax, 1024				; 00000400H
	shl	eax, 0
	add	eax, DWORD PTR _preds$[ebp]
	mov	ecx, DWORD PTR _row$1[ebp]
	add	ecx, DWORD PTR _x$[ebp]
	add	eax, ecx
	push	eax
	push	16					; 00000010H
	mov	edx, 1024				; 00000400H
	imul	eax, edx, 0
	add	eax, DWORD PTR _preds$[ebp]
	mov	ecx, DWORD PTR _row$1[ebp]
	add	ecx, DWORD PTR _x$[ebp]
	add	eax, ecx
	push	eax
	call	_kvz_satd_8bit_8x8_general_dual_avx2
	add	esp, 32					; 00000020H
	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, DWORD PTR _satds_out$[ebp]
	mov	eax, DWORD PTR [edx+ecx]
	add	eax, DWORD PTR _sum1$[ebp]
	mov	ecx, 4
	imul	edx, ecx, 0
	mov	ecx, DWORD PTR _satds_out$[ebp]
	mov	DWORD PTR [ecx+edx], eax
	mov	eax, 4
	shl	eax, 0
	mov	ecx, DWORD PTR _satds_out$[ebp]
	mov	edx, DWORD PTR [ecx+eax]
	add	edx, DWORD PTR _sum2$[ebp]
	mov	eax, 4
	shl	eax, 0
	mov	ecx, DWORD PTR _satds_out$[ebp]
	mov	DWORD PTR [ecx+eax], edx
	jmp	$LN5@satd_8bit_
$LN6@satd_8bit_:
	jmp	$LN2@satd_8bit_
$LN3@satd_8bit_:
	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, 4
	imul	eax, edx, 0
	mov	edx, DWORD PTR _satds_out$[ebp]
	mov	esi, DWORD PTR _satds_out$[ebp]
	mov	ecx, DWORD PTR [esi+ecx]
	mov	DWORD PTR [edx+eax], ecx
	mov	eax, 4
	shl	eax, 0
	mov	ecx, 4
	shl	ecx, 0
	mov	edx, DWORD PTR _satds_out$[ebp]
	mov	esi, DWORD PTR _satds_out$[ebp]
	mov	eax, DWORD PTR [esi+eax]
	mov	DWORD PTR [edx+ecx], eax
	push	edx
	mov	ecx, ebp
	push	eax
	lea	edx, DWORD PTR $LN12@satd_8bit_
	call	@_RTC_CheckStackVars@8
	pop	eax
	pop	edx
	pop	edi
	pop	esi
	pop	ebx
	add	esp, 252				; 000000fcH
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
$LN12@satd_8bit_:
	DD	2
	DD	$LN11@satd_8bit_
$LN11@satd_8bit_:
	DD	-32					; ffffffe0H
	DD	4
	DD	$LN9@satd_8bit_
	DD	-44					; ffffffd4H
	DD	4
	DD	$LN10@satd_8bit_
$LN10@satd_8bit_:
	DB	115					; 00000073H
	DB	117					; 00000075H
	DB	109					; 0000006dH
	DB	50					; 00000032H
	DB	0
$LN9@satd_8bit_:
	DB	115					; 00000073H
	DB	117					; 00000075H
	DB	109					; 0000006dH
	DB	49					; 00000031H
	DB	0
_satd_8bit_16x16_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _satd_8bit_8x8_dual_avx2
_TEXT	SEGMENT
_row$1 = -56						; size = 4
_sum2$ = -44						; size = 4
_sum1$ = -32						; size = 4
_y$ = -20						; size = 4
_x$ = -8						; size = 4
_preds$ = 8						; size = 4
_orig$ = 12						; size = 4
_num_modes$ = 16					; size = 4
_satds_out$ = 20					; size = 4
_satd_8bit_8x8_dual_avx2 PROC				; COMDAT

; 607  : { 

	push	ebp
	mov	ebp, esp
	sub	esp, 252				; 000000fcH
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-252]
	mov	ecx, 63					; 0000003fH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 608  :   unsigned x, y; 
; 609  :   satds_out[0] = 0;

	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, DWORD PTR _satds_out$[ebp]
	mov	DWORD PTR [edx+ecx], 0

; 610  :   satds_out[1] = 0;

	mov	eax, 4
	shl	eax, 0
	mov	ecx, DWORD PTR _satds_out$[ebp]
	mov	DWORD PTR [ecx+eax], 0

; 611  :   unsigned sum1 = 0;

	mov	DWORD PTR _sum1$[ebp], 0

; 612  :   unsigned sum2 = 0;

	mov	DWORD PTR _sum2$[ebp], 0

; 613  :   for (y = 0; y < (8); y += 8) { 

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@satd_8bit_
$LN2@satd_8bit_:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 8
	mov	DWORD PTR _y$[ebp], eax
$LN4@satd_8bit_:
	cmp	DWORD PTR _y$[ebp], 8
	jae	$LN3@satd_8bit_

; 614  :   unsigned row = y * (8); 

	mov	eax, DWORD PTR _y$[ebp]
	shl	eax, 3
	mov	DWORD PTR _row$1[ebp], eax

; 615  :   for (x = 0; x < (8); x += 8) { 

	mov	DWORD PTR _x$[ebp], 0
	jmp	SHORT $LN7@satd_8bit_
$LN5@satd_8bit_:
	mov	eax, DWORD PTR _x$[ebp]
	add	eax, 8
	mov	DWORD PTR _x$[ebp], eax
$LN7@satd_8bit_:
	cmp	DWORD PTR _x$[ebp], 8
	jae	$LN6@satd_8bit_

; 616  :   kvz_satd_8bit_8x8_general_dual_avx2(&preds[0][row + x], (8), &preds[1][row + x], (8), &orig[row + x], (8), &sum1, &sum2); 

	lea	eax, DWORD PTR _sum2$[ebp]
	push	eax
	lea	ecx, DWORD PTR _sum1$[ebp]
	push	ecx
	push	8
	mov	edx, DWORD PTR _row$1[ebp]
	add	edx, DWORD PTR _x$[ebp]
	add	edx, DWORD PTR _orig$[ebp]
	push	edx
	push	8
	mov	eax, 1024				; 00000400H
	shl	eax, 0
	add	eax, DWORD PTR _preds$[ebp]
	mov	ecx, DWORD PTR _row$1[ebp]
	add	ecx, DWORD PTR _x$[ebp]
	add	eax, ecx
	push	eax
	push	8
	mov	edx, 1024				; 00000400H
	imul	eax, edx, 0
	add	eax, DWORD PTR _preds$[ebp]
	mov	ecx, DWORD PTR _row$1[ebp]
	add	ecx, DWORD PTR _x$[ebp]
	add	eax, ecx
	push	eax
	call	_kvz_satd_8bit_8x8_general_dual_avx2
	add	esp, 32					; 00000020H

; 617  :   satds_out[0] += sum1;

	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, DWORD PTR _satds_out$[ebp]
	mov	eax, DWORD PTR [edx+ecx]
	add	eax, DWORD PTR _sum1$[ebp]
	mov	ecx, 4
	imul	edx, ecx, 0
	mov	ecx, DWORD PTR _satds_out$[ebp]
	mov	DWORD PTR [ecx+edx], eax

; 618  :   satds_out[1] += sum2;

	mov	eax, 4
	shl	eax, 0
	mov	ecx, DWORD PTR _satds_out$[ebp]
	mov	edx, DWORD PTR [ecx+eax]
	add	edx, DWORD PTR _sum2$[ebp]
	mov	eax, 4
	shl	eax, 0
	mov	ecx, DWORD PTR _satds_out$[ebp]
	mov	DWORD PTR [ecx+eax], edx

; 619  :       } 

	jmp	$LN5@satd_8bit_
$LN6@satd_8bit_:

; 620  :       } 

	jmp	$LN2@satd_8bit_
$LN3@satd_8bit_:

; 621  :   satds_out[0] >>= (KVZ_BIT_DEPTH-8);

	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, 4
	imul	eax, edx, 0
	mov	edx, DWORD PTR _satds_out$[ebp]
	mov	esi, DWORD PTR _satds_out$[ebp]
	mov	ecx, DWORD PTR [esi+ecx]
	mov	DWORD PTR [edx+eax], ecx

; 622  :   satds_out[1] >>= (KVZ_BIT_DEPTH-8);

	mov	eax, 4
	shl	eax, 0
	mov	ecx, 4
	shl	ecx, 0
	mov	edx, DWORD PTR _satds_out$[ebp]
	mov	esi, DWORD PTR _satds_out$[ebp]
	mov	eax, DWORD PTR [esi+eax]
	mov	DWORD PTR [edx+ecx], eax

; 623  : }

	push	edx
	mov	ecx, ebp
	push	eax
	lea	edx, DWORD PTR $LN12@satd_8bit_
	call	@_RTC_CheckStackVars@8
	pop	eax
	pop	edx
	pop	edi
	pop	esi
	pop	ebx
	add	esp, 252				; 000000fcH
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
$LN12@satd_8bit_:
	DD	2
	DD	$LN11@satd_8bit_
$LN11@satd_8bit_:
	DD	-32					; ffffffe0H
	DD	4
	DD	$LN9@satd_8bit_
	DD	-44					; ffffffd4H
	DD	4
	DD	$LN10@satd_8bit_
$LN10@satd_8bit_:
	DB	115					; 00000073H
	DB	117					; 00000075H
	DB	109					; 0000006dH
	DB	50					; 00000032H
	DB	0
$LN9@satd_8bit_:
	DB	115					; 00000073H
	DB	117					; 00000075H
	DB	109					; 0000006dH
	DB	49					; 00000031H
	DB	0
_satd_8bit_8x8_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _satd_any_size_8bit_avx2
_TEXT	SEGMENT
_x$1 = -80						; size = 4
_row2$2 = -68						; size = 4
_row1$3 = -56						; size = 4
_y$4 = -44						; size = 4
_x$5 = -32						; size = 4
_y$6 = -20						; size = 4
_sum$ = -8						; size = 4
_width$ = 8						; size = 4
_height$ = 12						; size = 4
_block1$ = 16						; size = 4
_stride1$ = 20						; size = 4
_block2$ = 24						; size = 4
_stride2$ = 28						; size = 4
_satd_any_size_8bit_avx2 PROC				; COMDAT

; 579  : SATD_ANY_SIZE(8bit_avx2)

	push	ebp
	mov	ebp, esp
	sub	esp, 276				; 00000114H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-276]
	mov	ecx, 69					; 00000045H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4
	mov	DWORD PTR _sum$[ebp], 0
	mov	eax, DWORD PTR _width$[ebp]
	and	eax, -2147483641			; 80000007H
	jns	SHORT $LN17@satd_any_s
	dec	eax
	or	eax, -8					; fffffff8H
	inc	eax
$LN17@satd_any_s:
	test	eax, eax
	je	SHORT $LN14@satd_any_s
	mov	DWORD PTR _y$6[ebp], 0
	jmp	SHORT $LN4@satd_any_s
$LN2@satd_any_s:
	mov	eax, DWORD PTR _y$6[ebp]
	add	eax, 4
	mov	DWORD PTR _y$6[ebp], eax
$LN4@satd_any_s:
	mov	eax, DWORD PTR _y$6[ebp]
	cmp	eax, DWORD PTR _height$[ebp]
	jge	SHORT $LN3@satd_any_s
	mov	eax, DWORD PTR _stride2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _y$6[ebp]
	imul	ecx, DWORD PTR _stride2$[ebp]
	add	ecx, DWORD PTR _block2$[ebp]
	push	ecx
	mov	edx, DWORD PTR _stride1$[ebp]
	push	edx
	mov	eax, DWORD PTR _y$6[ebp]
	imul	eax, DWORD PTR _stride1$[ebp]
	add	eax, DWORD PTR _block1$[ebp]
	push	eax
	call	_kvz_satd_4x4_subblock_8bit_avx2
	add	esp, 16					; 00000010H
	add	eax, DWORD PTR _sum$[ebp]
	mov	DWORD PTR _sum$[ebp], eax
	jmp	SHORT $LN2@satd_any_s
$LN3@satd_any_s:
	mov	eax, DWORD PTR _block1$[ebp]
	add	eax, 4
	mov	DWORD PTR _block1$[ebp], eax
	mov	eax, DWORD PTR _block2$[ebp]
	add	eax, 4
	mov	DWORD PTR _block2$[ebp], eax
	mov	eax, DWORD PTR _width$[ebp]
	sub	eax, 4
	mov	DWORD PTR _width$[ebp], eax
$LN14@satd_any_s:
	mov	eax, DWORD PTR _height$[ebp]
	and	eax, -2147483641			; 80000007H
	jns	SHORT $LN18@satd_any_s
	dec	eax
	or	eax, -8					; fffffff8H
	inc	eax
$LN18@satd_any_s:
	test	eax, eax
	je	SHORT $LN15@satd_any_s
	mov	DWORD PTR _x$5[ebp], 0
	jmp	SHORT $LN7@satd_any_s
$LN5@satd_any_s:
	mov	eax, DWORD PTR _x$5[ebp]
	add	eax, 4
	mov	DWORD PTR _x$5[ebp], eax
$LN7@satd_any_s:
	mov	eax, DWORD PTR _x$5[ebp]
	cmp	eax, DWORD PTR _width$[ebp]
	jge	SHORT $LN6@satd_any_s
	mov	eax, DWORD PTR _stride2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _block2$[ebp]
	add	ecx, DWORD PTR _x$5[ebp]
	push	ecx
	mov	edx, DWORD PTR _stride1$[ebp]
	push	edx
	mov	eax, DWORD PTR _block1$[ebp]
	add	eax, DWORD PTR _x$5[ebp]
	push	eax
	call	_kvz_satd_4x4_subblock_8bit_avx2
	add	esp, 16					; 00000010H
	add	eax, DWORD PTR _sum$[ebp]
	mov	DWORD PTR _sum$[ebp], eax
	jmp	SHORT $LN5@satd_any_s
$LN6@satd_any_s:
	mov	eax, DWORD PTR _stride1$[ebp]
	mov	ecx, DWORD PTR _block1$[ebp]
	lea	edx, DWORD PTR [ecx+eax*4]
	mov	DWORD PTR _block1$[ebp], edx
	mov	eax, DWORD PTR _stride2$[ebp]
	mov	ecx, DWORD PTR _block2$[ebp]
	lea	edx, DWORD PTR [ecx+eax*4]
	mov	DWORD PTR _block2$[ebp], edx
	mov	eax, DWORD PTR _height$[ebp]
	sub	eax, 4
	mov	DWORD PTR _height$[ebp], eax
$LN15@satd_any_s:
	mov	DWORD PTR _y$4[ebp], 0
	jmp	SHORT $LN10@satd_any_s
$LN8@satd_any_s:
	mov	eax, DWORD PTR _y$4[ebp]
	add	eax, 8
	mov	DWORD PTR _y$4[ebp], eax
$LN10@satd_any_s:
	mov	eax, DWORD PTR _y$4[ebp]
	cmp	eax, DWORD PTR _height$[ebp]
	jge	SHORT $LN9@satd_any_s
	mov	eax, DWORD PTR _y$4[ebp]
	imul	eax, DWORD PTR _stride1$[ebp]
	add	eax, DWORD PTR _block1$[ebp]
	mov	DWORD PTR _row1$3[ebp], eax
	mov	eax, DWORD PTR _y$4[ebp]
	imul	eax, DWORD PTR _stride2$[ebp]
	add	eax, DWORD PTR _block2$[ebp]
	mov	DWORD PTR _row2$2[ebp], eax
	mov	DWORD PTR _x$1[ebp], 0
	jmp	SHORT $LN13@satd_any_s
$LN11@satd_any_s:
	mov	eax, DWORD PTR _x$1[ebp]
	add	eax, 8
	mov	DWORD PTR _x$1[ebp], eax
$LN13@satd_any_s:
	mov	eax, DWORD PTR _x$1[ebp]
	cmp	eax, DWORD PTR _width$[ebp]
	jge	SHORT $LN12@satd_any_s
	mov	eax, DWORD PTR _stride2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _row2$2[ebp]
	add	ecx, DWORD PTR _x$1[ebp]
	push	ecx
	mov	edx, DWORD PTR _stride1$[ebp]
	push	edx
	mov	eax, DWORD PTR _row1$3[ebp]
	add	eax, DWORD PTR _x$1[ebp]
	push	eax
	call	_satd_8x8_subblock_8bit_avx2
	add	esp, 16					; 00000010H
	add	eax, DWORD PTR _sum$[ebp]
	mov	DWORD PTR _sum$[ebp], eax
	jmp	SHORT $LN11@satd_any_s
$LN12@satd_any_s:
	jmp	SHORT $LN8@satd_any_s
$LN9@satd_any_s:
	mov	eax, DWORD PTR _sum$[ebp]
	pop	edi
	pop	esi
	pop	ebx
	add	esp, 276				; 00000114H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_satd_any_size_8bit_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _satd_64x64_8bit_avx2
_TEXT	SEGMENT
_x$1 = -44						; size = 4
_row$2 = -32						; size = 4
_y$3 = -20						; size = 4
_sum$ = -8						; size = 4
_block1$ = 8						; size = 4
_block2$ = 12						; size = 4
_satd_64x64_8bit_avx2 PROC				; COMDAT

; 578  : SATD_NxN(8bit_avx2, 64)

	push	ebp
	mov	ebp, esp
	sub	esp, 240				; 000000f0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-240]
	mov	ecx, 60					; 0000003cH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4
	mov	DWORD PTR _sum$[ebp], 0
	mov	DWORD PTR _y$3[ebp], 0
	jmp	SHORT $LN4@satd_64x64
$LN2@satd_64x64:
	mov	eax, DWORD PTR _y$3[ebp]
	add	eax, 8
	mov	DWORD PTR _y$3[ebp], eax
$LN4@satd_64x64:
	cmp	DWORD PTR _y$3[ebp], 64			; 00000040H
	jae	SHORT $LN3@satd_64x64
	mov	eax, DWORD PTR _y$3[ebp]
	shl	eax, 6
	mov	DWORD PTR _row$2[ebp], eax
	mov	DWORD PTR _x$1[ebp], 0
	jmp	SHORT $LN7@satd_64x64
$LN5@satd_64x64:
	mov	eax, DWORD PTR _x$1[ebp]
	add	eax, 8
	mov	DWORD PTR _x$1[ebp], eax
$LN7@satd_64x64:
	cmp	DWORD PTR _x$1[ebp], 64			; 00000040H
	jae	SHORT $LN6@satd_64x64
	push	64					; 00000040H
	mov	eax, DWORD PTR _row$2[ebp]
	add	eax, DWORD PTR _x$1[ebp]
	add	eax, DWORD PTR _block2$[ebp]
	push	eax
	push	64					; 00000040H
	mov	ecx, DWORD PTR _row$2[ebp]
	add	ecx, DWORD PTR _x$1[ebp]
	add	ecx, DWORD PTR _block1$[ebp]
	push	ecx
	call	_satd_8x8_subblock_8bit_avx2
	add	esp, 16					; 00000010H
	add	eax, DWORD PTR _sum$[ebp]
	mov	DWORD PTR _sum$[ebp], eax
	jmp	SHORT $LN5@satd_64x64
$LN6@satd_64x64:
	jmp	SHORT $LN2@satd_64x64
$LN3@satd_64x64:
	mov	eax, DWORD PTR _sum$[ebp]
	pop	edi
	pop	esi
	pop	ebx
	add	esp, 240				; 000000f0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_satd_64x64_8bit_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _satd_32x32_8bit_avx2
_TEXT	SEGMENT
_x$1 = -44						; size = 4
_row$2 = -32						; size = 4
_y$3 = -20						; size = 4
_sum$ = -8						; size = 4
_block1$ = 8						; size = 4
_block2$ = 12						; size = 4
_satd_32x32_8bit_avx2 PROC				; COMDAT

; 577  : SATD_NxN(8bit_avx2, 32)

	push	ebp
	mov	ebp, esp
	sub	esp, 240				; 000000f0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-240]
	mov	ecx, 60					; 0000003cH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4
	mov	DWORD PTR _sum$[ebp], 0
	mov	DWORD PTR _y$3[ebp], 0
	jmp	SHORT $LN4@satd_32x32
$LN2@satd_32x32:
	mov	eax, DWORD PTR _y$3[ebp]
	add	eax, 8
	mov	DWORD PTR _y$3[ebp], eax
$LN4@satd_32x32:
	cmp	DWORD PTR _y$3[ebp], 32			; 00000020H
	jae	SHORT $LN3@satd_32x32
	mov	eax, DWORD PTR _y$3[ebp]
	shl	eax, 5
	mov	DWORD PTR _row$2[ebp], eax
	mov	DWORD PTR _x$1[ebp], 0
	jmp	SHORT $LN7@satd_32x32
$LN5@satd_32x32:
	mov	eax, DWORD PTR _x$1[ebp]
	add	eax, 8
	mov	DWORD PTR _x$1[ebp], eax
$LN7@satd_32x32:
	cmp	DWORD PTR _x$1[ebp], 32			; 00000020H
	jae	SHORT $LN6@satd_32x32
	push	32					; 00000020H
	mov	eax, DWORD PTR _row$2[ebp]
	add	eax, DWORD PTR _x$1[ebp]
	add	eax, DWORD PTR _block2$[ebp]
	push	eax
	push	32					; 00000020H
	mov	ecx, DWORD PTR _row$2[ebp]
	add	ecx, DWORD PTR _x$1[ebp]
	add	ecx, DWORD PTR _block1$[ebp]
	push	ecx
	call	_satd_8x8_subblock_8bit_avx2
	add	esp, 16					; 00000010H
	add	eax, DWORD PTR _sum$[ebp]
	mov	DWORD PTR _sum$[ebp], eax
	jmp	SHORT $LN5@satd_32x32
$LN6@satd_32x32:
	jmp	SHORT $LN2@satd_32x32
$LN3@satd_32x32:
	mov	eax, DWORD PTR _sum$[ebp]
	pop	edi
	pop	esi
	pop	ebx
	add	esp, 240				; 000000f0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_satd_32x32_8bit_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _satd_16x16_8bit_avx2
_TEXT	SEGMENT
_x$1 = -44						; size = 4
_row$2 = -32						; size = 4
_y$3 = -20						; size = 4
_sum$ = -8						; size = 4
_block1$ = 8						; size = 4
_block2$ = 12						; size = 4
_satd_16x16_8bit_avx2 PROC				; COMDAT

; 576  : SATD_NxN(8bit_avx2, 16)

	push	ebp
	mov	ebp, esp
	sub	esp, 240				; 000000f0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-240]
	mov	ecx, 60					; 0000003cH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4
	mov	DWORD PTR _sum$[ebp], 0
	mov	DWORD PTR _y$3[ebp], 0
	jmp	SHORT $LN4@satd_16x16
$LN2@satd_16x16:
	mov	eax, DWORD PTR _y$3[ebp]
	add	eax, 8
	mov	DWORD PTR _y$3[ebp], eax
$LN4@satd_16x16:
	cmp	DWORD PTR _y$3[ebp], 16			; 00000010H
	jae	SHORT $LN3@satd_16x16
	mov	eax, DWORD PTR _y$3[ebp]
	shl	eax, 4
	mov	DWORD PTR _row$2[ebp], eax
	mov	DWORD PTR _x$1[ebp], 0
	jmp	SHORT $LN7@satd_16x16
$LN5@satd_16x16:
	mov	eax, DWORD PTR _x$1[ebp]
	add	eax, 8
	mov	DWORD PTR _x$1[ebp], eax
$LN7@satd_16x16:
	cmp	DWORD PTR _x$1[ebp], 16			; 00000010H
	jae	SHORT $LN6@satd_16x16
	push	16					; 00000010H
	mov	eax, DWORD PTR _row$2[ebp]
	add	eax, DWORD PTR _x$1[ebp]
	add	eax, DWORD PTR _block2$[ebp]
	push	eax
	push	16					; 00000010H
	mov	ecx, DWORD PTR _row$2[ebp]
	add	ecx, DWORD PTR _x$1[ebp]
	add	ecx, DWORD PTR _block1$[ebp]
	push	ecx
	call	_satd_8x8_subblock_8bit_avx2
	add	esp, 16					; 00000010H
	add	eax, DWORD PTR _sum$[ebp]
	mov	DWORD PTR _sum$[ebp], eax
	jmp	SHORT $LN5@satd_16x16
$LN6@satd_16x16:
	jmp	SHORT $LN2@satd_16x16
$LN3@satd_16x16:
	mov	eax, DWORD PTR _sum$[ebp]
	pop	edi
	pop	esi
	pop	ebx
	add	esp, 240				; 000000f0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_satd_16x16_8bit_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _satd_8x8_8bit_avx2
_TEXT	SEGMENT
_x$1 = -44						; size = 4
_row$2 = -32						; size = 4
_y$3 = -20						; size = 4
_sum$ = -8						; size = 4
_block1$ = 8						; size = 4
_block2$ = 12						; size = 4
_satd_8x8_8bit_avx2 PROC				; COMDAT

; 575  : SATD_NxN(8bit_avx2,  8)

	push	ebp
	mov	ebp, esp
	sub	esp, 240				; 000000f0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-240]
	mov	ecx, 60					; 0000003cH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4
	mov	DWORD PTR _sum$[ebp], 0
	mov	DWORD PTR _y$3[ebp], 0
	jmp	SHORT $LN4@satd_8x8_8
$LN2@satd_8x8_8:
	mov	eax, DWORD PTR _y$3[ebp]
	add	eax, 8
	mov	DWORD PTR _y$3[ebp], eax
$LN4@satd_8x8_8:
	cmp	DWORD PTR _y$3[ebp], 8
	jae	SHORT $LN3@satd_8x8_8
	mov	eax, DWORD PTR _y$3[ebp]
	shl	eax, 3
	mov	DWORD PTR _row$2[ebp], eax
	mov	DWORD PTR _x$1[ebp], 0
	jmp	SHORT $LN7@satd_8x8_8
$LN5@satd_8x8_8:
	mov	eax, DWORD PTR _x$1[ebp]
	add	eax, 8
	mov	DWORD PTR _x$1[ebp], eax
$LN7@satd_8x8_8:
	cmp	DWORD PTR _x$1[ebp], 8
	jae	SHORT $LN6@satd_8x8_8
	push	8
	mov	eax, DWORD PTR _row$2[ebp]
	add	eax, DWORD PTR _x$1[ebp]
	add	eax, DWORD PTR _block2$[ebp]
	push	eax
	push	8
	mov	ecx, DWORD PTR _row$2[ebp]
	add	ecx, DWORD PTR _x$1[ebp]
	add	ecx, DWORD PTR _block1$[ebp]
	push	ecx
	call	_satd_8x8_subblock_8bit_avx2
	add	esp, 16					; 00000010H
	add	eax, DWORD PTR _sum$[ebp]
	mov	DWORD PTR _sum$[ebp], eax
	jmp	SHORT $LN5@satd_8x8_8
$LN6@satd_8x8_8:
	jmp	SHORT $LN2@satd_8x8_8
$LN3@satd_8x8_8:
	mov	eax, DWORD PTR _sum$[ebp]
	pop	edi
	pop	esi
	pop	ebx
	add	esp, 240				; 000000f0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_satd_8x8_8bit_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _satd_8x8_subblock_quad_avx2
_TEXT	SEGMENT
_preds$ = 8						; size = 4
_stride$ = 12						; size = 4
_orig$ = 16						; size = 4
_orig_stride$ = 20					; size = 4
_costs$ = 24						; size = 4
_satd_8x8_subblock_quad_avx2 PROC			; COMDAT

; 570  : {

	push	ebp
	mov	ebp, esp
	sub	esp, 192				; 000000c0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-192]
	mov	ecx, 48					; 00000030H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 571  :   kvz_satd_8bit_8x8_general_dual_avx2(preds[0], stride, preds[1], stride, orig, orig_stride, &costs[0], &costs[1]);

	mov	eax, 4
	shl	eax, 0
	add	eax, DWORD PTR _costs$[ebp]
	push	eax
	mov	ecx, 4
	imul	edx, ecx, 0
	add	edx, DWORD PTR _costs$[ebp]
	push	edx
	mov	eax, DWORD PTR _orig_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _orig$[ebp]
	push	ecx
	mov	edx, DWORD PTR _stride$[ebp]
	push	edx
	mov	eax, 4
	shl	eax, 0
	mov	ecx, DWORD PTR _preds$[ebp]
	mov	edx, DWORD PTR [ecx+eax]
	push	edx
	mov	eax, DWORD PTR _stride$[ebp]
	push	eax
	mov	ecx, 4
	imul	edx, ecx, 0
	mov	eax, DWORD PTR _preds$[ebp]
	mov	ecx, DWORD PTR [eax+edx]
	push	ecx
	call	_kvz_satd_8bit_8x8_general_dual_avx2
	add	esp, 32					; 00000020H

; 572  :   kvz_satd_8bit_8x8_general_dual_avx2(preds[2], stride, preds[3], stride, orig, orig_stride, &costs[2], &costs[3]);

	mov	eax, 4
	imul	ecx, eax, 3
	add	ecx, DWORD PTR _costs$[ebp]
	push	ecx
	mov	edx, 4
	shl	edx, 1
	add	edx, DWORD PTR _costs$[ebp]
	push	edx
	mov	eax, DWORD PTR _orig_stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _orig$[ebp]
	push	ecx
	mov	edx, DWORD PTR _stride$[ebp]
	push	edx
	mov	eax, 4
	imul	ecx, eax, 3
	mov	edx, DWORD PTR _preds$[ebp]
	mov	eax, DWORD PTR [edx+ecx]
	push	eax
	mov	ecx, DWORD PTR _stride$[ebp]
	push	ecx
	mov	edx, 4
	shl	edx, 1
	mov	eax, DWORD PTR _preds$[ebp]
	mov	ecx, DWORD PTR [eax+edx]
	push	ecx
	call	_kvz_satd_8bit_8x8_general_dual_avx2
	add	esp, 32					; 00000020H

; 573  : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 192				; 000000c0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_satd_8x8_subblock_quad_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _satd_8x8_subblock_8bit_avx2
_TEXT	SEGMENT
_result$ = -168						; size = 4
_sad$ = -156						; size = 4
_temp$ = -144						; size = 128
__$ArrayPad$ = -4					; size = 4
_buf1$ = 8						; size = 4
_stride1$ = 12						; size = 4
_buf2$ = 16						; size = 4
_stride2$ = 20						; size = 4
_satd_8x8_subblock_8bit_avx2 PROC			; COMDAT

; 552  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 376				; 00000178H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-376]
	mov	ecx, 94					; 0000005eH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	eax, DWORD PTR ___security_cookie
	xor	eax, ebp
	mov	DWORD PTR __$ArrayPad$[ebp], eax
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 553  :   __m128i temp[8];
; 554  : 
; 555  :   diff_blocks_avx2(&temp, buf1, stride1, buf2, stride2);

	mov	eax, DWORD PTR _stride2$[ebx]
	push	eax
	mov	ecx, DWORD PTR _buf2$[ebx]
	push	ecx
	mov	edx, DWORD PTR _stride1$[ebx]
	push	edx
	mov	eax, DWORD PTR _buf1$[ebx]
	push	eax
	lea	ecx, DWORD PTR _temp$[ebp]
	push	ecx
	call	_diff_blocks_avx2
	add	esp, 20					; 00000014H

; 556  :   hor_transform_block_avx2(&temp);

	lea	eax, DWORD PTR _temp$[ebp]
	push	eax
	call	_hor_transform_block_avx2
	add	esp, 4

; 557  :   ver_transform_block_avx2(&temp);

	lea	eax, DWORD PTR _temp$[ebp]
	push	eax
	call	_ver_transform_block_avx2
	add	esp, 4

; 558  :   
; 559  :   unsigned sad = sum_block_avx2(temp);

	lea	eax, DWORD PTR _temp$[ebp]
	push	eax
	call	_sum_block_avx2
	add	esp, 4
	mov	DWORD PTR _sad$[ebp], eax

; 560  : 
; 561  :   unsigned result = (sad + 2) >> 2;

	mov	eax, DWORD PTR _sad$[ebp]
	add	eax, 2
	shr	eax, 2
	mov	DWORD PTR _result$[ebp], eax

; 562  :   return result;

	mov	eax, DWORD PTR _result$[ebp]

; 563  : }

	push	edx
	mov	ecx, ebp
	push	eax
	lea	edx, DWORD PTR $LN5@satd_8x8_s
	call	@_RTC_CheckStackVars@8
	pop	eax
	pop	edx
	pop	edi
	pop	esi
	mov	ecx, DWORD PTR __$ArrayPad$[ebp]
	xor	ecx, ebp
	call	@__security_check_cookie@4
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
	npad	2
$LN5@satd_8x8_s:
	DD	1
	DD	$LN4@satd_8x8_s
$LN4@satd_8x8_s:
	DD	-144					; ffffff70H
	DD	128					; 00000080H
	DD	$LN3@satd_8x8_s
$LN3@satd_8x8_s:
	DB	116					; 00000074H
	DB	101					; 00000065H
	DB	109					; 0000006dH
	DB	112					; 00000070H
	DB	0
_satd_8x8_subblock_8bit_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _kvz_satd_4x4_subblock_quad_avx2
_TEXT	SEGMENT
_preds$ = 8						; size = 4
_stride$ = 12						; size = 4
_orig$ = 16						; size = 4
_orig_stride$ = 20					; size = 4
_costs$ = 24						; size = 4
_kvz_satd_4x4_subblock_quad_avx2 PROC			; COMDAT

; 546  : {

	push	ebp
	mov	ebp, esp
	sub	esp, 192				; 000000c0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-192]
	mov	ecx, 48					; 00000030H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 547  :   // TODO: AVX2 implementation
; 548  :   kvz_satd_4x4_subblock_quad_generic(preds, stride, orig, orig_stride, costs);

	mov	eax, DWORD PTR _costs$[ebp]
	push	eax
	mov	ecx, DWORD PTR _orig_stride$[ebp]
	push	ecx
	mov	edx, DWORD PTR _orig$[ebp]
	push	edx
	mov	eax, DWORD PTR _stride$[ebp]
	push	eax
	mov	ecx, DWORD PTR _preds$[ebp]
	push	ecx
	call	_kvz_satd_4x4_subblock_quad_generic
	add	esp, 20					; 00000014H

; 549  : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 192				; 000000c0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_kvz_satd_4x4_subblock_quad_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _kvz_satd_4x4_subblock_8bit_avx2
_TEXT	SEGMENT
_buf1$ = 8						; size = 4
_stride1$ = 12						; size = 4
_buf2$ = 16						; size = 4
_stride2$ = 20						; size = 4
_kvz_satd_4x4_subblock_8bit_avx2 PROC			; COMDAT

; 536  : {

	push	ebp
	mov	ebp, esp
	sub	esp, 192				; 000000c0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-192]
	mov	ecx, 48					; 00000030H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 537  :   // TODO: AVX2 implementation
; 538  :   return kvz_satd_4x4_subblock_generic(buf1, stride1, buf2, stride2);

	mov	eax, DWORD PTR _stride2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _buf2$[ebp]
	push	ecx
	mov	edx, DWORD PTR _stride1$[ebp]
	push	edx
	mov	eax, DWORD PTR _buf1$[ebp]
	push	eax
	call	_kvz_satd_4x4_subblock_generic
	add	esp, 16					; 00000010H

; 539  : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 192				; 000000c0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_kvz_satd_4x4_subblock_8bit_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _kvz_satd_8bit_8x8_general_dual_avx2
_TEXT	SEGMENT
_temp$ = -288						; size = 256
__$ArrayPad$ = -4					; size = 4
_buf1$ = 8						; size = 4
_stride1$ = 12						; size = 4
_buf2$ = 16						; size = 4
_stride2$ = 20						; size = 4
_orig$ = 24						; size = 4
_stride_orig$ = 28					; size = 4
_sum0$ = 32						; size = 4
_sum1$ = 36						; size = 4
_kvz_satd_8bit_8x8_general_dual_avx2 PROC		; COMDAT

; 516  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 504				; 000001f8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-504]
	mov	ecx, 126				; 0000007eH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	eax, DWORD PTR ___security_cookie
	xor	eax, ebp
	mov	DWORD PTR __$ArrayPad$[ebp], eax
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 517  :   __m256i temp[8];
; 518  : 
; 519  :   diff_blocks_dual_avx2(&temp, buf1, stride1, buf2, stride2, orig, stride_orig);

	mov	eax, DWORD PTR _stride_orig$[ebx]
	push	eax
	mov	ecx, DWORD PTR _orig$[ebx]
	push	ecx
	mov	edx, DWORD PTR _stride2$[ebx]
	push	edx
	mov	eax, DWORD PTR _buf2$[ebx]
	push	eax
	mov	ecx, DWORD PTR _stride1$[ebx]
	push	ecx
	mov	edx, DWORD PTR _buf1$[ebx]
	push	edx
	lea	eax, DWORD PTR _temp$[ebp]
	push	eax
	call	_diff_blocks_dual_avx2
	add	esp, 28					; 0000001cH

; 520  :   hor_transform_block_dual_avx2(&temp);

	lea	eax, DWORD PTR _temp$[ebp]
	push	eax
	call	_hor_transform_block_dual_avx2
	add	esp, 4

; 521  :   ver_transform_block_dual_avx2(&temp);

	lea	eax, DWORD PTR _temp$[ebp]
	push	eax
	call	_ver_transform_block_dual_avx2
	add	esp, 4

; 522  :   
; 523  :   sum_block_dual_avx2(temp, sum0, sum1);

	mov	eax, DWORD PTR _sum1$[ebx]
	push	eax
	mov	ecx, DWORD PTR _sum0$[ebx]
	push	ecx
	lea	edx, DWORD PTR _temp$[ebp]
	push	edx
	call	_sum_block_dual_avx2
	add	esp, 12					; 0000000cH

; 524  : 
; 525  :   *sum0 = (*sum0 + 2) >> 2;

	mov	eax, DWORD PTR _sum0$[ebx]
	mov	ecx, DWORD PTR [eax]
	add	ecx, 2
	shr	ecx, 2
	mov	edx, DWORD PTR _sum0$[ebx]
	mov	DWORD PTR [edx], ecx

; 526  :   *sum1 = (*sum1 + 2) >> 2;

	mov	eax, DWORD PTR _sum1$[ebx]
	mov	ecx, DWORD PTR [eax]
	add	ecx, 2
	shr	ecx, 2
	mov	edx, DWORD PTR _sum1$[ebx]
	mov	DWORD PTR [edx], ecx

; 527  : }

	push	edx
	mov	ecx, ebp
	push	eax
	lea	edx, DWORD PTR $LN5@kvz_satd_8
	call	@_RTC_CheckStackVars@8
	pop	eax
	pop	edx
	pop	edi
	pop	esi
	mov	ecx, DWORD PTR __$ArrayPad$[ebp]
	xor	ecx, ebp
	call	@__security_check_cookie@4
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
$LN5@kvz_satd_8:
	DD	1
	DD	$LN4@kvz_satd_8
$LN4@kvz_satd_8:
	DD	-288					; fffffee0H
	DD	256					; 00000100H
	DD	$LN3@kvz_satd_8
$LN3@kvz_satd_8:
	DB	116					; 00000074H
	DB	101					; 00000065H
	DB	109					; 0000006dH
	DB	112					; 00000070H
	DB	0
_kvz_satd_8bit_8x8_general_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _hor_transform_block_dual_avx2
_TEXT	SEGMENT
_row_diff$ = 8						; size = 4
_hor_transform_block_dual_avx2 PROC			; COMDAT

; 501  : {

	push	ebp
	mov	ebp, esp
	sub	esp, 192				; 000000c0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-192]
	mov	ecx, 48					; 00000030H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 502  :   hor_transform_row_dual_avx2((*row_diff) + 0);

	mov	eax, DWORD PTR _row_diff$[ebp]
	push	eax
	call	_hor_transform_row_dual_avx2
	add	esp, 4

; 503  :   hor_transform_row_dual_avx2((*row_diff) + 1);

	mov	eax, DWORD PTR _row_diff$[ebp]
	add	eax, 32					; 00000020H
	push	eax
	call	_hor_transform_row_dual_avx2
	add	esp, 4

; 504  :   hor_transform_row_dual_avx2((*row_diff) + 2);

	mov	eax, DWORD PTR _row_diff$[ebp]
	add	eax, 64					; 00000040H
	push	eax
	call	_hor_transform_row_dual_avx2
	add	esp, 4

; 505  :   hor_transform_row_dual_avx2((*row_diff) + 3);

	mov	eax, DWORD PTR _row_diff$[ebp]
	add	eax, 96					; 00000060H
	push	eax
	call	_hor_transform_row_dual_avx2
	add	esp, 4

; 506  :   hor_transform_row_dual_avx2((*row_diff) + 4);

	mov	eax, DWORD PTR _row_diff$[ebp]
	add	eax, 128				; 00000080H
	push	eax
	call	_hor_transform_row_dual_avx2
	add	esp, 4

; 507  :   hor_transform_row_dual_avx2((*row_diff) + 5);

	mov	eax, DWORD PTR _row_diff$[ebp]
	add	eax, 160				; 000000a0H
	push	eax
	call	_hor_transform_row_dual_avx2
	add	esp, 4

; 508  :   hor_transform_row_dual_avx2((*row_diff) + 6);

	mov	eax, DWORD PTR _row_diff$[ebp]
	add	eax, 192				; 000000c0H
	push	eax
	call	_hor_transform_row_dual_avx2
	add	esp, 4

; 509  :   hor_transform_row_dual_avx2((*row_diff) + 7);

	mov	eax, DWORD PTR _row_diff$[ebp]
	add	eax, 224				; 000000e0H
	push	eax
	call	_hor_transform_row_dual_avx2
	add	esp, 4

; 510  : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 192				; 000000c0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_hor_transform_block_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _hor_transform_block_avx2
_TEXT	SEGMENT
_row_diff$ = 8						; size = 4
_hor_transform_block_avx2 PROC				; COMDAT

; 489  : {

	push	ebp
	mov	ebp, esp
	sub	esp, 192				; 000000c0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-192]
	mov	ecx, 48					; 00000030H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 490  :   hor_transform_row_avx2((*row_diff) + 0);

	mov	eax, DWORD PTR _row_diff$[ebp]
	push	eax
	call	_hor_transform_row_avx2
	add	esp, 4

; 491  :   hor_transform_row_avx2((*row_diff) + 1);

	mov	eax, DWORD PTR _row_diff$[ebp]
	add	eax, 16					; 00000010H
	push	eax
	call	_hor_transform_row_avx2
	add	esp, 4

; 492  :   hor_transform_row_avx2((*row_diff) + 2);

	mov	eax, DWORD PTR _row_diff$[ebp]
	add	eax, 32					; 00000020H
	push	eax
	call	_hor_transform_row_avx2
	add	esp, 4

; 493  :   hor_transform_row_avx2((*row_diff) + 3);

	mov	eax, DWORD PTR _row_diff$[ebp]
	add	eax, 48					; 00000030H
	push	eax
	call	_hor_transform_row_avx2
	add	esp, 4

; 494  :   hor_transform_row_avx2((*row_diff) + 4);

	mov	eax, DWORD PTR _row_diff$[ebp]
	add	eax, 64					; 00000040H
	push	eax
	call	_hor_transform_row_avx2
	add	esp, 4

; 495  :   hor_transform_row_avx2((*row_diff) + 5);

	mov	eax, DWORD PTR _row_diff$[ebp]
	add	eax, 80					; 00000050H
	push	eax
	call	_hor_transform_row_avx2
	add	esp, 4

; 496  :   hor_transform_row_avx2((*row_diff) + 6);

	mov	eax, DWORD PTR _row_diff$[ebp]
	add	eax, 96					; 00000060H
	push	eax
	call	_hor_transform_row_avx2
	add	esp, 4

; 497  :   hor_transform_row_avx2((*row_diff) + 7);

	mov	eax, DWORD PTR _row_diff$[ebp]
	add	eax, 112				; 00000070H
	push	eax
	call	_hor_transform_row_avx2
	add	esp, 4

; 498  : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 192				; 000000c0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_hor_transform_block_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _diff_blocks_dual_avx2
_TEXT	SEGMENT
_row_diff$ = 8						; size = 4
_buf1$ = 12						; size = 4
_stride1$ = 16						; size = 4
_buf2$ = 20						; size = 4
_stride2$ = 24						; size = 4
_orig$ = 28						; size = 4
_stride_orig$ = 32					; size = 4
_diff_blocks_dual_avx2 PROC				; COMDAT

; 476  : {

	push	ebp
	mov	ebp, esp
	sub	esp, 192				; 000000c0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-192]
	mov	ecx, 48					; 00000030H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 477  :   (*row_diff)[0] = diff_row_dual_avx2(buf1 + 0 * stride1, buf2 + 0 * stride2, orig + 0 * stride_orig);

	imul	eax, DWORD PTR _stride_orig$[ebp], 0
	add	eax, DWORD PTR _orig$[ebp]
	push	eax
	imul	ecx, DWORD PTR _stride2$[ebp], 0
	add	ecx, DWORD PTR _buf2$[ebp]
	push	ecx
	imul	edx, DWORD PTR _stride1$[ebp], 0
	add	edx, DWORD PTR _buf1$[ebp]
	push	edx
	call	_diff_row_dual_avx2
	add	esp, 12					; 0000000cH
	mov	eax, 32					; 00000020H
	imul	ecx, eax, 0
	add	ecx, DWORD PTR _row_diff$[ebp]
	vmovdqu	YMMWORD PTR [ecx], ymm0

; 478  :   (*row_diff)[1] = diff_row_dual_avx2(buf1 + 1 * stride1, buf2 + 1 * stride2, orig + 1 * stride_orig);

	mov	eax, DWORD PTR _orig$[ebp]
	add	eax, DWORD PTR _stride_orig$[ebp]
	push	eax
	mov	ecx, DWORD PTR _buf2$[ebp]
	add	ecx, DWORD PTR _stride2$[ebp]
	push	ecx
	mov	edx, DWORD PTR _buf1$[ebp]
	add	edx, DWORD PTR _stride1$[ebp]
	push	edx
	call	_diff_row_dual_avx2
	add	esp, 12					; 0000000cH
	mov	eax, 32					; 00000020H
	shl	eax, 0
	add	eax, DWORD PTR _row_diff$[ebp]
	vmovdqu	YMMWORD PTR [eax], ymm0

; 479  :   (*row_diff)[2] = diff_row_dual_avx2(buf1 + 2 * stride1, buf2 + 2 * stride2, orig + 2 * stride_orig);

	mov	eax, DWORD PTR _stride_orig$[ebp]
	mov	ecx, DWORD PTR _orig$[ebp]
	lea	edx, DWORD PTR [ecx+eax*2]
	push	edx
	mov	eax, DWORD PTR _stride2$[ebp]
	mov	ecx, DWORD PTR _buf2$[ebp]
	lea	edx, DWORD PTR [ecx+eax*2]
	push	edx
	mov	eax, DWORD PTR _stride1$[ebp]
	mov	ecx, DWORD PTR _buf1$[ebp]
	lea	edx, DWORD PTR [ecx+eax*2]
	push	edx
	call	_diff_row_dual_avx2
	add	esp, 12					; 0000000cH
	mov	eax, 32					; 00000020H
	shl	eax, 1
	add	eax, DWORD PTR _row_diff$[ebp]
	vmovdqu	YMMWORD PTR [eax], ymm0

; 480  :   (*row_diff)[3] = diff_row_dual_avx2(buf1 + 3 * stride1, buf2 + 3 * stride2, orig + 3 * stride_orig);

	imul	eax, DWORD PTR _stride_orig$[ebp], 3
	add	eax, DWORD PTR _orig$[ebp]
	push	eax
	imul	ecx, DWORD PTR _stride2$[ebp], 3
	add	ecx, DWORD PTR _buf2$[ebp]
	push	ecx
	imul	edx, DWORD PTR _stride1$[ebp], 3
	add	edx, DWORD PTR _buf1$[ebp]
	push	edx
	call	_diff_row_dual_avx2
	add	esp, 12					; 0000000cH
	mov	eax, 32					; 00000020H
	imul	ecx, eax, 3
	add	ecx, DWORD PTR _row_diff$[ebp]
	vmovdqu	YMMWORD PTR [ecx], ymm0

; 481  :   (*row_diff)[4] = diff_row_dual_avx2(buf1 + 4 * stride1, buf2 + 4 * stride2, orig + 4 * stride_orig);

	mov	eax, DWORD PTR _stride_orig$[ebp]
	mov	ecx, DWORD PTR _orig$[ebp]
	lea	edx, DWORD PTR [ecx+eax*4]
	push	edx
	mov	eax, DWORD PTR _stride2$[ebp]
	mov	ecx, DWORD PTR _buf2$[ebp]
	lea	edx, DWORD PTR [ecx+eax*4]
	push	edx
	mov	eax, DWORD PTR _stride1$[ebp]
	mov	ecx, DWORD PTR _buf1$[ebp]
	lea	edx, DWORD PTR [ecx+eax*4]
	push	edx
	call	_diff_row_dual_avx2
	add	esp, 12					; 0000000cH
	mov	eax, 32					; 00000020H
	shl	eax, 2
	add	eax, DWORD PTR _row_diff$[ebp]
	vmovdqu	YMMWORD PTR [eax], ymm0

; 482  :   (*row_diff)[5] = diff_row_dual_avx2(buf1 + 5 * stride1, buf2 + 5 * stride2, orig + 5 * stride_orig);

	imul	eax, DWORD PTR _stride_orig$[ebp], 5
	add	eax, DWORD PTR _orig$[ebp]
	push	eax
	imul	ecx, DWORD PTR _stride2$[ebp], 5
	add	ecx, DWORD PTR _buf2$[ebp]
	push	ecx
	imul	edx, DWORD PTR _stride1$[ebp], 5
	add	edx, DWORD PTR _buf1$[ebp]
	push	edx
	call	_diff_row_dual_avx2
	add	esp, 12					; 0000000cH
	mov	eax, 32					; 00000020H
	imul	ecx, eax, 5
	add	ecx, DWORD PTR _row_diff$[ebp]
	vmovdqu	YMMWORD PTR [ecx], ymm0

; 483  :   (*row_diff)[6] = diff_row_dual_avx2(buf1 + 6 * stride1, buf2 + 6 * stride2, orig + 6 * stride_orig);

	imul	eax, DWORD PTR _stride_orig$[ebp], 6
	add	eax, DWORD PTR _orig$[ebp]
	push	eax
	imul	ecx, DWORD PTR _stride2$[ebp], 6
	add	ecx, DWORD PTR _buf2$[ebp]
	push	ecx
	imul	edx, DWORD PTR _stride1$[ebp], 6
	add	edx, DWORD PTR _buf1$[ebp]
	push	edx
	call	_diff_row_dual_avx2
	add	esp, 12					; 0000000cH
	mov	eax, 32					; 00000020H
	imul	ecx, eax, 6
	add	ecx, DWORD PTR _row_diff$[ebp]
	vmovdqu	YMMWORD PTR [ecx], ymm0

; 484  :   (*row_diff)[7] = diff_row_dual_avx2(buf1 + 7 * stride1, buf2 + 7 * stride2, orig + 7 * stride_orig);

	imul	eax, DWORD PTR _stride_orig$[ebp], 7
	add	eax, DWORD PTR _orig$[ebp]
	push	eax
	imul	ecx, DWORD PTR _stride2$[ebp], 7
	add	ecx, DWORD PTR _buf2$[ebp]
	push	ecx
	imul	edx, DWORD PTR _stride1$[ebp], 7
	add	edx, DWORD PTR _buf1$[ebp]
	push	edx
	call	_diff_row_dual_avx2
	add	esp, 12					; 0000000cH
	mov	eax, 32					; 00000020H
	imul	ecx, eax, 7
	add	ecx, DWORD PTR _row_diff$[ebp]
	vmovdqu	YMMWORD PTR [ecx], ymm0

; 485  : 
; 486  : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 192				; 000000c0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_diff_blocks_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _diff_blocks_avx2
_TEXT	SEGMENT
_row_diff$ = 8						; size = 4
_buf1$ = 12						; size = 4
_stride1$ = 16						; size = 4
_orig$ = 20						; size = 4
_stride_orig$ = 24					; size = 4
_diff_blocks_avx2 PROC					; COMDAT

; 460  : {

	push	ebp
	mov	ebp, esp
	sub	esp, 192				; 000000c0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-192]
	mov	ecx, 48					; 00000030H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 461  :   (*row_diff)[0] = diff_row_avx2(buf1 + 0 * stride1, orig + 0 * stride_orig);

	imul	eax, DWORD PTR _stride_orig$[ebp], 0
	add	eax, DWORD PTR _orig$[ebp]
	push	eax
	imul	ecx, DWORD PTR _stride1$[ebp], 0
	add	ecx, DWORD PTR _buf1$[ebp]
	push	ecx
	call	_diff_row_avx2
	add	esp, 8
	mov	edx, 16					; 00000010H
	imul	eax, edx, 0
	add	eax, DWORD PTR _row_diff$[ebp]
	vmovdqu	XMMWORD PTR [eax], xmm0

; 462  :   (*row_diff)[1] = diff_row_avx2(buf1 + 1 * stride1, orig + 1 * stride_orig);

	mov	eax, DWORD PTR _orig$[ebp]
	add	eax, DWORD PTR _stride_orig$[ebp]
	push	eax
	mov	ecx, DWORD PTR _buf1$[ebp]
	add	ecx, DWORD PTR _stride1$[ebp]
	push	ecx
	call	_diff_row_avx2
	add	esp, 8
	mov	edx, 16					; 00000010H
	shl	edx, 0
	add	edx, DWORD PTR _row_diff$[ebp]
	vmovdqu	XMMWORD PTR [edx], xmm0

; 463  :   (*row_diff)[2] = diff_row_avx2(buf1 + 2 * stride1, orig + 2 * stride_orig);

	mov	eax, DWORD PTR _stride_orig$[ebp]
	mov	ecx, DWORD PTR _orig$[ebp]
	lea	edx, DWORD PTR [ecx+eax*2]
	push	edx
	mov	eax, DWORD PTR _stride1$[ebp]
	mov	ecx, DWORD PTR _buf1$[ebp]
	lea	edx, DWORD PTR [ecx+eax*2]
	push	edx
	call	_diff_row_avx2
	add	esp, 8
	mov	eax, 16					; 00000010H
	shl	eax, 1
	add	eax, DWORD PTR _row_diff$[ebp]
	vmovdqu	XMMWORD PTR [eax], xmm0

; 464  :   (*row_diff)[3] = diff_row_avx2(buf1 + 3 * stride1, orig + 3 * stride_orig);

	imul	eax, DWORD PTR _stride_orig$[ebp], 3
	add	eax, DWORD PTR _orig$[ebp]
	push	eax
	imul	ecx, DWORD PTR _stride1$[ebp], 3
	add	ecx, DWORD PTR _buf1$[ebp]
	push	ecx
	call	_diff_row_avx2
	add	esp, 8
	mov	edx, 16					; 00000010H
	imul	eax, edx, 3
	add	eax, DWORD PTR _row_diff$[ebp]
	vmovdqu	XMMWORD PTR [eax], xmm0

; 465  :   (*row_diff)[4] = diff_row_avx2(buf1 + 4 * stride1, orig + 4 * stride_orig);

	mov	eax, DWORD PTR _stride_orig$[ebp]
	mov	ecx, DWORD PTR _orig$[ebp]
	lea	edx, DWORD PTR [ecx+eax*4]
	push	edx
	mov	eax, DWORD PTR _stride1$[ebp]
	mov	ecx, DWORD PTR _buf1$[ebp]
	lea	edx, DWORD PTR [ecx+eax*4]
	push	edx
	call	_diff_row_avx2
	add	esp, 8
	mov	eax, 16					; 00000010H
	shl	eax, 2
	add	eax, DWORD PTR _row_diff$[ebp]
	vmovdqu	XMMWORD PTR [eax], xmm0

; 466  :   (*row_diff)[5] = diff_row_avx2(buf1 + 5 * stride1, orig + 5 * stride_orig);

	imul	eax, DWORD PTR _stride_orig$[ebp], 5
	add	eax, DWORD PTR _orig$[ebp]
	push	eax
	imul	ecx, DWORD PTR _stride1$[ebp], 5
	add	ecx, DWORD PTR _buf1$[ebp]
	push	ecx
	call	_diff_row_avx2
	add	esp, 8
	mov	edx, 16					; 00000010H
	imul	eax, edx, 5
	add	eax, DWORD PTR _row_diff$[ebp]
	vmovdqu	XMMWORD PTR [eax], xmm0

; 467  :   (*row_diff)[6] = diff_row_avx2(buf1 + 6 * stride1, orig + 6 * stride_orig);

	imul	eax, DWORD PTR _stride_orig$[ebp], 6
	add	eax, DWORD PTR _orig$[ebp]
	push	eax
	imul	ecx, DWORD PTR _stride1$[ebp], 6
	add	ecx, DWORD PTR _buf1$[ebp]
	push	ecx
	call	_diff_row_avx2
	add	esp, 8
	mov	edx, 16					; 00000010H
	imul	eax, edx, 6
	add	eax, DWORD PTR _row_diff$[ebp]
	vmovdqu	XMMWORD PTR [eax], xmm0

; 468  :   (*row_diff)[7] = diff_row_avx2(buf1 + 7 * stride1, orig + 7 * stride_orig);

	imul	eax, DWORD PTR _stride_orig$[ebp], 7
	add	eax, DWORD PTR _orig$[ebp]
	push	eax
	imul	ecx, DWORD PTR _stride1$[ebp], 7
	add	ecx, DWORD PTR _buf1$[ebp]
	push	ecx
	call	_diff_row_avx2
	add	esp, 8
	mov	edx, 16					; 00000010H
	imul	eax, edx, 7
	add	eax, DWORD PTR _row_diff$[ebp]
	vmovdqu	XMMWORD PTR [eax], xmm0

; 469  : 
; 470  : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 192				; 000000c0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_diff_blocks_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _diff_row_dual_avx2
_TEXT	SEGMENT
_buf2_row$ = -224					; size = 32
_buf1_row$ = -160					; size = 32
_temp3$ = -96						; size = 16
_temp2$ = -64						; size = 16
_temp1$ = -32						; size = 16
_buf1$ = 8						; size = 4
_buf2$ = 12						; size = 4
_orig$ = 16						; size = 4
_diff_row_dual_avx2 PROC				; COMDAT

; 447  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 440				; 000001b8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-440]
	mov	ecx, 110				; 0000006eH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 448  :   __m128i temp1 = _mm_loadl_epi64((__m128i*)buf1);

	mov	eax, DWORD PTR _buf1$[ebx]
	vmovq	xmm0, QWORD PTR [eax]
	vmovdqa	XMMWORD PTR _temp1$[ebp], xmm0

; 449  :   __m128i temp2 = _mm_loadl_epi64((__m128i*)buf2);

	mov	eax, DWORD PTR _buf2$[ebx]
	vmovq	xmm0, QWORD PTR [eax]
	vmovdqa	XMMWORD PTR _temp2$[ebp], xmm0

; 450  :   __m128i temp3 = _mm_loadl_epi64((__m128i*)orig);

	mov	eax, DWORD PTR _orig$[ebx]
	vmovq	xmm0, QWORD PTR [eax]
	vmovdqa	XMMWORD PTR _temp3$[ebp], xmm0

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vmovdqa	xmm0, XMMWORD PTR _temp1$[ebp]
	vpunpcklqdq xmm0, xmm0, XMMWORD PTR _temp2$[ebp]
	vpmovzxbw ymm0, xmm0
	vmovdqu	YMMWORD PTR _buf1_row$[ebp], ymm0

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpbroadcastq xmm0, XMMWORD PTR _temp3$[ebp]
	vpmovzxbw ymm0, xmm0
	vmovdqu	YMMWORD PTR _buf2_row$[ebp], ymm0

; 453  : 
; 454  :   return _mm256_sub_epi16(buf1_row, buf2_row);

	vmovdqu	ymm0, YMMWORD PTR _buf1_row$[ebp]
	vpsubw	ymm0, ymm0, YMMWORD PTR _buf2_row$[ebp]

; 455  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_diff_row_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _diff_row_avx2
_TEXT	SEGMENT
_buf2_row$ = -64					; size = 16
_buf1_row$ = -32					; size = 16
_buf1$ = 8						; size = 4
_buf2$ = 12						; size = 4
_diff_row_avx2 PROC					; COMDAT

; 440  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 264				; 00000108H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-264]
	mov	ecx, 66					; 00000042H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	mov	eax, DWORD PTR _buf1$[ebx]
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxbw xmm0, xmm0
	vmovdqa	XMMWORD PTR _buf1_row$[ebp], xmm0

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	mov	eax, DWORD PTR _buf2$[ebx]
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxbw xmm0, xmm0
	vmovdqa	XMMWORD PTR _buf2_row$[ebp], xmm0

; 443  :   return _mm_sub_epi16(buf1_row, buf2_row);

	vmovdqa	xmm0, XMMWORD PTR _buf1_row$[ebp]
	vpsubw	xmm0, xmm0, XMMWORD PTR _buf2_row$[ebp]

; 444  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_diff_row_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _sum_block_dual_avx2
_TEXT	SEGMENT
_sad$ = -64						; size = 32
_ver_row$ = 8						; size = 4
_sum0$ = 12						; size = 4
_sum1$ = 16						; size = 4
_sum_block_dual_avx2 PROC				; COMDAT

; 421  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 280				; 00000118H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-280]
	mov	ecx, 70					; 00000046H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 422  :   __m256i sad = _mm256_setzero_si256();

	vpxor	xmm0, xmm0, xmm0
	vmovdqu	YMMWORD PTR _sad$[ebp], ymm0

; 423  :   haddwd_accumulate_dual_avx2(&sad, ver_row + 0);

	mov	eax, DWORD PTR _ver_row$[ebx]
	push	eax
	lea	ecx, DWORD PTR _sad$[ebp]
	push	ecx
	call	_haddwd_accumulate_dual_avx2
	add	esp, 8

; 424  :   haddwd_accumulate_dual_avx2(&sad, ver_row + 1);

	mov	eax, DWORD PTR _ver_row$[ebx]
	add	eax, 32					; 00000020H
	push	eax
	lea	ecx, DWORD PTR _sad$[ebp]
	push	ecx
	call	_haddwd_accumulate_dual_avx2
	add	esp, 8

; 425  :   haddwd_accumulate_dual_avx2(&sad, ver_row + 2);

	mov	eax, DWORD PTR _ver_row$[ebx]
	add	eax, 64					; 00000040H
	push	eax
	lea	ecx, DWORD PTR _sad$[ebp]
	push	ecx
	call	_haddwd_accumulate_dual_avx2
	add	esp, 8

; 426  :   haddwd_accumulate_dual_avx2(&sad, ver_row + 3); 

	mov	eax, DWORD PTR _ver_row$[ebx]
	add	eax, 96					; 00000060H
	push	eax
	lea	ecx, DWORD PTR _sad$[ebp]
	push	ecx
	call	_haddwd_accumulate_dual_avx2
	add	esp, 8

; 427  :   haddwd_accumulate_dual_avx2(&sad, ver_row + 4);

	mov	eax, DWORD PTR _ver_row$[ebx]
	add	eax, 128				; 00000080H
	push	eax
	lea	ecx, DWORD PTR _sad$[ebp]
	push	ecx
	call	_haddwd_accumulate_dual_avx2
	add	esp, 8

; 428  :   haddwd_accumulate_dual_avx2(&sad, ver_row + 5);

	mov	eax, DWORD PTR _ver_row$[ebx]
	add	eax, 160				; 000000a0H
	push	eax
	lea	ecx, DWORD PTR _sad$[ebp]
	push	ecx
	call	_haddwd_accumulate_dual_avx2
	add	esp, 8

; 429  :   haddwd_accumulate_dual_avx2(&sad, ver_row + 6);

	mov	eax, DWORD PTR _ver_row$[ebx]
	add	eax, 192				; 000000c0H
	push	eax
	lea	ecx, DWORD PTR _sad$[ebp]
	push	ecx
	call	_haddwd_accumulate_dual_avx2
	add	esp, 8

; 430  :   haddwd_accumulate_dual_avx2(&sad, ver_row + 7);

	mov	eax, DWORD PTR _ver_row$[ebx]
	add	eax, 224				; 000000e0H
	push	eax
	lea	ecx, DWORD PTR _sad$[ebp]
	push	ecx
	call	_haddwd_accumulate_dual_avx2
	add	esp, 8

; 431  : 
; 432  :   sad = _mm256_add_epi32(sad, _mm256_shuffle_epi32(sad, _MM_SHUFFLE(1, 0, 3, 2)));

	vpshufd	ymm0, YMMWORD PTR _sad$[ebp], 78	; 0000004eH
	vmovdqu	ymm1, YMMWORD PTR _sad$[ebp]
	vpaddd	ymm0, ymm1, ymm0
	vmovdqu	YMMWORD PTR _sad$[ebp], ymm0

; 433  :   sad = _mm256_add_epi32(sad, _mm256_shuffle_epi32(sad, _MM_SHUFFLE(0, 1, 0, 1)));

	vpshufd	ymm0, YMMWORD PTR _sad$[ebp], 17	; 00000011H
	vmovdqu	ymm1, YMMWORD PTR _sad$[ebp]
	vpaddd	ymm0, ymm1, ymm0
	vmovdqu	YMMWORD PTR _sad$[ebp], ymm0

; 434  : 
; 435  :   *sum0 = _mm_cvtsi128_si32(_mm256_extracti128_si256(sad, 0));

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0
	mov	ecx, DWORD PTR _sum0$[ebx]
	mov	DWORD PTR [ecx], eax

; 436  :   *sum1 = _mm_cvtsi128_si32(_mm256_extracti128_si256(sad, 1));

	vmovdqu	ymm0, YMMWORD PTR _sad$[ebp]
	vextracti128 xmm0, ymm0, 1
	vmovd	eax, xmm0
	mov	ecx, DWORD PTR _sum1$[ebx]
	mov	DWORD PTR [ecx], eax

; 437  : }

	push	edx
	mov	ecx, ebp
	push	eax
	lea	edx, DWORD PTR $LN5@sum_block_
	call	@_RTC_CheckStackVars@8
	pop	eax
	pop	edx
	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
$LN5@sum_block_:
	DD	1
	DD	$LN4@sum_block_
$LN4@sum_block_:
	DD	-64					; ffffffc0H
	DD	32					; 00000020H
	DD	$LN3@sum_block_
$LN3@sum_block_:
	DB	115					; 00000073H
	DB	97					; 00000061H
	DB	100					; 00000064H
	DB	0
_sum_block_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _sum_block_avx2
_TEXT	SEGMENT
_sad$ = -32						; size = 16
_ver_row$ = 8						; size = 4
_sum_block_avx2 PROC					; COMDAT

; 403  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 232				; 000000e8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-232]
	mov	ecx, 58					; 0000003aH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 404  :   __m128i sad = _mm_setzero_si128();

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 405  :   haddwd_accumulate_avx2(&sad, ver_row + 0);

	mov	eax, DWORD PTR _ver_row$[ebx]
	push	eax
	lea	ecx, DWORD PTR _sad$[ebp]
	push	ecx
	call	_haddwd_accumulate_avx2
	add	esp, 8

; 406  :   haddwd_accumulate_avx2(&sad, ver_row + 1);

	mov	eax, DWORD PTR _ver_row$[ebx]
	add	eax, 16					; 00000010H
	push	eax
	lea	ecx, DWORD PTR _sad$[ebp]
	push	ecx
	call	_haddwd_accumulate_avx2
	add	esp, 8

; 407  :   haddwd_accumulate_avx2(&sad, ver_row + 2);

	mov	eax, DWORD PTR _ver_row$[ebx]
	add	eax, 32					; 00000020H
	push	eax
	lea	ecx, DWORD PTR _sad$[ebp]
	push	ecx
	call	_haddwd_accumulate_avx2
	add	esp, 8

; 408  :   haddwd_accumulate_avx2(&sad, ver_row + 3); 

	mov	eax, DWORD PTR _ver_row$[ebx]
	add	eax, 48					; 00000030H
	push	eax
	lea	ecx, DWORD PTR _sad$[ebp]
	push	ecx
	call	_haddwd_accumulate_avx2
	add	esp, 8

; 409  :   haddwd_accumulate_avx2(&sad, ver_row + 4);

	mov	eax, DWORD PTR _ver_row$[ebx]
	add	eax, 64					; 00000040H
	push	eax
	lea	ecx, DWORD PTR _sad$[ebp]
	push	ecx
	call	_haddwd_accumulate_avx2
	add	esp, 8

; 410  :   haddwd_accumulate_avx2(&sad, ver_row + 5);

	mov	eax, DWORD PTR _ver_row$[ebx]
	add	eax, 80					; 00000050H
	push	eax
	lea	ecx, DWORD PTR _sad$[ebp]
	push	ecx
	call	_haddwd_accumulate_avx2
	add	esp, 8

; 411  :   haddwd_accumulate_avx2(&sad, ver_row + 6);

	mov	eax, DWORD PTR _ver_row$[ebx]
	add	eax, 96					; 00000060H
	push	eax
	lea	ecx, DWORD PTR _sad$[ebp]
	push	ecx
	call	_haddwd_accumulate_avx2
	add	esp, 8

; 412  :   haddwd_accumulate_avx2(&sad, ver_row + 7);

	mov	eax, DWORD PTR _ver_row$[ebx]
	add	eax, 112				; 00000070H
	push	eax
	lea	ecx, DWORD PTR _sad$[ebp]
	push	ecx
	call	_haddwd_accumulate_avx2
	add	esp, 8

; 413  : 
; 414  :   sad = _mm_add_epi32(sad, _mm_shuffle_epi32(sad, _MM_SHUFFLE(1, 0, 3, 2)));

	vpshufd	xmm0, XMMWORD PTR _sad$[ebp], 78	; 0000004eH
	vmovdqa	xmm1, XMMWORD PTR _sad$[ebp]
	vpaddd	xmm0, xmm1, xmm0
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 415  :   sad = _mm_add_epi32(sad, _mm_shuffle_epi32(sad, _MM_SHUFFLE(0, 1, 0, 1)));

	vpshufd	xmm0, XMMWORD PTR _sad$[ebp], 17	; 00000011H
	vmovdqa	xmm1, XMMWORD PTR _sad$[ebp]
	vpaddd	xmm0, xmm1, xmm0
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 416  : 
; 417  :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 418  : }

	push	edx
	mov	ecx, ebp
	push	eax
	lea	edx, DWORD PTR $LN5@sum_block_
	call	@_RTC_CheckStackVars@8
	pop	eax
	pop	edx
	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
	npad	1
$LN5@sum_block_:
	DD	1
	DD	$LN4@sum_block_
$LN4@sum_block_:
	DD	-32					; ffffffe0H
	DD	16					; 00000010H
	DD	$LN3@sum_block_
$LN3@sum_block_:
	DB	115					; 00000073H
	DB	97					; 00000061H
	DB	100					; 00000064H
	DB	0
_sum_block_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _haddwd_accumulate_dual_avx2
_TEXT	SEGMENT
_abs_value$ = -64					; size = 32
_accumulate$ = 8					; size = 4
_ver_row$ = 12						; size = 4
_haddwd_accumulate_dual_avx2 PROC			; COMDAT

; 397  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 280				; 00000118H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-280]
	mov	ecx, 70					; 00000046H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 398  :   __m256i abs_value = _mm256_abs_epi16(*ver_row);

	mov	eax, DWORD PTR _ver_row$[ebx]
	vpabsw	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _abs_value$[ebp], ymm0

; 399  :   *accumulate = _mm256_add_epi32(*accumulate, _mm256_madd_epi16(abs_value, _mm256_set1_epi16(1)));

	vmovdqu	ymm0, YMMWORD PTR _abs_value$[ebp]
	vpmaddwd ymm0, ymm0, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	mov	eax, DWORD PTR _accumulate$[ebx]
	vmovdqu	ymm1, YMMWORD PTR [eax]
	vpaddd	ymm0, ymm1, ymm0
	mov	ecx, DWORD PTR _accumulate$[ebx]
	vmovdqu	YMMWORD PTR [ecx], ymm0

; 400  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_haddwd_accumulate_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _haddwd_accumulate_avx2
_TEXT	SEGMENT
_abs_value$ = -32					; size = 16
_accumulate$ = 8					; size = 4
_ver_row$ = 12						; size = 4
_haddwd_accumulate_avx2 PROC				; COMDAT

; 391  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 232				; 000000e8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-232]
	mov	ecx, 58					; 0000003aH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 392  :   __m128i abs_value = _mm_abs_epi16(*ver_row);

	mov	eax, DWORD PTR _ver_row$[ebx]
	vpabsw	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _abs_value$[ebp], xmm0

; 393  :   *accumulate = _mm_add_epi32(*accumulate, _mm_madd_epi16(abs_value, _mm_set1_epi16(1)));

	vmovdqa	xmm0, XMMWORD PTR _abs_value$[ebp]
	vpmaddwd xmm0, xmm0, XMMWORD PTR __xmm@00010001000100010001000100010001
	mov	eax, DWORD PTR _accumulate$[ebx]
	vmovdqu	xmm1, XMMWORD PTR [eax]
	vpaddd	xmm0, xmm1, xmm0
	mov	ecx, DWORD PTR _accumulate$[ebx]
	vmovdqu	XMMWORD PTR [ecx], xmm0

; 394  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_haddwd_accumulate_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _ver_transform_block_dual_avx2
_TEXT	SEGMENT
_temp1$ = -576						; size = 256
_temp0$ = -288						; size = 256
__$ArrayPad$ = -4					; size = 4
_rows$ = 8						; size = 4
_ver_transform_block_dual_avx2 PROC			; COMDAT

; 369  : static INLINE void ver_transform_block_dual_avx2(__m256i (*rows)[8]){

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 984				; 000003d8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-984]
	mov	ecx, 246				; 000000f6H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	eax, DWORD PTR ___security_cookie
	xor	eax, ebp
	mov	DWORD PTR __$ArrayPad$[ebp], eax
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 370  : 
; 371  :   __m256i temp0[8];
; 372  :   add_sub_dual_avx2(temp0, (*rows), 0, 1, 0, 1);

	push	1
	push	0
	push	1
	push	0
	mov	eax, DWORD PTR _rows$[ebx]
	push	eax
	lea	ecx, DWORD PTR _temp0$[ebp]
	push	ecx
	call	_add_sub_dual_avx2
	add	esp, 24					; 00000018H

; 373  :   add_sub_dual_avx2(temp0, (*rows), 2, 3, 2, 3);

	push	3
	push	2
	push	3
	push	2
	mov	eax, DWORD PTR _rows$[ebx]
	push	eax
	lea	ecx, DWORD PTR _temp0$[ebp]
	push	ecx
	call	_add_sub_dual_avx2
	add	esp, 24					; 00000018H

; 374  :   add_sub_dual_avx2(temp0, (*rows), 4, 5, 4, 5);

	push	5
	push	4
	push	5
	push	4
	mov	eax, DWORD PTR _rows$[ebx]
	push	eax
	lea	ecx, DWORD PTR _temp0$[ebp]
	push	ecx
	call	_add_sub_dual_avx2
	add	esp, 24					; 00000018H

; 375  :   add_sub_dual_avx2(temp0, (*rows), 6, 7, 6, 7);

	push	7
	push	6
	push	7
	push	6
	mov	eax, DWORD PTR _rows$[ebx]
	push	eax
	lea	ecx, DWORD PTR _temp0$[ebp]
	push	ecx
	call	_add_sub_dual_avx2
	add	esp, 24					; 00000018H

; 376  : 
; 377  :   __m256i temp1[8];
; 378  :   add_sub_dual_avx2(temp1, temp0, 0, 1, 0, 2);

	push	2
	push	0
	push	1
	push	0
	lea	eax, DWORD PTR _temp0$[ebp]
	push	eax
	lea	ecx, DWORD PTR _temp1$[ebp]
	push	ecx
	call	_add_sub_dual_avx2
	add	esp, 24					; 00000018H

; 379  :   add_sub_dual_avx2(temp1, temp0, 2, 3, 1, 3);

	push	3
	push	1
	push	3
	push	2
	lea	eax, DWORD PTR _temp0$[ebp]
	push	eax
	lea	ecx, DWORD PTR _temp1$[ebp]
	push	ecx
	call	_add_sub_dual_avx2
	add	esp, 24					; 00000018H

; 380  :   add_sub_dual_avx2(temp1, temp0, 4, 5, 4, 6);

	push	6
	push	4
	push	5
	push	4
	lea	eax, DWORD PTR _temp0$[ebp]
	push	eax
	lea	ecx, DWORD PTR _temp1$[ebp]
	push	ecx
	call	_add_sub_dual_avx2
	add	esp, 24					; 00000018H

; 381  :   add_sub_dual_avx2(temp1, temp0, 6, 7, 5, 7);

	push	7
	push	5
	push	7
	push	6
	lea	eax, DWORD PTR _temp0$[ebp]
	push	eax
	lea	ecx, DWORD PTR _temp1$[ebp]
	push	ecx
	call	_add_sub_dual_avx2
	add	esp, 24					; 00000018H

; 382  : 
; 383  :   add_sub_dual_avx2((*rows), temp1, 0, 1, 0, 4);

	push	4
	push	0
	push	1
	push	0
	lea	eax, DWORD PTR _temp1$[ebp]
	push	eax
	mov	ecx, DWORD PTR _rows$[ebx]
	push	ecx
	call	_add_sub_dual_avx2
	add	esp, 24					; 00000018H

; 384  :   add_sub_dual_avx2((*rows), temp1, 2, 3, 1, 5);

	push	5
	push	1
	push	3
	push	2
	lea	eax, DWORD PTR _temp1$[ebp]
	push	eax
	mov	ecx, DWORD PTR _rows$[ebx]
	push	ecx
	call	_add_sub_dual_avx2
	add	esp, 24					; 00000018H

; 385  :   add_sub_dual_avx2((*rows), temp1, 4, 5, 2, 6);

	push	6
	push	2
	push	5
	push	4
	lea	eax, DWORD PTR _temp1$[ebp]
	push	eax
	mov	ecx, DWORD PTR _rows$[ebx]
	push	ecx
	call	_add_sub_dual_avx2
	add	esp, 24					; 00000018H

; 386  :   add_sub_dual_avx2((*rows), temp1, 6, 7, 3, 7);

	push	7
	push	3
	push	7
	push	6
	lea	eax, DWORD PTR _temp1$[ebp]
	push	eax
	mov	ecx, DWORD PTR _rows$[ebx]
	push	ecx
	call	_add_sub_dual_avx2
	add	esp, 24					; 00000018H

; 387  :   
; 388  : }

	push	edx
	mov	ecx, ebp
	push	eax
	lea	edx, DWORD PTR $LN6@ver_transf
	call	@_RTC_CheckStackVars@8
	pop	eax
	pop	edx
	pop	edi
	pop	esi
	mov	ecx, DWORD PTR __$ArrayPad$[ebp]
	xor	ecx, ebp
	call	@__security_check_cookie@4
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
$LN6@ver_transf:
	DD	2
	DD	$LN5@ver_transf
$LN5@ver_transf:
	DD	-288					; fffffee0H
	DD	256					; 00000100H
	DD	$LN3@ver_transf
	DD	-576					; fffffdc0H
	DD	256					; 00000100H
	DD	$LN4@ver_transf
$LN4@ver_transf:
	DB	116					; 00000074H
	DB	101					; 00000065H
	DB	109					; 0000006dH
	DB	112					; 00000070H
	DB	49					; 00000031H
	DB	0
$LN3@ver_transf:
	DB	116					; 00000074H
	DB	101					; 00000065H
	DB	109					; 0000006dH
	DB	112					; 00000070H
	DB	48					; 00000030H
	DB	0
_ver_transform_block_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _add_sub_dual_avx2
_TEXT	SEGMENT
_out$ = 8						; size = 4
_in$ = 12						; size = 4
_out_idx0$ = 16						; size = 4
_out_idx1$ = 20						; size = 4
_in_idx0$ = 24						; size = 4
_in_idx1$ = 28						; size = 4
_add_sub_dual_avx2 PROC					; COMDAT

; 363  : {

	push	ebp
	mov	ebp, esp
	sub	esp, 192				; 000000c0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-192]
	mov	ecx, 48					; 00000030H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 364  :   out[out_idx0] = _mm256_add_epi16(in[in_idx0], in[in_idx1]);

	mov	eax, DWORD PTR _in_idx1$[ebp]
	shl	eax, 5
	add	eax, DWORD PTR _in$[ebp]
	mov	ecx, DWORD PTR _in_idx0$[ebp]
	shl	ecx, 5
	add	ecx, DWORD PTR _in$[ebp]
	vmovdqu	ymm0, YMMWORD PTR [ecx]
	vpaddw	ymm0, ymm0, YMMWORD PTR [eax]
	mov	edx, DWORD PTR _out_idx0$[ebp]
	shl	edx, 5
	add	edx, DWORD PTR _out$[ebp]
	vmovdqu	YMMWORD PTR [edx], ymm0

; 365  :   out[out_idx1] = _mm256_sub_epi16(in[in_idx0], in[in_idx1]);

	mov	eax, DWORD PTR _in_idx1$[ebp]
	shl	eax, 5
	add	eax, DWORD PTR _in$[ebp]
	mov	ecx, DWORD PTR _in_idx0$[ebp]
	shl	ecx, 5
	add	ecx, DWORD PTR _in$[ebp]
	vmovdqu	ymm0, YMMWORD PTR [ecx]
	vpsubw	ymm0, ymm0, YMMWORD PTR [eax]
	mov	edx, DWORD PTR _out_idx1$[ebp]
	shl	edx, 5
	add	edx, DWORD PTR _out$[ebp]
	vmovdqu	YMMWORD PTR [edx], ymm0

; 366  : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 192				; 000000c0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_add_sub_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _ver_transform_block_avx2
_TEXT	SEGMENT
_temp1$ = -288						; size = 128
_temp0$ = -144						; size = 128
__$ArrayPad$ = -4					; size = 4
_rows$ = 8						; size = 4
_ver_transform_block_avx2 PROC				; COMDAT

; 341  : static INLINE void ver_transform_block_avx2(__m128i (*rows)[8]){

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 488				; 000001e8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-488]
	mov	ecx, 122				; 0000007aH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	eax, DWORD PTR ___security_cookie
	xor	eax, ebp
	mov	DWORD PTR __$ArrayPad$[ebp], eax
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 342  : 
; 343  :   __m128i temp0[8];
; 344  :   add_sub_avx2(temp0, (*rows), 0, 1, 0, 1);

	push	1
	push	0
	push	1
	push	0
	mov	eax, DWORD PTR _rows$[ebx]
	push	eax
	lea	ecx, DWORD PTR _temp0$[ebp]
	push	ecx
	call	_add_sub_avx2
	add	esp, 24					; 00000018H

; 345  :   add_sub_avx2(temp0, (*rows), 2, 3, 2, 3);

	push	3
	push	2
	push	3
	push	2
	mov	eax, DWORD PTR _rows$[ebx]
	push	eax
	lea	ecx, DWORD PTR _temp0$[ebp]
	push	ecx
	call	_add_sub_avx2
	add	esp, 24					; 00000018H

; 346  :   add_sub_avx2(temp0, (*rows), 4, 5, 4, 5);

	push	5
	push	4
	push	5
	push	4
	mov	eax, DWORD PTR _rows$[ebx]
	push	eax
	lea	ecx, DWORD PTR _temp0$[ebp]
	push	ecx
	call	_add_sub_avx2
	add	esp, 24					; 00000018H

; 347  :   add_sub_avx2(temp0, (*rows), 6, 7, 6, 7);

	push	7
	push	6
	push	7
	push	6
	mov	eax, DWORD PTR _rows$[ebx]
	push	eax
	lea	ecx, DWORD PTR _temp0$[ebp]
	push	ecx
	call	_add_sub_avx2
	add	esp, 24					; 00000018H

; 348  : 
; 349  :   __m128i temp1[8];
; 350  :   add_sub_avx2(temp1, temp0, 0, 1, 0, 2);

	push	2
	push	0
	push	1
	push	0
	lea	eax, DWORD PTR _temp0$[ebp]
	push	eax
	lea	ecx, DWORD PTR _temp1$[ebp]
	push	ecx
	call	_add_sub_avx2
	add	esp, 24					; 00000018H

; 351  :   add_sub_avx2(temp1, temp0, 2, 3, 1, 3);

	push	3
	push	1
	push	3
	push	2
	lea	eax, DWORD PTR _temp0$[ebp]
	push	eax
	lea	ecx, DWORD PTR _temp1$[ebp]
	push	ecx
	call	_add_sub_avx2
	add	esp, 24					; 00000018H

; 352  :   add_sub_avx2(temp1, temp0, 4, 5, 4, 6);

	push	6
	push	4
	push	5
	push	4
	lea	eax, DWORD PTR _temp0$[ebp]
	push	eax
	lea	ecx, DWORD PTR _temp1$[ebp]
	push	ecx
	call	_add_sub_avx2
	add	esp, 24					; 00000018H

; 353  :   add_sub_avx2(temp1, temp0, 6, 7, 5, 7);

	push	7
	push	5
	push	7
	push	6
	lea	eax, DWORD PTR _temp0$[ebp]
	push	eax
	lea	ecx, DWORD PTR _temp1$[ebp]
	push	ecx
	call	_add_sub_avx2
	add	esp, 24					; 00000018H

; 354  : 
; 355  :   add_sub_avx2((*rows), temp1, 0, 1, 0, 4);

	push	4
	push	0
	push	1
	push	0
	lea	eax, DWORD PTR _temp1$[ebp]
	push	eax
	mov	ecx, DWORD PTR _rows$[ebx]
	push	ecx
	call	_add_sub_avx2
	add	esp, 24					; 00000018H

; 356  :   add_sub_avx2((*rows), temp1, 2, 3, 1, 5);

	push	5
	push	1
	push	3
	push	2
	lea	eax, DWORD PTR _temp1$[ebp]
	push	eax
	mov	ecx, DWORD PTR _rows$[ebx]
	push	ecx
	call	_add_sub_avx2
	add	esp, 24					; 00000018H

; 357  :   add_sub_avx2((*rows), temp1, 4, 5, 2, 6);

	push	6
	push	2
	push	5
	push	4
	lea	eax, DWORD PTR _temp1$[ebp]
	push	eax
	mov	ecx, DWORD PTR _rows$[ebx]
	push	ecx
	call	_add_sub_avx2
	add	esp, 24					; 00000018H

; 358  :   add_sub_avx2((*rows), temp1, 6, 7, 3, 7);

	push	7
	push	3
	push	7
	push	6
	lea	eax, DWORD PTR _temp1$[ebp]
	push	eax
	mov	ecx, DWORD PTR _rows$[ebx]
	push	ecx
	call	_add_sub_avx2
	add	esp, 24					; 00000018H

; 359  :   
; 360  : }

	push	edx
	mov	ecx, ebp
	push	eax
	lea	edx, DWORD PTR $LN6@ver_transf
	call	@_RTC_CheckStackVars@8
	pop	eax
	pop	edx
	pop	edi
	pop	esi
	mov	ecx, DWORD PTR __$ArrayPad$[ebp]
	xor	ecx, ebp
	call	@__security_check_cookie@4
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
$LN6@ver_transf:
	DD	2
	DD	$LN5@ver_transf
$LN5@ver_transf:
	DD	-144					; ffffff70H
	DD	128					; 00000080H
	DD	$LN3@ver_transf
	DD	-288					; fffffee0H
	DD	128					; 00000080H
	DD	$LN4@ver_transf
$LN4@ver_transf:
	DB	116					; 00000074H
	DB	101					; 00000065H
	DB	109					; 0000006dH
	DB	112					; 00000070H
	DB	49					; 00000031H
	DB	0
$LN3@ver_transf:
	DB	116					; 00000074H
	DB	101					; 00000065H
	DB	109					; 0000006dH
	DB	112					; 00000070H
	DB	48					; 00000030H
	DB	0
_ver_transform_block_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _add_sub_avx2
_TEXT	SEGMENT
_out$ = 8						; size = 4
_in$ = 12						; size = 4
_out_idx0$ = 16						; size = 4
_out_idx1$ = 20						; size = 4
_in_idx0$ = 24						; size = 4
_in_idx1$ = 28						; size = 4
_add_sub_avx2 PROC					; COMDAT

; 336  : {

	push	ebp
	mov	ebp, esp
	sub	esp, 192				; 000000c0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-192]
	mov	ecx, 48					; 00000030H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	mov	eax, DWORD PTR _in_idx1$[ebp]
	shl	eax, 4
	add	eax, DWORD PTR _in$[ebp]
	mov	ecx, DWORD PTR _in_idx0$[ebp]
	shl	ecx, 4
	add	ecx, DWORD PTR _in$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [ecx]
	vpaddw	xmm0, xmm0, XMMWORD PTR [eax]
	mov	edx, DWORD PTR _out_idx0$[ebp]
	shl	edx, 4
	add	edx, DWORD PTR _out$[ebp]
	vmovdqu	XMMWORD PTR [edx], xmm0

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	mov	eax, DWORD PTR _in_idx1$[ebp]
	shl	eax, 4
	add	eax, DWORD PTR _in$[ebp]
	mov	ecx, DWORD PTR _in_idx0$[ebp]
	shl	ecx, 4
	add	ecx, DWORD PTR _in$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [ecx]
	vpsubw	xmm0, xmm0, XMMWORD PTR [eax]
	mov	edx, DWORD PTR _out_idx1$[ebp]
	shl	edx, 4
	add	edx, DWORD PTR _out$[ebp]
	vmovdqu	XMMWORD PTR [edx], xmm0

; 339  : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 192				; 000000c0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_add_sub_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _hor_transform_row_dual_avx2
_TEXT	SEGMENT
_temp$ = -256						; size = 32
_sign_mask$ = -192					; size = 32
_mask_neg$ = -128					; size = 32
_mask_pos$ = -64					; size = 32
_row$ = 8						; size = 4
_hor_transform_row_dual_avx2 PROC			; COMDAT

; 314  : static INLINE void hor_transform_row_dual_avx2(__m256i* row){

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 472				; 000001d8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-472]
	mov	ecx, 118				; 00000076H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 315  :   
; 316  :   __m256i mask_pos = _mm256_set1_epi16(1);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vmovdqu	YMMWORD PTR _mask_pos$[ebp], ymm0

; 317  :   __m256i mask_neg = _mm256_set1_epi16(-1);

	vmovdqu	ymm0, YMMWORD PTR __ymm@ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	YMMWORD PTR _mask_neg$[ebp], ymm0

; 318  :   __m256i sign_mask = _mm256_unpacklo_epi64(mask_pos, mask_neg);

	vmovdqu	ymm0, YMMWORD PTR _mask_pos$[ebp]
	vpunpcklqdq ymm0, ymm0, YMMWORD PTR _mask_neg$[ebp]
	vmovdqu	YMMWORD PTR _sign_mask$[ebp], ymm0

; 319  :   __m256i temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	mov	eax, DWORD PTR _row$[ebx]
	vpshufd	ymm0, YMMWORD PTR [eax], 78		; 0000004eH
	vmovdqu	YMMWORD PTR _temp$[ebp], ymm0

; 320  :   *row = _mm256_sign_epi16(*row, sign_mask);

	mov	eax, DWORD PTR _row$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vpsignw	ymm0, ymm0, YMMWORD PTR _sign_mask$[ebp]
	mov	ecx, DWORD PTR _row$[ebx]
	vmovdqu	YMMWORD PTR [ecx], ymm0

; 321  :   *row = _mm256_add_epi16(*row, temp);

	mov	eax, DWORD PTR _row$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vpaddw	ymm0, ymm0, YMMWORD PTR _temp$[ebp]
	mov	ecx, DWORD PTR _row$[ebx]
	vmovdqu	YMMWORD PTR [ecx], ymm0

; 322  : 
; 323  :   sign_mask = _mm256_unpacklo_epi32(mask_pos, mask_neg);

	vmovdqu	ymm0, YMMWORD PTR _mask_pos$[ebp]
	vpunpckldq ymm0, ymm0, YMMWORD PTR _mask_neg$[ebp]
	vmovdqu	YMMWORD PTR _sign_mask$[ebp], ymm0

; 324  :   temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	mov	eax, DWORD PTR _row$[ebx]
	vpshufd	ymm0, YMMWORD PTR [eax], 177		; 000000b1H
	vmovdqu	YMMWORD PTR _temp$[ebp], ymm0

; 325  :   *row = _mm256_sign_epi16(*row, sign_mask);

	mov	eax, DWORD PTR _row$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vpsignw	ymm0, ymm0, YMMWORD PTR _sign_mask$[ebp]
	mov	ecx, DWORD PTR _row$[ebx]
	vmovdqu	YMMWORD PTR [ecx], ymm0

; 326  :   *row = _mm256_add_epi16(*row, temp);

	mov	eax, DWORD PTR _row$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vpaddw	ymm0, ymm0, YMMWORD PTR _temp$[ebp]
	mov	ecx, DWORD PTR _row$[ebx]
	vmovdqu	YMMWORD PTR [ecx], ymm0

; 327  : 
; 328  :   sign_mask = _mm256_unpacklo_epi16(mask_pos, mask_neg);

	vmovdqu	ymm0, YMMWORD PTR _mask_pos$[ebp]
	vpunpcklwd ymm0, ymm0, YMMWORD PTR _mask_neg$[ebp]
	vmovdqu	YMMWORD PTR _sign_mask$[ebp], ymm0

; 329  :   temp = _mm256_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	mov	eax, DWORD PTR _row$[ebx]
	vpshuflw ymm0, YMMWORD PTR [eax], 177		; 000000b1H
	vmovdqu	YMMWORD PTR _temp$[ebp], ymm0

; 330  :   temp = _mm256_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw ymm0, YMMWORD PTR _temp$[ebp], 177	; 000000b1H
	vmovdqu	YMMWORD PTR _temp$[ebp], ymm0

; 331  :   *row = _mm256_sign_epi16(*row, sign_mask);

	mov	eax, DWORD PTR _row$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vpsignw	ymm0, ymm0, YMMWORD PTR _sign_mask$[ebp]
	mov	ecx, DWORD PTR _row$[ebx]
	vmovdqu	YMMWORD PTR [ecx], ymm0

; 332  :   *row = _mm256_add_epi16(*row, temp);

	mov	eax, DWORD PTR _row$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vpaddw	ymm0, ymm0, YMMWORD PTR _temp$[ebp]
	mov	ecx, DWORD PTR _row$[ebx]
	vmovdqu	YMMWORD PTR [ecx], ymm0

; 333  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_hor_transform_row_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _hor_transform_row_avx2
_TEXT	SEGMENT
_temp$ = -128						; size = 16
_sign_mask$ = -96					; size = 16
_mask_neg$ = -64					; size = 16
_mask_pos$ = -32					; size = 16
_row$ = 8						; size = 4
_hor_transform_row_avx2 PROC				; COMDAT

; 293  : static INLINE void hor_transform_row_avx2(__m128i* row){

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 328				; 00000148H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-328]
	mov	ecx, 82					; 00000052H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 294  :   
; 295  :   __m128i mask_pos = _mm_set1_epi16(1);

	vmovdqa	xmm0, XMMWORD PTR __xmm@00010001000100010001000100010001
	vmovdqa	XMMWORD PTR _mask_pos$[ebp], xmm0

; 296  :   __m128i mask_neg = _mm_set1_epi16(-1);

	vmovdqa	xmm0, XMMWORD PTR __xmm@ffffffffffffffffffffffffffffffff
	vmovdqa	XMMWORD PTR _mask_neg$[ebp], xmm0

; 297  :   __m128i sign_mask = _mm_unpacklo_epi64(mask_pos, mask_neg);

	vmovdqa	xmm0, XMMWORD PTR _mask_pos$[ebp]
	vpunpcklqdq xmm0, xmm0, XMMWORD PTR _mask_neg$[ebp]
	vmovdqa	XMMWORD PTR _sign_mask$[ebp], xmm0

; 298  :   __m128i temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	mov	eax, DWORD PTR _row$[ebx]
	vpshufd	xmm0, XMMWORD PTR [eax], 78		; 0000004eH
	vmovdqa	XMMWORD PTR _temp$[ebp], xmm0

; 299  :   *row = _mm_sign_epi16(*row, sign_mask);

	mov	eax, DWORD PTR _row$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vpsignw	xmm0, xmm0, XMMWORD PTR _sign_mask$[ebp]
	mov	ecx, DWORD PTR _row$[ebx]
	vmovdqu	XMMWORD PTR [ecx], xmm0

; 300  :   *row = _mm_add_epi16(*row, temp);

	mov	eax, DWORD PTR _row$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vpaddw	xmm0, xmm0, XMMWORD PTR _temp$[ebp]
	mov	ecx, DWORD PTR _row$[ebx]
	vmovdqu	XMMWORD PTR [ecx], xmm0

; 301  : 
; 302  :   sign_mask = _mm_unpacklo_epi32(mask_pos, mask_neg);

	vmovdqa	xmm0, XMMWORD PTR _mask_pos$[ebp]
	vpunpckldq xmm0, xmm0, XMMWORD PTR _mask_neg$[ebp]
	vmovdqa	XMMWORD PTR _sign_mask$[ebp], xmm0

; 303  :   temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	mov	eax, DWORD PTR _row$[ebx]
	vpshufd	xmm0, XMMWORD PTR [eax], 177		; 000000b1H
	vmovdqa	XMMWORD PTR _temp$[ebp], xmm0

; 304  :   *row = _mm_sign_epi16(*row, sign_mask);

	mov	eax, DWORD PTR _row$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vpsignw	xmm0, xmm0, XMMWORD PTR _sign_mask$[ebp]
	mov	ecx, DWORD PTR _row$[ebx]
	vmovdqu	XMMWORD PTR [ecx], xmm0

; 305  :   *row = _mm_add_epi16(*row, temp);

	mov	eax, DWORD PTR _row$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vpaddw	xmm0, xmm0, XMMWORD PTR _temp$[ebp]
	mov	ecx, DWORD PTR _row$[ebx]
	vmovdqu	XMMWORD PTR [ecx], xmm0

; 306  : 
; 307  :   sign_mask = _mm_unpacklo_epi16(mask_pos, mask_neg);

	vmovdqa	xmm0, XMMWORD PTR _mask_pos$[ebp]
	vpunpcklwd xmm0, xmm0, XMMWORD PTR _mask_neg$[ebp]
	vmovdqa	XMMWORD PTR _sign_mask$[ebp], xmm0

; 308  :   temp = _mm_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	mov	eax, DWORD PTR _row$[ebx]
	vpshuflw xmm0, XMMWORD PTR [eax], 177		; 000000b1H
	vmovdqa	XMMWORD PTR _temp$[ebp], xmm0

; 309  :   temp = _mm_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw xmm0, XMMWORD PTR _temp$[ebp], 177	; 000000b1H
	vmovdqa	XMMWORD PTR _temp$[ebp], xmm0

; 310  :   *row = _mm_sign_epi16(*row, sign_mask);

	mov	eax, DWORD PTR _row$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vpsignw	xmm0, xmm0, XMMWORD PTR _sign_mask$[ebp]
	mov	ecx, DWORD PTR _row$[ebx]
	vmovdqu	XMMWORD PTR [ecx], xmm0

; 311  :   *row = _mm_add_epi16(*row, temp);

	mov	eax, DWORD PTR _row$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vpaddw	xmm0, xmm0, XMMWORD PTR _temp$[ebp]
	mov	ecx, DWORD PTR _row$[ebx]
	vmovdqu	XMMWORD PTR [ecx], xmm0

; 312  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_hor_transform_row_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _satd_8bit_4x4_dual_avx2
_TEXT	SEGMENT
$T1 = -976						; size = 16
$T2 = -944						; size = 16
_sum2$ = -536						; size = 4
_sum1$ = -524						; size = 4
_row3$ = -512						; size = 32
_row2$ = -448						; size = 32
_row1$ = -384						; size = 32
_row0$ = -320						; size = 32
_diff_hi$ = -256					; size = 32
_diff_lo$ = -192					; size = 32
_pred$ = -128						; size = 32
_original$ = -64					; size = 32
_preds$ = 8						; size = 4
_orig$ = 12						; size = 4
_num_modes$ = 16					; size = 4
_satds_out$ = 20					; size = 4
_satd_8bit_4x4_dual_avx2 PROC				; COMDAT

; 245  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 984				; 000003d8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-984]
	mov	ecx, 246				; 000000f6H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 246  : 
; 247  :   __m256i original = _mm256_broadcastsi128_si256(_mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)orig)));

	mov	eax, DWORD PTR _orig$[ebx]
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxbw xmm0, xmm0
	vmovdqa	XMMWORD PTR $T2[ebp], xmm0
	vbroadcasti128 ymm0, XMMWORD PTR $T2[ebp]
	vmovdqu	YMMWORD PTR _original$[ebp], ymm0

; 248  :   __m256i pred = _mm256_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)preds[0]));

	mov	eax, 1024				; 00000400H
	imul	ecx, eax, 0
	mov	edx, DWORD PTR _preds$[ebx]
	vmovq	xmm0, QWORD PTR [edx+ecx]
	vpmovzxbw ymm0, xmm0
	vmovdqu	YMMWORD PTR _pred$[ebp], ymm0

; 249  :   pred = _mm256_inserti128_si256(pred, _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)preds[1])), 1);

	mov	eax, 1024				; 00000400H
	shl	eax, 0
	mov	ecx, DWORD PTR _preds$[ebx]
	vmovq	xmm0, QWORD PTR [ecx+eax]
	vpmovzxbw xmm0, xmm0
	vmovdqu	ymm1, YMMWORD PTR _pred$[ebp]
	vinserti128 ymm0, ymm1, xmm0, 1
	vmovdqu	YMMWORD PTR _pred$[ebp], ymm0

; 250  : 
; 251  :   __m256i diff_lo = _mm256_sub_epi16(pred, original);

	vmovdqu	ymm0, YMMWORD PTR _pred$[ebp]
	vpsubw	ymm0, ymm0, YMMWORD PTR _original$[ebp]
	vmovdqu	YMMWORD PTR _diff_lo$[ebp], ymm0

; 252  : 
; 253  :   original = _mm256_broadcastsi128_si256(_mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)(orig + 8))));

	mov	eax, DWORD PTR _orig$[ebx]
	vmovq	xmm0, QWORD PTR [eax+8]
	vpmovzxbw xmm0, xmm0
	vmovdqa	XMMWORD PTR $T1[ebp], xmm0
	vbroadcasti128 ymm0, XMMWORD PTR $T1[ebp]
	vmovdqu	YMMWORD PTR _original$[ebp], ymm0

; 254  :   pred = _mm256_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)(preds[0] + 8)));

	mov	eax, 1024				; 00000400H
	imul	ecx, eax, 0
	mov	edx, DWORD PTR _preds$[ebx]
	vmovq	xmm0, QWORD PTR [edx+ecx+8]
	vpmovzxbw ymm0, xmm0
	vmovdqu	YMMWORD PTR _pred$[ebp], ymm0

; 255  :   pred = _mm256_inserti128_si256(pred, _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)(preds[1] + 8))), 1);

	mov	eax, 1024				; 00000400H
	shl	eax, 0
	mov	ecx, DWORD PTR _preds$[ebx]
	vmovq	xmm0, QWORD PTR [ecx+eax+8]
	vpmovzxbw xmm0, xmm0
	vmovdqu	ymm1, YMMWORD PTR _pred$[ebp]
	vinserti128 ymm0, ymm1, xmm0, 1
	vmovdqu	YMMWORD PTR _pred$[ebp], ymm0

; 256  : 
; 257  :   __m256i diff_hi = _mm256_sub_epi16(pred, original);

	vmovdqu	ymm0, YMMWORD PTR _pred$[ebp]
	vpsubw	ymm0, ymm0, YMMWORD PTR _original$[ebp]
	vmovdqu	YMMWORD PTR _diff_hi$[ebp], ymm0

; 258  : 
; 259  :   //Hor
; 260  :   __m256i row0 = _mm256_hadd_epi16(diff_lo, diff_hi);

	vmovdqu	ymm0, YMMWORD PTR _diff_lo$[ebp]
	vphaddw	ymm0, ymm0, YMMWORD PTR _diff_hi$[ebp]
	vmovdqu	YMMWORD PTR _row0$[ebp], ymm0

; 261  :   __m256i row1 = _mm256_hsub_epi16(diff_lo, diff_hi);

	vmovdqu	ymm0, YMMWORD PTR _diff_lo$[ebp]
	vphsubw	ymm0, ymm0, YMMWORD PTR _diff_hi$[ebp]
	vmovdqu	YMMWORD PTR _row1$[ebp], ymm0

; 262  : 
; 263  :   __m256i row2 = _mm256_hadd_epi16(row0, row1);

	vmovdqu	ymm0, YMMWORD PTR _row0$[ebp]
	vphaddw	ymm0, ymm0, YMMWORD PTR _row1$[ebp]
	vmovdqu	YMMWORD PTR _row2$[ebp], ymm0

; 264  :   __m256i row3 = _mm256_hsub_epi16(row0, row1);

	vmovdqu	ymm0, YMMWORD PTR _row0$[ebp]
	vphsubw	ymm0, ymm0, YMMWORD PTR _row1$[ebp]
	vmovdqu	YMMWORD PTR _row3$[ebp], ymm0

; 265  : 
; 266  :   //Ver
; 267  :   row0 = _mm256_hadd_epi16(row2, row3);

	vmovdqu	ymm0, YMMWORD PTR _row2$[ebp]
	vphaddw	ymm0, ymm0, YMMWORD PTR _row3$[ebp]
	vmovdqu	YMMWORD PTR _row0$[ebp], ymm0

; 268  :   row1 = _mm256_hsub_epi16(row2, row3);

	vmovdqu	ymm0, YMMWORD PTR _row2$[ebp]
	vphsubw	ymm0, ymm0, YMMWORD PTR _row3$[ebp]
	vmovdqu	YMMWORD PTR _row1$[ebp], ymm0

; 269  : 
; 270  :   row2 = _mm256_hadd_epi16(row0, row1);

	vmovdqu	ymm0, YMMWORD PTR _row0$[ebp]
	vphaddw	ymm0, ymm0, YMMWORD PTR _row1$[ebp]
	vmovdqu	YMMWORD PTR _row2$[ebp], ymm0

; 271  :   row3 = _mm256_hsub_epi16(row0, row1);

	vmovdqu	ymm0, YMMWORD PTR _row0$[ebp]
	vphsubw	ymm0, ymm0, YMMWORD PTR _row1$[ebp]
	vmovdqu	YMMWORD PTR _row3$[ebp], ymm0

; 272  : 
; 273  :   //Abs and sum
; 274  :   row2 = _mm256_abs_epi16(row2);

	vpabsw	ymm0, YMMWORD PTR _row2$[ebp]
	vmovdqu	YMMWORD PTR _row2$[ebp], ymm0

; 275  :   row3 = _mm256_abs_epi16(row3);

	vpabsw	ymm0, YMMWORD PTR _row3$[ebp]
	vmovdqu	YMMWORD PTR _row3$[ebp], ymm0

; 276  : 
; 277  :   row3 = _mm256_add_epi16(row2, row3);

	vmovdqu	ymm0, YMMWORD PTR _row2$[ebp]
	vpaddw	ymm0, ymm0, YMMWORD PTR _row3$[ebp]
	vmovdqu	YMMWORD PTR _row3$[ebp], ymm0

; 278  : 
; 279  :   row3 = _mm256_add_epi16(row3, _mm256_shuffle_epi32(row3, _MM_SHUFFLE(1, 0, 3, 2) ));

	vpshufd	ymm0, YMMWORD PTR _row3$[ebp], 78	; 0000004eH
	vmovdqu	ymm1, YMMWORD PTR _row3$[ebp]
	vpaddw	ymm0, ymm1, ymm0
	vmovdqu	YMMWORD PTR _row3$[ebp], ymm0

; 280  :   row3 = _mm256_add_epi16(row3, _mm256_shuffle_epi32(row3, _MM_SHUFFLE(0, 1, 0, 1) ));

	vpshufd	ymm0, YMMWORD PTR _row3$[ebp], 17	; 00000011H
	vmovdqu	ymm1, YMMWORD PTR _row3$[ebp]
	vpaddw	ymm0, ymm1, ymm0
	vmovdqu	YMMWORD PTR _row3$[ebp], ymm0

; 281  :   row3 = _mm256_add_epi16(row3, _mm256_shufflelo_epi16(row3, _MM_SHUFFLE(0, 1, 0, 1) ));

	vpshuflw ymm0, YMMWORD PTR _row3$[ebp], 17	; 00000011H
	vmovdqu	ymm1, YMMWORD PTR _row3$[ebp]
	vpaddw	ymm0, ymm1, ymm0
	vmovdqu	YMMWORD PTR _row3$[ebp], ymm0

; 282  : 
; 283  :   unsigned sum1 = _mm_extract_epi16(_mm256_castsi256_si128(row3), 0);

	vmovdqa	xmm0, XMMWORD PTR _row3$[ebp]
	vpextrw	eax, xmm0, 0
	mov	DWORD PTR _sum1$[ebp], eax

; 284  :   sum1 = (sum1 + 1) >> 1;

	mov	eax, DWORD PTR _sum1$[ebp]
	add	eax, 1
	shr	eax, 1
	mov	DWORD PTR _sum1$[ebp], eax

; 285  : 
; 286  :   unsigned sum2 = _mm_extract_epi16(_mm256_extracti128_si256(row3, 1), 0);

	vmovdqu	ymm0, YMMWORD PTR _row3$[ebp]
	vextracti128 xmm0, ymm0, 1
	vpextrw	eax, xmm0, 0
	mov	DWORD PTR _sum2$[ebp], eax

; 287  :   sum2 = (sum2 + 1) >> 1;

	mov	eax, DWORD PTR _sum2$[ebp]
	add	eax, 1
	shr	eax, 1
	mov	DWORD PTR _sum2$[ebp], eax

; 288  : 
; 289  :   satds_out[0] = sum1;

	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, DWORD PTR _satds_out$[ebx]
	mov	eax, DWORD PTR _sum1$[ebp]
	mov	DWORD PTR [edx+ecx], eax

; 290  :   satds_out[1] = sum2;

	mov	eax, 4
	shl	eax, 0
	mov	ecx, DWORD PTR _satds_out$[ebx]
	mov	edx, DWORD PTR _sum2$[ebp]
	mov	DWORD PTR [ecx+eax], edx

; 291  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_satd_8bit_4x4_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _satd_4x4_8bit_avx2
_TEXT	SEGMENT
_satd$ = -280						; size = 4
_sum$ = -268						; size = 4
_row3$ = -256						; size = 16
_row2$ = -224						; size = 16
_row1$ = -192						; size = 16
_row0$ = -160						; size = 16
_diff_hi$ = -128					; size = 16
_diff_lo$ = -96						; size = 16
_current$ = -64						; size = 16
_original$ = -32					; size = 16
_org$ = 8						; size = 4
_cur$ = 12						; size = 4
_satd_4x4_8bit_avx2 PROC				; COMDAT

; 199  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 488				; 000001e8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-488]
	mov	ecx, 122				; 0000007aH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 200  : 
; 201  :   __m128i original = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)org));

	mov	eax, DWORD PTR _org$[ebx]
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxbw xmm0, xmm0
	vmovdqa	XMMWORD PTR _original$[ebp], xmm0

; 202  :   __m128i current = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)cur));

	mov	eax, DWORD PTR _cur$[ebx]
	vmovq	xmm0, QWORD PTR [eax]
	vpmovzxbw xmm0, xmm0
	vmovdqa	XMMWORD PTR _current$[ebp], xmm0

; 203  : 
; 204  :   __m128i diff_lo = _mm_sub_epi16(current, original);

	vmovdqa	xmm0, XMMWORD PTR _current$[ebp]
	vpsubw	xmm0, xmm0, XMMWORD PTR _original$[ebp]
	vmovdqa	XMMWORD PTR _diff_lo$[ebp], xmm0

; 205  : 
; 206  :   original = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)(org + 8)));

	mov	eax, DWORD PTR _org$[ebx]
	vmovq	xmm0, QWORD PTR [eax+8]
	vpmovzxbw xmm0, xmm0
	vmovdqa	XMMWORD PTR _original$[ebp], xmm0

; 207  :   current = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)(cur + 8)));

	mov	eax, DWORD PTR _cur$[ebx]
	vmovq	xmm0, QWORD PTR [eax+8]
	vpmovzxbw xmm0, xmm0
	vmovdqa	XMMWORD PTR _current$[ebp], xmm0

; 208  : 
; 209  :   __m128i diff_hi = _mm_sub_epi16(current, original);

	vmovdqa	xmm0, XMMWORD PTR _current$[ebp]
	vpsubw	xmm0, xmm0, XMMWORD PTR _original$[ebp]
	vmovdqa	XMMWORD PTR _diff_hi$[ebp], xmm0

; 210  : 
; 211  : 
; 212  :   //Hor
; 213  :   __m128i row0 = _mm_hadd_epi16(diff_lo, diff_hi);

	vmovdqa	xmm0, XMMWORD PTR _diff_lo$[ebp]
	vphaddw	xmm0, xmm0, XMMWORD PTR _diff_hi$[ebp]
	vmovdqa	XMMWORD PTR _row0$[ebp], xmm0

; 214  :   __m128i row1 = _mm_hsub_epi16(diff_lo, diff_hi);

	vmovdqa	xmm0, XMMWORD PTR _diff_lo$[ebp]
	vphsubw	xmm0, xmm0, XMMWORD PTR _diff_hi$[ebp]
	vmovdqa	XMMWORD PTR _row1$[ebp], xmm0

; 215  : 
; 216  :   __m128i row2 = _mm_hadd_epi16(row0, row1);

	vmovdqa	xmm0, XMMWORD PTR _row0$[ebp]
	vphaddw	xmm0, xmm0, XMMWORD PTR _row1$[ebp]
	vmovdqa	XMMWORD PTR _row2$[ebp], xmm0

; 217  :   __m128i row3 = _mm_hsub_epi16(row0, row1);

	vmovdqa	xmm0, XMMWORD PTR _row0$[ebp]
	vphsubw	xmm0, xmm0, XMMWORD PTR _row1$[ebp]
	vmovdqa	XMMWORD PTR _row3$[ebp], xmm0

; 218  : 
; 219  :   //Ver
; 220  :   row0 = _mm_hadd_epi16(row2, row3);

	vmovdqa	xmm0, XMMWORD PTR _row2$[ebp]
	vphaddw	xmm0, xmm0, XMMWORD PTR _row3$[ebp]
	vmovdqa	XMMWORD PTR _row0$[ebp], xmm0

; 221  :   row1 = _mm_hsub_epi16(row2, row3);

	vmovdqa	xmm0, XMMWORD PTR _row2$[ebp]
	vphsubw	xmm0, xmm0, XMMWORD PTR _row3$[ebp]
	vmovdqa	XMMWORD PTR _row1$[ebp], xmm0

; 222  : 
; 223  :   row2 = _mm_hadd_epi16(row0, row1);

	vmovdqa	xmm0, XMMWORD PTR _row0$[ebp]
	vphaddw	xmm0, xmm0, XMMWORD PTR _row1$[ebp]
	vmovdqa	XMMWORD PTR _row2$[ebp], xmm0

; 224  :   row3 = _mm_hsub_epi16(row0, row1);

	vmovdqa	xmm0, XMMWORD PTR _row0$[ebp]
	vphsubw	xmm0, xmm0, XMMWORD PTR _row1$[ebp]
	vmovdqa	XMMWORD PTR _row3$[ebp], xmm0

; 225  : 
; 226  :   //Abs and sum
; 227  :   row2 = _mm_abs_epi16(row2);

	vpabsw	xmm0, XMMWORD PTR _row2$[ebp]
	vmovdqa	XMMWORD PTR _row2$[ebp], xmm0

; 228  :   row3 = _mm_abs_epi16(row3);

	vpabsw	xmm0, XMMWORD PTR _row3$[ebp]
	vmovdqa	XMMWORD PTR _row3$[ebp], xmm0

; 229  : 
; 230  :   row3 = _mm_add_epi16(row2, row3);

	vmovdqa	xmm0, XMMWORD PTR _row2$[ebp]
	vpaddw	xmm0, xmm0, XMMWORD PTR _row3$[ebp]
	vmovdqa	XMMWORD PTR _row3$[ebp], xmm0

; 231  : 
; 232  :   row3 = _mm_add_epi16(row3, _mm_shuffle_epi32(row3, _MM_SHUFFLE(1, 0, 3, 2) ));

	vpshufd	xmm0, XMMWORD PTR _row3$[ebp], 78	; 0000004eH
	vmovdqa	xmm1, XMMWORD PTR _row3$[ebp]
	vpaddw	xmm0, xmm1, xmm0
	vmovdqa	XMMWORD PTR _row3$[ebp], xmm0

; 233  :   row3 = _mm_add_epi16(row3, _mm_shuffle_epi32(row3, _MM_SHUFFLE(0, 1, 0, 1) ));

	vpshufd	xmm0, XMMWORD PTR _row3$[ebp], 17	; 00000011H
	vmovdqa	xmm1, XMMWORD PTR _row3$[ebp]
	vpaddw	xmm0, xmm1, xmm0
	vmovdqa	XMMWORD PTR _row3$[ebp], xmm0

; 234  :   row3 = _mm_add_epi16(row3, _mm_shufflelo_epi16(row3, _MM_SHUFFLE(0, 1, 0, 1) ));

	vpshuflw xmm0, XMMWORD PTR _row3$[ebp], 17	; 00000011H
	vmovdqa	xmm1, XMMWORD PTR _row3$[ebp]
	vpaddw	xmm0, xmm1, xmm0
	vmovdqa	XMMWORD PTR _row3$[ebp], xmm0

; 235  : 
; 236  :   unsigned sum = _mm_extract_epi16(row3, 0);

	vmovdqa	xmm0, XMMWORD PTR _row3$[ebp]
	vpextrw	eax, xmm0, 0
	mov	DWORD PTR _sum$[ebp], eax

; 237  :   unsigned satd = (sum + 1) >> 1;

	mov	eax, DWORD PTR _sum$[ebp]
	add	eax, 1
	shr	eax, 1
	mov	DWORD PTR _satd$[ebp], eax

; 238  : 
; 239  :   return satd;

	mov	eax, DWORD PTR _satd$[ebp]

; 240  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_satd_4x4_8bit_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _sad_8bit_64x64_avx2
_TEXT	SEGMENT
_sum1$1 = -160						; size = 32
_i$2 = -108						; size = 4
_sum0$ = -96						; size = 32
_size_of_64x64$ = -44					; size = 4
_size_of_8x8$ = -32					; size = 4
_b$ = -20						; size = 4
_a$ = -8						; size = 4
_buf1$ = 8						; size = 4
_buf2$ = 12						; size = 4
_sad_8bit_64x64_avx2 PROC				; COMDAT

; 180  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 376				; 00000178H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-376]
	mov	ecx, 94					; 0000005eH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 181  :   const __m256i *const a = (const __m256i *)buf1;

	mov	eax, DWORD PTR _buf1$[ebx]
	mov	DWORD PTR _a$[ebp], eax

; 182  :   const __m256i *const b = (const __m256i *)buf2;

	mov	eax, DWORD PTR _buf2$[ebx]
	mov	DWORD PTR _b$[ebp], eax

; 183  : 
; 184  :   const unsigned size_of_8x8 = 8 * 8 / sizeof(__m256i);

	mov	DWORD PTR _size_of_8x8$[ebp], 2

; 185  :   const unsigned size_of_64x64 = 64 * 64 / sizeof(__m256i);

	mov	DWORD PTR _size_of_64x64$[ebp], 128	; 00000080H

; 186  : 
; 187  :   // Looping 512 bytes at a time seems faster than letting VC figure it out
; 188  :   // through inlining, like inline_8bit_sad_16x16_avx2 does.
; 189  :   __m256i sum0 = inline_8bit_sad_8x8_avx2(a, b);

	mov	eax, DWORD PTR _b$[ebp]
	push	eax
	mov	ecx, DWORD PTR _a$[ebp]
	push	ecx
	call	_inline_8bit_sad_8x8_avx2
	add	esp, 8
	vmovdqu	YMMWORD PTR _sum0$[ebp], ymm0

; 190  :   for (unsigned i = size_of_8x8; i < size_of_64x64; i += size_of_8x8) {

	mov	eax, DWORD PTR _size_of_8x8$[ebp]
	mov	DWORD PTR _i$2[ebp], eax
	jmp	SHORT $LN4@sad_8bit_6
$LN2@sad_8bit_6:
	mov	eax, DWORD PTR _i$2[ebp]
	add	eax, DWORD PTR _size_of_8x8$[ebp]
	mov	DWORD PTR _i$2[ebp], eax
$LN4@sad_8bit_6:
	mov	eax, DWORD PTR _i$2[ebp]
	cmp	eax, DWORD PTR _size_of_64x64$[ebp]
	jae	SHORT $LN3@sad_8bit_6

; 191  :     __m256i sum1 = inline_8bit_sad_8x8_avx2(a + i, b + i);

	mov	eax, DWORD PTR _i$2[ebp]
	shl	eax, 5
	add	eax, DWORD PTR _b$[ebp]
	push	eax
	mov	ecx, DWORD PTR _i$2[ebp]
	shl	ecx, 5
	add	ecx, DWORD PTR _a$[ebp]
	push	ecx
	call	_inline_8bit_sad_8x8_avx2
	add	esp, 8
	vmovdqu	YMMWORD PTR _sum1$1[ebp], ymm0

; 192  :     sum0 = _mm256_add_epi32(sum0, sum1);

	vmovdqu	ymm0, YMMWORD PTR _sum0$[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _sum1$1[ebp]
	vmovdqu	YMMWORD PTR _sum0$[ebp], ymm0

; 193  :   }

	jmp	SHORT $LN2@sad_8bit_6
$LN3@sad_8bit_6:

; 194  : 
; 195  :   return m256i_horizontal_sum(sum0);

	vmovdqu	ymm0, YMMWORD PTR _sum0$[ebp]
	call	_m256i_horizontal_sum

; 196  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_sad_8bit_64x64_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _sad_8bit_32x32_avx2
_TEXT	SEGMENT
_sum1$1 = -160						; size = 32
_i$2 = -108						; size = 4
_sum0$ = -96						; size = 32
_size_of_32x32$ = -44					; size = 4
_size_of_8x8$ = -32					; size = 4
_b$ = -20						; size = 4
_a$ = -8						; size = 4
_buf1$ = 8						; size = 4
_buf2$ = 12						; size = 4
_sad_8bit_32x32_avx2 PROC				; COMDAT

; 160  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 376				; 00000178H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-376]
	mov	ecx, 94					; 0000005eH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 161  :   const __m256i *const a = (const __m256i *)buf1;

	mov	eax, DWORD PTR _buf1$[ebx]
	mov	DWORD PTR _a$[ebp], eax

; 162  :   const __m256i *const b = (const __m256i *)buf2;

	mov	eax, DWORD PTR _buf2$[ebx]
	mov	DWORD PTR _b$[ebp], eax

; 163  : 
; 164  :   const unsigned size_of_8x8 = 8 * 8 / sizeof(__m256i);

	mov	DWORD PTR _size_of_8x8$[ebp], 2

; 165  :   const unsigned size_of_32x32 = 32 * 32 / sizeof(__m256i);

	mov	DWORD PTR _size_of_32x32$[ebp], 32	; 00000020H

; 166  : 
; 167  :   // Looping 512 bytes at a time seems faster than letting VC figure it out
; 168  :   // through inlining, like inline_8bit_sad_16x16_avx2 does.
; 169  :   __m256i sum0 = inline_8bit_sad_8x8_avx2(a, b);

	mov	eax, DWORD PTR _b$[ebp]
	push	eax
	mov	ecx, DWORD PTR _a$[ebp]
	push	ecx
	call	_inline_8bit_sad_8x8_avx2
	add	esp, 8
	vmovdqu	YMMWORD PTR _sum0$[ebp], ymm0

; 170  :   for (unsigned i = size_of_8x8; i < size_of_32x32; i += size_of_8x8) {

	mov	eax, DWORD PTR _size_of_8x8$[ebp]
	mov	DWORD PTR _i$2[ebp], eax
	jmp	SHORT $LN4@sad_8bit_3
$LN2@sad_8bit_3:
	mov	eax, DWORD PTR _i$2[ebp]
	add	eax, DWORD PTR _size_of_8x8$[ebp]
	mov	DWORD PTR _i$2[ebp], eax
$LN4@sad_8bit_3:
	mov	eax, DWORD PTR _i$2[ebp]
	cmp	eax, DWORD PTR _size_of_32x32$[ebp]
	jae	SHORT $LN3@sad_8bit_3

; 171  :     __m256i sum1 = inline_8bit_sad_8x8_avx2(a + i, b + i);

	mov	eax, DWORD PTR _i$2[ebp]
	shl	eax, 5
	add	eax, DWORD PTR _b$[ebp]
	push	eax
	mov	ecx, DWORD PTR _i$2[ebp]
	shl	ecx, 5
	add	ecx, DWORD PTR _a$[ebp]
	push	ecx
	call	_inline_8bit_sad_8x8_avx2
	add	esp, 8
	vmovdqu	YMMWORD PTR _sum1$1[ebp], ymm0

; 172  :     sum0 = _mm256_add_epi32(sum0, sum1);

	vmovdqu	ymm0, YMMWORD PTR _sum0$[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _sum1$1[ebp]
	vmovdqu	YMMWORD PTR _sum0$[ebp], ymm0

; 173  :   }

	jmp	SHORT $LN2@sad_8bit_3
$LN3@sad_8bit_3:

; 174  : 
; 175  :   return m256i_horizontal_sum(sum0);

	vmovdqu	ymm0, YMMWORD PTR _sum0$[ebp]
	call	_m256i_horizontal_sum

; 176  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_sad_8bit_32x32_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _sad_8bit_16x16_avx2
_TEXT	SEGMENT
_sum$ = -64						; size = 32
_b$ = -20						; size = 4
_a$ = -8						; size = 4
_buf1$ = 8						; size = 4
_buf2$ = 12						; size = 4
_sad_8bit_16x16_avx2 PROC				; COMDAT

; 150  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 280				; 00000118H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-280]
	mov	ecx, 70					; 00000046H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 151  :   const __m256i *const a = (const __m256i *)buf1;

	mov	eax, DWORD PTR _buf1$[ebx]
	mov	DWORD PTR _a$[ebp], eax

; 152  :   const __m256i *const b = (const __m256i *)buf2;

	mov	eax, DWORD PTR _buf2$[ebx]
	mov	DWORD PTR _b$[ebp], eax

; 153  :   __m256i sum = inline_8bit_sad_16x16_avx2(a, b);

	mov	eax, DWORD PTR _b$[ebp]
	push	eax
	mov	ecx, DWORD PTR _a$[ebp]
	push	ecx
	call	_inline_8bit_sad_16x16_avx2
	add	esp, 8
	vmovdqu	YMMWORD PTR _sum$[ebp], ymm0

; 154  : 
; 155  :   return m256i_horizontal_sum(sum);

	vmovdqu	ymm0, YMMWORD PTR _sum$[ebp]
	call	_m256i_horizontal_sum

; 156  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_sad_8bit_16x16_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _sad_8bit_8x8_avx2
_TEXT	SEGMENT
_sum$ = -64						; size = 32
_b$ = -20						; size = 4
_a$ = -8						; size = 4
_buf1$ = 8						; size = 4
_buf2$ = 12						; size = 4
_sad_8bit_8x8_avx2 PROC					; COMDAT

; 140  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 280				; 00000118H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-280]
	mov	ecx, 70					; 00000046H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 141  :   const __m256i *const a = (const __m256i *)buf1;

	mov	eax, DWORD PTR _buf1$[ebx]
	mov	DWORD PTR _a$[ebp], eax

; 142  :   const __m256i *const b = (const __m256i *)buf2;

	mov	eax, DWORD PTR _buf2$[ebx]
	mov	DWORD PTR _b$[ebp], eax

; 143  :   __m256i sum = inline_8bit_sad_8x8_avx2(a, b);

	mov	eax, DWORD PTR _b$[ebp]
	push	eax
	mov	ecx, DWORD PTR _a$[ebp]
	push	ecx
	call	_inline_8bit_sad_8x8_avx2
	add	esp, 8
	vmovdqu	YMMWORD PTR _sum$[ebp], ymm0

; 144  : 
; 145  :   return m256i_horizontal_sum(sum);

	vmovdqu	ymm0, YMMWORD PTR _sum$[ebp]
	call	_m256i_horizontal_sum

; 146  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_sad_8bit_8x8_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _m256i_horizontal_sum
_TEXT	SEGMENT
_result$ = -120						; size = 16
_mm128_result$ = -96					; size = 16
_sum$ = -64						; size = 32
__$ArrayPad$ = -4					; size = 4
_m256i_horizontal_sum PROC				; COMDAT
; _sum$ = ymm0

; 129  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 344				; 00000158H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-344]
	mov	ecx, 86					; 00000056H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	eax, DWORD PTR ___security_cookie
	xor	eax, ebp
	mov	DWORD PTR __$ArrayPad$[ebp], eax
	vmovups	YMMWORD PTR _sum$[ebp], ymm0
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 130  :   // Add the high 128 bits to low 128 bits.
; 131  :   __m128i mm128_result = _mm_add_epi32(_mm256_castsi256_si128(sum), _mm256_extractf128_si256(sum, 1));

	vmovdqu	ymm0, YMMWORD PTR _sum$[ebp]
	vextractf128 xmm0, ymm0, 1
	vmovdqa	xmm1, XMMWORD PTR _sum$[ebp]
	vpaddd	xmm0, xmm1, xmm0
	vmovdqa	XMMWORD PTR _mm128_result$[ebp], xmm0

; 132  :   // Add the high 64 bits  to low 64 bits.
; 133  :   uint32_t result[4];
; 134  :   _mm_storeu_si128((__m128i*)result, mm128_result);

	vmovdqu	xmm0, XMMWORD PTR _mm128_result$[ebp]
	vmovdqu	XMMWORD PTR _result$[ebp], xmm0

; 135  :   return result[0] + result[2];

	mov	eax, 4
	imul	ecx, eax, 0
	mov	edx, 4
	shl	edx, 1
	mov	eax, DWORD PTR _result$[ebp+ecx]
	add	eax, DWORD PTR _result$[ebp+edx]

; 136  : }

	push	edx
	mov	ecx, ebp
	push	eax
	lea	edx, DWORD PTR $LN5@m256i_hori
	call	@_RTC_CheckStackVars@8
	pop	eax
	pop	edx
	pop	edi
	pop	esi
	mov	ecx, DWORD PTR __$ArrayPad$[ebp]
	xor	ecx, ebp
	call	@__security_check_cookie@4
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
	npad	1
$LN5@m256i_hori:
	DD	1
	DD	$LN4@m256i_hori
$LN4@m256i_hori:
	DD	-120					; ffffff88H
	DD	16					; 00000010H
	DD	$LN3@m256i_hori
$LN3@m256i_hori:
	DB	114					; 00000072H
	DB	101					; 00000065H
	DB	115					; 00000073H
	DB	117					; 00000075H
	DB	108					; 0000006cH
	DB	116					; 00000074H
	DB	0
_m256i_horizontal_sum ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _inline_8bit_sad_16x16_avx2
_TEXT	SEGMENT
_sum3$ = -256						; size = 32
_sum2$ = -192						; size = 32
_sum1$ = -128						; size = 32
_sum0$ = -64						; size = 32
_size_of_8x8$ = -8					; size = 4
_a$ = 8							; size = 4
_b$ = 12						; size = 4
_inline_8bit_sad_16x16_avx2 PROC			; COMDAT

; 108  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 472				; 000001d8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-472]
	mov	ecx, 118				; 00000076H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 109  :   const unsigned size_of_8x8 = 8 * 8 / sizeof(__m256i);

	mov	DWORD PTR _size_of_8x8$[ebp], 2

; 110  : 
; 111  :   // Calculate in 4 chunks of 16x4.
; 112  :   __m256i sum0, sum1, sum2, sum3;
; 113  :   sum0 = inline_8bit_sad_8x8_avx2(a + 0 * size_of_8x8, b + 0 * size_of_8x8);

	imul	eax, DWORD PTR _size_of_8x8$[ebp], 0
	shl	eax, 5
	add	eax, DWORD PTR _b$[ebx]
	push	eax
	imul	ecx, DWORD PTR _size_of_8x8$[ebp], 0
	shl	ecx, 5
	add	ecx, DWORD PTR _a$[ebx]
	push	ecx
	call	_inline_8bit_sad_8x8_avx2
	add	esp, 8
	vmovdqu	YMMWORD PTR _sum0$[ebp], ymm0

; 114  :   sum1 = inline_8bit_sad_8x8_avx2(a + 1 * size_of_8x8, b + 1 * size_of_8x8);

	mov	eax, DWORD PTR _size_of_8x8$[ebp]
	shl	eax, 5
	add	eax, DWORD PTR _b$[ebx]
	push	eax
	mov	ecx, DWORD PTR _size_of_8x8$[ebp]
	shl	ecx, 5
	add	ecx, DWORD PTR _a$[ebx]
	push	ecx
	call	_inline_8bit_sad_8x8_avx2
	add	esp, 8
	vmovdqu	YMMWORD PTR _sum1$[ebp], ymm0

; 115  :   sum2 = inline_8bit_sad_8x8_avx2(a + 2 * size_of_8x8, b + 2 * size_of_8x8);

	mov	eax, DWORD PTR _size_of_8x8$[ebp]
	shl	eax, 1
	shl	eax, 5
	add	eax, DWORD PTR _b$[ebx]
	push	eax
	mov	ecx, DWORD PTR _size_of_8x8$[ebp]
	shl	ecx, 1
	shl	ecx, 5
	add	ecx, DWORD PTR _a$[ebx]
	push	ecx
	call	_inline_8bit_sad_8x8_avx2
	add	esp, 8
	vmovdqu	YMMWORD PTR _sum2$[ebp], ymm0

; 116  :   sum3 = inline_8bit_sad_8x8_avx2(a + 3 * size_of_8x8, b + 3 * size_of_8x8);

	imul	eax, DWORD PTR _size_of_8x8$[ebp], 3
	shl	eax, 5
	add	eax, DWORD PTR _b$[ebx]
	push	eax
	imul	ecx, DWORD PTR _size_of_8x8$[ebp], 3
	shl	ecx, 5
	add	ecx, DWORD PTR _a$[ebx]
	push	ecx
	call	_inline_8bit_sad_8x8_avx2
	add	esp, 8
	vmovdqu	YMMWORD PTR _sum3$[ebp], ymm0

; 117  : 
; 118  :   sum0 = _mm256_add_epi32(sum0, sum1);

	vmovdqu	ymm0, YMMWORD PTR _sum0$[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _sum1$[ebp]
	vmovdqu	YMMWORD PTR _sum0$[ebp], ymm0

; 119  :   sum2 = _mm256_add_epi32(sum2, sum3);

	vmovdqu	ymm0, YMMWORD PTR _sum2$[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _sum3$[ebp]
	vmovdqu	YMMWORD PTR _sum2$[ebp], ymm0

; 120  : 
; 121  :   return _mm256_add_epi32(sum0, sum2);

	vmovdqu	ymm0, YMMWORD PTR _sum0$[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _sum2$[ebp]

; 122  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_inline_8bit_sad_16x16_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _inline_8bit_sad_8x8_avx2
_TEXT	SEGMENT
_sum1$ = -128						; size = 32
_sum0$ = -64						; size = 32
_a$ = 8							; size = 4
_b$ = 12						; size = 4
_inline_8bit_sad_8x8_avx2 PROC				; COMDAT

; 95   : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 344				; 00000158H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-344]
	mov	ecx, 86					; 00000056H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 96   :   __m256i sum0, sum1;
; 97   :   sum0 = _mm256_sad_epu8(_mm256_load_si256(a + 0), _mm256_load_si256(b + 0));

	mov	eax, DWORD PTR _b$[ebx]
	mov	ecx, DWORD PTR _a$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx]
	vpsadbw	ymm0, ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _sum0$[ebp], ymm0

; 98   :   sum1 = _mm256_sad_epu8(_mm256_load_si256(a + 1), _mm256_load_si256(b + 1));

	mov	eax, DWORD PTR _b$[ebx]
	add	eax, 32					; 00000020H
	mov	ecx, DWORD PTR _a$[ebx]
	add	ecx, 32					; 00000020H
	vmovdqu	ymm0, YMMWORD PTR [ecx]
	vpsadbw	ymm0, ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _sum1$[ebp], ymm0

; 99   : 
; 100  :   return _mm256_add_epi32(sum0, sum1);

	vmovdqu	ymm0, YMMWORD PTR _sum0$[ebp]
	vpaddd	ymm0, ymm0, YMMWORD PTR _sum1$[ebp]

; 101  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_inline_8bit_sad_8x8_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _kvz_reg_sad_avx2
_TEXT	SEGMENT
_data1$ = 8						; size = 4
_data2$ = 12						; size = 4
_width$ = 16						; size = 4
_height$ = 20						; size = 4
_stride1$ = 24						; size = 4
_stride2$ = 28						; size = 4
_kvz_reg_sad_avx2 PROC					; COMDAT

; 70   : {

	push	ebp
	mov	ebp, esp
	sub	esp, 192				; 000000c0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-192]
	mov	ecx, 48					; 00000030H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 71   :   if (width == 0)

	cmp	DWORD PTR _width$[ebp], 0
	jne	SHORT $LN2@kvz_reg_sa

; 72   :     return 0;

	xor	eax, eax
	jmp	$LN1@kvz_reg_sa
$LN2@kvz_reg_sa:

; 73   :   if (width == 4)

	cmp	DWORD PTR _width$[ebp], 4
	jne	SHORT $LN3@kvz_reg_sa

; 74   :     return reg_sad_w4(data1, data2, height, stride1, stride2);

	mov	eax, DWORD PTR _stride2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _stride1$[ebp]
	push	ecx
	mov	edx, DWORD PTR _height$[ebp]
	push	edx
	mov	eax, DWORD PTR _data2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _data1$[ebp]
	push	ecx
	call	_reg_sad_w4
	add	esp, 20					; 00000014H
	jmp	$LN1@kvz_reg_sa
$LN3@kvz_reg_sa:

; 75   :   if (width == 8)

	cmp	DWORD PTR _width$[ebp], 8
	jne	SHORT $LN4@kvz_reg_sa

; 76   :     return reg_sad_w8(data1, data2, height, stride1, stride2);

	mov	eax, DWORD PTR _stride2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _stride1$[ebp]
	push	ecx
	mov	edx, DWORD PTR _height$[ebp]
	push	edx
	mov	eax, DWORD PTR _data2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _data1$[ebp]
	push	ecx
	call	_reg_sad_w8
	add	esp, 20					; 00000014H
	jmp	$LN1@kvz_reg_sa
$LN4@kvz_reg_sa:

; 77   :   if (width == 12)

	cmp	DWORD PTR _width$[ebp], 12		; 0000000cH
	jne	SHORT $LN5@kvz_reg_sa

; 78   :     return reg_sad_w12(data1, data2, height, stride1, stride2);

	mov	eax, DWORD PTR _stride2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _stride1$[ebp]
	push	ecx
	mov	edx, DWORD PTR _height$[ebp]
	push	edx
	mov	eax, DWORD PTR _data2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _data1$[ebp]
	push	ecx
	call	_reg_sad_w12
	add	esp, 20					; 00000014H
	jmp	$LN1@kvz_reg_sa
$LN5@kvz_reg_sa:

; 79   :   if (width == 16)

	cmp	DWORD PTR _width$[ebp], 16		; 00000010H
	jne	SHORT $LN6@kvz_reg_sa

; 80   :     return reg_sad_w16(data1, data2, height, stride1, stride2);

	mov	eax, DWORD PTR _stride2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _stride1$[ebp]
	push	ecx
	mov	edx, DWORD PTR _height$[ebp]
	push	edx
	mov	eax, DWORD PTR _data2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _data1$[ebp]
	push	ecx
	call	_reg_sad_w16
	add	esp, 20					; 00000014H
	jmp	$LN1@kvz_reg_sa
$LN6@kvz_reg_sa:

; 81   :   if (width == 24)

	cmp	DWORD PTR _width$[ebp], 24		; 00000018H
	jne	SHORT $LN7@kvz_reg_sa

; 82   :     return reg_sad_w24(data1, data2, height, stride1, stride2);

	mov	eax, DWORD PTR _stride2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _stride1$[ebp]
	push	ecx
	mov	edx, DWORD PTR _height$[ebp]
	push	edx
	mov	eax, DWORD PTR _data2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _data1$[ebp]
	push	ecx
	call	_reg_sad_w24
	add	esp, 20					; 00000014H
	jmp	SHORT $LN1@kvz_reg_sa
$LN7@kvz_reg_sa:

; 83   :   if (width == 32)

	cmp	DWORD PTR _width$[ebp], 32		; 00000020H
	jne	SHORT $LN8@kvz_reg_sa

; 84   :     return reg_sad_w32(data1, data2, height, stride1, stride2);

	mov	eax, DWORD PTR _stride2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _stride1$[ebp]
	push	ecx
	mov	edx, DWORD PTR _height$[ebp]
	push	edx
	mov	eax, DWORD PTR _data2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _data1$[ebp]
	push	ecx
	call	_reg_sad_w32
	add	esp, 20					; 00000014H
	jmp	SHORT $LN1@kvz_reg_sa
$LN8@kvz_reg_sa:

; 85   :   if (width == 64)

	cmp	DWORD PTR _width$[ebp], 64		; 00000040H
	jne	SHORT $LN9@kvz_reg_sa

; 86   :     return reg_sad_w64(data1, data2, height, stride1, stride2);

	mov	eax, DWORD PTR _stride2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _stride1$[ebp]
	push	ecx
	mov	edx, DWORD PTR _height$[ebp]
	push	edx
	mov	eax, DWORD PTR _data2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _data1$[ebp]
	push	ecx
	call	_reg_sad_w64
	add	esp, 20					; 00000014H
	jmp	SHORT $LN1@kvz_reg_sa
	jmp	SHORT $LN1@kvz_reg_sa
$LN9@kvz_reg_sa:

; 87   :   else
; 88   :     return reg_sad_arbitrary(data1, data2, width, height, stride1, stride2);

	mov	eax, DWORD PTR _stride2$[ebp]
	push	eax
	mov	ecx, DWORD PTR _stride1$[ebp]
	push	ecx
	mov	edx, DWORD PTR _height$[ebp]
	push	edx
	mov	eax, DWORD PTR _width$[ebp]
	push	eax
	mov	ecx, DWORD PTR _data2$[ebp]
	push	ecx
	mov	edx, DWORD PTR _data1$[ebp]
	push	edx
	call	_reg_sad_arbitrary
	add	esp, 24					; 00000018H
$LN1@kvz_reg_sa:

; 89   : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 192				; 000000c0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_kvz_reg_sad_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\reg_sad_pow2_widths-avx2.h
;	COMDAT _hor_sad_avx2_w32
_TEXT	SEGMENT
_sad$ = -1952						; size = 16
_sum_2$ = -1920						; size = 16
_sum_1$ = -1888						; size = 16
_inclo$ = -1856						; size = 16
_inchi$ = -1824						; size = 16
_curr_sads_ab$1 = -1792					; size = 32
_b_epoled$2 = -1728					; size = 32
_b_data_transfered$3 = -1664				; size = 32
_b_lanes_reversed$4 = -1600				; size = 32
_b_shifted$5 = -1536					; size = 32
_b$6 = -1472						; size = 32
_a$7 = -1408						; size = 32
_y$ = -1368						; size = 4
_ld_offset$ = -1356					; size = 4
_xchg_data_mask$ = -1344				; size = 32
_xchg_lane_mask$ = -1280				; size = 32
_lanes_llo_rhi$ = -1216					; size = 32
_xchg_mask1$ = -1152					; size = 32
_mlo2hi_mask_r$ = -1088					; size = 32
_mlo2hi_imask_r$ = -1024				; size = 32
_mlo2hi_mask_l$ = -960					; size = 32
_epol_mask$ = -896					; size = 32
_epol_mask_r$ = -832					; size = 32
_shufmask1$ = -768					; size = 32
_shufmask1_r$ = -704					; size = 32
_shufmask1_l$ = -640					; size = 32
_rightmost_good_idx$ = -576				; size = 32
_ns$ = -512						; size = 32
_unsign_mask$ = -448					; size = 32
_rights$ = -384						; size = 32
_lefts$ = -320						; size = 32
_lane_widths$ = -256					; size = 32
_zero$ = -192						; size = 32
_right_clamped$ = -148					; size = 4
_right_eq_wid$ = -136					; size = 4
_left_clamped$ = -124					; size = 4
_left_eq_wid$ = -112					; size = 4
_lane_width$ = -100					; size = 4
_block_width_log2$ = -88				; size = 4
_block_width$ = -76					; size = 4
_avx_inc$ = -64						; size = 32
_pic_data$ = 8						; size = 4
_ref_data$ = 12						; size = 4
_height$ = 16						; size = 4
_pic_stride$ = 20					; size = 4
_ref_stride$ = 24					; size = 4
_left$ = 28						; size = 4
_right$ = 32						; size = 4
_hor_sad_avx2_w32 PROC					; COMDAT

; 150  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 2744				; 00000ab8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-2744]
	mov	ecx, 686				; 000002aeH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __1E5A298B_reg_sad_pow2_widths-avx2@h
	call	@__CheckForDebuggerJustMyCode@4

; 151  :   __m256i avx_inc = _mm256_setzero_si256();

	vpxor	xmm0, xmm0, xmm0
	vmovdqu	YMMWORD PTR _avx_inc$[ebp], ymm0

; 152  : 
; 153  :   const size_t block_width      = 32;

	mov	DWORD PTR _block_width$[ebp], 32	; 00000020H

; 154  :   const size_t block_width_log2 = 5;

	mov	DWORD PTR _block_width_log2$[ebp], 5

; 155  :   const size_t lane_width       = 16;

	mov	DWORD PTR _lane_width$[ebp], 16		; 00000010H

; 156  : 
; 157  :   const int32_t left_eq_wid     = left  >> block_width_log2;

	mov	eax, DWORD PTR _block_width_log2$[ebp]
	shrx	ecx, DWORD PTR _left$[ebx], eax
	mov	DWORD PTR _left_eq_wid$[ebp], ecx

; 158  :   const int32_t left_clamped    = left  -  left_eq_wid;

	mov	eax, DWORD PTR _left$[ebx]
	sub	eax, DWORD PTR _left_eq_wid$[ebp]
	mov	DWORD PTR _left_clamped$[ebp], eax

; 159  :   const int32_t right_eq_wid    = right >> block_width_log2;

	mov	eax, DWORD PTR _block_width_log2$[ebp]
	shrx	ecx, DWORD PTR _right$[ebx], eax
	mov	DWORD PTR _right_eq_wid$[ebp], ecx

; 160  :   const int32_t right_clamped   = right -  right_eq_wid;

	mov	eax, DWORD PTR _right$[ebx]
	sub	eax, DWORD PTR _right_eq_wid$[ebp]
	mov	DWORD PTR _right_clamped$[ebp], eax

; 161  : 
; 162  :   const __m256i zero        = _mm256_setzero_si256();

	vpxor	xmm0, xmm0, xmm0
	vmovdqu	YMMWORD PTR _zero$[ebp], ymm0

; 163  :   const __m256i lane_widths = _mm256_set1_epi8((uint8_t)lane_width);

	movsx	eax, BYTE PTR _lane_width$[ebp]
	vmovd	xmm0, eax
	vpbroadcastb ymm0, xmm0
	vmovdqu	YMMWORD PTR _lane_widths$[ebp], ymm0

; 164  :   const __m256i lefts       = _mm256_set1_epi8((uint8_t)left_clamped);

	movsx	eax, BYTE PTR _left_clamped$[ebp]
	vmovd	xmm0, eax
	vpbroadcastb ymm0, xmm0
	vmovdqu	YMMWORD PTR _lefts$[ebp], ymm0

; 165  :   const __m256i rights      = _mm256_set1_epi8((uint8_t)right_clamped);

	movsx	eax, BYTE PTR _right_clamped$[ebp]
	vmovd	xmm0, eax
	vpbroadcastb ymm0, xmm0
	vmovdqu	YMMWORD PTR _rights$[ebp], ymm0

; 166  :   const __m256i unsign_mask = _mm256_set1_epi8(0x7f);

	vmovdqu	ymm0, YMMWORD PTR __ymm@7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f
	vmovdqu	YMMWORD PTR _unsign_mask$[ebp], ymm0

; 167  :   const __m256i ns          = _mm256_setr_epi8(0,  1,  2,  3,  4,  5,  6,  7,  8,  9,  10, 11, 12, 13, 14, 15,

	vmovdqu	ymm0, YMMWORD PTR __ymm@1f1e1d1c1b1a191817161514131211100f0e0d0c0b0a09080706050403020100
	vmovdqu	YMMWORD PTR _ns$[ebp], ymm0

; 168  :                                                16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);
; 169  : 
; 170  :   const __m256i rightmost_good_idx = _mm256_set1_epi8((uint8_t)(block_width - right - 1));

	mov	eax, DWORD PTR _block_width$[ebp]
	sub	eax, DWORD PTR _right$[ebx]
	sub	eax, 1
	movsx	ecx, al
	vmovd	xmm0, ecx
	vpbroadcastb ymm0, xmm0
	vmovdqu	YMMWORD PTR _rightmost_good_idx$[ebp], ymm0

; 171  : 
; 172  :   const __m256i shufmask1_l    = _mm256_sub_epi8  (ns,          lefts);

	vmovdqu	ymm0, YMMWORD PTR _ns$[ebp]
	vpsubb	ymm0, ymm0, YMMWORD PTR _lefts$[ebp]
	vmovdqu	YMMWORD PTR _shufmask1_l$[ebp], ymm0

; 173  :   const __m256i shufmask1_r    = _mm256_add_epi8  (shufmask1_l, rights);

	vmovdqu	ymm0, YMMWORD PTR _shufmask1_l$[ebp]
	vpaddb	ymm0, ymm0, YMMWORD PTR _rights$[ebp]
	vmovdqu	YMMWORD PTR _shufmask1_r$[ebp], ymm0

; 174  :   const __m256i shufmask1      = _mm256_and_si256 (shufmask1_r, unsign_mask);

	vmovdqu	ymm0, YMMWORD PTR _shufmask1_r$[ebp]
	vpand	ymm0, ymm0, YMMWORD PTR _unsign_mask$[ebp]
	vmovdqu	YMMWORD PTR _shufmask1$[ebp], ymm0

; 175  : 
; 176  :   const __m256i epol_mask_r    = _mm256_min_epi8  (ns,    rightmost_good_idx);

	vmovdqu	ymm0, YMMWORD PTR _ns$[ebp]
	vpminsb	ymm0, ymm0, YMMWORD PTR _rightmost_good_idx$[ebp]
	vmovdqu	YMMWORD PTR _epol_mask_r$[ebp], ymm0

; 177  :   const __m256i epol_mask      = _mm256_max_epi8  (lefts, epol_mask_r);

	vmovdqu	ymm0, YMMWORD PTR _lefts$[ebp]
	vpmaxsb	ymm0, ymm0, YMMWORD PTR _epol_mask_r$[ebp]
	vmovdqu	YMMWORD PTR _epol_mask$[ebp], ymm0

; 178  : 
; 179  :   const __m256i mlo2hi_mask_l  = _mm256_cmpgt_epi8(lefts, ns);

	vmovdqu	ymm0, YMMWORD PTR _lefts$[ebp]
	vpcmpgtb ymm0, ymm0, YMMWORD PTR _ns$[ebp]
	vmovdqu	YMMWORD PTR _mlo2hi_mask_l$[ebp], ymm0

; 180  :   const __m256i mlo2hi_imask_r = _mm256_cmpgt_epi8(lane_widths, shufmask1);

	vmovdqu	ymm0, YMMWORD PTR _lane_widths$[ebp]
	vpcmpgtb ymm0, ymm0, YMMWORD PTR _shufmask1$[ebp]
	vmovdqu	YMMWORD PTR _mlo2hi_imask_r$[ebp], ymm0

; 181  :   const __m256i mlo2hi_mask_r  = _mm256_cmpeq_epi8(mlo2hi_imask_r, zero);

	vmovdqu	ymm0, YMMWORD PTR _mlo2hi_imask_r$[ebp]
	vpcmpeqb ymm0, ymm0, YMMWORD PTR _zero$[ebp]
	vmovdqu	YMMWORD PTR _mlo2hi_mask_r$[ebp], ymm0

; 182  : 
; 183  :   // For left != 0,  use low lane of mlo2hi_mask_l as blend mask for high lane.
; 184  :   // For right != 0, use low lane of mlo2hi_mask_r as blend mask for low lane.
; 185  :   const __m256i xchg_mask1     = _mm256_permute2x128_si256(mlo2hi_mask_l, mlo2hi_mask_r, 0x02);

	vmovdqu	ymm0, YMMWORD PTR _mlo2hi_mask_l$[ebp]
	vperm2i128 ymm0, ymm0, YMMWORD PTR _mlo2hi_mask_r$[ebp], 2
	vmovdqu	YMMWORD PTR _xchg_mask1$[ebp], ymm0

; 186  : 
; 187  :   // If left != 0 (ie. right == 0), the xchg should only affect high lane,
; 188  :   // if right != 0 (ie. left == 0), the low lane. Set bits on the lane that
; 189  :   // the xchg should affect. left == right == 0 should never happen, this'll
; 190  :   // break if it does.
; 191  :   const __m256i lanes_llo_rhi  = _mm256_blend_epi32(lefts, rights, 0xf0);

	vmovdqu	ymm0, YMMWORD PTR _lefts$[ebp]
	vpblendd ymm0, ymm0, YMMWORD PTR _rights$[ebp], 240 ; 000000f0H
	vmovdqu	YMMWORD PTR _lanes_llo_rhi$[ebp], ymm0

; 192  :   const __m256i xchg_lane_mask = _mm256_cmpeq_epi32(lanes_llo_rhi, zero);

	vmovdqu	ymm0, YMMWORD PTR _lanes_llo_rhi$[ebp]
	vpcmpeqd ymm0, ymm0, YMMWORD PTR _zero$[ebp]
	vmovdqu	YMMWORD PTR _xchg_lane_mask$[ebp], ymm0

; 193  : 
; 194  :   const __m256i xchg_data_mask = _mm256_and_si256(xchg_mask1, xchg_lane_mask);

	vmovdqu	ymm0, YMMWORD PTR _xchg_mask1$[ebp]
	vpand	ymm0, ymm0, YMMWORD PTR _xchg_lane_mask$[ebp]
	vmovdqu	YMMWORD PTR _xchg_data_mask$[ebp], ymm0

; 195  : 
; 196  :   // If we're straddling the left border, start from the left border instead,
; 197  :   // and if right border, end on the border
; 198  :   const int32_t ld_offset = left - right;

	mov	eax, DWORD PTR _left$[ebx]
	sub	eax, DWORD PTR _right$[ebx]
	mov	DWORD PTR _ld_offset$[ebp], eax

; 199  : 
; 200  :   int32_t y;
; 201  :   for (y = 0; y < height; y++) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@hor_sad_av
$LN2@hor_sad_av:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN4@hor_sad_av:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	$LN3@hor_sad_av

; 202  :     __m256i a = _mm256_loadu_si256((__m256i *)(pic_data + (y + 0) * pic_stride + 0));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _a$7[ebp], ymm0

; 203  :     __m256i b = _mm256_loadu_si256((__m256i *)(ref_data + (y + 0) * ref_stride + 0  + ld_offset));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _ref_data$[ebx]
	add	eax, DWORD PTR _ld_offset$[ebp]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _b$6[ebp], ymm0

; 204  : 
; 205  :     __m256i b_shifted            = _mm256_shuffle_epi8     (b, shufmask1);

	vmovdqu	ymm0, YMMWORD PTR _b$6[ebp]
	vpshufb	ymm0, ymm0, YMMWORD PTR _shufmask1$[ebp]
	vmovdqu	YMMWORD PTR _b_shifted$5[ebp], ymm0

; 206  :     __m256i b_lanes_reversed     = _mm256_permute4x64_epi64(b_shifted,   _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm0, YMMWORD PTR _b_shifted$5[ebp], 78	; 0000004eH
	vmovdqu	YMMWORD PTR _b_lanes_reversed$4[ebp], ymm0

; 207  :     __m256i b_data_transfered    = _mm256_blendv_epi8      (b_shifted, b_lanes_reversed, xchg_data_mask);

	vmovdqu	ymm0, YMMWORD PTR _xchg_data_mask$[ebp]
	vmovdqu	ymm1, YMMWORD PTR _b_shifted$5[ebp]
	vpblendvb ymm0, ymm1, YMMWORD PTR _b_lanes_reversed$4[ebp], ymm0
	vmovdqu	YMMWORD PTR _b_data_transfered$3[ebp], ymm0

; 208  :     __m256i b_epoled             = _mm256_shuffle_epi8     (b_data_transfered, epol_mask);

	vmovdqu	ymm0, YMMWORD PTR _b_data_transfered$3[ebp]
	vpshufb	ymm0, ymm0, YMMWORD PTR _epol_mask$[ebp]
	vmovdqu	YMMWORD PTR _b_epoled$2[ebp], ymm0

; 209  : 
; 210  :     __m256i curr_sads_ab         = _mm256_sad_epu8(a, b_epoled);

	vmovdqu	ymm0, YMMWORD PTR _a$7[ebp]
	vpsadbw	ymm0, ymm0, YMMWORD PTR _b_epoled$2[ebp]
	vmovdqu	YMMWORD PTR _curr_sads_ab$1[ebp], ymm0

; 211  : 
; 212  :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_ab);

	vmovdqu	ymm0, YMMWORD PTR _avx_inc$[ebp]
	vpaddq	ymm0, ymm0, YMMWORD PTR _curr_sads_ab$1[ebp]
	vmovdqu	YMMWORD PTR _avx_inc$[ebp], ymm0

; 213  :   }

	jmp	$LN2@hor_sad_av
$LN3@hor_sad_av:

; 214  :   __m128i inchi = _mm256_extracti128_si256(avx_inc, 1);

	vmovdqu	ymm0, YMMWORD PTR _avx_inc$[ebp]
	vextracti128 xmm0, ymm0, 1
	vmovdqa	XMMWORD PTR _inchi$[ebp], xmm0

; 215  :   __m128i inclo = _mm256_castsi256_si128  (avx_inc);

	vmovdqu	xmm0, XMMWORD PTR _avx_inc$[ebp]
	vmovdqa	XMMWORD PTR _inclo$[ebp], xmm0

; 216  : 
; 217  :   __m128i sum_1 = _mm_add_epi64    (inclo, inchi);

	vmovdqa	xmm0, XMMWORD PTR _inclo$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _inchi$[ebp]
	vmovdqa	XMMWORD PTR _sum_1$[ebp], xmm0

; 218  :   __m128i sum_2 = _mm_shuffle_epi32(sum_1, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sum_1$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sum_2$[ebp], xmm0

; 219  :   __m128i sad   = _mm_add_epi64    (sum_1, sum_2);

	vmovdqa	xmm0, XMMWORD PTR _sum_1$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sum_2$[ebp]
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 220  : 
; 221  :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 222  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_hor_sad_avx2_w32 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\reg_sad_pow2_widths-avx2.h
;	COMDAT _reg_sad_w64
_TEXT	SEGMENT
_sad$ = -1408						; size = 16
_sum_2$ = -1376						; size = 16
_sum_1$ = -1344						; size = 16
_inclo$ = -1312						; size = 16
_inchi$ = -1280						; size = 16
_curr_sads_cd$1 = -1248					; size = 32
_curr_sads_ab$2 = -1184					; size = 32
_d$3 = -1120						; size = 32
_c$4 = -1056						; size = 32
_b$5 = -992						; size = 32
_a$6 = -928						; size = 32
_curr_sads_gh$7 = -864					; size = 32
_curr_sads_ef$8 = -800					; size = 32
_curr_sads_cd$9 = -736					; size = 32
_curr_sads_ab$10 = -672					; size = 32
_h$11 = -608						; size = 32
_g$12 = -544						; size = 32
_f$13 = -480						; size = 32
_e$14 = -416						; size = 32
_d$15 = -352						; size = 32
_c$16 = -288						; size = 32
_b$17 = -224						; size = 32
_a$18 = -160						; size = 32
_height_residual_lines$ = -100				; size = 4
_height_twoline_groups$ = -88				; size = 4
_y$ = -76						; size = 4
_avx_inc$ = -64						; size = 32
_data1$ = 8						; size = 4
_data2$ = 12						; size = 4
_height$ = 16						; size = 4
_stride1$ = 20						; size = 4
_stride2$ = 24						; size = 4
_reg_sad_w64 PROC					; COMDAT

; 95   : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 2008				; 000007d8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-2008]
	mov	ecx, 502				; 000001f6H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __1E5A298B_reg_sad_pow2_widths-avx2@h
	call	@__CheckForDebuggerJustMyCode@4

; 96   :   __m256i avx_inc = _mm256_setzero_si256();

	vpxor	xmm0, xmm0, xmm0
	vmovdqu	YMMWORD PTR _avx_inc$[ebp], ymm0

; 97   :   int32_t y;
; 98   : 
; 99   :   const int32_t height_twoline_groups = height & ~1;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, -2					; fffffffeH
	mov	DWORD PTR _height_twoline_groups$[ebp], eax

; 100  :   const int32_t height_residual_lines = height &  1;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, 1
	mov	DWORD PTR _height_residual_lines$[ebp], eax

; 101  : 
; 102  :   for (y = 0; y < height_twoline_groups; y += 2) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@reg_sad_w6
$LN2@reg_sad_w6:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	mov	DWORD PTR _y$[ebp], eax
$LN4@reg_sad_w6:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height_twoline_groups$[ebp]
	jge	$LN3@reg_sad_w6

; 103  :     __m256i a = _mm256_loadu_si256((const __m256i *)(data1 + (y + 0) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _a$18[ebp], ymm0

; 104  :     __m256i b = _mm256_loadu_si256((const __m256i *)(data2 + (y + 0) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _b$17[ebp], ymm0

; 105  :     __m256i c = _mm256_loadu_si256((const __m256i *)(data1 + (y + 0) * stride1 + 32));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	mov	ecx, DWORD PTR _data1$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx+eax+32]
	vmovdqu	YMMWORD PTR _c$16[ebp], ymm0

; 106  :     __m256i d = _mm256_loadu_si256((const __m256i *)(data2 + (y + 0) * stride2 + 32));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	mov	ecx, DWORD PTR _data2$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx+eax+32]
	vmovdqu	YMMWORD PTR _d$15[ebp], ymm0

; 107  : 
; 108  :     __m256i e = _mm256_loadu_si256((const __m256i *)(data1 + (y + 1) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _e$14[ebp], ymm0

; 109  :     __m256i f = _mm256_loadu_si256((const __m256i *)(data2 + (y + 1) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _f$13[ebp], ymm0

; 110  :     __m256i g = _mm256_loadu_si256((const __m256i *)(data1 + (y + 1) * stride1 + 32));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride1$[ebx]
	mov	ecx, DWORD PTR _data1$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx+eax+32]
	vmovdqu	YMMWORD PTR _g$12[ebp], ymm0

; 111  :     __m256i h = _mm256_loadu_si256((const __m256i *)(data2 + (y + 1) * stride2 + 32));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride2$[ebx]
	mov	ecx, DWORD PTR _data2$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx+eax+32]
	vmovdqu	YMMWORD PTR _h$11[ebp], ymm0

; 112  : 
; 113  :     __m256i curr_sads_ab = _mm256_sad_epu8(a, b);

	vmovdqu	ymm0, YMMWORD PTR _a$18[ebp]
	vpsadbw	ymm0, ymm0, YMMWORD PTR _b$17[ebp]
	vmovdqu	YMMWORD PTR _curr_sads_ab$10[ebp], ymm0

; 114  :     __m256i curr_sads_cd = _mm256_sad_epu8(c, d);

	vmovdqu	ymm0, YMMWORD PTR _c$16[ebp]
	vpsadbw	ymm0, ymm0, YMMWORD PTR _d$15[ebp]
	vmovdqu	YMMWORD PTR _curr_sads_cd$9[ebp], ymm0

; 115  :     __m256i curr_sads_ef = _mm256_sad_epu8(e, f);

	vmovdqu	ymm0, YMMWORD PTR _e$14[ebp]
	vpsadbw	ymm0, ymm0, YMMWORD PTR _f$13[ebp]
	vmovdqu	YMMWORD PTR _curr_sads_ef$8[ebp], ymm0

; 116  :     __m256i curr_sads_gh = _mm256_sad_epu8(g, h);

	vmovdqu	ymm0, YMMWORD PTR _g$12[ebp]
	vpsadbw	ymm0, ymm0, YMMWORD PTR _h$11[ebp]
	vmovdqu	YMMWORD PTR _curr_sads_gh$7[ebp], ymm0

; 117  : 
; 118  :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_ab);

	vmovdqu	ymm0, YMMWORD PTR _avx_inc$[ebp]
	vpaddq	ymm0, ymm0, YMMWORD PTR _curr_sads_ab$10[ebp]
	vmovdqu	YMMWORD PTR _avx_inc$[ebp], ymm0

; 119  :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_cd);

	vmovdqu	ymm0, YMMWORD PTR _avx_inc$[ebp]
	vpaddq	ymm0, ymm0, YMMWORD PTR _curr_sads_cd$9[ebp]
	vmovdqu	YMMWORD PTR _avx_inc$[ebp], ymm0

; 120  :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_ef);

	vmovdqu	ymm0, YMMWORD PTR _avx_inc$[ebp]
	vpaddq	ymm0, ymm0, YMMWORD PTR _curr_sads_ef$8[ebp]
	vmovdqu	YMMWORD PTR _avx_inc$[ebp], ymm0

; 121  :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_gh);

	vmovdqu	ymm0, YMMWORD PTR _avx_inc$[ebp]
	vpaddq	ymm0, ymm0, YMMWORD PTR _curr_sads_gh$7[ebp]
	vmovdqu	YMMWORD PTR _avx_inc$[ebp], ymm0

; 122  :   }

	jmp	$LN2@reg_sad_w6
$LN3@reg_sad_w6:

; 123  :   if (height_residual_lines) {

	cmp	DWORD PTR _height_residual_lines$[ebp], 0
	je	$LN8@reg_sad_w6

; 124  :     for (; y < height; y++) {

	jmp	SHORT $LN7@reg_sad_w6
$LN5@reg_sad_w6:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN7@reg_sad_w6:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	$LN8@reg_sad_w6

; 125  :       __m256i a = _mm256_loadu_si256((const __m256i *)(data1 + (y + 0) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _a$6[ebp], ymm0

; 126  :       __m256i b = _mm256_loadu_si256((const __m256i *)(data2 + (y + 0) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _b$5[ebp], ymm0

; 127  :       __m256i c = _mm256_loadu_si256((const __m256i *)(data1 + (y + 0) * stride1 + 32));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	mov	ecx, DWORD PTR _data1$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx+eax+32]
	vmovdqu	YMMWORD PTR _c$4[ebp], ymm0

; 128  :       __m256i d = _mm256_loadu_si256((const __m256i *)(data2 + (y + 0) * stride2 + 32));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	mov	ecx, DWORD PTR _data2$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [ecx+eax+32]
	vmovdqu	YMMWORD PTR _d$3[ebp], ymm0

; 129  : 
; 130  :       __m256i curr_sads_ab = _mm256_sad_epu8(a, b);

	vmovdqu	ymm0, YMMWORD PTR _a$6[ebp]
	vpsadbw	ymm0, ymm0, YMMWORD PTR _b$5[ebp]
	vmovdqu	YMMWORD PTR _curr_sads_ab$2[ebp], ymm0

; 131  :       __m256i curr_sads_cd = _mm256_sad_epu8(c, d);

	vmovdqu	ymm0, YMMWORD PTR _c$4[ebp]
	vpsadbw	ymm0, ymm0, YMMWORD PTR _d$3[ebp]
	vmovdqu	YMMWORD PTR _curr_sads_cd$1[ebp], ymm0

; 132  :       avx_inc = _mm256_add_epi64(avx_inc, curr_sads_ab);

	vmovdqu	ymm0, YMMWORD PTR _avx_inc$[ebp]
	vpaddq	ymm0, ymm0, YMMWORD PTR _curr_sads_ab$2[ebp]
	vmovdqu	YMMWORD PTR _avx_inc$[ebp], ymm0

; 133  :       avx_inc = _mm256_add_epi64(avx_inc, curr_sads_cd);

	vmovdqu	ymm0, YMMWORD PTR _avx_inc$[ebp]
	vpaddq	ymm0, ymm0, YMMWORD PTR _curr_sads_cd$1[ebp]
	vmovdqu	YMMWORD PTR _avx_inc$[ebp], ymm0

; 134  :     }

	jmp	$LN5@reg_sad_w6
$LN8@reg_sad_w6:

; 135  :   }
; 136  : 
; 137  :   __m128i inchi = _mm256_extracti128_si256(avx_inc, 1);

	vmovdqu	ymm0, YMMWORD PTR _avx_inc$[ebp]
	vextracti128 xmm0, ymm0, 1
	vmovdqa	XMMWORD PTR _inchi$[ebp], xmm0

; 138  :   __m128i inclo = _mm256_castsi256_si128  (avx_inc);

	vmovdqu	xmm0, XMMWORD PTR _avx_inc$[ebp]
	vmovdqa	XMMWORD PTR _inclo$[ebp], xmm0

; 139  : 
; 140  :   __m128i sum_1 = _mm_add_epi64    (inclo, inchi);

	vmovdqa	xmm0, XMMWORD PTR _inclo$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _inchi$[ebp]
	vmovdqa	XMMWORD PTR _sum_1$[ebp], xmm0

; 141  :   __m128i sum_2 = _mm_shuffle_epi32(sum_1, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sum_1$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sum_2$[ebp], xmm0

; 142  :   __m128i sad   = _mm_add_epi64    (sum_1, sum_2);

	vmovdqa	xmm0, XMMWORD PTR _sum_1$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sum_2$[ebp]
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 143  : 
; 144  :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 145  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_reg_sad_w64 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\reg_sad_pow2_widths-avx2.h
;	COMDAT _reg_sad_w32
_TEXT	SEGMENT
_sad$ = -1216						; size = 16
_sum_2$ = -1184						; size = 16
_sum_1$ = -1152						; size = 16
_inclo$ = -1120						; size = 16
_inchi$ = -1088						; size = 16
_curr_sads$1 = -1056					; size = 32
_b$2 = -992						; size = 32
_a$3 = -928						; size = 32
_curr_sads_gh$4 = -864					; size = 32
_curr_sads_ef$5 = -800					; size = 32
_curr_sads_cd$6 = -736					; size = 32
_curr_sads_ab$7 = -672					; size = 32
_h$8 = -608						; size = 32
_g$9 = -544						; size = 32
_f$10 = -480						; size = 32
_e$11 = -416						; size = 32
_d$12 = -352						; size = 32
_c$13 = -288						; size = 32
_b$14 = -224						; size = 32
_a$15 = -160						; size = 32
_height_residual_lines$ = -100				; size = 4
_height_fourline_groups$ = -88				; size = 4
_y$ = -76						; size = 4
_avx_inc$ = -64						; size = 32
_data1$ = 8						; size = 4
_data2$ = 12						; size = 4
_height$ = 16						; size = 4
_stride1$ = 20						; size = 4
_stride2$ = 24						; size = 4
_reg_sad_w32 PROC					; COMDAT

; 45   : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -32				; ffffffe0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 1816				; 00000718H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-1816]
	mov	ecx, 454				; 000001c6H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __1E5A298B_reg_sad_pow2_widths-avx2@h
	call	@__CheckForDebuggerJustMyCode@4

; 46   :   __m256i avx_inc = _mm256_setzero_si256();

	vpxor	xmm0, xmm0, xmm0
	vmovdqu	YMMWORD PTR _avx_inc$[ebp], ymm0

; 47   :   int32_t y;
; 48   : 
; 49   :   const int32_t height_fourline_groups = height & ~3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, -4					; fffffffcH
	mov	DWORD PTR _height_fourline_groups$[ebp], eax

; 50   :   const int32_t height_residual_lines  = height &  3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, 3
	mov	DWORD PTR _height_residual_lines$[ebp], eax

; 51   : 
; 52   :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@reg_sad_w3
$LN2@reg_sad_w3:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 4
	mov	DWORD PTR _y$[ebp], eax
$LN4@reg_sad_w3:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height_fourline_groups$[ebp]
	jge	$LN3@reg_sad_w3

; 53   :     __m256i a = _mm256_loadu_si256((const __m256i *)(data1 + (y + 0) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _a$15[ebp], ymm0

; 54   :     __m256i b = _mm256_loadu_si256((const __m256i *)(data2 + (y + 0) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _b$14[ebp], ymm0

; 55   :     __m256i c = _mm256_loadu_si256((const __m256i *)(data1 + (y + 1) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _c$13[ebp], ymm0

; 56   :     __m256i d = _mm256_loadu_si256((const __m256i *)(data2 + (y + 1) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _d$12[ebp], ymm0

; 57   :     __m256i e = _mm256_loadu_si256((const __m256i *)(data1 + (y + 2) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _e$11[ebp], ymm0

; 58   :     __m256i f = _mm256_loadu_si256((const __m256i *)(data2 + (y + 2) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _f$10[ebp], ymm0

; 59   :     __m256i g = _mm256_loadu_si256((const __m256i *)(data1 + (y + 3) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _g$9[ebp], ymm0

; 60   :     __m256i h = _mm256_loadu_si256((const __m256i *)(data2 + (y + 3) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _h$8[ebp], ymm0

; 61   : 
; 62   :     __m256i curr_sads_ab = _mm256_sad_epu8(a, b);

	vmovdqu	ymm0, YMMWORD PTR _a$15[ebp]
	vpsadbw	ymm0, ymm0, YMMWORD PTR _b$14[ebp]
	vmovdqu	YMMWORD PTR _curr_sads_ab$7[ebp], ymm0

; 63   :     __m256i curr_sads_cd = _mm256_sad_epu8(c, d);

	vmovdqu	ymm0, YMMWORD PTR _c$13[ebp]
	vpsadbw	ymm0, ymm0, YMMWORD PTR _d$12[ebp]
	vmovdqu	YMMWORD PTR _curr_sads_cd$6[ebp], ymm0

; 64   :     __m256i curr_sads_ef = _mm256_sad_epu8(e, f);

	vmovdqu	ymm0, YMMWORD PTR _e$11[ebp]
	vpsadbw	ymm0, ymm0, YMMWORD PTR _f$10[ebp]
	vmovdqu	YMMWORD PTR _curr_sads_ef$5[ebp], ymm0

; 65   :     __m256i curr_sads_gh = _mm256_sad_epu8(g, h);

	vmovdqu	ymm0, YMMWORD PTR _g$9[ebp]
	vpsadbw	ymm0, ymm0, YMMWORD PTR _h$8[ebp]
	vmovdqu	YMMWORD PTR _curr_sads_gh$4[ebp], ymm0

; 66   : 
; 67   :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_ab);

	vmovdqu	ymm0, YMMWORD PTR _avx_inc$[ebp]
	vpaddq	ymm0, ymm0, YMMWORD PTR _curr_sads_ab$7[ebp]
	vmovdqu	YMMWORD PTR _avx_inc$[ebp], ymm0

; 68   :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_cd);

	vmovdqu	ymm0, YMMWORD PTR _avx_inc$[ebp]
	vpaddq	ymm0, ymm0, YMMWORD PTR _curr_sads_cd$6[ebp]
	vmovdqu	YMMWORD PTR _avx_inc$[ebp], ymm0

; 69   :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_ef);

	vmovdqu	ymm0, YMMWORD PTR _avx_inc$[ebp]
	vpaddq	ymm0, ymm0, YMMWORD PTR _curr_sads_ef$5[ebp]
	vmovdqu	YMMWORD PTR _avx_inc$[ebp], ymm0

; 70   :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_gh);

	vmovdqu	ymm0, YMMWORD PTR _avx_inc$[ebp]
	vpaddq	ymm0, ymm0, YMMWORD PTR _curr_sads_gh$4[ebp]
	vmovdqu	YMMWORD PTR _avx_inc$[ebp], ymm0

; 71   :   }

	jmp	$LN2@reg_sad_w3
$LN3@reg_sad_w3:

; 72   :   if (height_residual_lines) {

	cmp	DWORD PTR _height_residual_lines$[ebp], 0
	je	SHORT $LN8@reg_sad_w3

; 73   :     for (; y < height; y++) {

	jmp	SHORT $LN7@reg_sad_w3
$LN5@reg_sad_w3:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN7@reg_sad_w3:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	SHORT $LN8@reg_sad_w3

; 74   :       __m256i a = _mm256_loadu_si256((const __m256i *)(data1 + (y + 0) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _a$3[ebp], ymm0

; 75   :       __m256i b = _mm256_loadu_si256((const __m256i *)(data2 + (y + 0) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovdqu	ymm0, YMMWORD PTR [eax]
	vmovdqu	YMMWORD PTR _b$2[ebp], ymm0

; 76   : 
; 77   :       __m256i curr_sads = _mm256_sad_epu8(a, b);

	vmovdqu	ymm0, YMMWORD PTR _a$3[ebp]
	vpsadbw	ymm0, ymm0, YMMWORD PTR _b$2[ebp]
	vmovdqu	YMMWORD PTR _curr_sads$1[ebp], ymm0

; 78   :       avx_inc = _mm256_add_epi64(avx_inc, curr_sads);

	vmovdqu	ymm0, YMMWORD PTR _avx_inc$[ebp]
	vpaddq	ymm0, ymm0, YMMWORD PTR _curr_sads$1[ebp]
	vmovdqu	YMMWORD PTR _avx_inc$[ebp], ymm0

; 79   :     }

	jmp	SHORT $LN5@reg_sad_w3
$LN8@reg_sad_w3:

; 80   :   }
; 81   : 
; 82   :   __m128i inchi = _mm256_extracti128_si256(avx_inc, 1);

	vmovdqu	ymm0, YMMWORD PTR _avx_inc$[ebp]
	vextracti128 xmm0, ymm0, 1
	vmovdqa	XMMWORD PTR _inchi$[ebp], xmm0

; 83   :   __m128i inclo = _mm256_castsi256_si128  (avx_inc);

	vmovdqu	xmm0, XMMWORD PTR _avx_inc$[ebp]
	vmovdqa	XMMWORD PTR _inclo$[ebp], xmm0

; 84   : 
; 85   :   __m128i sum_1 = _mm_add_epi64    (inclo, inchi);

	vmovdqa	xmm0, XMMWORD PTR _inclo$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _inchi$[ebp]
	vmovdqa	XMMWORD PTR _sum_1$[ebp], xmm0

; 86   :   __m128i sum_2 = _mm_shuffle_epi32(sum_1, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sum_1$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sum_2$[ebp], xmm0

; 87   :   __m128i sad   = _mm_add_epi64    (sum_1, sum_2);

	vmovdqa	xmm0, XMMWORD PTR _sum_1$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sum_2$[ebp]
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 88   : 
; 89   :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 90   : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_reg_sad_w32 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
;	COMDAT _hor_sad_sse41_arbitrary
_TEXT	SEGMENT
_sad$ = -3120						; size = 16
_sse_inc_2$ = -3088					; size = 16
_sad_ab$1 = -3056					; size = 16
_b_unread$2 = -3024					; size = 16
_unrd_mask$3 = -2992					; size = 16
_unrd_imask$4 = -2960					; size = 16
_curr_ns$5 = -2928					; size = 16
_startoffs$6 = -2896					; size = 16
_startoff$7 = -2857					; size = 1
_b_with_old$8 = -2848					; size = 16
_b_shifted$9 = -2816					; size = 16
_b$10 = -2784						; size = 16
_a$11 = -2752						; size = 16
_old_b$12 = -2720					; size = 16
_leftoff_with_sign_neg$13 = -2696			; size = 4
_a_off$14 = -2684					; size = 4
_sad_ab$15 = -2672					; size = 16
_b_unread$16 = -2640					; size = 16
_unrd_mask$17 = -2608					; size = 16
_unrd_imask$18 = -2576					; size = 16
_ns$19 = -2544						; size = 16
_startoffs$20 = -2512					; size = 16
_a$21 = -2480						; size = 16
_borderpx_vec$22 = -2448				; size = 16
_sad_gh$23 = -2416					; size = 16
_sad_ef$24 = -2384					; size = 16
_sad_cd$25 = -2352					; size = 16
_sad_ab$26 = -2320					; size = 16
_h_unread$27 = -2288					; size = 16
_f_unread$28 = -2256					; size = 16
_d_unread$29 = -2224					; size = 16
_b_unread$30 = -2192					; size = 16
_unrd_mask$31 = -2160					; size = 16
_unrd_imask$32 = -2128					; size = 16
_curr_ns$33 = -2096					; size = 16
_startoffs$34 = -2064					; size = 16
_startoff$35 = -2025					; size = 1
_h_with_old$36 = -2016					; size = 16
_f_with_old$37 = -1984					; size = 16
_d_with_old$38 = -1952					; size = 16
_b_with_old$39 = -1920					; size = 16
_h_shifted$40 = -1888					; size = 16
_f_shifted$41 = -1856					; size = 16
_d_shifted$42 = -1824					; size = 16
_b_shifted$43 = -1792					; size = 16
_h$44 = -1760						; size = 16
_f$45 = -1728						; size = 16
_d$46 = -1696						; size = 16
_b$47 = -1664						; size = 16
_g$48 = -1632						; size = 16
_e$49 = -1600						; size = 16
_c$50 = -1568						; size = 16
_a$51 = -1536						; size = 16
_old_h$52 = -1504					; size = 16
_old_f$53 = -1472					; size = 16
_old_d$54 = -1440					; size = 16
_old_b$55 = -1408					; size = 16
_leftoff_with_sign_neg$56 = -1384			; size = 4
_a_off$57 = -1372					; size = 4
_sad_gh$58 = -1360					; size = 16
_sad_ef$59 = -1328					; size = 16
_sad_cd$60 = -1296					; size = 16
_sad_ab$61 = -1264					; size = 16
_h_unread$62 = -1232					; size = 16
_f_unread$63 = -1200					; size = 16
_d_unread$64 = -1168					; size = 16
_b_unread$65 = -1136					; size = 16
_unrd_mask$66 = -1104					; size = 16
_unrd_imask$67 = -1072					; size = 16
_ns$68 = -1040						; size = 16
_startoffs$69 = -1008					; size = 16
_g$70 = -976						; size = 16
_e$71 = -944						; size = 16
_c$72 = -912						; size = 16
_a$73 = -880						; size = 16
_borderpx_vec_h$74 = -848				; size = 16
_borderpx_vec_f$75 = -816				; size = 16
_borderpx_vec_d$76 = -784				; size = 16
_borderpx_vec_b$77 = -752				; size = 16
_y$ = -724						; size = 4
_x$ = -712						; size = 4
_outvec_offset$ = -700					; size = 4
_move_old_to_b_imask$ = -688				; size = 16
_mo2bimask_r$ = -656					; size = 16
_mo2bimask_l$ = -624					; size = 16
_mo2bmask_l$ = -592					; size = 16
_shufmask1$ = -560					; size = 16
_ns_for_sm1$ = -528					; size = 16
_offs_for_sm1$ = -496					; size = 16
_offs_neg$ = -464					; size = 16
_vw_for_left$ = -432					; size = 16
_is_left$ = -400					; size = 16
_left_offsets$ = -368					; size = 16
_invec_linc$ = -344					; size = 4
_invec_lend$ = -332					; size = 4
_invec_lstart$ = -320					; size = 4
_border_off$ = -308					; size = 4
_inside_width$ = -296					; size = 4
_outside_width$ = -284					; size = 4
_is_left_bm$ = -272					; size = 4
_left_offset$ = -260					; size = 4
_inside_vecs$ = -248					; size = 4
_outside_vecs$ = -236					; size = 4
_nslo$ = -224						; size = 16
_vec_widths$ = -192					; size = 16
_blk_widths$ = -160					; size = 16
_rights$ = -128						; size = 16
_height_residual_lines$ = -92				; size = 4
_height_fourline_groups$ = -80				; size = 4
_vec_width_log2$ = -68					; size = 4
_vecwid_bitmask$ = -56					; size = 4
_vec_width$ = -44					; size = 4
_sse_inc$ = -32						; size = 16
_pic_data$ = 8						; size = 4
_ref_data$ = 12						; size = 4
_width$ = 16						; size = 4
_height$ = 20						; size = 4
_pic_stride$ = 24					; size = 4
_ref_stride$ = 28					; size = 4
_left$ = 32						; size = 4
_right$ = 36						; size = 4
_hor_sad_sse41_arbitrary PROC				; COMDAT

; 842  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	mov	eax, 4472				; 00001178H
	call	__chkstk
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-4472]
	mov	ecx, 1118				; 0000045eH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __9CB4F737_reg_sad_pow2_widths-sse41@h
	call	@__CheckForDebuggerJustMyCode@4

; 843  :   __m128i sse_inc = _mm_setzero_si128();

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 844  : 
; 845  :   const size_t vec_width = 16;

	mov	DWORD PTR _vec_width$[ebp], 16		; 00000010H

; 846  :   const size_t vecwid_bitmask = 15;

	mov	DWORD PTR _vecwid_bitmask$[ebp], 15	; 0000000fH

; 847  :   const size_t vec_width_log2 = 4;

	mov	DWORD PTR _vec_width_log2$[ebp], 4

; 848  : 
; 849  :   const int32_t height_fourline_groups = height & ~3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, -4					; fffffffcH
	mov	DWORD PTR _height_fourline_groups$[ebp], eax

; 850  :   const int32_t height_residual_lines  = height &  3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, 3
	mov	DWORD PTR _height_residual_lines$[ebp], eax

; 851  : 
; 852  :   const __m128i rights     = _mm_set1_epi8((uint8_t)right);

	movsx	eax, BYTE PTR _right$[ebx]
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _rights$[ebp], xmm0

; 853  :   const __m128i blk_widths = _mm_set1_epi8((uint8_t)width);

	movsx	eax, BYTE PTR _width$[ebx]
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _blk_widths$[ebp], xmm0

; 854  :   const __m128i vec_widths = _mm_set1_epi8((uint8_t)vec_width);

	movsx	eax, BYTE PTR _vec_width$[ebp]
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _vec_widths$[ebp], xmm0

; 855  :   const __m128i nslo       = _mm_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);

	vmovdqa	xmm0, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	vmovdqa	XMMWORD PTR _nslo$[ebp], xmm0

; 856  : 
; 857  :   uint32_t outside_vecs,  inside_vecs,  left_offset, is_left_bm;
; 858  :   int32_t  outside_width, inside_width, border_off,  invec_lstart,
; 859  :            invec_lend,    invec_linc;
; 860  :   if (left) {

	cmp	DWORD PTR _left$[ebx], 0
	je	$LN20@hor_sad_ss

; 861  :     outside_vecs  =    left                              >> vec_width_log2;

	mov	eax, DWORD PTR _vec_width_log2$[ebp]
	shrx	ecx, DWORD PTR _left$[ebx], eax
	mov	DWORD PTR _outside_vecs$[ebp], ecx

; 862  :     inside_vecs   = (( width           + vecwid_bitmask) >> vec_width_log2) - outside_vecs;

	mov	eax, DWORD PTR _width$[ebx]
	add	eax, DWORD PTR _vecwid_bitmask$[ebp]
	mov	ecx, DWORD PTR _vec_width_log2$[ebp]
	shrx	edx, eax, ecx
	sub	edx, DWORD PTR _outside_vecs$[ebp]
	mov	DWORD PTR _inside_vecs$[ebp], edx

; 863  :     outside_width =    outside_vecs * vec_width;

	mov	eax, DWORD PTR _outside_vecs$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	mov	DWORD PTR _outside_width$[ebp], eax

; 864  :     inside_width  =    inside_vecs  * vec_width;

	mov	eax, DWORD PTR _inside_vecs$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	mov	DWORD PTR _inside_width$[ebp], eax

; 865  :     left_offset   =    left;

	mov	eax, DWORD PTR _left$[ebx]
	mov	DWORD PTR _left_offset$[ebp], eax

; 866  :     border_off    =    left;

	mov	eax, DWORD PTR _left$[ebx]
	mov	DWORD PTR _border_off$[ebp], eax

; 867  :     invec_lstart  =    0;

	mov	DWORD PTR _invec_lstart$[ebp], 0

; 868  :     invec_lend    =    inside_vecs;

	mov	eax, DWORD PTR _inside_vecs$[ebp]
	mov	DWORD PTR _invec_lend$[ebp], eax

; 869  :     invec_linc    =    1;

	mov	DWORD PTR _invec_linc$[ebp], 1

; 870  :     is_left_bm    =    -1;

	mov	DWORD PTR _is_left_bm$[ebp], -1

; 871  :   } else {

	jmp	$LN21@hor_sad_ss
$LN20@hor_sad_ss:

; 872  :     inside_vecs   =  ((width - right) + vecwid_bitmask)  >> vec_width_log2;

	mov	eax, DWORD PTR _width$[ebx]
	sub	eax, DWORD PTR _right$[ebx]
	add	eax, DWORD PTR _vecwid_bitmask$[ebp]
	mov	ecx, DWORD PTR _vec_width_log2$[ebp]
	shrx	edx, eax, ecx
	mov	DWORD PTR _inside_vecs$[ebp], edx

; 873  :     outside_vecs  = (( width          + vecwid_bitmask)  >> vec_width_log2) - inside_vecs;

	mov	eax, DWORD PTR _width$[ebx]
	add	eax, DWORD PTR _vecwid_bitmask$[ebp]
	mov	ecx, DWORD PTR _vec_width_log2$[ebp]
	shrx	edx, eax, ecx
	sub	edx, DWORD PTR _inside_vecs$[ebp]
	mov	DWORD PTR _outside_vecs$[ebp], edx

; 874  :     outside_width =    outside_vecs * vec_width;

	mov	eax, DWORD PTR _outside_vecs$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	mov	DWORD PTR _outside_width$[ebp], eax

; 875  :     inside_width  =    inside_vecs  * vec_width;

	mov	eax, DWORD PTR _inside_vecs$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	mov	DWORD PTR _inside_width$[ebp], eax

; 876  :     left_offset   =    right - width;

	mov	eax, DWORD PTR _right$[ebx]
	sub	eax, DWORD PTR _width$[ebx]
	mov	DWORD PTR _left_offset$[ebp], eax

; 877  :     border_off    =    width - 1 - right;

	mov	eax, DWORD PTR _width$[ebx]
	sub	eax, 1
	sub	eax, DWORD PTR _right$[ebx]
	mov	DWORD PTR _border_off$[ebp], eax

; 878  :     invec_lstart  =    inside_vecs - 1;

	mov	eax, DWORD PTR _inside_vecs$[ebp]
	sub	eax, 1
	mov	DWORD PTR _invec_lstart$[ebp], eax

; 879  :     invec_lend    =    -1;

	mov	DWORD PTR _invec_lend$[ebp], -1

; 880  :     invec_linc    =    -1;

	mov	DWORD PTR _invec_linc$[ebp], -1

; 881  :     is_left_bm    =    0;

	mov	DWORD PTR _is_left_bm$[ebp], 0
$LN21@hor_sad_ss:

; 882  :   }
; 883  :   left_offset &= vecwid_bitmask;

	mov	eax, DWORD PTR _left_offset$[ebp]
	and	eax, DWORD PTR _vecwid_bitmask$[ebp]
	mov	DWORD PTR _left_offset$[ebp], eax

; 884  : 
; 885  :   const __m128i left_offsets = _mm_set1_epi8 ((uint8_t)left_offset);

	movsx	eax, BYTE PTR _left_offset$[ebp]
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _left_offsets$[ebp], xmm0

; 886  :   const __m128i is_left      = _mm_cmpeq_epi8(rights, _mm_setzero_si128());

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	xmm1, XMMWORD PTR _rights$[ebp]
	vpcmpeqb xmm0, xmm1, xmm0
	vmovdqa	XMMWORD PTR _is_left$[ebp], xmm0

; 887  :   const __m128i vw_for_left  = _mm_and_si128 (is_left, vec_widths);

	vmovdqa	xmm0, XMMWORD PTR _is_left$[ebp]
	vpand	xmm0, xmm0, XMMWORD PTR _vec_widths$[ebp]
	vmovdqa	XMMWORD PTR _vw_for_left$[ebp], xmm0

; 888  : 
; 889  :   // -x == (x ^ 0xff) + 1 = (x ^ 0xff) - 0xff. Also x == (x ^ 0x00) - 0x00.
; 890  :   // in other words, calculate inverse of left_offsets if is_left is true.
; 891  :   const __m128i offs_neg            = _mm_xor_si128 (left_offsets, is_left);

	vmovdqa	xmm0, XMMWORD PTR _left_offsets$[ebp]
	vpxor	xmm0, xmm0, XMMWORD PTR _is_left$[ebp]
	vmovdqa	XMMWORD PTR _offs_neg$[ebp], xmm0

; 892  :   const __m128i offs_for_sm1        = _mm_sub_epi8  (offs_neg,     is_left);

	vmovdqa	xmm0, XMMWORD PTR _offs_neg$[ebp]
	vpsubb	xmm0, xmm0, XMMWORD PTR _is_left$[ebp]
	vmovdqa	XMMWORD PTR _offs_for_sm1$[ebp], xmm0

; 893  : 
; 894  :   const __m128i ns_for_sm1          = _mm_or_si128  (vw_for_left,  nslo);

	vmovdqa	xmm0, XMMWORD PTR _vw_for_left$[ebp]
	vpor	xmm0, xmm0, XMMWORD PTR _nslo$[ebp]
	vmovdqa	XMMWORD PTR _ns_for_sm1$[ebp], xmm0

; 895  :   const __m128i shufmask1           = _mm_add_epi8  (ns_for_sm1,   offs_for_sm1);

	vmovdqa	xmm0, XMMWORD PTR _ns_for_sm1$[ebp]
	vpaddb	xmm0, xmm0, XMMWORD PTR _offs_for_sm1$[ebp]
	vmovdqa	XMMWORD PTR _shufmask1$[ebp], xmm0

; 896  : 
; 897  :   const __m128i mo2bmask_l          = _mm_cmpgt_epi8(left_offsets, nslo);

	vmovdqa	xmm0, XMMWORD PTR _left_offsets$[ebp]
	vpcmpgtb xmm0, xmm0, XMMWORD PTR _nslo$[ebp]
	vmovdqa	XMMWORD PTR _mo2bmask_l$[ebp], xmm0

; 898  :   const __m128i mo2bimask_l         = _mm_cmpeq_epi8(mo2bmask_l,   _mm_setzero_si128());

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	xmm1, XMMWORD PTR _mo2bmask_l$[ebp]
	vpcmpeqb xmm0, xmm1, xmm0
	vmovdqa	XMMWORD PTR _mo2bimask_l$[ebp], xmm0

; 899  :   const __m128i mo2bimask_r         = _mm_cmpgt_epi8(vec_widths,   shufmask1);

	vmovdqa	xmm0, XMMWORD PTR _vec_widths$[ebp]
	vpcmpgtb xmm0, xmm0, XMMWORD PTR _shufmask1$[ebp]
	vmovdqa	XMMWORD PTR _mo2bimask_r$[ebp], xmm0

; 900  :   const __m128i move_old_to_b_imask = _mm_blendv_epi8(mo2bimask_r, mo2bimask_l, is_left);

	vmovdqa	xmm0, XMMWORD PTR _is_left$[ebp]
	vmovdqa	xmm1, XMMWORD PTR _mo2bimask_r$[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _mo2bimask_l$[ebp], xmm0
	vmovdqa	XMMWORD PTR _move_old_to_b_imask$[ebp], xmm0

; 901  : 
; 902  :   const int32_t outvec_offset = (~is_left_bm) & inside_width;

	mov	eax, DWORD PTR _is_left_bm$[ebp]
	not	eax
	and	eax, DWORD PTR _inside_width$[ebp]
	mov	DWORD PTR _outvec_offset$[ebp], eax

; 903  :   int32_t x, y;
; 904  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@hor_sad_ss
$LN2@hor_sad_ss:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 4
	mov	DWORD PTR _y$[ebp], eax
$LN4@hor_sad_ss:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height_fourline_groups$[ebp]
	jge	$LN3@hor_sad_ss

; 905  :     __m128i borderpx_vec_b = _mm_set1_epi8(ref_data[(int32_t)((y + 0) * ref_stride + border_off)]);

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _border_off$[ebp]
	mov	ecx, DWORD PTR _ref_data$[ebx]
	movsx	edx, BYTE PTR [ecx+eax]
	vmovd	xmm0, edx
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _borderpx_vec_b$77[ebp], xmm0

; 906  :     __m128i borderpx_vec_d = _mm_set1_epi8(ref_data[(int32_t)((y + 1) * ref_stride + border_off)]);

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _border_off$[ebp]
	mov	ecx, DWORD PTR _ref_data$[ebx]
	movsx	edx, BYTE PTR [ecx+eax]
	vmovd	xmm0, edx
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _borderpx_vec_d$76[ebp], xmm0

; 907  :     __m128i borderpx_vec_f = _mm_set1_epi8(ref_data[(int32_t)((y + 2) * ref_stride + border_off)]);

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _border_off$[ebp]
	mov	ecx, DWORD PTR _ref_data$[ebx]
	movsx	edx, BYTE PTR [ecx+eax]
	vmovd	xmm0, edx
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _borderpx_vec_f$75[ebp], xmm0

; 908  :     __m128i borderpx_vec_h = _mm_set1_epi8(ref_data[(int32_t)((y + 3) * ref_stride + border_off)]);

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _border_off$[ebp]
	mov	ecx, DWORD PTR _ref_data$[ebx]
	movsx	edx, BYTE PTR [ecx+eax]
	vmovd	xmm0, edx
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _borderpx_vec_h$74[ebp], xmm0

; 909  : 
; 910  :     for (x = 0; x < outside_vecs; x++) {

	mov	DWORD PTR _x$[ebp], 0
	jmp	SHORT $LN7@hor_sad_ss
$LN5@hor_sad_ss:
	mov	eax, DWORD PTR _x$[ebp]
	add	eax, 1
	mov	DWORD PTR _x$[ebp], eax
$LN7@hor_sad_ss:
	mov	eax, DWORD PTR _x$[ebp]
	cmp	eax, DWORD PTR _outside_vecs$[ebp]
	jae	$LN6@hor_sad_ss

; 911  :       __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + outvec_offset));

	mov	eax, DWORD PTR _x$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	add	eax, DWORD PTR _pic_data$[ebx]
	mov	ecx, DWORD PTR _y$[ebp]
	imul	ecx, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _outvec_offset$[ebp]
	add	ecx, eax
	vmovdqu	xmm0, XMMWORD PTR [ecx]
	vmovdqa	XMMWORD PTR _a$73[ebp], xmm0

; 912  :       __m128i c = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 1) * pic_stride + outvec_offset));

	mov	eax, DWORD PTR _x$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	add	eax, DWORD PTR _pic_data$[ebx]
	mov	ecx, DWORD PTR _y$[ebp]
	add	ecx, 1
	imul	ecx, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _outvec_offset$[ebp]
	add	ecx, eax
	vmovdqu	xmm0, XMMWORD PTR [ecx]
	vmovdqa	XMMWORD PTR _c$72[ebp], xmm0

; 913  :       __m128i e = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 2) * pic_stride + outvec_offset));

	mov	eax, DWORD PTR _x$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	add	eax, DWORD PTR _pic_data$[ebx]
	mov	ecx, DWORD PTR _y$[ebp]
	add	ecx, 2
	imul	ecx, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _outvec_offset$[ebp]
	add	ecx, eax
	vmovdqu	xmm0, XMMWORD PTR [ecx]
	vmovdqa	XMMWORD PTR _e$71[ebp], xmm0

; 914  :       __m128i g = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 3) * pic_stride + outvec_offset));

	mov	eax, DWORD PTR _x$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	add	eax, DWORD PTR _pic_data$[ebx]
	mov	ecx, DWORD PTR _y$[ebp]
	add	ecx, 3
	imul	ecx, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _outvec_offset$[ebp]
	add	ecx, eax
	vmovdqu	xmm0, XMMWORD PTR [ecx]
	vmovdqa	XMMWORD PTR _g$70[ebp], xmm0

; 915  : 
; 916  :       __m128i startoffs  = _mm_set1_epi8  ((x + inside_vecs) << vec_width_log2);

	mov	eax, DWORD PTR _x$[ebp]
	add	eax, DWORD PTR _inside_vecs$[ebp]
	mov	ecx, DWORD PTR _vec_width_log2$[ebp]
	shlx	edx, eax, ecx
	movsx	eax, dl
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _startoffs$69[ebp], xmm0

; 917  :       __m128i ns         = _mm_add_epi8   (startoffs, nslo);

	vmovdqa	xmm0, XMMWORD PTR _startoffs$69[ebp]
	vpaddb	xmm0, xmm0, XMMWORD PTR _nslo$[ebp]
	vmovdqa	XMMWORD PTR _ns$68[ebp], xmm0

; 918  : 
; 919  :       // Unread imask is (is_left NOR unrd_imask_for_right), do the maths etc
; 920  :       __m128i unrd_imask = _mm_cmpgt_epi8 (blk_widths, ns);

	vmovdqa	xmm0, XMMWORD PTR _blk_widths$[ebp]
	vpcmpgtb xmm0, xmm0, XMMWORD PTR _ns$68[ebp]
	vmovdqa	XMMWORD PTR _unrd_imask$67[ebp], xmm0

; 921  :               unrd_imask = _mm_or_si128   (unrd_imask, is_left);

	vmovdqa	xmm0, XMMWORD PTR _unrd_imask$67[ebp]
	vpor	xmm0, xmm0, XMMWORD PTR _is_left$[ebp]
	vmovdqa	XMMWORD PTR _unrd_imask$67[ebp], xmm0

; 922  :       __m128i unrd_mask  = _mm_cmpeq_epi8 (unrd_imask, _mm_setzero_si128());

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	xmm1, XMMWORD PTR _unrd_imask$67[ebp]
	vpcmpeqb xmm0, xmm1, xmm0
	vmovdqa	XMMWORD PTR _unrd_mask$66[ebp], xmm0

; 923  : 
; 924  :       __m128i b_unread   = _mm_blendv_epi8(borderpx_vec_b, a, unrd_mask);

	vmovdqa	xmm0, XMMWORD PTR _unrd_mask$66[ebp]
	vmovdqa	xmm1, XMMWORD PTR _borderpx_vec_b$77[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _a$73[ebp], xmm0
	vmovdqa	XMMWORD PTR _b_unread$65[ebp], xmm0

; 925  :       __m128i d_unread   = _mm_blendv_epi8(borderpx_vec_d, c, unrd_mask);

	vmovdqa	xmm0, XMMWORD PTR _unrd_mask$66[ebp]
	vmovdqa	xmm1, XMMWORD PTR _borderpx_vec_d$76[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _c$72[ebp], xmm0
	vmovdqa	XMMWORD PTR _d_unread$64[ebp], xmm0

; 926  :       __m128i f_unread   = _mm_blendv_epi8(borderpx_vec_f, e, unrd_mask);

	vmovdqa	xmm0, XMMWORD PTR _unrd_mask$66[ebp]
	vmovdqa	xmm1, XMMWORD PTR _borderpx_vec_f$75[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _e$71[ebp], xmm0
	vmovdqa	XMMWORD PTR _f_unread$63[ebp], xmm0

; 927  :       __m128i h_unread   = _mm_blendv_epi8(borderpx_vec_h, g, unrd_mask);

	vmovdqa	xmm0, XMMWORD PTR _unrd_mask$66[ebp]
	vmovdqa	xmm1, XMMWORD PTR _borderpx_vec_h$74[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _g$70[ebp], xmm0
	vmovdqa	XMMWORD PTR _h_unread$62[ebp], xmm0

; 928  : 
; 929  :       __m128i sad_ab     = _mm_sad_epu8   (a, b_unread);

	vmovdqa	xmm0, XMMWORD PTR _a$73[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b_unread$65[ebp]
	vmovdqa	XMMWORD PTR _sad_ab$61[ebp], xmm0

; 930  :       __m128i sad_cd     = _mm_sad_epu8   (c, d_unread);

	vmovdqa	xmm0, XMMWORD PTR _c$72[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _d_unread$64[ebp]
	vmovdqa	XMMWORD PTR _sad_cd$60[ebp], xmm0

; 931  :       __m128i sad_ef     = _mm_sad_epu8   (e, f_unread);

	vmovdqa	xmm0, XMMWORD PTR _e$71[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _f_unread$63[ebp]
	vmovdqa	XMMWORD PTR _sad_ef$59[ebp], xmm0

; 932  :       __m128i sad_gh     = _mm_sad_epu8   (g, h_unread);

	vmovdqa	xmm0, XMMWORD PTR _g$70[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _h_unread$62[ebp]
	vmovdqa	XMMWORD PTR _sad_gh$58[ebp], xmm0

; 933  : 
; 934  :       sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sad_ab$61[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 935  :       sse_inc = _mm_add_epi64(sse_inc, sad_cd);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sad_cd$60[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 936  :       sse_inc = _mm_add_epi64(sse_inc, sad_ef);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sad_ef$59[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 937  :       sse_inc = _mm_add_epi64(sse_inc, sad_gh);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sad_gh$58[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 938  :     }

	jmp	$LN5@hor_sad_ss
$LN6@hor_sad_ss:

; 939  :     int32_t a_off = outside_width & is_left_bm;

	mov	eax, DWORD PTR _outside_width$[ebp]
	and	eax, DWORD PTR _is_left_bm$[ebp]
	mov	DWORD PTR _a_off$57[ebp], eax

; 940  :     int32_t leftoff_with_sign_neg = (left_offset ^ is_left_bm) - is_left_bm;

	mov	eax, DWORD PTR _left_offset$[ebp]
	xor	eax, DWORD PTR _is_left_bm$[ebp]
	sub	eax, DWORD PTR _is_left_bm$[ebp]
	mov	DWORD PTR _leftoff_with_sign_neg$56[ebp], eax

; 941  : 
; 942  :     __m128i old_b = borderpx_vec_b;

	vmovdqa	xmm0, XMMWORD PTR _borderpx_vec_b$77[ebp]
	vmovdqa	XMMWORD PTR _old_b$55[ebp], xmm0

; 943  :     __m128i old_d = borderpx_vec_d;

	vmovdqa	xmm0, XMMWORD PTR _borderpx_vec_d$76[ebp]
	vmovdqa	XMMWORD PTR _old_d$54[ebp], xmm0

; 944  :     __m128i old_f = borderpx_vec_f;

	vmovdqa	xmm0, XMMWORD PTR _borderpx_vec_f$75[ebp]
	vmovdqa	XMMWORD PTR _old_f$53[ebp], xmm0

; 945  :     __m128i old_h = borderpx_vec_h;

	vmovdqa	xmm0, XMMWORD PTR _borderpx_vec_h$74[ebp]
	vmovdqa	XMMWORD PTR _old_h$52[ebp], xmm0

; 946  : 
; 947  :     for (x = invec_lstart; x != invec_lend; x += invec_linc) {

	mov	eax, DWORD PTR _invec_lstart$[ebp]
	mov	DWORD PTR _x$[ebp], eax
	jmp	SHORT $LN10@hor_sad_ss
$LN8@hor_sad_ss:
	mov	eax, DWORD PTR _x$[ebp]
	add	eax, DWORD PTR _invec_linc$[ebp]
	mov	DWORD PTR _x$[ebp], eax
$LN10@hor_sad_ss:
	mov	eax, DWORD PTR _x$[ebp]
	cmp	eax, DWORD PTR _invec_lend$[ebp]
	je	$LN9@hor_sad_ss

; 948  :       __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + a_off));

	mov	eax, DWORD PTR _x$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	add	eax, DWORD PTR _pic_data$[ebx]
	mov	ecx, DWORD PTR _y$[ebp]
	imul	ecx, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _a_off$57[ebp]
	add	ecx, eax
	vmovdqu	xmm0, XMMWORD PTR [ecx]
	vmovdqa	XMMWORD PTR _a$51[ebp], xmm0

; 949  :       __m128i c = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 1) * pic_stride + a_off));

	mov	eax, DWORD PTR _x$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	add	eax, DWORD PTR _pic_data$[ebx]
	mov	ecx, DWORD PTR _y$[ebp]
	add	ecx, 1
	imul	ecx, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _a_off$57[ebp]
	add	ecx, eax
	vmovdqu	xmm0, XMMWORD PTR [ecx]
	vmovdqa	XMMWORD PTR _c$50[ebp], xmm0

; 950  :       __m128i e = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 2) * pic_stride + a_off));

	mov	eax, DWORD PTR _x$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	add	eax, DWORD PTR _pic_data$[ebx]
	mov	ecx, DWORD PTR _y$[ebp]
	add	ecx, 2
	imul	ecx, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _a_off$57[ebp]
	add	ecx, eax
	vmovdqu	xmm0, XMMWORD PTR [ecx]
	vmovdqa	XMMWORD PTR _e$49[ebp], xmm0

; 951  :       __m128i g = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 3) * pic_stride + a_off));

	mov	eax, DWORD PTR _x$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	add	eax, DWORD PTR _pic_data$[ebx]
	mov	ecx, DWORD PTR _y$[ebp]
	add	ecx, 3
	imul	ecx, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _a_off$57[ebp]
	add	ecx, eax
	vmovdqu	xmm0, XMMWORD PTR [ecx]
	vmovdqa	XMMWORD PTR _g$48[ebp], xmm0

; 952  :       __m128i b = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 0) * ref_stride + a_off - leftoff_with_sign_neg));

	mov	eax, DWORD PTR _x$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	add	eax, DWORD PTR _ref_data$[ebx]
	mov	ecx, DWORD PTR _y$[ebp]
	imul	ecx, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _a_off$57[ebp]
	add	ecx, eax
	sub	ecx, DWORD PTR _leftoff_with_sign_neg$56[ebp]
	vmovdqu	xmm0, XMMWORD PTR [ecx]
	vmovdqa	XMMWORD PTR _b$47[ebp], xmm0

; 953  :       __m128i d = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 1) * ref_stride + a_off - leftoff_with_sign_neg));

	mov	eax, DWORD PTR _x$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	add	eax, DWORD PTR _ref_data$[ebx]
	mov	ecx, DWORD PTR _y$[ebp]
	add	ecx, 1
	imul	ecx, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _a_off$57[ebp]
	add	ecx, eax
	sub	ecx, DWORD PTR _leftoff_with_sign_neg$56[ebp]
	vmovdqu	xmm0, XMMWORD PTR [ecx]
	vmovdqa	XMMWORD PTR _d$46[ebp], xmm0

; 954  :       __m128i f = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 2) * ref_stride + a_off - leftoff_with_sign_neg));

	mov	eax, DWORD PTR _x$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	add	eax, DWORD PTR _ref_data$[ebx]
	mov	ecx, DWORD PTR _y$[ebp]
	add	ecx, 2
	imul	ecx, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _a_off$57[ebp]
	add	ecx, eax
	sub	ecx, DWORD PTR _leftoff_with_sign_neg$56[ebp]
	vmovdqu	xmm0, XMMWORD PTR [ecx]
	vmovdqa	XMMWORD PTR _f$45[ebp], xmm0

; 955  :       __m128i h = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 3) * ref_stride + a_off - leftoff_with_sign_neg));

	mov	eax, DWORD PTR _x$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	add	eax, DWORD PTR _ref_data$[ebx]
	mov	ecx, DWORD PTR _y$[ebp]
	add	ecx, 3
	imul	ecx, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _a_off$57[ebp]
	add	ecx, eax
	sub	ecx, DWORD PTR _leftoff_with_sign_neg$56[ebp]
	vmovdqu	xmm0, XMMWORD PTR [ecx]
	vmovdqa	XMMWORD PTR _h$44[ebp], xmm0

; 956  : 
; 957  :       __m128i b_shifted    = _mm_shuffle_epi8(b,     shufmask1);

	vmovdqa	xmm0, XMMWORD PTR _b$47[ebp]
	vpshufb	xmm0, xmm0, XMMWORD PTR _shufmask1$[ebp]
	vmovdqa	XMMWORD PTR _b_shifted$43[ebp], xmm0

; 958  :       __m128i d_shifted    = _mm_shuffle_epi8(d,     shufmask1);

	vmovdqa	xmm0, XMMWORD PTR _d$46[ebp]
	vpshufb	xmm0, xmm0, XMMWORD PTR _shufmask1$[ebp]
	vmovdqa	XMMWORD PTR _d_shifted$42[ebp], xmm0

; 959  :       __m128i f_shifted    = _mm_shuffle_epi8(f,     shufmask1);

	vmovdqa	xmm0, XMMWORD PTR _f$45[ebp]
	vpshufb	xmm0, xmm0, XMMWORD PTR _shufmask1$[ebp]
	vmovdqa	XMMWORD PTR _f_shifted$41[ebp], xmm0

; 960  :       __m128i h_shifted    = _mm_shuffle_epi8(h,     shufmask1);

	vmovdqa	xmm0, XMMWORD PTR _h$44[ebp]
	vpshufb	xmm0, xmm0, XMMWORD PTR _shufmask1$[ebp]
	vmovdqa	XMMWORD PTR _h_shifted$40[ebp], xmm0

; 961  : 
; 962  :       __m128i b_with_old   = _mm_blendv_epi8 (old_b, b_shifted, move_old_to_b_imask);

	vmovdqa	xmm0, XMMWORD PTR _move_old_to_b_imask$[ebp]
	vmovdqa	xmm1, XMMWORD PTR _old_b$55[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _b_shifted$43[ebp], xmm0
	vmovdqa	XMMWORD PTR _b_with_old$39[ebp], xmm0

; 963  :       __m128i d_with_old   = _mm_blendv_epi8 (old_d, d_shifted, move_old_to_b_imask);

	vmovdqa	xmm0, XMMWORD PTR _move_old_to_b_imask$[ebp]
	vmovdqa	xmm1, XMMWORD PTR _old_d$54[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _d_shifted$42[ebp], xmm0
	vmovdqa	XMMWORD PTR _d_with_old$38[ebp], xmm0

; 964  :       __m128i f_with_old   = _mm_blendv_epi8 (old_f, f_shifted, move_old_to_b_imask);

	vmovdqa	xmm0, XMMWORD PTR _move_old_to_b_imask$[ebp]
	vmovdqa	xmm1, XMMWORD PTR _old_f$53[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _f_shifted$41[ebp], xmm0
	vmovdqa	XMMWORD PTR _f_with_old$37[ebp], xmm0

; 965  :       __m128i h_with_old   = _mm_blendv_epi8 (old_h, h_shifted, move_old_to_b_imask);

	vmovdqa	xmm0, XMMWORD PTR _move_old_to_b_imask$[ebp]
	vmovdqa	xmm1, XMMWORD PTR _old_h$52[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _h_shifted$40[ebp], xmm0
	vmovdqa	XMMWORD PTR _h_with_old$36[ebp], xmm0

; 966  : 
; 967  :       uint8_t startoff     = (x << vec_width_log2) + a_off;

	mov	eax, DWORD PTR _vec_width_log2$[ebp]
	shlx	ecx, DWORD PTR _x$[ebp], eax
	add	ecx, DWORD PTR _a_off$57[ebp]
	mov	BYTE PTR _startoff$35[ebp], cl

; 968  :       __m128i startoffs    = _mm_set1_epi8   (startoff);

	movsx	eax, BYTE PTR _startoff$35[ebp]
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _startoffs$34[ebp], xmm0

; 969  :       __m128i curr_ns      = _mm_add_epi8    (startoffs,    nslo);

	vmovdqa	xmm0, XMMWORD PTR _startoffs$34[ebp]
	vpaddb	xmm0, xmm0, XMMWORD PTR _nslo$[ebp]
	vmovdqa	XMMWORD PTR _curr_ns$33[ebp], xmm0

; 970  :       __m128i unrd_imask   = _mm_cmpgt_epi8  (blk_widths,   curr_ns);

	vmovdqa	xmm0, XMMWORD PTR _blk_widths$[ebp]
	vpcmpgtb xmm0, xmm0, XMMWORD PTR _curr_ns$33[ebp]
	vmovdqa	XMMWORD PTR _unrd_imask$32[ebp], xmm0

; 971  :       __m128i unrd_mask    = _mm_cmpeq_epi8  (unrd_imask,   _mm_setzero_si128());

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	xmm1, XMMWORD PTR _unrd_imask$32[ebp]
	vpcmpeqb xmm0, xmm1, xmm0
	vmovdqa	XMMWORD PTR _unrd_mask$31[ebp], xmm0

; 972  : 
; 973  :       __m128i b_unread     = _mm_blendv_epi8 (b_with_old,   a, unrd_mask);

	vmovdqa	xmm0, XMMWORD PTR _unrd_mask$31[ebp]
	vmovdqa	xmm1, XMMWORD PTR _b_with_old$39[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _a$51[ebp], xmm0
	vmovdqa	XMMWORD PTR _b_unread$30[ebp], xmm0

; 974  :       __m128i d_unread     = _mm_blendv_epi8 (d_with_old,   c, unrd_mask);

	vmovdqa	xmm0, XMMWORD PTR _unrd_mask$31[ebp]
	vmovdqa	xmm1, XMMWORD PTR _d_with_old$38[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _c$50[ebp], xmm0
	vmovdqa	XMMWORD PTR _d_unread$29[ebp], xmm0

; 975  :       __m128i f_unread     = _mm_blendv_epi8 (f_with_old,   e, unrd_mask);

	vmovdqa	xmm0, XMMWORD PTR _unrd_mask$31[ebp]
	vmovdqa	xmm1, XMMWORD PTR _f_with_old$37[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _e$49[ebp], xmm0
	vmovdqa	XMMWORD PTR _f_unread$28[ebp], xmm0

; 976  :       __m128i h_unread     = _mm_blendv_epi8 (h_with_old,   g, unrd_mask);

	vmovdqa	xmm0, XMMWORD PTR _unrd_mask$31[ebp]
	vmovdqa	xmm1, XMMWORD PTR _h_with_old$36[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _g$48[ebp], xmm0
	vmovdqa	XMMWORD PTR _h_unread$27[ebp], xmm0

; 977  : 
; 978  :       old_b = b_shifted;

	vmovdqa	xmm0, XMMWORD PTR _b_shifted$43[ebp]
	vmovdqa	XMMWORD PTR _old_b$55[ebp], xmm0

; 979  :       old_d = d_shifted;

	vmovdqa	xmm0, XMMWORD PTR _d_shifted$42[ebp]
	vmovdqa	XMMWORD PTR _old_d$54[ebp], xmm0

; 980  :       old_f = f_shifted;

	vmovdqa	xmm0, XMMWORD PTR _f_shifted$41[ebp]
	vmovdqa	XMMWORD PTR _old_f$53[ebp], xmm0

; 981  :       old_h = h_shifted;

	vmovdqa	xmm0, XMMWORD PTR _h_shifted$40[ebp]
	vmovdqa	XMMWORD PTR _old_h$52[ebp], xmm0

; 982  : 
; 983  :       __m128i sad_ab     = _mm_sad_epu8(a, b_unread);

	vmovdqa	xmm0, XMMWORD PTR _a$51[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b_unread$30[ebp]
	vmovdqa	XMMWORD PTR _sad_ab$26[ebp], xmm0

; 984  :       __m128i sad_cd     = _mm_sad_epu8(c, d_unread);

	vmovdqa	xmm0, XMMWORD PTR _c$50[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _d_unread$29[ebp]
	vmovdqa	XMMWORD PTR _sad_cd$25[ebp], xmm0

; 985  :       __m128i sad_ef     = _mm_sad_epu8(e, f_unread);

	vmovdqa	xmm0, XMMWORD PTR _e$49[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _f_unread$28[ebp]
	vmovdqa	XMMWORD PTR _sad_ef$24[ebp], xmm0

; 986  :       __m128i sad_gh     = _mm_sad_epu8(g, h_unread);

	vmovdqa	xmm0, XMMWORD PTR _g$48[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _h_unread$27[ebp]
	vmovdqa	XMMWORD PTR _sad_gh$23[ebp], xmm0

; 987  : 
; 988  :       sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sad_ab$26[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 989  :       sse_inc = _mm_add_epi64(sse_inc, sad_cd);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sad_cd$25[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 990  :       sse_inc = _mm_add_epi64(sse_inc, sad_ef);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sad_ef$24[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 991  :       sse_inc = _mm_add_epi64(sse_inc, sad_gh);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sad_gh$23[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 992  :     }

	jmp	$LN8@hor_sad_ss
$LN9@hor_sad_ss:

; 993  :   }

	jmp	$LN2@hor_sad_ss
$LN3@hor_sad_ss:

; 994  :   if (height_residual_lines) {

	cmp	DWORD PTR _height_residual_lines$[ebp], 0
	je	$LN22@hor_sad_ss

; 995  :     for (; y < height; y++) {

	jmp	SHORT $LN13@hor_sad_ss
$LN11@hor_sad_ss:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN13@hor_sad_ss:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	$LN22@hor_sad_ss

; 996  :       __m128i borderpx_vec = _mm_set1_epi8(ref_data[(int32_t)((y + 0) * ref_stride + border_off)]);

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _border_off$[ebp]
	mov	ecx, DWORD PTR _ref_data$[ebx]
	movsx	edx, BYTE PTR [ecx+eax]
	vmovd	xmm0, edx
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _borderpx_vec$22[ebp], xmm0

; 997  :       for (x = 0; x < outside_vecs; x++) {

	mov	DWORD PTR _x$[ebp], 0
	jmp	SHORT $LN16@hor_sad_ss
$LN14@hor_sad_ss:
	mov	eax, DWORD PTR _x$[ebp]
	add	eax, 1
	mov	DWORD PTR _x$[ebp], eax
$LN16@hor_sad_ss:
	mov	eax, DWORD PTR _x$[ebp]
	cmp	eax, DWORD PTR _outside_vecs$[ebp]
	jae	$LN15@hor_sad_ss

; 998  :         __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + outvec_offset));

	mov	eax, DWORD PTR _x$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	add	eax, DWORD PTR _pic_data$[ebx]
	mov	ecx, DWORD PTR _y$[ebp]
	imul	ecx, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _outvec_offset$[ebp]
	add	ecx, eax
	vmovdqu	xmm0, XMMWORD PTR [ecx]
	vmovdqa	XMMWORD PTR _a$21[ebp], xmm0

; 999  : 
; 1000 :         __m128i startoffs  = _mm_set1_epi8  ((x + inside_vecs) << vec_width_log2);

	mov	eax, DWORD PTR _x$[ebp]
	add	eax, DWORD PTR _inside_vecs$[ebp]
	mov	ecx, DWORD PTR _vec_width_log2$[ebp]
	shlx	edx, eax, ecx
	movsx	eax, dl
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _startoffs$20[ebp], xmm0

; 1001 :         __m128i ns         = _mm_add_epi8   (startoffs, nslo);

	vmovdqa	xmm0, XMMWORD PTR _startoffs$20[ebp]
	vpaddb	xmm0, xmm0, XMMWORD PTR _nslo$[ebp]
	vmovdqa	XMMWORD PTR _ns$19[ebp], xmm0

; 1002 : 
; 1003 :         // Unread imask is (is_left NOR unrd_imask_for_right), do the maths etc
; 1004 :         __m128i unrd_imask = _mm_cmpgt_epi8 (blk_widths, ns);

	vmovdqa	xmm0, XMMWORD PTR _blk_widths$[ebp]
	vpcmpgtb xmm0, xmm0, XMMWORD PTR _ns$19[ebp]
	vmovdqa	XMMWORD PTR _unrd_imask$18[ebp], xmm0

; 1005 :                 unrd_imask = _mm_or_si128   (unrd_imask, is_left);

	vmovdqa	xmm0, XMMWORD PTR _unrd_imask$18[ebp]
	vpor	xmm0, xmm0, XMMWORD PTR _is_left$[ebp]
	vmovdqa	XMMWORD PTR _unrd_imask$18[ebp], xmm0

; 1006 :         __m128i unrd_mask  = _mm_cmpeq_epi8 (unrd_imask, _mm_setzero_si128());

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	xmm1, XMMWORD PTR _unrd_imask$18[ebp]
	vpcmpeqb xmm0, xmm1, xmm0
	vmovdqa	XMMWORD PTR _unrd_mask$17[ebp], xmm0

; 1007 :         __m128i b_unread   = _mm_blendv_epi8(borderpx_vec, a, unrd_mask);

	vmovdqa	xmm0, XMMWORD PTR _unrd_mask$17[ebp]
	vmovdqa	xmm1, XMMWORD PTR _borderpx_vec$22[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _a$21[ebp], xmm0
	vmovdqa	XMMWORD PTR _b_unread$16[ebp], xmm0

; 1008 : 
; 1009 :         __m128i sad_ab     = _mm_sad_epu8   (a, b_unread);

	vmovdqa	xmm0, XMMWORD PTR _a$21[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b_unread$16[ebp]
	vmovdqa	XMMWORD PTR _sad_ab$15[ebp], xmm0

; 1010 :         sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sad_ab$15[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 1011 :       }

	jmp	$LN14@hor_sad_ss
$LN15@hor_sad_ss:

; 1012 :       int32_t a_off = outside_width & is_left_bm;

	mov	eax, DWORD PTR _outside_width$[ebp]
	and	eax, DWORD PTR _is_left_bm$[ebp]
	mov	DWORD PTR _a_off$14[ebp], eax

; 1013 :       int32_t leftoff_with_sign_neg = (left_offset ^ is_left_bm) - is_left_bm;

	mov	eax, DWORD PTR _left_offset$[ebp]
	xor	eax, DWORD PTR _is_left_bm$[ebp]
	sub	eax, DWORD PTR _is_left_bm$[ebp]
	mov	DWORD PTR _leftoff_with_sign_neg$13[ebp], eax

; 1014 : 
; 1015 :       __m128i old_b = borderpx_vec;

	vmovdqa	xmm0, XMMWORD PTR _borderpx_vec$22[ebp]
	vmovdqa	XMMWORD PTR _old_b$12[ebp], xmm0

; 1016 :       for (x = invec_lstart; x != invec_lend; x += invec_linc) {

	mov	eax, DWORD PTR _invec_lstart$[ebp]
	mov	DWORD PTR _x$[ebp], eax
	jmp	SHORT $LN19@hor_sad_ss
$LN17@hor_sad_ss:
	mov	eax, DWORD PTR _x$[ebp]
	add	eax, DWORD PTR _invec_linc$[ebp]
	mov	DWORD PTR _x$[ebp], eax
$LN19@hor_sad_ss:
	mov	eax, DWORD PTR _x$[ebp]
	cmp	eax, DWORD PTR _invec_lend$[ebp]
	je	$LN18@hor_sad_ss

; 1017 :         __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + a_off));

	mov	eax, DWORD PTR _x$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	add	eax, DWORD PTR _pic_data$[ebx]
	mov	ecx, DWORD PTR _y$[ebp]
	imul	ecx, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _a_off$14[ebp]
	add	ecx, eax
	vmovdqu	xmm0, XMMWORD PTR [ecx]
	vmovdqa	XMMWORD PTR _a$11[ebp], xmm0

; 1018 :         __m128i b = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 0) * ref_stride + a_off - leftoff_with_sign_neg));

	mov	eax, DWORD PTR _x$[ebp]
	imul	eax, DWORD PTR _vec_width$[ebp]
	add	eax, DWORD PTR _ref_data$[ebx]
	mov	ecx, DWORD PTR _y$[ebp]
	imul	ecx, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _a_off$14[ebp]
	add	ecx, eax
	sub	ecx, DWORD PTR _leftoff_with_sign_neg$13[ebp]
	vmovdqu	xmm0, XMMWORD PTR [ecx]
	vmovdqa	XMMWORD PTR _b$10[ebp], xmm0

; 1019 : 
; 1020 :         __m128i b_shifted    = _mm_shuffle_epi8(b,     shufmask1);

	vmovdqa	xmm0, XMMWORD PTR _b$10[ebp]
	vpshufb	xmm0, xmm0, XMMWORD PTR _shufmask1$[ebp]
	vmovdqa	XMMWORD PTR _b_shifted$9[ebp], xmm0

; 1021 :         __m128i b_with_old   = _mm_blendv_epi8 (old_b, b_shifted, move_old_to_b_imask);

	vmovdqa	xmm0, XMMWORD PTR _move_old_to_b_imask$[ebp]
	vmovdqa	xmm1, XMMWORD PTR _old_b$12[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _b_shifted$9[ebp], xmm0
	vmovdqa	XMMWORD PTR _b_with_old$8[ebp], xmm0

; 1022 : 
; 1023 :         uint8_t startoff     = (x << vec_width_log2) + a_off;

	mov	eax, DWORD PTR _vec_width_log2$[ebp]
	shlx	ecx, DWORD PTR _x$[ebp], eax
	add	ecx, DWORD PTR _a_off$14[ebp]
	mov	BYTE PTR _startoff$7[ebp], cl

; 1024 :         __m128i startoffs    = _mm_set1_epi8   (startoff);

	movsx	eax, BYTE PTR _startoff$7[ebp]
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _startoffs$6[ebp], xmm0

; 1025 :         __m128i curr_ns      = _mm_add_epi8    (startoffs,    nslo);

	vmovdqa	xmm0, XMMWORD PTR _startoffs$6[ebp]
	vpaddb	xmm0, xmm0, XMMWORD PTR _nslo$[ebp]
	vmovdqa	XMMWORD PTR _curr_ns$5[ebp], xmm0

; 1026 :         __m128i unrd_imask   = _mm_cmpgt_epi8  (blk_widths,   curr_ns);

	vmovdqa	xmm0, XMMWORD PTR _blk_widths$[ebp]
	vpcmpgtb xmm0, xmm0, XMMWORD PTR _curr_ns$5[ebp]
	vmovdqa	XMMWORD PTR _unrd_imask$4[ebp], xmm0

; 1027 :         __m128i unrd_mask    = _mm_cmpeq_epi8  (unrd_imask,   _mm_setzero_si128());

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	xmm1, XMMWORD PTR _unrd_imask$4[ebp]
	vpcmpeqb xmm0, xmm1, xmm0
	vmovdqa	XMMWORD PTR _unrd_mask$3[ebp], xmm0

; 1028 :         __m128i b_unread     = _mm_blendv_epi8 (b_with_old,   a, unrd_mask);

	vmovdqa	xmm0, XMMWORD PTR _unrd_mask$3[ebp]
	vmovdqa	xmm1, XMMWORD PTR _b_with_old$8[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _a$11[ebp], xmm0
	vmovdqa	XMMWORD PTR _b_unread$2[ebp], xmm0

; 1029 : 
; 1030 :         old_b = b_shifted;

	vmovdqa	xmm0, XMMWORD PTR _b_shifted$9[ebp]
	vmovdqa	XMMWORD PTR _old_b$12[ebp], xmm0

; 1031 : 
; 1032 :         __m128i sad_ab     = _mm_sad_epu8(a, b_unread);

	vmovdqa	xmm0, XMMWORD PTR _a$11[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b_unread$2[ebp]
	vmovdqa	XMMWORD PTR _sad_ab$1[ebp], xmm0

; 1033 :         sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sad_ab$1[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 1034 :       }

	jmp	$LN17@hor_sad_ss
$LN18@hor_sad_ss:

; 1035 :     }

	jmp	$LN11@hor_sad_ss
$LN22@hor_sad_ss:

; 1036 :   }
; 1037 :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sse_inc$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sse_inc_2$[ebp], xmm0

; 1038 :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sse_inc_2$[ebp]
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 1039 :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 1040 : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_hor_sad_sse41_arbitrary ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
;	COMDAT _hor_sad_sse41_w16
_TEXT	SEGMENT
tv66 = -1608						; size = 4
_sad$ = -1024						; size = 16
_sse_inc_2$ = -992					; size = 16
_curr_sads$1 = -960					; size = 16
_b_epol$2 = -928					; size = 16
_b$3 = -896						; size = 16
_a$4 = -864						; size = 16
_curr_sads_gh$5 = -832					; size = 16
_curr_sads_ef$6 = -800					; size = 16
_curr_sads_cd$7 = -768					; size = 16
_curr_sads_ab$8 = -736					; size = 16
_h_epol$9 = -704					; size = 16
_f_epol$10 = -672					; size = 16
_d_epol$11 = -640					; size = 16
_b_epol$12 = -608					; size = 16
_h$13 = -576						; size = 16
_g$14 = -544						; size = 16
_f$15 = -512						; size = 16
_e$16 = -480						; size = 16
_d$17 = -448						; size = 16
_c$18 = -416						; size = 16
_b$19 = -384						; size = 16
_a$20 = -352						; size = 16
_y$ = -316						; size = 4
_sse_inc$ = -304					; size = 16
_height_residual_lines$ = -280				; size = 4
_height_fourline_groups$ = -268				; size = 4
_epol_mask$ = -256					; size = 16
_mask1$ = -224						; size = 16
_mask_right$ = -192					; size = 16
_left_128$ = -160					; size = 16
_right_border_idxs$ = -128				; size = 16
_leftoff$ = -104					; size = 4
_border_idx_negative$ = -92				; size = 4
_zero$ = -80						; size = 16
_ns$ = -48						; size = 16
_border_idx$ = -20					; size = 4
_right_border_idx$ = -8					; size = 4
_pic_data$ = 8						; size = 4
_ref_data$ = 12						; size = 4
_height$ = 16						; size = 4
_pic_stride$ = 20					; size = 4
_ref_stride$ = 24					; size = 4
_left$ = 28						; size = 4
_right$ = 32						; size = 4
_hor_sad_sse41_w16 PROC					; COMDAT

; 759  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 1608				; 00000648H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-1608]
	mov	ecx, 402				; 00000192H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __9CB4F737_reg_sad_pow2_widths-sse41@h
	call	@__CheckForDebuggerJustMyCode@4

; 760  :   // right is the number of overhanging pixels in the vector, so it has to be
; 761  :   // handled this way to produce the index of last valid (border) pixel
; 762  :   const int32_t right_border_idx = 15 - right;

	mov	eax, 15					; 0000000fH
	sub	eax, DWORD PTR _right$[ebx]
	mov	DWORD PTR _right_border_idx$[ebp], eax

; 763  :   const int32_t border_idx       = left ? left : right_border_idx;

	cmp	DWORD PTR _left$[ebx], 0
	je	SHORT $LN10@hor_sad_ss
	mov	eax, DWORD PTR _left$[ebx]
	mov	DWORD PTR tv66[ebp], eax
	jmp	SHORT $LN11@hor_sad_ss
$LN10@hor_sad_ss:
	mov	ecx, DWORD PTR _right_border_idx$[ebp]
	mov	DWORD PTR tv66[ebp], ecx
$LN11@hor_sad_ss:
	mov	edx, DWORD PTR tv66[ebp]
	mov	DWORD PTR _border_idx$[ebp], edx

; 764  : 
; 765  :   const __m128i ns               = _mm_setr_epi8(0,  1,  2,  3,  4,  5,  6,  7,

	vmovdqa	xmm0, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	vmovdqa	XMMWORD PTR _ns$[ebp], xmm0

; 766  :                                                  8,  9,  10, 11, 12, 13, 14, 15);
; 767  :   const __m128i zero             = _mm_setzero_si128();

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	XMMWORD PTR _zero$[ebp], xmm0

; 768  : 
; 769  :   // Dirty hack alert! If right == block_width (ie. the entire vector is
; 770  :   // outside the frame), move the block offset one pixel to the left (so
; 771  :   // that the leftmost pixel in vector is actually the valid border pixel
; 772  :   // from which we want to extrapolate), and use an epol mask that will
; 773  :   // simply stretch the pixel all over the vector.
; 774  :   //
; 775  :   // To avoid a branch here:
; 776  :   // The mask will be -1 (0xffffffff) for border_idx -1 and 0 for >= 0
; 777  :   const int32_t border_idx_negative = border_idx >> 31;

	mov	eax, DWORD PTR _border_idx$[ebp]
	sar	eax, 31					; 0000001fH
	mov	DWORD PTR _border_idx_negative$[ebp], eax

; 778  :   const int32_t leftoff             = border_idx_negative | left;

	mov	eax, DWORD PTR _border_idx_negative$[ebp]
	or	eax, DWORD PTR _left$[ebx]
	mov	DWORD PTR _leftoff$[ebp], eax

; 779  : 
; 780  :   __m128i right_border_idxs = _mm_set1_epi8((int8_t)right_border_idx);

	movsx	eax, BYTE PTR _right_border_idx$[ebp]
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _right_border_idxs$[ebp], xmm0

; 781  :   __m128i left_128          = _mm_set1_epi8((int8_t)left);

	movsx	eax, BYTE PTR _left$[ebx]
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _left_128$[ebp], xmm0

; 782  : 
; 783  :   // If we're straddling the left border, right_border_idx is 15 and the first
; 784  :   // operation does nothing. If right border, left is 0 and the second
; 785  :   // operation does nothing.
; 786  :   __m128i mask_right        = _mm_min_epi8 (ns,         right_border_idxs);

	vmovdqa	xmm0, XMMWORD PTR _ns$[ebp]
	vpminsb	xmm0, xmm0, XMMWORD PTR _right_border_idxs$[ebp]
	vmovdqa	XMMWORD PTR _mask_right$[ebp], xmm0

; 787  :   __m128i mask1             = _mm_sub_epi8 (mask_right, left_128);

	vmovdqa	xmm0, XMMWORD PTR _mask_right$[ebp]
	vpsubb	xmm0, xmm0, XMMWORD PTR _left_128$[ebp]
	vmovdqa	XMMWORD PTR _mask1$[ebp], xmm0

; 788  : 
; 789  :   // If right == 16 (we're completely outside the frame), right_border_idx is
; 790  :   // -1 and so is mask1. Clamp negative values to zero and as discussed
; 791  :   // earlier, adjust the load offset instead to load the "-1'st" pixel and
; 792  :   // using an all-zero shuffle mask, broadcast it all over the vector.
; 793  :   const __m128i epol_mask = _mm_max_epi8(mask1, zero);

	vmovdqa	xmm0, XMMWORD PTR _mask1$[ebp]
	vpmaxsb	xmm0, xmm0, XMMWORD PTR _zero$[ebp]
	vmovdqa	XMMWORD PTR _epol_mask$[ebp], xmm0

; 794  : 
; 795  :   const int32_t height_fourline_groups = height & ~3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, -4					; fffffffcH
	mov	DWORD PTR _height_fourline_groups$[ebp], eax

; 796  :   const int32_t height_residual_lines  = height &  3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, 3
	mov	DWORD PTR _height_residual_lines$[ebp], eax

; 797  : 
; 798  :   __m128i sse_inc = _mm_setzero_si128();

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 799  :   int32_t y;
; 800  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@hor_sad_ss
$LN2@hor_sad_ss:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 4
	mov	DWORD PTR _y$[ebp], eax
$LN4@hor_sad_ss:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height_fourline_groups$[ebp]
	jge	$LN3@hor_sad_ss

; 801  :     __m128i a = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * pic_stride));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _a$20[ebp], xmm0

; 802  :     __m128i b = _mm_loadu_si128((__m128i *)(ref_data + (y + 0) * ref_stride + leftoff));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _ref_data$[ebx]
	add	eax, DWORD PTR _leftoff$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _b$19[ebp], xmm0

; 803  :     __m128i c = _mm_loadu_si128((__m128i *)(pic_data + (y + 1) * pic_stride));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _c$18[ebp], xmm0

; 804  :     __m128i d = _mm_loadu_si128((__m128i *)(ref_data + (y + 1) * ref_stride + leftoff));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _ref_data$[ebx]
	add	eax, DWORD PTR _leftoff$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _d$17[ebp], xmm0

; 805  :     __m128i e = _mm_loadu_si128((__m128i *)(pic_data + (y + 2) * pic_stride));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _e$16[ebp], xmm0

; 806  :     __m128i f = _mm_loadu_si128((__m128i *)(ref_data + (y + 2) * ref_stride + leftoff));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _ref_data$[ebx]
	add	eax, DWORD PTR _leftoff$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _f$15[ebp], xmm0

; 807  :     __m128i g = _mm_loadu_si128((__m128i *)(pic_data + (y + 3) * pic_stride));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _g$14[ebp], xmm0

; 808  :     __m128i h = _mm_loadu_si128((__m128i *)(ref_data + (y + 3) * ref_stride + leftoff));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _ref_data$[ebx]
	add	eax, DWORD PTR _leftoff$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _h$13[ebp], xmm0

; 809  : 
; 810  :     __m128i b_epol = _mm_shuffle_epi8(b, epol_mask);

	vmovdqa	xmm0, XMMWORD PTR _b$19[ebp]
	vpshufb	xmm0, xmm0, XMMWORD PTR _epol_mask$[ebp]
	vmovdqa	XMMWORD PTR _b_epol$12[ebp], xmm0

; 811  :     __m128i d_epol = _mm_shuffle_epi8(d, epol_mask);

	vmovdqa	xmm0, XMMWORD PTR _d$17[ebp]
	vpshufb	xmm0, xmm0, XMMWORD PTR _epol_mask$[ebp]
	vmovdqa	XMMWORD PTR _d_epol$11[ebp], xmm0

; 812  :     __m128i f_epol = _mm_shuffle_epi8(f, epol_mask);

	vmovdqa	xmm0, XMMWORD PTR _f$15[ebp]
	vpshufb	xmm0, xmm0, XMMWORD PTR _epol_mask$[ebp]
	vmovdqa	XMMWORD PTR _f_epol$10[ebp], xmm0

; 813  :     __m128i h_epol = _mm_shuffle_epi8(h, epol_mask);

	vmovdqa	xmm0, XMMWORD PTR _h$13[ebp]
	vpshufb	xmm0, xmm0, XMMWORD PTR _epol_mask$[ebp]
	vmovdqa	XMMWORD PTR _h_epol$9[ebp], xmm0

; 814  : 
; 815  :     __m128i curr_sads_ab = _mm_sad_epu8(a, b_epol);

	vmovdqa	xmm0, XMMWORD PTR _a$20[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b_epol$12[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_ab$8[ebp], xmm0

; 816  :     __m128i curr_sads_cd = _mm_sad_epu8(c, d_epol);

	vmovdqa	xmm0, XMMWORD PTR _c$18[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _d_epol$11[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_cd$7[ebp], xmm0

; 817  :     __m128i curr_sads_ef = _mm_sad_epu8(e, f_epol);

	vmovdqa	xmm0, XMMWORD PTR _e$16[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _f_epol$10[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_ef$6[ebp], xmm0

; 818  :     __m128i curr_sads_gh = _mm_sad_epu8(g, h_epol);

	vmovdqa	xmm0, XMMWORD PTR _g$14[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _h_epol$9[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_gh$5[ebp], xmm0

; 819  : 
; 820  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_ab$8[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 821  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_cd$7[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 822  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_ef$6[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 823  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_gh$5[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 824  :   }

	jmp	$LN2@hor_sad_ss
$LN3@hor_sad_ss:

; 825  :   if (height_residual_lines) {

	cmp	DWORD PTR _height_residual_lines$[ebp], 0
	je	$LN8@hor_sad_ss

; 826  :     for (; y < height; y++) {

	jmp	SHORT $LN7@hor_sad_ss
$LN5@hor_sad_ss:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN7@hor_sad_ss:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	$LN8@hor_sad_ss

; 827  :       __m128i a = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * pic_stride));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _a$4[ebp], xmm0

; 828  :       __m128i b = _mm_loadu_si128((__m128i *)(ref_data + (y + 0) * ref_stride + leftoff));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _ref_data$[ebx]
	add	eax, DWORD PTR _leftoff$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _b$3[ebp], xmm0

; 829  :       __m128i b_epol = _mm_shuffle_epi8(b, epol_mask);

	vmovdqa	xmm0, XMMWORD PTR _b$3[ebp]
	vpshufb	xmm0, xmm0, XMMWORD PTR _epol_mask$[ebp]
	vmovdqa	XMMWORD PTR _b_epol$2[ebp], xmm0

; 830  :       __m128i curr_sads = _mm_sad_epu8(a, b_epol);

	vmovdqa	xmm0, XMMWORD PTR _a$4[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b_epol$2[ebp]
	vmovdqa	XMMWORD PTR _curr_sads$1[ebp], xmm0

; 831  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads$1[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 832  :     }

	jmp	$LN5@hor_sad_ss
$LN8@hor_sad_ss:

; 833  :   }
; 834  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sse_inc$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sse_inc_2$[ebp], xmm0

; 835  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sse_inc_2$[ebp]
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 836  :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 837  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_hor_sad_sse41_w16 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
;	COMDAT _hor_sad_sse41_w8
_TEXT	SEGMENT
tv66 = -1288						; size = 4
_sad$ = -896						; size = 16
_sse_inc_2$ = -864					; size = 16
_curr_sads_ab$1 = -832					; size = 16
_b_epol$2 = -800					; size = 16
_b$3 = -768						; size = 16
_a$4 = -736						; size = 16
_curr_sads_cd$5 = -704					; size = 16
_curr_sads_ab$6 = -672					; size = 16
_d_epol$7 = -640					; size = 16
_b_epol$8 = -608					; size = 16
_d$9 = -576						; size = 16
_c$10 = -544						; size = 16
_b$11 = -512						; size = 16
_a$12 = -480						; size = 16
_d_d$13 = -448						; size = 16
_c_d$14 = -416						; size = 16
_b_d$15 = -384						; size = 16
_a_d$16 = -352						; size = 16
_y$ = -316						; size = 4
_sse_inc$ = -304					; size = 16
_height_residual_lines$ = -280				; size = 4
_height_fourline_groups$ = -268				; size = 4
_epol_mask$ = -256					; size = 16
_mask1$ = -224						; size = 16
_mask_right$ = -192					; size = 16
_left_128$ = -160					; size = 16
_right_border_idxs$ = -128				; size = 16
_leftoff$ = -104					; size = 4
_border_idx_negative$ = -92				; size = 4
_qwbaseids$ = -80					; size = 16
_ns$ = -48						; size = 16
_border_idx$ = -20					; size = 4
_right_border_idx$ = -8					; size = 4
_pic_data$ = 8						; size = 4
_ref_data$ = 12						; size = 4
_height$ = 16						; size = 4
_pic_stride$ = 20					; size = 4
_ref_stride$ = 24					; size = 4
_left$ = 28						; size = 4
_right$ = 32						; size = 4
_hor_sad_sse41_w8 PROC					; COMDAT

; 646  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 1288				; 00000508H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-1288]
	mov	ecx, 322				; 00000142H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __9CB4F737_reg_sad_pow2_widths-sse41@h
	call	@__CheckForDebuggerJustMyCode@4

; 647  :   // right is the number of overhanging pixels in the vector, so it has to be
; 648  :   // handled this way to produce the index of last valid (border) pixel
; 649  :   const int32_t right_border_idx = 7 - right;

	mov	eax, 7
	sub	eax, DWORD PTR _right$[ebx]
	mov	DWORD PTR _right_border_idx$[ebp], eax

; 650  :   const int32_t border_idx       = left ? left : right_border_idx;

	cmp	DWORD PTR _left$[ebx], 0
	je	SHORT $LN10@hor_sad_ss
	mov	eax, DWORD PTR _left$[ebx]
	mov	DWORD PTR tv66[ebp], eax
	jmp	SHORT $LN11@hor_sad_ss
$LN10@hor_sad_ss:
	mov	ecx, DWORD PTR _right_border_idx$[ebp]
	mov	DWORD PTR tv66[ebp], ecx
$LN11@hor_sad_ss:
	mov	edx, DWORD PTR tv66[ebp]
	mov	DWORD PTR _border_idx$[ebp], edx

; 651  : 
; 652  :   const __m128i ns               = _mm_setr_epi8(0,  1,  2,  3,  4,  5,  6,  7,

	vmovdqa	xmm0, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	vmovdqa	XMMWORD PTR _ns$[ebp], xmm0

; 653  :                                                  8,  9,  10, 11, 12, 13, 14, 15);
; 654  : 
; 655  :   // Quadword (ie. line) base indexes, ie. the edges the lines read will be
; 656  :   // clamped towards; higher qword (lower line) bytes tend towards 8 and lower
; 657  :   // qword (higher line) bytes towards 0
; 658  :   const __m128i qwbaseids   = _mm_setr_epi8(0, 0, 0, 0, 0, 0, 0, 0,

	vmovdqa	xmm0, XMMWORD PTR __xmm@08080808080808080000000000000000
	vmovdqa	XMMWORD PTR _qwbaseids$[ebp], xmm0

; 659  :                                             8, 8, 8, 8, 8, 8, 8, 8);
; 660  : 
; 661  :   // Dirty hack alert! If right == block_width (ie. the entire vector is
; 662  :   // outside the frame), move the block offset one pixel to the left (so
; 663  :   // that the leftmost pixel in vector is actually the valid border pixel
; 664  :   // from which we want to extrapolate), and use an epol mask that will
; 665  :   // simply stretch the pixel all over the vector.
; 666  :   //
; 667  :   // To avoid a branch here:
; 668  :   // The mask will be -1 (0xffffffff) for border_idx -1 and 0 for >= 0
; 669  :   const int32_t border_idx_negative = border_idx >> 31;

	mov	eax, DWORD PTR _border_idx$[ebp]
	sar	eax, 31					; 0000001fH
	mov	DWORD PTR _border_idx_negative$[ebp], eax

; 670  :   const int32_t leftoff             = border_idx_negative | left;

	mov	eax, DWORD PTR _border_idx_negative$[ebp]
	or	eax, DWORD PTR _left$[ebx]
	mov	DWORD PTR _leftoff$[ebp], eax

; 671  : 
; 672  :   __m128i right_border_idxs = _mm_set1_epi8((int8_t)right_border_idx);

	movsx	eax, BYTE PTR _right_border_idx$[ebp]
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _right_border_idxs$[ebp], xmm0

; 673  :   __m128i left_128          = _mm_set1_epi8((int8_t)left);

	movsx	eax, BYTE PTR _left$[ebx]
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _left_128$[ebp], xmm0

; 674  : 
; 675  :   right_border_idxs         = _mm_add_epi8 (right_border_idxs, qwbaseids);

	vmovdqa	xmm0, XMMWORD PTR _right_border_idxs$[ebp]
	vpaddb	xmm0, xmm0, XMMWORD PTR _qwbaseids$[ebp]
	vmovdqa	XMMWORD PTR _right_border_idxs$[ebp], xmm0

; 676  : 
; 677  :   // If we're straddling the left border, right_border_idx is 7 and the first
; 678  :   // operation does nothing. If right border, left is 0 and the second
; 679  :   // operation does nothing.
; 680  :   __m128i mask_right        = _mm_min_epi8 (ns,         right_border_idxs);

	vmovdqa	xmm0, XMMWORD PTR _ns$[ebp]
	vpminsb	xmm0, xmm0, XMMWORD PTR _right_border_idxs$[ebp]
	vmovdqa	XMMWORD PTR _mask_right$[ebp], xmm0

; 681  :   __m128i mask1             = _mm_sub_epi8 (mask_right, left_128);

	vmovdqa	xmm0, XMMWORD PTR _mask_right$[ebp]
	vpsubb	xmm0, xmm0, XMMWORD PTR _left_128$[ebp]
	vmovdqa	XMMWORD PTR _mask1$[ebp], xmm0

; 682  : 
; 683  :   // If right == 8 (we're completely outside the frame), right_border_idx is
; 684  :   // -1 and so is mask1. Clamp negative values to qwbaseid and as discussed
; 685  :   // earlier, adjust the load offset instead to load the "-1'st" pixels and
; 686  :   // using qwbaseids as the shuffle mask, broadcast it all over the rows.
; 687  :   const __m128i epol_mask = _mm_max_epi8(mask1, qwbaseids);

	vmovdqa	xmm0, XMMWORD PTR _mask1$[ebp]
	vpmaxsb	xmm0, xmm0, XMMWORD PTR _qwbaseids$[ebp]
	vmovdqa	XMMWORD PTR _epol_mask$[ebp], xmm0

; 688  : 
; 689  :   const int32_t height_fourline_groups = height & ~3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, -4					; fffffffcH
	mov	DWORD PTR _height_fourline_groups$[ebp], eax

; 690  :   const int32_t height_residual_lines  = height &  3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, 3
	mov	DWORD PTR _height_residual_lines$[ebp], eax

; 691  : 
; 692  :   __m128i sse_inc = _mm_setzero_si128();

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 693  :   int32_t y;
; 694  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@hor_sad_ss
$LN2@hor_sad_ss:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 4
	mov	DWORD PTR _y$[ebp], eax
$LN4@hor_sad_ss:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height_fourline_groups$[ebp]
	jge	$LN3@hor_sad_ss

; 695  :     __m128d a_d = _mm_setzero_pd();

	vxorps	xmm0, xmm0, xmm0
	vmovaps	XMMWORD PTR _a_d$16[ebp], xmm0

; 696  :     __m128d b_d = _mm_setzero_pd();

	vxorps	xmm0, xmm0, xmm0
	vmovaps	XMMWORD PTR _b_d$15[ebp], xmm0

; 697  :     __m128d c_d = _mm_setzero_pd();

	vxorps	xmm0, xmm0, xmm0
	vmovaps	XMMWORD PTR _c_d$14[ebp], xmm0

; 698  :     __m128d d_d = _mm_setzero_pd();

	vxorps	xmm0, xmm0, xmm0
	vmovaps	XMMWORD PTR _d_d$13[ebp], xmm0

; 699  : 
; 700  :     a_d = _mm_loadl_pd(a_d, (const double *)(pic_data + (y + 0) * pic_stride));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovaps	xmm0, XMMWORD PTR _a_d$16[ebp]
	vmovlpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _a_d$16[ebp], xmm0

; 701  :     b_d = _mm_loadl_pd(b_d, (const double *)(ref_data + (y + 0) * ref_stride + leftoff));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _ref_data$[ebx]
	add	eax, DWORD PTR _leftoff$[ebp]
	vmovaps	xmm0, XMMWORD PTR _b_d$15[ebp]
	vmovlpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _b_d$15[ebp], xmm0

; 702  :     a_d = _mm_loadh_pd(a_d, (const double *)(pic_data + (y + 1) * pic_stride));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovaps	xmm0, XMMWORD PTR _a_d$16[ebp]
	vmovhpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _a_d$16[ebp], xmm0

; 703  :     b_d = _mm_loadh_pd(b_d, (const double *)(ref_data + (y + 1) * ref_stride + leftoff));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _ref_data$[ebx]
	add	eax, DWORD PTR _leftoff$[ebp]
	vmovaps	xmm0, XMMWORD PTR _b_d$15[ebp]
	vmovhpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _b_d$15[ebp], xmm0

; 704  : 
; 705  :     c_d = _mm_loadl_pd(c_d, (const double *)(pic_data + (y + 2) * pic_stride));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovaps	xmm0, XMMWORD PTR _c_d$14[ebp]
	vmovlpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _c_d$14[ebp], xmm0

; 706  :     d_d = _mm_loadl_pd(d_d, (const double *)(ref_data + (y + 2) * ref_stride + leftoff));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _ref_data$[ebx]
	add	eax, DWORD PTR _leftoff$[ebp]
	vmovaps	xmm0, XMMWORD PTR _d_d$13[ebp]
	vmovlpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _d_d$13[ebp], xmm0

; 707  :     c_d = _mm_loadh_pd(c_d, (const double *)(pic_data + (y + 3) * pic_stride));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _pic_stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovaps	xmm0, XMMWORD PTR _c_d$14[ebp]
	vmovhpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _c_d$14[ebp], xmm0

; 708  :     d_d = _mm_loadh_pd(d_d, (const double *)(ref_data + (y + 3) * ref_stride + leftoff));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _ref_data$[ebx]
	add	eax, DWORD PTR _leftoff$[ebp]
	vmovaps	xmm0, XMMWORD PTR _d_d$13[ebp]
	vmovhpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _d_d$13[ebp], xmm0

; 709  : 
; 710  :     __m128i a = _mm_castpd_si128(a_d);

	vmovaps	xmm0, XMMWORD PTR _a_d$16[ebp]
	vmovdqa	XMMWORD PTR _a$12[ebp], xmm0

; 711  :     __m128i b = _mm_castpd_si128(b_d);

	vmovaps	xmm0, XMMWORD PTR _b_d$15[ebp]
	vmovdqa	XMMWORD PTR _b$11[ebp], xmm0

; 712  :     __m128i c = _mm_castpd_si128(c_d);

	vmovaps	xmm0, XMMWORD PTR _c_d$14[ebp]
	vmovdqa	XMMWORD PTR _c$10[ebp], xmm0

; 713  :     __m128i d = _mm_castpd_si128(d_d);

	vmovaps	xmm0, XMMWORD PTR _d_d$13[ebp]
	vmovdqa	XMMWORD PTR _d$9[ebp], xmm0

; 714  : 
; 715  :     __m128i b_epol = _mm_shuffle_epi8(b, epol_mask);

	vmovdqa	xmm0, XMMWORD PTR _b$11[ebp]
	vpshufb	xmm0, xmm0, XMMWORD PTR _epol_mask$[ebp]
	vmovdqa	XMMWORD PTR _b_epol$8[ebp], xmm0

; 716  :     __m128i d_epol = _mm_shuffle_epi8(d, epol_mask);

	vmovdqa	xmm0, XMMWORD PTR _d$9[ebp]
	vpshufb	xmm0, xmm0, XMMWORD PTR _epol_mask$[ebp]
	vmovdqa	XMMWORD PTR _d_epol$7[ebp], xmm0

; 717  : 
; 718  :     __m128i curr_sads_ab = _mm_sad_epu8(a, b_epol);

	vmovdqa	xmm0, XMMWORD PTR _a$12[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b_epol$8[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_ab$6[ebp], xmm0

; 719  :     __m128i curr_sads_cd = _mm_sad_epu8(c, d_epol);

	vmovdqa	xmm0, XMMWORD PTR _c$10[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _d_epol$7[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_cd$5[ebp], xmm0

; 720  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_ab$6[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 721  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_cd$5[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 722  :   }

	jmp	$LN2@hor_sad_ss
$LN3@hor_sad_ss:

; 723  :   if (height_residual_lines) {

	cmp	DWORD PTR _height_residual_lines$[ebp], 0
	je	$LN8@hor_sad_ss

; 724  :     for (; y < height; y++) {

	jmp	SHORT $LN7@hor_sad_ss
$LN5@hor_sad_ss:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN7@hor_sad_ss:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	$LN8@hor_sad_ss

; 725  :       __m128i a = _mm_loadl_epi64((__m128i *)(pic_data + y * pic_stride));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _pic_stride$[ebx]
	mov	ecx, DWORD PTR _pic_data$[ebx]
	vmovq	xmm0, QWORD PTR [ecx+eax]
	vmovdqa	XMMWORD PTR _a$4[ebp], xmm0

; 726  :       __m128i b = _mm_loadl_epi64((__m128i *)(ref_data + y * ref_stride + leftoff));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _ref_data$[ebx]
	mov	ecx, DWORD PTR _leftoff$[ebp]
	vmovq	xmm0, QWORD PTR [eax+ecx]
	vmovdqa	XMMWORD PTR _b$3[ebp], xmm0

; 727  : 
; 728  :       __m128i b_epol = _mm_shuffle_epi8(b, epol_mask);

	vmovdqa	xmm0, XMMWORD PTR _b$3[ebp]
	vpshufb	xmm0, xmm0, XMMWORD PTR _epol_mask$[ebp]
	vmovdqa	XMMWORD PTR _b_epol$2[ebp], xmm0

; 729  : 
; 730  :       __m128i curr_sads_ab = _mm_sad_epu8(a, b_epol);

	vmovdqa	xmm0, XMMWORD PTR _a$4[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b_epol$2[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_ab$1[ebp], xmm0

; 731  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_ab$1[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 732  :     }

	jmp	$LN5@hor_sad_ss
$LN8@hor_sad_ss:

; 733  :   }
; 734  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sse_inc$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sse_inc_2$[ebp], xmm0

; 735  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sse_inc_2$[ebp]
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 736  :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 737  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_hor_sad_sse41_w8 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
;	COMDAT _hor_sad_sse41_w4
_TEXT	SEGMENT
tv66 = -1032						; size = 4
_sad$ = -640						; size = 16
_sse_inc_2$ = -608					; size = 16
_curr_sads$1 = -576					; size = 16
_b_epol$2 = -544					; size = 16
_b$3 = -512						; size = 16
_a$4 = -480						; size = 16
_curr_sads$5 = -448					; size = 16
_b_epol$6 = -416					; size = 16
_b$7 = -384						; size = 16
_a$8 = -352						; size = 16
_y$ = -316						; size = 4
_sse_inc$ = -304					; size = 16
_height_residual_lines$ = -280				; size = 4
_height_fourline_groups$ = -268				; size = 4
_epol_mask$ = -256					; size = 16
_mask1$ = -224						; size = 16
_mask_right$ = -192					; size = 16
_left_128$ = -160					; size = 16
_right_border_idxs$ = -128				; size = 16
_dwbaseids$ = -96					; size = 16
_leftoff$ = -72						; size = 4
_border_idx_negative$ = -60				; size = 4
_ns$ = -48						; size = 16
_border_idx$ = -20					; size = 4
_right_border_idx$ = -8					; size = 4
_pic_data$ = 8						; size = 4
_ref_data$ = 12						; size = 4
_height$ = 16						; size = 4
_pic_stride$ = 20					; size = 4
_ref_stride$ = 24					; size = 4
_left$ = 28						; size = 4
_right$ = 32						; size = 4
_hor_sad_sse41_w4 PROC					; COMDAT

; 582  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 1032				; 00000408H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-1032]
	mov	ecx, 258				; 00000102H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __9CB4F737_reg_sad_pow2_widths-sse41@h
	call	@__CheckForDebuggerJustMyCode@4

; 583  :   const int32_t right_border_idx = 3 - right;

	mov	eax, 3
	sub	eax, DWORD PTR _right$[ebx]
	mov	DWORD PTR _right_border_idx$[ebp], eax

; 584  :   const int32_t border_idx       = left ? left : right_border_idx;

	cmp	DWORD PTR _left$[ebx], 0
	je	SHORT $LN10@hor_sad_ss
	mov	eax, DWORD PTR _left$[ebx]
	mov	DWORD PTR tv66[ebp], eax
	jmp	SHORT $LN11@hor_sad_ss
$LN10@hor_sad_ss:
	mov	ecx, DWORD PTR _right_border_idx$[ebp]
	mov	DWORD PTR tv66[ebp], ecx
$LN11@hor_sad_ss:
	mov	edx, DWORD PTR tv66[ebp]
	mov	DWORD PTR _border_idx$[ebp], edx

; 585  : 
; 586  :   const __m128i ns               = _mm_setr_epi8(0,  1,  2,  3,  4,  5,  6,  7,

	vmovdqa	xmm0, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	vmovdqa	XMMWORD PTR _ns$[ebp], xmm0

; 587  :                                                  8,  9,  10, 11, 12, 13, 14, 15);
; 588  : 
; 589  :   const int32_t border_idx_negative = border_idx >> 31;

	mov	eax, DWORD PTR _border_idx$[ebp]
	sar	eax, 31					; 0000001fH
	mov	DWORD PTR _border_idx_negative$[ebp], eax

; 590  :   const int32_t leftoff             = border_idx_negative | left;

	mov	eax, DWORD PTR _border_idx_negative$[ebp]
	or	eax, DWORD PTR _left$[ebx]
	mov	DWORD PTR _leftoff$[ebp], eax

; 591  : 
; 592  :   // Dualword (ie. line) base indexes, ie. the edges the lines read will be
; 593  :   // clamped towards
; 594  :   const __m128i dwbaseids   = _mm_setr_epi8(0, 0, 0, 0, 4, 4, 4, 4,

	vmovdqa	xmm0, XMMWORD PTR __xmm@0c0c0c0c080808080404040400000000
	vmovdqa	XMMWORD PTR _dwbaseids$[ebp], xmm0

; 595  :                                             8, 8, 8, 8, 12, 12, 12, 12);
; 596  : 
; 597  :   __m128i right_border_idxs = _mm_set1_epi8((int8_t)right_border_idx);

	movsx	eax, BYTE PTR _right_border_idx$[ebp]
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _right_border_idxs$[ebp], xmm0

; 598  :   __m128i left_128          = _mm_set1_epi8((int8_t)left);

	movsx	eax, BYTE PTR _left$[ebx]
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _left_128$[ebp], xmm0

; 599  : 
; 600  :   right_border_idxs         = _mm_add_epi8 (right_border_idxs, dwbaseids);

	vmovdqa	xmm0, XMMWORD PTR _right_border_idxs$[ebp]
	vpaddb	xmm0, xmm0, XMMWORD PTR _dwbaseids$[ebp]
	vmovdqa	XMMWORD PTR _right_border_idxs$[ebp], xmm0

; 601  : 
; 602  :   __m128i mask_right        = _mm_min_epi8 (ns,         right_border_idxs);

	vmovdqa	xmm0, XMMWORD PTR _ns$[ebp]
	vpminsb	xmm0, xmm0, XMMWORD PTR _right_border_idxs$[ebp]
	vmovdqa	XMMWORD PTR _mask_right$[ebp], xmm0

; 603  :   __m128i mask1             = _mm_sub_epi8 (mask_right, left_128);

	vmovdqa	xmm0, XMMWORD PTR _mask_right$[ebp]
	vpsubb	xmm0, xmm0, XMMWORD PTR _left_128$[ebp]
	vmovdqa	XMMWORD PTR _mask1$[ebp], xmm0

; 604  : 
; 605  :   const __m128i epol_mask   = _mm_max_epi8(mask1, dwbaseids);

	vmovdqa	xmm0, XMMWORD PTR _mask1$[ebp]
	vpmaxsb	xmm0, xmm0, XMMWORD PTR _dwbaseids$[ebp]
	vmovdqa	XMMWORD PTR _epol_mask$[ebp], xmm0

; 606  : 
; 607  :   const int32_t height_fourline_groups = height & ~3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, -4					; fffffffcH
	mov	DWORD PTR _height_fourline_groups$[ebp], eax

; 608  :   const int32_t height_residual_lines  = height &  3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, 3
	mov	DWORD PTR _height_residual_lines$[ebp], eax

; 609  : 
; 610  :   __m128i sse_inc = _mm_setzero_si128();

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 611  :   int32_t y;
; 612  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@hor_sad_ss
$LN2@hor_sad_ss:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 4
	mov	DWORD PTR _y$[ebp], eax
$LN4@hor_sad_ss:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height_fourline_groups$[ebp]
	jge	$LN3@hor_sad_ss

; 613  :     __m128i a = _mm_cvtsi32_si128(*(const uint32_t *)(pic_data + y * pic_stride));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _pic_stride$[ebx]
	mov	ecx, DWORD PTR _pic_data$[ebx]
	vmovd	xmm0, DWORD PTR [ecx+eax]
	vmovdqa	XMMWORD PTR _a$8[ebp], xmm0

; 614  :     __m128i b = _mm_cvtsi32_si128(*(const uint32_t *)(ref_data + y * ref_stride + leftoff));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _ref_data$[ebx]
	mov	ecx, DWORD PTR _leftoff$[ebp]
	vmovd	xmm0, DWORD PTR [eax+ecx]
	vmovdqa	XMMWORD PTR _b$7[ebp], xmm0

; 615  : 
; 616  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 1) * pic_stride),           1);

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _pic_stride$[ebx]
	mov	ecx, DWORD PTR _pic_data$[ebx]
	vmovdqa	xmm0, XMMWORD PTR _a$8[ebp]
	vpinsrd	xmm0, xmm0, DWORD PTR [ecx+eax], 1
	vmovdqa	XMMWORD PTR _a$8[ebp], xmm0

; 617  :     b = _mm_insert_epi32(b, *(const uint32_t *)(ref_data + (y + 1) * ref_stride + leftoff), 1);

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _ref_data$[ebx]
	mov	ecx, DWORD PTR _leftoff$[ebp]
	vmovdqa	xmm0, XMMWORD PTR _b$7[ebp]
	vpinsrd	xmm0, xmm0, DWORD PTR [eax+ecx], 1
	vmovdqa	XMMWORD PTR _b$7[ebp], xmm0

; 618  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 2) * pic_stride),           2);

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _pic_stride$[ebx]
	mov	ecx, DWORD PTR _pic_data$[ebx]
	vmovdqa	xmm0, XMMWORD PTR _a$8[ebp]
	vpinsrd	xmm0, xmm0, DWORD PTR [ecx+eax], 2
	vmovdqa	XMMWORD PTR _a$8[ebp], xmm0

; 619  :     b = _mm_insert_epi32(b, *(const uint32_t *)(ref_data + (y + 2) * ref_stride + leftoff), 2);

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _ref_data$[ebx]
	mov	ecx, DWORD PTR _leftoff$[ebp]
	vmovdqa	xmm0, XMMWORD PTR _b$7[ebp]
	vpinsrd	xmm0, xmm0, DWORD PTR [eax+ecx], 2
	vmovdqa	XMMWORD PTR _b$7[ebp], xmm0

; 620  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 3) * pic_stride),           3);

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _pic_stride$[ebx]
	mov	ecx, DWORD PTR _pic_data$[ebx]
	vmovdqa	xmm0, XMMWORD PTR _a$8[ebp]
	vpinsrd	xmm0, xmm0, DWORD PTR [ecx+eax], 3
	vmovdqa	XMMWORD PTR _a$8[ebp], xmm0

; 621  :     b = _mm_insert_epi32(b, *(const uint32_t *)(ref_data + (y + 3) * ref_stride + leftoff), 3);

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _ref_data$[ebx]
	mov	ecx, DWORD PTR _leftoff$[ebp]
	vmovdqa	xmm0, XMMWORD PTR _b$7[ebp]
	vpinsrd	xmm0, xmm0, DWORD PTR [eax+ecx], 3
	vmovdqa	XMMWORD PTR _b$7[ebp], xmm0

; 622  : 
; 623  :     __m128i b_epol    = _mm_shuffle_epi8(b,       epol_mask);

	vmovdqa	xmm0, XMMWORD PTR _b$7[ebp]
	vpshufb	xmm0, xmm0, XMMWORD PTR _epol_mask$[ebp]
	vmovdqa	XMMWORD PTR _b_epol$6[ebp], xmm0

; 624  :     __m128i curr_sads = _mm_sad_epu8    (a,       b_epol);

	vmovdqa	xmm0, XMMWORD PTR _a$8[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b_epol$6[ebp]
	vmovdqa	XMMWORD PTR _curr_sads$5[ebp], xmm0

; 625  :             sse_inc   = _mm_add_epi64   (sse_inc, curr_sads);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads$5[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 626  :   }

	jmp	$LN2@hor_sad_ss
$LN3@hor_sad_ss:

; 627  :   if (height_residual_lines) {

	cmp	DWORD PTR _height_residual_lines$[ebp], 0
	je	$LN8@hor_sad_ss

; 628  :     for (; y < height; y++) {

	jmp	SHORT $LN7@hor_sad_ss
$LN5@hor_sad_ss:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN7@hor_sad_ss:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	$LN8@hor_sad_ss

; 629  :       __m128i a = _mm_cvtsi32_si128(*(const uint32_t *)(pic_data + y * pic_stride));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _pic_stride$[ebx]
	mov	ecx, DWORD PTR _pic_data$[ebx]
	vmovd	xmm0, DWORD PTR [ecx+eax]
	vmovdqa	XMMWORD PTR _a$4[ebp], xmm0

; 630  :       __m128i b = _mm_cvtsi32_si128(*(const uint32_t *)(ref_data + y * ref_stride + leftoff));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _ref_stride$[ebx]
	add	eax, DWORD PTR _ref_data$[ebx]
	mov	ecx, DWORD PTR _leftoff$[ebp]
	vmovd	xmm0, DWORD PTR [eax+ecx]
	vmovdqa	XMMWORD PTR _b$3[ebp], xmm0

; 631  : 
; 632  :       __m128i b_epol = _mm_shuffle_epi8(b, epol_mask);

	vmovdqa	xmm0, XMMWORD PTR _b$3[ebp]
	vpshufb	xmm0, xmm0, XMMWORD PTR _epol_mask$[ebp]
	vmovdqa	XMMWORD PTR _b_epol$2[ebp], xmm0

; 633  :       __m128i curr_sads = _mm_sad_epu8 (a, b_epol);

	vmovdqa	xmm0, XMMWORD PTR _a$4[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b_epol$2[ebp]
	vmovdqa	XMMWORD PTR _curr_sads$1[ebp], xmm0

; 634  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads$1[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 635  :     }

	jmp	$LN5@hor_sad_ss
$LN8@hor_sad_ss:

; 636  :   }
; 637  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sse_inc$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sse_inc_2$[ebp], xmm0

; 638  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sse_inc_2$[ebp]
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 639  : 
; 640  :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 641  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_hor_sad_sse41_w4 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
;	COMDAT _ver_sad_arbitrary
_TEXT	SEGMENT
_sad$ = -1120						; size = 16
_sse_inc_2$ = -1088					; size = 16
_curr_sads$1 = -1056					; size = 16
_a_masked$2 = -1024					; size = 16
_a$3 = -992						; size = 16
_curr_sads_gh$4 = -960					; size = 16
_curr_sads_ef$5 = -928					; size = 16
_curr_sads_cd$6 = -896					; size = 16
_curr_sads_ab$7 = -864					; size = 16
_g_masked$8 = -832					; size = 16
_e_masked$9 = -800					; size = 16
_c_masked$10 = -768					; size = 16
_a_masked$11 = -736					; size = 16
_g$12 = -704						; size = 16
_e$13 = -672						; size = 16
_c$14 = -640						; size = 16
_a$15 = -608						; size = 16
_ref_row$16 = -576					; size = 16
_curr_sads$17 = -544					; size = 16
_a$18 = -512						; size = 16
_curr_sads_gh$19 = -480					; size = 16
_curr_sads_ef$20 = -448					; size = 16
_curr_sads_cd$21 = -416					; size = 16
_curr_sads_ab$22 = -384					; size = 16
_g$23 = -352						; size = 16
_e$24 = -320						; size = 16
_c$25 = -288						; size = 16
_a$26 = -256						; size = 16
_ref_row$27 = -224					; size = 16
_rdmask$ = -192						; size = 16
_ns$ = -160						; size = 16
_rds$ = -128						; size = 16
_height_residual_lines$ = -96				; size = 4
_height_fourline_groups$ = -84				; size = 4
_width_residual_pixels$ = -72				; size = 4
_width_xmms$ = -60					; size = 4
_sse_inc$ = -48						; size = 16
_x$ = -20						; size = 4
_y$ = -8						; size = 4
_pic_data$ = 8						; size = 4
_ref_data$ = 12						; size = 4
_width$ = 16						; size = 4
_height$ = 20						; size = 4
_stride$ = 24						; size = 4
_ver_sad_arbitrary PROC					; COMDAT

; 494  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 1704				; 000006a8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-1704]
	mov	ecx, 426				; 000001aaH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __9CB4F737_reg_sad_pow2_widths-sse41@h
	call	@__CheckForDebuggerJustMyCode@4

; 495  :   int32_t y, x;
; 496  :   __m128i sse_inc = _mm_setzero_si128();

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 497  : 
; 498  :   // Bytes in block in 128-bit blocks per each scanline, and remainder
; 499  :   const int32_t width_xmms             = width  & ~15;

	mov	eax, DWORD PTR _width$[ebx]
	and	eax, -16				; fffffff0H
	mov	DWORD PTR _width_xmms$[ebp], eax

; 500  :   const int32_t width_residual_pixels  = width  &  15;

	mov	eax, DWORD PTR _width$[ebx]
	and	eax, 15					; 0000000fH
	mov	DWORD PTR _width_residual_pixels$[ebp], eax

; 501  : 
; 502  :   const int32_t height_fourline_groups = height & ~3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, -4					; fffffffcH
	mov	DWORD PTR _height_fourline_groups$[ebp], eax

; 503  :   const int32_t height_residual_lines  = height &  3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, 3
	mov	DWORD PTR _height_residual_lines$[ebp], eax

; 504  : 
; 505  :   const __m128i rds    = _mm_set1_epi8 (width_residual_pixels);

	movsx	eax, BYTE PTR _width_residual_pixels$[ebp]
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _rds$[ebp], xmm0

; 506  :   const __m128i ns     = _mm_setr_epi8 (0,  1,  2,  3,  4,  5,  6,  7,

	vmovdqa	xmm0, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	vmovdqa	XMMWORD PTR _ns$[ebp], xmm0

; 507  :                                         8,  9,  10, 11, 12, 13, 14, 15);
; 508  :   const __m128i rdmask = _mm_cmpgt_epi8(rds, ns);

	vmovdqa	xmm0, XMMWORD PTR _rds$[ebp]
	vpcmpgtb xmm0, xmm0, XMMWORD PTR _ns$[ebp]
	vmovdqa	XMMWORD PTR _rdmask$[ebp], xmm0

; 509  : 
; 510  :   for (x = 0; x < width_xmms; x += 16) {

	mov	DWORD PTR _x$[ebp], 0
	jmp	SHORT $LN4@ver_sad_ar
$LN2@ver_sad_ar:
	mov	eax, DWORD PTR _x$[ebp]
	add	eax, 16					; 00000010H
	mov	DWORD PTR _x$[ebp], eax
$LN4@ver_sad_ar:
	mov	eax, DWORD PTR _x$[ebp]
	cmp	eax, DWORD PTR _width_xmms$[ebp]
	jge	$LN3@ver_sad_ar

; 511  :     const __m128i ref_row = _mm_loadu_si128((__m128i *)(ref_data + x));

	mov	eax, DWORD PTR _ref_data$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _ref_row$27[ebp], xmm0

; 512  :     for (y = 0; y < height_fourline_groups; y += 4) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN7@ver_sad_ar
$LN5@ver_sad_ar:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 4
	mov	DWORD PTR _y$[ebp], eax
$LN7@ver_sad_ar:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height_fourline_groups$[ebp]
	jge	$LN6@ver_sad_ar

; 513  :       __m128i a = _mm_loadu_si128((const __m128i *)(pic_data + (y + 0) * stride + x));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _a$26[ebp], xmm0

; 514  :       __m128i c = _mm_loadu_si128((const __m128i *)(pic_data + (y + 1) * stride + x));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _c$25[ebp], xmm0

; 515  :       __m128i e = _mm_loadu_si128((const __m128i *)(pic_data + (y + 2) * stride + x));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _e$24[ebp], xmm0

; 516  :       __m128i g = _mm_loadu_si128((const __m128i *)(pic_data + (y + 3) * stride + x));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _g$23[ebp], xmm0

; 517  : 
; 518  :       __m128i curr_sads_ab = _mm_sad_epu8(ref_row, a);

	vmovdqa	xmm0, XMMWORD PTR _ref_row$27[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _a$26[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_ab$22[ebp], xmm0

; 519  :       __m128i curr_sads_cd = _mm_sad_epu8(ref_row, c);

	vmovdqa	xmm0, XMMWORD PTR _ref_row$27[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _c$25[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_cd$21[ebp], xmm0

; 520  :       __m128i curr_sads_ef = _mm_sad_epu8(ref_row, e);

	vmovdqa	xmm0, XMMWORD PTR _ref_row$27[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _e$24[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_ef$20[ebp], xmm0

; 521  :       __m128i curr_sads_gh = _mm_sad_epu8(ref_row, g);

	vmovdqa	xmm0, XMMWORD PTR _ref_row$27[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _g$23[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_gh$19[ebp], xmm0

; 522  : 
; 523  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_ab$22[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 524  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_cd$21[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 525  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_ef$20[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 526  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_gh$19[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 527  :     }

	jmp	$LN5@ver_sad_ar
$LN6@ver_sad_ar:

; 528  :     if (height_residual_lines) {

	cmp	DWORD PTR _height_residual_lines$[ebp], 0
	je	SHORT $LN17@ver_sad_ar

; 529  :       for (; y < height; y++) {

	jmp	SHORT $LN10@ver_sad_ar
$LN8@ver_sad_ar:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN10@ver_sad_ar:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	SHORT $LN17@ver_sad_ar

; 530  :         __m128i a = _mm_loadu_si128((const __m128i *)(pic_data + y * stride + x));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _a$18[ebp], xmm0

; 531  : 
; 532  :         __m128i curr_sads = _mm_sad_epu8(a, ref_row);

	vmovdqa	xmm0, XMMWORD PTR _a$18[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _ref_row$27[ebp]
	vmovdqa	XMMWORD PTR _curr_sads$17[ebp], xmm0

; 533  : 
; 534  :         sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads$17[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 535  :       }

	jmp	SHORT $LN8@ver_sad_ar
$LN17@ver_sad_ar:

; 536  :     }
; 537  :   }

	jmp	$LN2@ver_sad_ar
$LN3@ver_sad_ar:

; 538  : 
; 539  :   if (width_residual_pixels) {

	cmp	DWORD PTR _width_residual_pixels$[ebp], 0
	je	$LN18@ver_sad_ar

; 540  :     const __m128i ref_row = _mm_loadu_si128((__m128i *)(ref_data + x));

	mov	eax, DWORD PTR _ref_data$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _ref_row$16[ebp], xmm0

; 541  :     for (y = 0; y < height_fourline_groups; y += 4) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN13@ver_sad_ar
$LN11@ver_sad_ar:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 4
	mov	DWORD PTR _y$[ebp], eax
$LN13@ver_sad_ar:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height_fourline_groups$[ebp]
	jge	$LN12@ver_sad_ar

; 542  :       __m128i a = _mm_loadu_si128((const __m128i *)(pic_data + (y + 0) * stride + x));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _a$15[ebp], xmm0

; 543  :       __m128i c = _mm_loadu_si128((const __m128i *)(pic_data + (y + 1) * stride + x));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _c$14[ebp], xmm0

; 544  :       __m128i e = _mm_loadu_si128((const __m128i *)(pic_data + (y + 2) * stride + x));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _e$13[ebp], xmm0

; 545  :       __m128i g = _mm_loadu_si128((const __m128i *)(pic_data + (y + 3) * stride + x));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _g$12[ebp], xmm0

; 546  : 
; 547  :       __m128i a_masked     = _mm_blendv_epi8(ref_row, a, rdmask);

	vmovdqa	xmm0, XMMWORD PTR _rdmask$[ebp]
	vmovdqa	xmm1, XMMWORD PTR _ref_row$16[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _a$15[ebp], xmm0
	vmovdqa	XMMWORD PTR _a_masked$11[ebp], xmm0

; 548  :       __m128i c_masked     = _mm_blendv_epi8(ref_row, c, rdmask);

	vmovdqa	xmm0, XMMWORD PTR _rdmask$[ebp]
	vmovdqa	xmm1, XMMWORD PTR _ref_row$16[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _c$14[ebp], xmm0
	vmovdqa	XMMWORD PTR _c_masked$10[ebp], xmm0

; 549  :       __m128i e_masked     = _mm_blendv_epi8(ref_row, e, rdmask);

	vmovdqa	xmm0, XMMWORD PTR _rdmask$[ebp]
	vmovdqa	xmm1, XMMWORD PTR _ref_row$16[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _e$13[ebp], xmm0
	vmovdqa	XMMWORD PTR _e_masked$9[ebp], xmm0

; 550  :       __m128i g_masked     = _mm_blendv_epi8(ref_row, g, rdmask);

	vmovdqa	xmm0, XMMWORD PTR _rdmask$[ebp]
	vmovdqa	xmm1, XMMWORD PTR _ref_row$16[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _g$12[ebp], xmm0
	vmovdqa	XMMWORD PTR _g_masked$8[ebp], xmm0

; 551  : 
; 552  :       __m128i curr_sads_ab = _mm_sad_epu8   (ref_row, a_masked);

	vmovdqa	xmm0, XMMWORD PTR _ref_row$16[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _a_masked$11[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_ab$7[ebp], xmm0

; 553  :       __m128i curr_sads_cd = _mm_sad_epu8   (ref_row, c_masked);

	vmovdqa	xmm0, XMMWORD PTR _ref_row$16[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _c_masked$10[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_cd$6[ebp], xmm0

; 554  :       __m128i curr_sads_ef = _mm_sad_epu8   (ref_row, e_masked);

	vmovdqa	xmm0, XMMWORD PTR _ref_row$16[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _e_masked$9[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_ef$5[ebp], xmm0

; 555  :       __m128i curr_sads_gh = _mm_sad_epu8   (ref_row, g_masked);

	vmovdqa	xmm0, XMMWORD PTR _ref_row$16[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _g_masked$8[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_gh$4[ebp], xmm0

; 556  : 
; 557  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_ab$7[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 558  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_cd$6[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 559  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_ef$5[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 560  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_gh$4[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 561  :     }

	jmp	$LN11@ver_sad_ar
$LN12@ver_sad_ar:

; 562  :     if (height_residual_lines) {

	cmp	DWORD PTR _height_residual_lines$[ebp], 0
	je	SHORT $LN18@ver_sad_ar

; 563  :       for (; y < height; y++) {

	jmp	SHORT $LN16@ver_sad_ar
$LN14@ver_sad_ar:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN16@ver_sad_ar:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	SHORT $LN18@ver_sad_ar

; 564  :         __m128i a = _mm_loadu_si128((const __m128i *)(pic_data + y * stride + x));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _a$3[ebp], xmm0

; 565  : 
; 566  :         __m128i a_masked  = _mm_blendv_epi8(ref_row, a, rdmask);

	vmovdqa	xmm0, XMMWORD PTR _rdmask$[ebp]
	vmovdqa	xmm1, XMMWORD PTR _ref_row$16[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _a$3[ebp], xmm0
	vmovdqa	XMMWORD PTR _a_masked$2[ebp], xmm0

; 567  :         __m128i curr_sads = _mm_sad_epu8   (ref_row, a_masked);

	vmovdqa	xmm0, XMMWORD PTR _ref_row$16[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _a_masked$2[ebp]
	vmovdqa	XMMWORD PTR _curr_sads$1[ebp], xmm0

; 568  : 
; 569  :         sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads$1[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 570  :       }

	jmp	SHORT $LN14@ver_sad_ar
$LN18@ver_sad_ar:

; 571  :     }
; 572  :   }
; 573  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sse_inc$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sse_inc_2$[ebp], xmm0

; 574  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sse_inc_2$[ebp]
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 575  : 
; 576  :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 577  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_ver_sad_arbitrary ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
;	COMDAT _ver_sad_w16
_TEXT	SEGMENT
_sad$ = -480						; size = 16
_sse_inc_2$ = -448					; size = 16
_curr_sads$1 = -416					; size = 16
_pic_row$2 = -384					; size = 16
_curr_sads_4$3 = -352					; size = 16
_curr_sads_3$4 = -320					; size = 16
_curr_sads_2$5 = -288					; size = 16
_curr_sads_1$6 = -256					; size = 16
_pic_row_4$7 = -224					; size = 16
_pic_row_3$8 = -192					; size = 16
_pic_row_2$9 = -160					; size = 16
_pic_row_1$10 = -128					; size = 16
_height_residual_lines$ = -100				; size = 4
_height_fourline_groups$ = -88				; size = 4
_y$ = -76						; size = 4
_sse_inc$ = -64						; size = 16
_ref_row$ = -32						; size = 16
_pic_data$ = 8						; size = 4
_ref_data$ = 12						; size = 4
_height$ = 16						; size = 4
_stride$ = 20						; size = 4
_ver_sad_w16 PROC					; COMDAT

; 454  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 680				; 000002a8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-680]
	mov	ecx, 170				; 000000aaH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __9CB4F737_reg_sad_pow2_widths-sse41@h
	call	@__CheckForDebuggerJustMyCode@4

; 455  :   const __m128i ref_row = _mm_loadu_si128((__m128i *)ref_data);

	mov	eax, DWORD PTR _ref_data$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _ref_row$[ebp], xmm0

; 456  :   __m128i sse_inc       = _mm_setzero_si128();

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 457  :   int32_t y;
; 458  : 
; 459  :   const int32_t height_fourline_groups = height & ~3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, -4					; fffffffcH
	mov	DWORD PTR _height_fourline_groups$[ebp], eax

; 460  :   const int32_t height_residual_lines  = height &  3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, 3
	mov	DWORD PTR _height_residual_lines$[ebp], eax

; 461  : 
; 462  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@ver_sad_w1
$LN2@ver_sad_w1:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 4
	mov	DWORD PTR _y$[ebp], eax
$LN4@ver_sad_w1:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height_fourline_groups$[ebp]
	jge	$LN3@ver_sad_w1

; 463  :     __m128i pic_row_1   = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * stride));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _pic_row_1$10[ebp], xmm0

; 464  :     __m128i pic_row_2   = _mm_loadu_si128((__m128i *)(pic_data + (y + 1) * stride));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _pic_row_2$9[ebp], xmm0

; 465  :     __m128i pic_row_3   = _mm_loadu_si128((__m128i *)(pic_data + (y + 2) * stride));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _pic_row_3$8[ebp], xmm0

; 466  :     __m128i pic_row_4   = _mm_loadu_si128((__m128i *)(pic_data + (y + 3) * stride));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _pic_row_4$7[ebp], xmm0

; 467  : 
; 468  :     __m128i curr_sads_1 = _mm_sad_epu8   (pic_row_1, ref_row);

	vmovdqa	xmm0, XMMWORD PTR _pic_row_1$10[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _ref_row$[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_1$6[ebp], xmm0

; 469  :     __m128i curr_sads_2 = _mm_sad_epu8   (pic_row_2, ref_row);

	vmovdqa	xmm0, XMMWORD PTR _pic_row_2$9[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _ref_row$[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_2$5[ebp], xmm0

; 470  :     __m128i curr_sads_3 = _mm_sad_epu8   (pic_row_3, ref_row);

	vmovdqa	xmm0, XMMWORD PTR _pic_row_3$8[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _ref_row$[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_3$4[ebp], xmm0

; 471  :     __m128i curr_sads_4 = _mm_sad_epu8   (pic_row_4, ref_row);

	vmovdqa	xmm0, XMMWORD PTR _pic_row_4$7[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _ref_row$[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_4$3[ebp], xmm0

; 472  : 
; 473  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_1);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_1$6[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 474  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_2);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_2$5[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 475  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_3);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_3$4[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 476  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_4);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_4$3[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 477  :   }

	jmp	$LN2@ver_sad_w1
$LN3@ver_sad_w1:

; 478  :   if (height_residual_lines) {

	cmp	DWORD PTR _height_residual_lines$[ebp], 0
	je	SHORT $LN8@ver_sad_w1

; 479  :     for (; y < height; y++) {

	jmp	SHORT $LN7@ver_sad_w1
$LN5@ver_sad_w1:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN7@ver_sad_w1:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	SHORT $LN8@ver_sad_w1

; 480  :       __m128i pic_row   = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * stride));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _pic_row$2[ebp], xmm0

; 481  :       __m128i curr_sads = _mm_sad_epu8   (pic_row, ref_row);

	vmovdqa	xmm0, XMMWORD PTR _pic_row$2[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _ref_row$[ebp]
	vmovdqa	XMMWORD PTR _curr_sads$1[ebp], xmm0

; 482  : 
; 483  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads$1[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 484  :     }

	jmp	SHORT $LN5@ver_sad_w1
$LN8@ver_sad_w1:

; 485  :   }
; 486  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sse_inc$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sse_inc_2$[ebp], xmm0

; 487  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sse_inc_2$[ebp]
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 488  : 
; 489  :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 490  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_ver_sad_w16 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
;	COMDAT _ver_sad_w12
_TEXT	SEGMENT
_sad$ = -240						; size = 16
_sse_inc_2$ = -208					; size = 16
_curr_sads$1 = -176					; size = 16
_a_masked$2 = -144					; size = 16
_a$3 = -112						; size = 16
_y$ = -76						; size = 4
_sse_inc$ = -64						; size = 16
_ref_row$ = -32						; size = 16
_pic_data$ = 8						; size = 4
_ref_data$ = 12						; size = 4
_height$ = 16						; size = 4
_stride$ = 20						; size = 4
_ver_sad_w12 PROC					; COMDAT

; 435  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 440				; 000001b8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-440]
	mov	ecx, 110				; 0000006eH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __9CB4F737_reg_sad_pow2_widths-sse41@h
	call	@__CheckForDebuggerJustMyCode@4

; 436  :   const __m128i ref_row = _mm_loadu_si128((__m128i *)ref_data);

	mov	eax, DWORD PTR _ref_data$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _ref_row$[ebp], xmm0

; 437  :   __m128i sse_inc = _mm_setzero_si128();

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 438  :   int32_t y;
; 439  : 
; 440  :   for (y = 0; y < height; y++) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@ver_sad_w1
$LN2@ver_sad_w1:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN4@ver_sad_w1:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	SHORT $LN3@ver_sad_w1

; 441  :     __m128i a = _mm_loadu_si128((const __m128i *)(pic_data + y * stride));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _a$3[ebp], xmm0

; 442  : 
; 443  :     __m128i a_masked  = _mm_blend_epi16(ref_row, a, 0x3f);

	vmovdqa	xmm0, XMMWORD PTR _ref_row$[ebp]
	vpblendw xmm0, xmm0, XMMWORD PTR _a$3[ebp], 63	; 0000003fH
	vmovdqa	XMMWORD PTR _a_masked$2[ebp], xmm0

; 444  :     __m128i curr_sads = _mm_sad_epu8   (ref_row, a_masked);

	vmovdqa	xmm0, XMMWORD PTR _ref_row$[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _a_masked$2[ebp]
	vmovdqa	XMMWORD PTR _curr_sads$1[ebp], xmm0

; 445  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads$1[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 446  :   }

	jmp	SHORT $LN2@ver_sad_w1
$LN3@ver_sad_w1:

; 447  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sse_inc$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sse_inc_2$[ebp], xmm0

; 448  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sse_inc_2$[ebp]
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 449  :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 450  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_ver_sad_w12 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
;	COMDAT _ver_sad_w8
_TEXT	SEGMENT
_sad$ = -448						; size = 16
_sse_inc_2$ = -416					; size = 16
_curr_sads_ab$1 = -384					; size = 16
_a$2 = -352						; size = 16
_b$3 = -320						; size = 16
_curr_sads_cd$4 = -288					; size = 16
_curr_sads_ab$5 = -256					; size = 16
_c$6 = -224						; size = 16
_a$7 = -192						; size = 16
_c_d$8 = -160						; size = 16
_a_d$9 = -128						; size = 16
_height_residual_lines$ = -100				; size = 4
_height_fourline_groups$ = -88				; size = 4
_y$ = -76						; size = 4
_sse_inc$ = -64						; size = 16
_ref_row$ = -32						; size = 16
_pic_data$ = 8						; size = 4
_ref_data$ = 12						; size = 4
_height$ = 16						; size = 4
_stride$ = 20						; size = 4
_ver_sad_w8 PROC					; COMDAT

; 391  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 648				; 00000288H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-648]
	mov	ecx, 162				; 000000a2H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __9CB4F737_reg_sad_pow2_widths-sse41@h
	call	@__CheckForDebuggerJustMyCode@4

; 392  :   const __m128i ref_row = _mm_set1_epi64x(*(const uint64_t *)ref_data);

	mov	eax, DWORD PTR _ref_data$[ebx]
	vmovq	xmm0, QWORD PTR [eax]
	vmovq	xmm0, xmm0
	vpbroadcastq xmm0, xmm0
	vmovdqa	XMMWORD PTR _ref_row$[ebp], xmm0

; 393  :   __m128i sse_inc = _mm_setzero_si128();

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 394  :   int32_t y;
; 395  : 
; 396  :   const int32_t height_fourline_groups = height & ~3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, -4					; fffffffcH
	mov	DWORD PTR _height_fourline_groups$[ebp], eax

; 397  :   const int32_t height_residual_lines  = height &  3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, 3
	mov	DWORD PTR _height_residual_lines$[ebp], eax

; 398  : 
; 399  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@ver_sad_w8
$LN2@ver_sad_w8:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 4
	mov	DWORD PTR _y$[ebp], eax
$LN4@ver_sad_w8:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height_fourline_groups$[ebp]
	jge	$LN3@ver_sad_w8

; 400  :     __m128d a_d = _mm_setzero_pd();

	vxorps	xmm0, xmm0, xmm0
	vmovaps	XMMWORD PTR _a_d$9[ebp], xmm0

; 401  :     __m128d c_d = _mm_setzero_pd();

	vxorps	xmm0, xmm0, xmm0
	vmovaps	XMMWORD PTR _c_d$8[ebp], xmm0

; 402  : 
; 403  :     a_d = _mm_loadl_pd(a_d, (const double *)(pic_data + (y + 0) * stride));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovaps	xmm0, XMMWORD PTR _a_d$9[ebp]
	vmovlpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _a_d$9[ebp], xmm0

; 404  :     a_d = _mm_loadh_pd(a_d, (const double *)(pic_data + (y + 1) * stride));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovaps	xmm0, XMMWORD PTR _a_d$9[ebp]
	vmovhpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _a_d$9[ebp], xmm0

; 405  : 
; 406  :     c_d = _mm_loadl_pd(c_d, (const double *)(pic_data + (y + 2) * stride));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovaps	xmm0, XMMWORD PTR _c_d$8[ebp]
	vmovlpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _c_d$8[ebp], xmm0

; 407  :     c_d = _mm_loadh_pd(c_d, (const double *)(pic_data + (y + 3) * stride));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _stride$[ebx]
	add	eax, DWORD PTR _pic_data$[ebx]
	vmovaps	xmm0, XMMWORD PTR _c_d$8[ebp]
	vmovhpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _c_d$8[ebp], xmm0

; 408  : 
; 409  :     __m128i a = _mm_castpd_si128(a_d);

	vmovaps	xmm0, XMMWORD PTR _a_d$9[ebp]
	vmovdqa	XMMWORD PTR _a$7[ebp], xmm0

; 410  :     __m128i c = _mm_castpd_si128(c_d);

	vmovaps	xmm0, XMMWORD PTR _c_d$8[ebp]
	vmovdqa	XMMWORD PTR _c$6[ebp], xmm0

; 411  : 
; 412  :     __m128i curr_sads_ab = _mm_sad_epu8(a, ref_row);

	vmovdqa	xmm0, XMMWORD PTR _a$7[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _ref_row$[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_ab$5[ebp], xmm0

; 413  :     __m128i curr_sads_cd = _mm_sad_epu8(c, ref_row);

	vmovdqa	xmm0, XMMWORD PTR _c$6[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _ref_row$[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_cd$4[ebp], xmm0

; 414  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_ab$5[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 415  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_cd$4[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 416  :   }

	jmp	$LN2@ver_sad_w8
$LN3@ver_sad_w8:

; 417  :   if (height_residual_lines) {

	cmp	DWORD PTR _height_residual_lines$[ebp], 0
	je	SHORT $LN8@ver_sad_w8

; 418  :     __m128i b = _mm_move_epi64(ref_row);

	vmovq	xmm0, QWORD PTR _ref_row$[ebp]
	vmovdqa	XMMWORD PTR _b$3[ebp], xmm0

; 419  : 
; 420  :     for (; y < height; y++) {

	jmp	SHORT $LN7@ver_sad_w8
$LN5@ver_sad_w8:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN7@ver_sad_w8:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	SHORT $LN8@ver_sad_w8

; 421  :       __m128i a = _mm_loadl_epi64((__m128i *)(pic_data + y * stride));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride$[ebx]
	mov	ecx, DWORD PTR _pic_data$[ebx]
	vmovq	xmm0, QWORD PTR [ecx+eax]
	vmovdqa	XMMWORD PTR _a$2[ebp], xmm0

; 422  : 
; 423  :       __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	vmovdqa	xmm0, XMMWORD PTR _a$2[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b$3[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_ab$1[ebp], xmm0

; 424  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_ab$1[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 425  :     }

	jmp	SHORT $LN5@ver_sad_w8
$LN8@ver_sad_w8:

; 426  :   }
; 427  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sse_inc$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sse_inc_2$[ebp], xmm0

; 428  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sse_inc_2$[ebp]
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 429  : 
; 430  :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 431  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_ver_sad_w8 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
;	COMDAT _ver_sad_w4
_TEXT	SEGMENT
_sad$ = -288						; size = 16
_sse_inc_2$ = -256					; size = 16
_curr_sads$1 = -224					; size = 16
_a$2 = -192						; size = 16
_curr_sads$3 = -160					; size = 16
_a$4 = -128						; size = 16
_height_residual_lines$ = -100				; size = 4
_height_fourline_groups$ = -88				; size = 4
_y$ = -76						; size = 4
_sse_inc$ = -64						; size = 16
_ref_row$ = -32						; size = 16
_pic_data$ = 8						; size = 4
_ref_data$ = 12						; size = 4
_height$ = 16						; size = 4
_stride$ = 20						; size = 4
_ver_sad_w4 PROC					; COMDAT

; 354  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 488				; 000001e8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-488]
	mov	ecx, 122				; 0000007aH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __9CB4F737_reg_sad_pow2_widths-sse41@h
	call	@__CheckForDebuggerJustMyCode@4

; 355  :   __m128i ref_row = _mm_set1_epi32(*(const uint32_t *)ref_data);

	mov	eax, DWORD PTR _ref_data$[ebx]
	vmovd	xmm0, DWORD PTR [eax]
	vpbroadcastd xmm0, xmm0
	vmovdqa	XMMWORD PTR _ref_row$[ebp], xmm0

; 356  :   __m128i sse_inc = _mm_setzero_si128();

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 357  :   int32_t y;
; 358  : 
; 359  :   const int32_t height_fourline_groups = height & ~3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, -4					; fffffffcH
	mov	DWORD PTR _height_fourline_groups$[ebp], eax

; 360  :   const int32_t height_residual_lines  = height &  3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, 3
	mov	DWORD PTR _height_residual_lines$[ebp], eax

; 361  : 
; 362  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@ver_sad_w4
$LN2@ver_sad_w4:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 4
	mov	DWORD PTR _y$[ebp], eax
$LN4@ver_sad_w4:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height_fourline_groups$[ebp]
	jge	$LN3@ver_sad_w4

; 363  :     __m128i a = _mm_cvtsi32_si128(*(uint32_t *)(pic_data + y * stride));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride$[ebx]
	mov	ecx, DWORD PTR _pic_data$[ebx]
	vmovd	xmm0, DWORD PTR [ecx+eax]
	vmovdqa	XMMWORD PTR _a$4[ebp], xmm0

; 364  : 
; 365  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 1) * stride), 1);

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride$[ebx]
	mov	ecx, DWORD PTR _pic_data$[ebx]
	vmovdqa	xmm0, XMMWORD PTR _a$4[ebp]
	vpinsrd	xmm0, xmm0, DWORD PTR [ecx+eax], 1
	vmovdqa	XMMWORD PTR _a$4[ebp], xmm0

; 366  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 2) * stride), 2);

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _stride$[ebx]
	mov	ecx, DWORD PTR _pic_data$[ebx]
	vmovdqa	xmm0, XMMWORD PTR _a$4[ebp]
	vpinsrd	xmm0, xmm0, DWORD PTR [ecx+eax], 2
	vmovdqa	XMMWORD PTR _a$4[ebp], xmm0

; 367  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 3) * stride), 3);

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _stride$[ebx]
	mov	ecx, DWORD PTR _pic_data$[ebx]
	vmovdqa	xmm0, XMMWORD PTR _a$4[ebp]
	vpinsrd	xmm0, xmm0, DWORD PTR [ecx+eax], 3
	vmovdqa	XMMWORD PTR _a$4[ebp], xmm0

; 368  : 
; 369  :     __m128i curr_sads = _mm_sad_epu8(a, ref_row);

	vmovdqa	xmm0, XMMWORD PTR _a$4[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _ref_row$[ebp]
	vmovdqa	XMMWORD PTR _curr_sads$3[ebp], xmm0

; 370  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads$3[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 371  :   }

	jmp	$LN2@ver_sad_w4
$LN3@ver_sad_w4:

; 372  :   if (height_residual_lines) {

	cmp	DWORD PTR _height_residual_lines$[ebp], 0
	je	SHORT $LN8@ver_sad_w4

; 373  :     // Only pick the last dword, because we're comparing single dwords (lines)
; 374  :     ref_row = _mm_bsrli_si128(ref_row, 12);

	vmovdqa	xmm0, XMMWORD PTR _ref_row$[ebp]
	vpsrldq	xmm0, xmm0, 12				; 0000000cH
	vmovdqa	XMMWORD PTR _ref_row$[ebp], xmm0

; 375  : 
; 376  :     for (; y < height; y++) {

	jmp	SHORT $LN7@ver_sad_w4
$LN5@ver_sad_w4:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN7@ver_sad_w4:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	SHORT $LN8@ver_sad_w4

; 377  :       __m128i a = _mm_cvtsi32_si128(*(const uint32_t *)(pic_data + y * stride));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride$[ebx]
	mov	ecx, DWORD PTR _pic_data$[ebx]
	vmovd	xmm0, DWORD PTR [ecx+eax]
	vmovdqa	XMMWORD PTR _a$2[ebp], xmm0

; 378  : 
; 379  :       __m128i curr_sads = _mm_sad_epu8(a, ref_row);

	vmovdqa	xmm0, XMMWORD PTR _a$2[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _ref_row$[ebp]
	vmovdqa	XMMWORD PTR _curr_sads$1[ebp], xmm0

; 380  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads$1[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 381  :     }

	jmp	SHORT $LN5@ver_sad_w4
$LN8@ver_sad_w4:

; 382  :   }
; 383  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sse_inc$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sse_inc_2$[ebp], xmm0

; 384  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sse_inc_2$[ebp]
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 385  : 
; 386  :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 387  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_ver_sad_w4 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
;	COMDAT _reg_sad_arbitrary
_TEXT	SEGMENT
_sad$ = -1376						; size = 16
_sse_inc_2$ = -1344					; size = 16
_curr_sads$1 = -1312					; size = 16
_b_masked$2 = -1280					; size = 16
_b$3 = -1248						; size = 16
_a$4 = -1216						; size = 16
_curr_sads_gh$5 = -1184					; size = 16
_curr_sads_ef$6 = -1152					; size = 16
_curr_sads_cd$7 = -1120					; size = 16
_curr_sads_ab$8 = -1088					; size = 16
_h_masked$9 = -1056					; size = 16
_f_masked$10 = -1024					; size = 16
_d_masked$11 = -992					; size = 16
_b_masked$12 = -960					; size = 16
_h$13 = -928						; size = 16
_g$14 = -896						; size = 16
_f$15 = -864						; size = 16
_e$16 = -832						; size = 16
_d$17 = -800						; size = 16
_c$18 = -768						; size = 16
_b$19 = -736						; size = 16
_a$20 = -704						; size = 16
_curr_sads$21 = -672					; size = 16
_b$22 = -640						; size = 16
_a$23 = -608						; size = 16
_curr_sads_gh$24 = -576					; size = 16
_curr_sads_ef$25 = -544					; size = 16
_curr_sads_cd$26 = -512					; size = 16
_curr_sads_ab$27 = -480					; size = 16
_h$28 = -448						; size = 16
_g$29 = -416						; size = 16
_f$30 = -384						; size = 16
_e$31 = -352						; size = 16
_d$32 = -320						; size = 16
_c$33 = -288						; size = 16
_b$34 = -256						; size = 16
_a$35 = -224						; size = 16
_rdmask$ = -192						; size = 16
_ns$ = -160						; size = 16
_rds$ = -128						; size = 16
_height_residual_lines$ = -96				; size = 4
_height_fourline_groups$ = -84				; size = 4
_width_residual_pixels$ = -72				; size = 4
_width_xmms$ = -60					; size = 4
_sse_inc$ = -48						; size = 16
_x$ = -20						; size = 4
_y$ = -8						; size = 4
_data1$ = 8						; size = 4
_data2$ = 12						; size = 4
_width$ = 16						; size = 4
_height$ = 20						; size = 4
_stride1$ = 24						; size = 4
_stride2$ = 28						; size = 4
_reg_sad_arbitrary PROC					; COMDAT

; 259  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 1960				; 000007a8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-1960]
	mov	ecx, 490				; 000001eaH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __9CB4F737_reg_sad_pow2_widths-sse41@h
	call	@__CheckForDebuggerJustMyCode@4

; 260  :   int32_t y, x;
; 261  :   __m128i sse_inc = _mm_setzero_si128();

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 262  :   
; 263  :   // Bytes in block in 128-bit blocks per each scanline, and remainder
; 264  :   const int32_t width_xmms             = width  & ~15;

	mov	eax, DWORD PTR _width$[ebx]
	and	eax, -16				; fffffff0H
	mov	DWORD PTR _width_xmms$[ebp], eax

; 265  :   const int32_t width_residual_pixels  = width  &  15;

	mov	eax, DWORD PTR _width$[ebx]
	and	eax, 15					; 0000000fH
	mov	DWORD PTR _width_residual_pixels$[ebp], eax

; 266  : 
; 267  :   const int32_t height_fourline_groups = height & ~3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, -4					; fffffffcH
	mov	DWORD PTR _height_fourline_groups$[ebp], eax

; 268  :   const int32_t height_residual_lines  = height &  3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, 3
	mov	DWORD PTR _height_residual_lines$[ebp], eax

; 269  : 
; 270  :   const __m128i rds    = _mm_set1_epi8 (width_residual_pixels);

	movsx	eax, BYTE PTR _width_residual_pixels$[ebp]
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vmovdqa	XMMWORD PTR _rds$[ebp], xmm0

; 271  :   const __m128i ns     = _mm_setr_epi8 (0,  1,  2,  3,  4,  5,  6,  7,

	vmovdqa	xmm0, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	vmovdqa	XMMWORD PTR _ns$[ebp], xmm0

; 272  :                                         8,  9,  10, 11, 12, 13, 14, 15);
; 273  :   const __m128i rdmask = _mm_cmpgt_epi8(rds, ns);

	vmovdqa	xmm0, XMMWORD PTR _rds$[ebp]
	vpcmpgtb xmm0, xmm0, XMMWORD PTR _ns$[ebp]
	vmovdqa	XMMWORD PTR _rdmask$[ebp], xmm0

; 274  : 
; 275  :   for (x = 0; x < width_xmms; x += 16) {

	mov	DWORD PTR _x$[ebp], 0
	jmp	SHORT $LN4@reg_sad_ar
$LN2@reg_sad_ar:
	mov	eax, DWORD PTR _x$[ebp]
	add	eax, 16					; 00000010H
	mov	DWORD PTR _x$[ebp], eax
$LN4@reg_sad_ar:
	mov	eax, DWORD PTR _x$[ebp]
	cmp	eax, DWORD PTR _width_xmms$[ebp]
	jge	$LN3@reg_sad_ar

; 276  :     for (y = 0; y < height_fourline_groups; y += 4) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN7@reg_sad_ar
$LN5@reg_sad_ar:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 4
	mov	DWORD PTR _y$[ebp], eax
$LN7@reg_sad_ar:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height_fourline_groups$[ebp]
	jge	$LN6@reg_sad_ar

; 277  :       __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1 + x));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _a$35[ebp], xmm0

; 278  :       __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2 + x));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _b$34[ebp], xmm0

; 279  :       __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1 + x));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _c$33[ebp], xmm0

; 280  :       __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2 + x));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _d$32[ebp], xmm0

; 281  :       __m128i e = _mm_loadu_si128((const __m128i *)(data1 + (y + 2) * stride1 + x));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _e$31[ebp], xmm0

; 282  :       __m128i f = _mm_loadu_si128((const __m128i *)(data2 + (y + 2) * stride2 + x));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _f$30[ebp], xmm0

; 283  :       __m128i g = _mm_loadu_si128((const __m128i *)(data1 + (y + 3) * stride1 + x));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _g$29[ebp], xmm0

; 284  :       __m128i h = _mm_loadu_si128((const __m128i *)(data2 + (y + 3) * stride2 + x));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _h$28[ebp], xmm0

; 285  : 
; 286  :       __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	vmovdqa	xmm0, XMMWORD PTR _a$35[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b$34[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_ab$27[ebp], xmm0

; 287  :       __m128i curr_sads_cd = _mm_sad_epu8(c, d);

	vmovdqa	xmm0, XMMWORD PTR _c$33[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _d$32[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_cd$26[ebp], xmm0

; 288  :       __m128i curr_sads_ef = _mm_sad_epu8(e, f);

	vmovdqa	xmm0, XMMWORD PTR _e$31[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _f$30[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_ef$25[ebp], xmm0

; 289  :       __m128i curr_sads_gh = _mm_sad_epu8(g, h);

	vmovdqa	xmm0, XMMWORD PTR _g$29[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _h$28[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_gh$24[ebp], xmm0

; 290  : 
; 291  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_ab$27[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 292  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_cd$26[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 293  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_ef$25[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 294  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_gh$24[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 295  :     }

	jmp	$LN5@reg_sad_ar
$LN6@reg_sad_ar:

; 296  :     if (height_residual_lines) {

	cmp	DWORD PTR _height_residual_lines$[ebp], 0
	je	SHORT $LN17@reg_sad_ar

; 297  :       for (; y < height; y++) {

	jmp	SHORT $LN10@reg_sad_ar
$LN8@reg_sad_ar:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN10@reg_sad_ar:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	SHORT $LN17@reg_sad_ar

; 298  :         __m128i a = _mm_loadu_si128((const __m128i *)(data1 + y * stride1 + x));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _a$23[ebp], xmm0

; 299  :         __m128i b = _mm_loadu_si128((const __m128i *)(data2 + y * stride2 + x));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _b$22[ebp], xmm0

; 300  : 
; 301  :         __m128i curr_sads = _mm_sad_epu8(a, b);

	vmovdqa	xmm0, XMMWORD PTR _a$23[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b$22[ebp]
	vmovdqa	XMMWORD PTR _curr_sads$21[ebp], xmm0

; 302  : 
; 303  :         sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads$21[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 304  :       }

	jmp	SHORT $LN8@reg_sad_ar
$LN17@reg_sad_ar:

; 305  :     }
; 306  :   }

	jmp	$LN2@reg_sad_ar
$LN3@reg_sad_ar:

; 307  : 
; 308  :   if (width_residual_pixels) {

	cmp	DWORD PTR _width_residual_pixels$[ebp], 0
	je	$LN18@reg_sad_ar

; 309  :     for (y = 0; y < height_fourline_groups; y += 4) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN13@reg_sad_ar
$LN11@reg_sad_ar:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 4
	mov	DWORD PTR _y$[ebp], eax
$LN13@reg_sad_ar:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height_fourline_groups$[ebp]
	jge	$LN12@reg_sad_ar

; 310  :       __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1 + x));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _a$20[ebp], xmm0

; 311  :       __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2 + x));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _b$19[ebp], xmm0

; 312  :       __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1 + x));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _c$18[ebp], xmm0

; 313  :       __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2 + x));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _d$17[ebp], xmm0

; 314  :       __m128i e = _mm_loadu_si128((const __m128i *)(data1 + (y + 2) * stride1 + x));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _e$16[ebp], xmm0

; 315  :       __m128i f = _mm_loadu_si128((const __m128i *)(data2 + (y + 2) * stride2 + x));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _f$15[ebp], xmm0

; 316  :       __m128i g = _mm_loadu_si128((const __m128i *)(data1 + (y + 3) * stride1 + x));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _g$14[ebp], xmm0

; 317  :       __m128i h = _mm_loadu_si128((const __m128i *)(data2 + (y + 3) * stride2 + x));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _h$13[ebp], xmm0

; 318  : 
; 319  :       __m128i b_masked     = _mm_blendv_epi8(a, b, rdmask);

	vmovdqa	xmm0, XMMWORD PTR _rdmask$[ebp]
	vmovdqa	xmm1, XMMWORD PTR _a$20[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _b$19[ebp], xmm0
	vmovdqa	XMMWORD PTR _b_masked$12[ebp], xmm0

; 320  :       __m128i d_masked     = _mm_blendv_epi8(c, d, rdmask);

	vmovdqa	xmm0, XMMWORD PTR _rdmask$[ebp]
	vmovdqa	xmm1, XMMWORD PTR _c$18[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _d$17[ebp], xmm0
	vmovdqa	XMMWORD PTR _d_masked$11[ebp], xmm0

; 321  :       __m128i f_masked     = _mm_blendv_epi8(e, f, rdmask);

	vmovdqa	xmm0, XMMWORD PTR _rdmask$[ebp]
	vmovdqa	xmm1, XMMWORD PTR _e$16[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _f$15[ebp], xmm0
	vmovdqa	XMMWORD PTR _f_masked$10[ebp], xmm0

; 322  :       __m128i h_masked     = _mm_blendv_epi8(g, h, rdmask);

	vmovdqa	xmm0, XMMWORD PTR _rdmask$[ebp]
	vmovdqa	xmm1, XMMWORD PTR _g$14[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _h$13[ebp], xmm0
	vmovdqa	XMMWORD PTR _h_masked$9[ebp], xmm0

; 323  : 
; 324  :       __m128i curr_sads_ab = _mm_sad_epu8   (a, b_masked);

	vmovdqa	xmm0, XMMWORD PTR _a$20[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b_masked$12[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_ab$8[ebp], xmm0

; 325  :       __m128i curr_sads_cd = _mm_sad_epu8   (c, d_masked);

	vmovdqa	xmm0, XMMWORD PTR _c$18[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _d_masked$11[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_cd$7[ebp], xmm0

; 326  :       __m128i curr_sads_ef = _mm_sad_epu8   (e, f_masked);

	vmovdqa	xmm0, XMMWORD PTR _e$16[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _f_masked$10[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_ef$6[ebp], xmm0

; 327  :       __m128i curr_sads_gh = _mm_sad_epu8   (g, h_masked);

	vmovdqa	xmm0, XMMWORD PTR _g$14[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _h_masked$9[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_gh$5[ebp], xmm0

; 328  : 
; 329  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_ab$8[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 330  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_cd$7[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 331  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_ef$6[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 332  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_gh$5[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 333  :     }

	jmp	$LN11@reg_sad_ar
$LN12@reg_sad_ar:

; 334  :     if (height_residual_lines) {

	cmp	DWORD PTR _height_residual_lines$[ebp], 0
	je	$LN18@reg_sad_ar

; 335  :       for (; y < height; y++) {

	jmp	SHORT $LN16@reg_sad_ar
$LN14@reg_sad_ar:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN16@reg_sad_ar:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	$LN18@reg_sad_ar

; 336  :         __m128i a = _mm_loadu_si128((const __m128i *)(data1 + y * stride1 + x));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _a$4[ebp], xmm0

; 337  :         __m128i b = _mm_loadu_si128((const __m128i *)(data2 + y * stride2 + x));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	add	eax, DWORD PTR _x$[ebp]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _b$3[ebp], xmm0

; 338  : 
; 339  :         __m128i b_masked  = _mm_blendv_epi8(a, b, rdmask);

	vmovdqa	xmm0, XMMWORD PTR _rdmask$[ebp]
	vmovdqa	xmm1, XMMWORD PTR _a$4[ebp]
	vpblendvb xmm0, xmm1, XMMWORD PTR _b$3[ebp], xmm0
	vmovdqa	XMMWORD PTR _b_masked$2[ebp], xmm0

; 340  :         __m128i curr_sads = _mm_sad_epu8   (a, b_masked);

	vmovdqa	xmm0, XMMWORD PTR _a$4[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b_masked$2[ebp]
	vmovdqa	XMMWORD PTR _curr_sads$1[ebp], xmm0

; 341  : 
; 342  :         sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads$1[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 343  :       }

	jmp	$LN14@reg_sad_ar
$LN18@reg_sad_ar:

; 344  :     }
; 345  :   }
; 346  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sse_inc$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sse_inc_2$[ebp], xmm0

; 347  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sse_inc_2$[ebp]
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 348  : 
; 349  :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 350  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_reg_sad_arbitrary ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
;	COMDAT _reg_sad_w24
_TEXT	SEGMENT
_sad$ = -672						; size = 16
_sse_inc_2$ = -640					; size = 16
_curr_sads_2$1 = -608					; size = 16
_curr_sads_1$2 = -576					; size = 16
_d$3 = -544						; size = 16
_c$4 = -512						; size = 16
_b$5 = -480						; size = 16
_a$6 = -448						; size = 16
_curr_sads_3$7 = -416					; size = 16
_curr_sads_2$8 = -384					; size = 16
_curr_sads_1$9 = -352					; size = 16
_f$10 = -320						; size = 16
_e$11 = -288						; size = 16
_f_d$12 = -256						; size = 16
_e_d$13 = -224						; size = 16
_d$14 = -192						; size = 16
_c$15 = -160						; size = 16
_b$16 = -128						; size = 16
_a$17 = -96						; size = 16
_height_parity$ = -68					; size = 4
_height_doublelines$ = -56				; size = 4
_y$ = -44						; size = 4
_sse_inc$ = -32						; size = 16
_data1$ = 8						; size = 4
_data2$ = 12						; size = 4
_height$ = 16						; size = 4
_stride1$ = 20						; size = 4
_stride2$ = 24						; size = 4
_reg_sad_w24 PROC					; COMDAT

; 207  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 1064				; 00000428H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-1064]
	mov	ecx, 266				; 0000010aH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __9CB4F737_reg_sad_pow2_widths-sse41@h
	call	@__CheckForDebuggerJustMyCode@4

; 208  :   __m128i sse_inc = _mm_setzero_si128();

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 209  :   int32_t y;
; 210  : 
; 211  :   const int32_t height_doublelines = height & ~1;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, -2					; fffffffeH
	mov	DWORD PTR _height_doublelines$[ebp], eax

; 212  :   const int32_t height_parity      = height &  1;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, 1
	mov	DWORD PTR _height_parity$[ebp], eax

; 213  : 
; 214  :   for (y = 0; y < height_doublelines; y += 2) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@reg_sad_w2
$LN2@reg_sad_w2:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	mov	DWORD PTR _y$[ebp], eax
$LN4@reg_sad_w2:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height_doublelines$[ebp]
	jge	$LN3@reg_sad_w2

; 215  :     __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _a$17[ebp], xmm0

; 216  :     __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _b$16[ebp], xmm0

; 217  :     __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _c$15[ebp], xmm0

; 218  :     __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _d$14[ebp], xmm0

; 219  : 
; 220  :     __m128d e_d = _mm_setzero_pd();

	vxorps	xmm0, xmm0, xmm0
	vmovaps	XMMWORD PTR _e_d$13[ebp], xmm0

; 221  :     __m128d f_d = _mm_setzero_pd();

	vxorps	xmm0, xmm0, xmm0
	vmovaps	XMMWORD PTR _f_d$12[ebp], xmm0

; 222  : 
; 223  :     e_d = _mm_loadl_pd(e_d, (const double *)(data1 + (y + 0) * stride1 + 16));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	mov	ecx, DWORD PTR _data1$[ebx]
	vmovaps	xmm0, XMMWORD PTR _e_d$13[ebp]
	vmovlpd	xmm0, xmm0, QWORD PTR [ecx+eax+16]
	vmovaps	XMMWORD PTR _e_d$13[ebp], xmm0

; 224  :     f_d = _mm_loadl_pd(f_d, (const double *)(data2 + (y + 0) * stride2 + 16));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	mov	ecx, DWORD PTR _data2$[ebx]
	vmovaps	xmm0, XMMWORD PTR _f_d$12[ebp]
	vmovlpd	xmm0, xmm0, QWORD PTR [ecx+eax+16]
	vmovaps	XMMWORD PTR _f_d$12[ebp], xmm0

; 225  :     e_d = _mm_loadh_pd(e_d, (const double *)(data1 + (y + 1) * stride1 + 16));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride1$[ebx]
	mov	ecx, DWORD PTR _data1$[ebx]
	vmovaps	xmm0, XMMWORD PTR _e_d$13[ebp]
	vmovhpd	xmm0, xmm0, QWORD PTR [ecx+eax+16]
	vmovaps	XMMWORD PTR _e_d$13[ebp], xmm0

; 226  :     f_d = _mm_loadh_pd(f_d, (const double *)(data2 + (y + 1) * stride2 + 16));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride2$[ebx]
	mov	ecx, DWORD PTR _data2$[ebx]
	vmovaps	xmm0, XMMWORD PTR _f_d$12[ebp]
	vmovhpd	xmm0, xmm0, QWORD PTR [ecx+eax+16]
	vmovaps	XMMWORD PTR _f_d$12[ebp], xmm0

; 227  : 
; 228  :     __m128i e = _mm_castpd_si128(e_d);

	vmovaps	xmm0, XMMWORD PTR _e_d$13[ebp]
	vmovdqa	XMMWORD PTR _e$11[ebp], xmm0

; 229  :     __m128i f = _mm_castpd_si128(f_d);

	vmovaps	xmm0, XMMWORD PTR _f_d$12[ebp]
	vmovdqa	XMMWORD PTR _f$10[ebp], xmm0

; 230  : 
; 231  :     __m128i curr_sads_1 = _mm_sad_epu8(a, b);

	vmovdqa	xmm0, XMMWORD PTR _a$17[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b$16[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_1$9[ebp], xmm0

; 232  :     __m128i curr_sads_2 = _mm_sad_epu8(c, d);

	vmovdqa	xmm0, XMMWORD PTR _c$15[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _d$14[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_2$8[ebp], xmm0

; 233  :     __m128i curr_sads_3 = _mm_sad_epu8(e, f);

	vmovdqa	xmm0, XMMWORD PTR _e$11[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _f$10[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_3$7[ebp], xmm0

; 234  : 
; 235  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_1);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_1$9[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 236  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_2);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_2$8[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 237  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_3);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_3$7[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 238  :   }

	jmp	$LN2@reg_sad_w2
$LN3@reg_sad_w2:

; 239  :   if (height_parity) {

	cmp	DWORD PTR _height_parity$[ebp], 0
	je	$LN5@reg_sad_w2

; 240  :     __m128i a = _mm_loadu_si128   ((const __m128i *)(data1 + y * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _a$6[ebp], xmm0

; 241  :     __m128i b = _mm_loadu_si128   ((const __m128i *)(data2 + y * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _b$5[ebp], xmm0

; 242  :     __m128i c = _mm_loadl_epi64   ((const __m128i *)(data1 + y * stride1 + 16));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	mov	ecx, DWORD PTR _data1$[ebx]
	vmovq	xmm0, QWORD PTR [ecx+eax+16]
	vmovdqa	XMMWORD PTR _c$4[ebp], xmm0

; 243  :     __m128i d = _mm_loadl_epi64   ((const __m128i *)(data2 + y * stride2 + 16));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	mov	ecx, DWORD PTR _data2$[ebx]
	vmovq	xmm0, QWORD PTR [ecx+eax+16]
	vmovdqa	XMMWORD PTR _d$3[ebp], xmm0

; 244  : 
; 245  :     __m128i curr_sads_1 = _mm_sad_epu8(a, b);

	vmovdqa	xmm0, XMMWORD PTR _a$6[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b$5[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_1$2[ebp], xmm0

; 246  :     __m128i curr_sads_2 = _mm_sad_epu8(c, d);

	vmovdqa	xmm0, XMMWORD PTR _c$4[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _d$3[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_2$1[ebp], xmm0

; 247  : 
; 248  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_1);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_1$2[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 249  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_2);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_2$1[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0
$LN5@reg_sad_w2:

; 250  :   }
; 251  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sse_inc$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sse_inc_2$[ebp], xmm0

; 252  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sse_inc_2$[ebp]
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 253  :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 254  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_reg_sad_w24 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
;	COMDAT _reg_sad_w16
_TEXT	SEGMENT
_sad$ = -608						; size = 16
_sse_inc_2$ = -576					; size = 16
_curr_sads$1 = -544					; size = 16
_b$2 = -512						; size = 16
_a$3 = -480						; size = 16
_curr_sads_gh$4 = -448					; size = 16
_curr_sads_ef$5 = -416					; size = 16
_curr_sads_cd$6 = -384					; size = 16
_curr_sads_ab$7 = -352					; size = 16
_h$8 = -320						; size = 16
_g$9 = -288						; size = 16
_f$10 = -256						; size = 16
_e$11 = -224						; size = 16
_d$12 = -192						; size = 16
_c$13 = -160						; size = 16
_b$14 = -128						; size = 16
_a$15 = -96						; size = 16
_height_residual_lines$ = -68				; size = 4
_height_fourline_groups$ = -56				; size = 4
_y$ = -44						; size = 4
_sse_inc$ = -32						; size = 16
_data1$ = 8						; size = 4
_data2$ = 12						; size = 4
_height$ = 16						; size = 4
_stride1$ = 20						; size = 4
_stride2$ = 24						; size = 4
_reg_sad_w16 PROC					; COMDAT

; 162  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 1000				; 000003e8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-1000]
	mov	ecx, 250				; 000000faH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __9CB4F737_reg_sad_pow2_widths-sse41@h
	call	@__CheckForDebuggerJustMyCode@4

; 163  :   __m128i sse_inc = _mm_setzero_si128();

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 164  :   int32_t y;
; 165  : 
; 166  :   const int32_t height_fourline_groups = height & ~3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, -4					; fffffffcH
	mov	DWORD PTR _height_fourline_groups$[ebp], eax

; 167  :   const int32_t height_residual_lines  = height &  3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, 3
	mov	DWORD PTR _height_residual_lines$[ebp], eax

; 168  : 
; 169  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@reg_sad_w1
$LN2@reg_sad_w1:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 4
	mov	DWORD PTR _y$[ebp], eax
$LN4@reg_sad_w1:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height_fourline_groups$[ebp]
	jge	$LN3@reg_sad_w1

; 170  :     __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _a$15[ebp], xmm0

; 171  :     __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _b$14[ebp], xmm0

; 172  :     __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _c$13[ebp], xmm0

; 173  :     __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _d$12[ebp], xmm0

; 174  :     __m128i e = _mm_loadu_si128((const __m128i *)(data1 + (y + 2) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _e$11[ebp], xmm0

; 175  :     __m128i f = _mm_loadu_si128((const __m128i *)(data2 + (y + 2) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _f$10[ebp], xmm0

; 176  :     __m128i g = _mm_loadu_si128((const __m128i *)(data1 + (y + 3) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _g$9[ebp], xmm0

; 177  :     __m128i h = _mm_loadu_si128((const __m128i *)(data2 + (y + 3) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _h$8[ebp], xmm0

; 178  : 
; 179  :     __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	vmovdqa	xmm0, XMMWORD PTR _a$15[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b$14[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_ab$7[ebp], xmm0

; 180  :     __m128i curr_sads_cd = _mm_sad_epu8(c, d);

	vmovdqa	xmm0, XMMWORD PTR _c$13[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _d$12[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_cd$6[ebp], xmm0

; 181  :     __m128i curr_sads_ef = _mm_sad_epu8(e, f);

	vmovdqa	xmm0, XMMWORD PTR _e$11[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _f$10[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_ef$5[ebp], xmm0

; 182  :     __m128i curr_sads_gh = _mm_sad_epu8(g, h);

	vmovdqa	xmm0, XMMWORD PTR _g$9[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _h$8[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_gh$4[ebp], xmm0

; 183  : 
; 184  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_ab$7[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 185  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_cd$6[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 186  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_ef$5[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 187  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_gh$4[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 188  :   }

	jmp	$LN2@reg_sad_w1
$LN3@reg_sad_w1:

; 189  :   if (height_residual_lines) {

	cmp	DWORD PTR _height_residual_lines$[ebp], 0
	je	SHORT $LN8@reg_sad_w1

; 190  :     for (; y < height; y++) {

	jmp	SHORT $LN7@reg_sad_w1
$LN5@reg_sad_w1:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN7@reg_sad_w1:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	SHORT $LN8@reg_sad_w1

; 191  :       __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _a$3[ebp], xmm0

; 192  :       __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _b$2[ebp], xmm0

; 193  : 
; 194  :       __m128i curr_sads = _mm_sad_epu8(a, b);

	vmovdqa	xmm0, XMMWORD PTR _a$3[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b$2[ebp]
	vmovdqa	XMMWORD PTR _curr_sads$1[ebp], xmm0

; 195  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads$1[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 196  :     }

	jmp	SHORT $LN5@reg_sad_w1
$LN8@reg_sad_w1:

; 197  :   }
; 198  : 
; 199  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sse_inc$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sse_inc_2$[ebp], xmm0

; 200  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sse_inc_2$[ebp]
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 201  :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 202  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_reg_sad_w16 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
;	COMDAT _reg_sad_w12
_TEXT	SEGMENT
_sad$ = -240						; size = 16
_sse_inc_2$ = -208					; size = 16
_curr_sads$1 = -176					; size = 16
_b_masked$2 = -144					; size = 16
_b$3 = -112						; size = 16
_a$4 = -80						; size = 16
_y$ = -44						; size = 4
_sse_inc$ = -32						; size = 16
_data1$ = 8						; size = 4
_data2$ = 12						; size = 4
_height$ = 16						; size = 4
_stride1$ = 20						; size = 4
_stride2$ = 24						; size = 4
_reg_sad_w12 PROC					; COMDAT

; 143  : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 440				; 000001b8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-440]
	mov	ecx, 110				; 0000006eH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __9CB4F737_reg_sad_pow2_widths-sse41@h
	call	@__CheckForDebuggerJustMyCode@4

; 144  :   __m128i sse_inc = _mm_setzero_si128();

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 145  :   int32_t y;
; 146  :   for (y = 0; y < height; y++) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@reg_sad_w1
$LN2@reg_sad_w1:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN4@reg_sad_w1:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	SHORT $LN3@reg_sad_w1

; 147  :     __m128i a = _mm_loadu_si128((const __m128i *)(data1 + y * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _a$4[ebp], xmm0

; 148  :     __m128i b = _mm_loadu_si128((const __m128i *)(data2 + y * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovdqu	xmm0, XMMWORD PTR [eax]
	vmovdqa	XMMWORD PTR _b$3[ebp], xmm0

; 149  : 
; 150  :     __m128i b_masked  = _mm_blend_epi16(a, b, 0x3f);

	vmovdqa	xmm0, XMMWORD PTR _a$4[ebp]
	vpblendw xmm0, xmm0, XMMWORD PTR _b$3[ebp], 63	; 0000003fH
	vmovdqa	XMMWORD PTR _b_masked$2[ebp], xmm0

; 151  :     __m128i curr_sads = _mm_sad_epu8   (a, b_masked);

	vmovdqa	xmm0, XMMWORD PTR _a$4[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b_masked$2[ebp]
	vmovdqa	XMMWORD PTR _curr_sads$1[ebp], xmm0

; 152  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads$1[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 153  :   }

	jmp	SHORT $LN2@reg_sad_w1
$LN3@reg_sad_w1:

; 154  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sse_inc$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sse_inc_2$[ebp], xmm0

; 155  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sse_inc_2$[ebp]
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 156  :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 157  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_reg_sad_w12 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
;	COMDAT _reg_sad_w8
_TEXT	SEGMENT
_sad$ = -544						; size = 16
_sse_inc_2$ = -512					; size = 16
_curr_sads_ab$1 = -480					; size = 16
_b$2 = -448						; size = 16
_a$3 = -416						; size = 16
_curr_sads_cd$4 = -384					; size = 16
_curr_sads_ab$5 = -352					; size = 16
_d$6 = -320						; size = 16
_c$7 = -288						; size = 16
_b$8 = -256						; size = 16
_a$9 = -224						; size = 16
_d_d$10 = -192						; size = 16
_c_d$11 = -160						; size = 16
_b_d$12 = -128						; size = 16
_a_d$13 = -96						; size = 16
_height_residual_lines$ = -68				; size = 4
_height_fourline_groups$ = -56				; size = 4
_y$ = -44						; size = 4
_sse_inc$ = -32						; size = 16
_data1$ = 8						; size = 4
_data2$ = 12						; size = 4
_height$ = 16						; size = 4
_stride1$ = 20						; size = 4
_stride2$ = 24						; size = 4
_reg_sad_w8 PROC					; COMDAT

; 92   : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 936				; 000003a8H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-936]
	mov	ecx, 234				; 000000eaH
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __9CB4F737_reg_sad_pow2_widths-sse41@h
	call	@__CheckForDebuggerJustMyCode@4

; 93   :   __m128i sse_inc = _mm_setzero_si128();

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 94   :   int32_t y;
; 95   : 
; 96   :   const int32_t height_fourline_groups = height & ~3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, -4					; fffffffcH
	mov	DWORD PTR _height_fourline_groups$[ebp], eax

; 97   :   const int32_t height_residual_lines  = height &  3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, 3
	mov	DWORD PTR _height_residual_lines$[ebp], eax

; 98   : 
; 99   :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@reg_sad_w8
$LN2@reg_sad_w8:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 4
	mov	DWORD PTR _y$[ebp], eax
$LN4@reg_sad_w8:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height_fourline_groups$[ebp]
	jge	$LN3@reg_sad_w8

; 100  :     __m128d a_d = _mm_setzero_pd();

	vxorps	xmm0, xmm0, xmm0
	vmovaps	XMMWORD PTR _a_d$13[ebp], xmm0

; 101  :     __m128d b_d = _mm_setzero_pd();

	vxorps	xmm0, xmm0, xmm0
	vmovaps	XMMWORD PTR _b_d$12[ebp], xmm0

; 102  :     __m128d c_d = _mm_setzero_pd();

	vxorps	xmm0, xmm0, xmm0
	vmovaps	XMMWORD PTR _c_d$11[ebp], xmm0

; 103  :     __m128d d_d = _mm_setzero_pd();

	vxorps	xmm0, xmm0, xmm0
	vmovaps	XMMWORD PTR _d_d$10[ebp], xmm0

; 104  : 
; 105  :     a_d = _mm_loadl_pd(a_d, (const double *)(data1 + (y + 0) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovaps	xmm0, XMMWORD PTR _a_d$13[ebp]
	vmovlpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _a_d$13[ebp], xmm0

; 106  :     b_d = _mm_loadl_pd(b_d, (const double *)(data2 + (y + 0) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovaps	xmm0, XMMWORD PTR _b_d$12[ebp]
	vmovlpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _b_d$12[ebp], xmm0

; 107  :     a_d = _mm_loadh_pd(a_d, (const double *)(data1 + (y + 1) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovaps	xmm0, XMMWORD PTR _a_d$13[ebp]
	vmovhpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _a_d$13[ebp], xmm0

; 108  :     b_d = _mm_loadh_pd(b_d, (const double *)(data2 + (y + 1) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovaps	xmm0, XMMWORD PTR _b_d$12[ebp]
	vmovhpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _b_d$12[ebp], xmm0

; 109  : 
; 110  :     c_d = _mm_loadl_pd(c_d, (const double *)(data1 + (y + 2) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovaps	xmm0, XMMWORD PTR _c_d$11[ebp]
	vmovlpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _c_d$11[ebp], xmm0

; 111  :     d_d = _mm_loadl_pd(d_d, (const double *)(data2 + (y + 2) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovaps	xmm0, XMMWORD PTR _d_d$10[ebp]
	vmovlpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _d_d$10[ebp], xmm0

; 112  :     c_d = _mm_loadh_pd(c_d, (const double *)(data1 + (y + 3) * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _stride1$[ebx]
	add	eax, DWORD PTR _data1$[ebx]
	vmovaps	xmm0, XMMWORD PTR _c_d$11[ebp]
	vmovhpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _c_d$11[ebp], xmm0

; 113  :     d_d = _mm_loadh_pd(d_d, (const double *)(data2 + (y + 3) * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _stride2$[ebx]
	add	eax, DWORD PTR _data2$[ebx]
	vmovaps	xmm0, XMMWORD PTR _d_d$10[ebp]
	vmovhpd	xmm0, xmm0, QWORD PTR [eax]
	vmovaps	XMMWORD PTR _d_d$10[ebp], xmm0

; 114  : 
; 115  :     __m128i a = _mm_castpd_si128(a_d);

	vmovaps	xmm0, XMMWORD PTR _a_d$13[ebp]
	vmovdqa	XMMWORD PTR _a$9[ebp], xmm0

; 116  :     __m128i b = _mm_castpd_si128(b_d);

	vmovaps	xmm0, XMMWORD PTR _b_d$12[ebp]
	vmovdqa	XMMWORD PTR _b$8[ebp], xmm0

; 117  :     __m128i c = _mm_castpd_si128(c_d);

	vmovaps	xmm0, XMMWORD PTR _c_d$11[ebp]
	vmovdqa	XMMWORD PTR _c$7[ebp], xmm0

; 118  :     __m128i d = _mm_castpd_si128(d_d);

	vmovaps	xmm0, XMMWORD PTR _d_d$10[ebp]
	vmovdqa	XMMWORD PTR _d$6[ebp], xmm0

; 119  : 
; 120  :     __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	vmovdqa	xmm0, XMMWORD PTR _a$9[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b$8[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_ab$5[ebp], xmm0

; 121  :     __m128i curr_sads_cd = _mm_sad_epu8(c, d);

	vmovdqa	xmm0, XMMWORD PTR _c$7[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _d$6[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_cd$4[ebp], xmm0

; 122  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_ab$5[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 123  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_cd$4[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 124  :   }

	jmp	$LN2@reg_sad_w8
$LN3@reg_sad_w8:

; 125  :   if (height_residual_lines) {

	cmp	DWORD PTR _height_residual_lines$[ebp], 0
	je	SHORT $LN8@reg_sad_w8

; 126  :     for (; y < height; y++) {

	jmp	SHORT $LN7@reg_sad_w8
$LN5@reg_sad_w8:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN7@reg_sad_w8:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	SHORT $LN8@reg_sad_w8

; 127  :       __m128i a = _mm_loadl_epi64((__m128i *)(data1 + y * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	mov	ecx, DWORD PTR _data1$[ebx]
	vmovq	xmm0, QWORD PTR [ecx+eax]
	vmovdqa	XMMWORD PTR _a$3[ebp], xmm0

; 128  :       __m128i b = _mm_loadl_epi64((__m128i *)(data2 + y * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	mov	ecx, DWORD PTR _data2$[ebx]
	vmovq	xmm0, QWORD PTR [ecx+eax]
	vmovdqa	XMMWORD PTR _b$2[ebp], xmm0

; 129  : 
; 130  :       __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	vmovdqa	xmm0, XMMWORD PTR _a$3[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b$2[ebp]
	vmovdqa	XMMWORD PTR _curr_sads_ab$1[ebp], xmm0

; 131  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads_ab$1[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 132  :     }

	jmp	SHORT $LN5@reg_sad_w8
$LN8@reg_sad_w8:

; 133  :   }
; 134  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sse_inc$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sse_inc_2$[ebp], xmm0

; 135  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sse_inc_2$[ebp]
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 136  : 
; 137  :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 138  : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_reg_sad_w8 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
;	COMDAT _reg_sad_w4
_TEXT	SEGMENT
_sad$ = -320						; size = 16
_sse_inc_2$ = -288					; size = 16
_curr_sads$1 = -256					; size = 16
_b$2 = -224						; size = 16
_a$3 = -192						; size = 16
_curr_sads$4 = -160					; size = 16
_b$5 = -128						; size = 16
_a$6 = -96						; size = 16
_height_residual_lines$ = -68				; size = 4
_height_fourline_groups$ = -56				; size = 4
_y$ = -44						; size = 4
_sse_inc$ = -32						; size = 16
_data1$ = 8						; size = 4
_data2$ = 12						; size = 4
_height$ = 16						; size = 4
_stride1$ = 20						; size = 4
_stride2$ = 24						; size = 4
_reg_sad_w4 PROC					; COMDAT

; 53   : {

	push	ebx
	mov	ebx, esp
	sub	esp, 8
	and	esp, -16				; fffffff0H
	add	esp, 4
	push	ebp
	mov	ebp, DWORD PTR [ebx+4]
	mov	DWORD PTR [esp+4], ebp
	mov	ebp, esp
	sub	esp, 520				; 00000208H
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-520]
	mov	ecx, 130				; 00000082H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __9CB4F737_reg_sad_pow2_widths-sse41@h
	call	@__CheckForDebuggerJustMyCode@4

; 54   :   __m128i sse_inc = _mm_setzero_si128();

	vxorps	xmm0, xmm0, xmm0
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 55   :   int32_t y;
; 56   : 
; 57   :   const int32_t height_fourline_groups = height & ~3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, -4					; fffffffcH
	mov	DWORD PTR _height_fourline_groups$[ebp], eax

; 58   :   const int32_t height_residual_lines  = height &  3;

	mov	eax, DWORD PTR _height$[ebx]
	and	eax, 3
	mov	DWORD PTR _height_residual_lines$[ebp], eax

; 59   : 
; 60   :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	DWORD PTR _y$[ebp], 0
	jmp	SHORT $LN4@reg_sad_w4
$LN2@reg_sad_w4:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 4
	mov	DWORD PTR _y$[ebp], eax
$LN4@reg_sad_w4:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height_fourline_groups$[ebp]
	jge	$LN3@reg_sad_w4

; 61   :     __m128i a = _mm_cvtsi32_si128(*(uint32_t *)(data1 + y * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	mov	ecx, DWORD PTR _data1$[ebx]
	vmovd	xmm0, DWORD PTR [ecx+eax]
	vmovdqa	XMMWORD PTR _a$6[ebp], xmm0

; 62   :     __m128i b = _mm_cvtsi32_si128(*(uint32_t *)(data2 + y * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	mov	ecx, DWORD PTR _data2$[ebx]
	vmovd	xmm0, DWORD PTR [ecx+eax]
	vmovdqa	XMMWORD PTR _b$5[ebp], xmm0

; 63   : 
; 64   :     a = _mm_insert_epi32(a, *(const uint32_t *)(data1 + (y + 1) * stride1), 1);

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride1$[ebx]
	mov	ecx, DWORD PTR _data1$[ebx]
	vmovdqa	xmm0, XMMWORD PTR _a$6[ebp]
	vpinsrd	xmm0, xmm0, DWORD PTR [ecx+eax], 1
	vmovdqa	XMMWORD PTR _a$6[ebp], xmm0

; 65   :     b = _mm_insert_epi32(b, *(const uint32_t *)(data2 + (y + 1) * stride2), 1);

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	imul	eax, DWORD PTR _stride2$[ebx]
	mov	ecx, DWORD PTR _data2$[ebx]
	vmovdqa	xmm0, XMMWORD PTR _b$5[ebp]
	vpinsrd	xmm0, xmm0, DWORD PTR [ecx+eax], 1
	vmovdqa	XMMWORD PTR _b$5[ebp], xmm0

; 66   :     a = _mm_insert_epi32(a, *(const uint32_t *)(data1 + (y + 2) * stride1), 2);

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _stride1$[ebx]
	mov	ecx, DWORD PTR _data1$[ebx]
	vmovdqa	xmm0, XMMWORD PTR _a$6[ebp]
	vpinsrd	xmm0, xmm0, DWORD PTR [ecx+eax], 2
	vmovdqa	XMMWORD PTR _a$6[ebp], xmm0

; 67   :     b = _mm_insert_epi32(b, *(const uint32_t *)(data2 + (y + 2) * stride2), 2);

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 2
	imul	eax, DWORD PTR _stride2$[ebx]
	mov	ecx, DWORD PTR _data2$[ebx]
	vmovdqa	xmm0, XMMWORD PTR _b$5[ebp]
	vpinsrd	xmm0, xmm0, DWORD PTR [ecx+eax], 2
	vmovdqa	XMMWORD PTR _b$5[ebp], xmm0

; 68   :     a = _mm_insert_epi32(a, *(const uint32_t *)(data1 + (y + 3) * stride1), 3);

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _stride1$[ebx]
	mov	ecx, DWORD PTR _data1$[ebx]
	vmovdqa	xmm0, XMMWORD PTR _a$6[ebp]
	vpinsrd	xmm0, xmm0, DWORD PTR [ecx+eax], 3
	vmovdqa	XMMWORD PTR _a$6[ebp], xmm0

; 69   :     b = _mm_insert_epi32(b, *(const uint32_t *)(data2 + (y + 3) * stride2), 3);

	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 3
	imul	eax, DWORD PTR _stride2$[ebx]
	mov	ecx, DWORD PTR _data2$[ebx]
	vmovdqa	xmm0, XMMWORD PTR _b$5[ebp]
	vpinsrd	xmm0, xmm0, DWORD PTR [ecx+eax], 3
	vmovdqa	XMMWORD PTR _b$5[ebp], xmm0

; 70   : 
; 71   :     __m128i curr_sads = _mm_sad_epu8(a, b);

	vmovdqa	xmm0, XMMWORD PTR _a$6[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b$5[ebp]
	vmovdqa	XMMWORD PTR _curr_sads$4[ebp], xmm0

; 72   :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads$4[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 73   :   }

	jmp	$LN2@reg_sad_w4
$LN3@reg_sad_w4:

; 74   :   if (height_residual_lines) {

	cmp	DWORD PTR _height_residual_lines$[ebp], 0
	je	SHORT $LN8@reg_sad_w4

; 75   :     for (; y < height; y++) {

	jmp	SHORT $LN7@reg_sad_w4
$LN5@reg_sad_w4:
	mov	eax, DWORD PTR _y$[ebp]
	add	eax, 1
	mov	DWORD PTR _y$[ebp], eax
$LN7@reg_sad_w4:
	mov	eax, DWORD PTR _y$[ebp]
	cmp	eax, DWORD PTR _height$[ebx]
	jge	SHORT $LN8@reg_sad_w4

; 76   :       __m128i a = _mm_cvtsi32_si128(*(const uint32_t *)(data1 + y * stride1));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride1$[ebx]
	mov	ecx, DWORD PTR _data1$[ebx]
	vmovd	xmm0, DWORD PTR [ecx+eax]
	vmovdqa	XMMWORD PTR _a$3[ebp], xmm0

; 77   :       __m128i b = _mm_cvtsi32_si128(*(const uint32_t *)(data2 + y * stride2));

	mov	eax, DWORD PTR _y$[ebp]
	imul	eax, DWORD PTR _stride2$[ebx]
	mov	ecx, DWORD PTR _data2$[ebx]
	vmovd	xmm0, DWORD PTR [ecx+eax]
	vmovdqa	XMMWORD PTR _b$2[ebp], xmm0

; 78   : 
; 79   :       __m128i curr_sads = _mm_sad_epu8(a, b);

	vmovdqa	xmm0, XMMWORD PTR _a$3[ebp]
	vpsadbw	xmm0, xmm0, XMMWORD PTR _b$2[ebp]
	vmovdqa	XMMWORD PTR _curr_sads$1[ebp], xmm0

; 80   :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _curr_sads$1[ebp]
	vmovdqa	XMMWORD PTR _sse_inc$[ebp], xmm0

; 81   :     }

	jmp	SHORT $LN5@reg_sad_w4
$LN8@reg_sad_w4:

; 82   :   }
; 83   :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, XMMWORD PTR _sse_inc$[ebp], 78	; 0000004eH
	vmovdqa	XMMWORD PTR _sse_inc_2$[ebp], xmm0

; 84   :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vmovdqa	xmm0, XMMWORD PTR _sse_inc$[ebp]
	vpaddq	xmm0, xmm0, XMMWORD PTR _sse_inc_2$[ebp]
	vmovdqa	XMMWORD PTR _sad$[ebp], xmm0

; 85   : 
; 86   :   return _mm_cvtsi128_si32(sad);

	vmovdqa	xmm0, XMMWORD PTR _sad$[ebp]
	vmovd	eax, xmm0

; 87   : }

	pop	edi
	pop	esi
	mov	esp, ebp
	pop	ebp
	mov	esp, ebx
	pop	ebx
	ret	0
_reg_sad_w4 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
;	COMDAT _reg_sad_w0
_TEXT	SEGMENT
_data1$ = 8						; size = 4
_data2$ = 12						; size = 4
_height$ = 16						; size = 4
_stride1$ = 20						; size = 4
_stride2$ = 24						; size = 4
_reg_sad_w0 PROC					; COMDAT

; 46   : {

	push	ebp
	mov	ebp, esp
	sub	esp, 192				; 000000c0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-192]
	mov	ecx, 48					; 00000030H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __9CB4F737_reg_sad_pow2_widths-sse41@h
	call	@__CheckForDebuggerJustMyCode@4

; 47   :   return 0;

	xor	eax, eax

; 48   : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 192				; 000000c0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_reg_sad_w0 ENDP
_TEXT	ENDS
; Function compile flags: /Odtp /RTCsu /ZI
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
;	COMDAT _kvz_strategy_register_picture_avx2
_TEXT	SEGMENT
tv349 = -208						; size = 4
tv338 = -208						; size = 4
tv327 = -208						; size = 4
tv316 = -208						; size = 4
tv305 = -208						; size = 4
tv294 = -208						; size = 4
tv283 = -208						; size = 4
tv272 = -208						; size = 4
tv261 = -208						; size = 4
tv250 = -208						; size = 4
tv239 = -208						; size = 4
tv228 = -208						; size = 4
tv217 = -208						; size = 4
tv206 = -208						; size = 4
tv195 = -208						; size = 4
tv184 = -208						; size = 4
tv173 = -208						; size = 4
tv162 = -208						; size = 4
tv151 = -208						; size = 4
tv140 = -208						; size = 4
tv129 = -208						; size = 4
tv86 = -208						; size = 4
tv75 = -208						; size = 4
_success$ = -5						; size = 1
_opaque$ = 8						; size = 4
_bitdepth$ = 12						; size = 1
_kvz_strategy_register_picture_avx2 PROC		; COMDAT

; 1716 : {

	push	ebp
	mov	ebp, esp
	sub	esp, 208				; 000000d0H
	push	ebx
	push	esi
	push	edi
	lea	edi, DWORD PTR [ebp-208]
	mov	ecx, 52					; 00000034H
	mov	eax, -858993460				; ccccccccH
	rep stosd
	mov	ecx, OFFSET __5DE43F84_picture-avx2@c
	call	@__CheckForDebuggerJustMyCode@4

; 1717 :   bool success = true;

	mov	BYTE PTR _success$[ebp], 1

; 1718 : #if COMPILE_INTEL_AVX2
; 1719 : #if KVZ_BIT_DEPTH == 8
; 1720 :   // We don't actually use SAD for intra right now, other than 4x4 for
; 1721 :   // transform skip, but we might again one day and this is some of the
; 1722 :   // simplest code to look at for anyone interested in doing more
; 1723 :   // optimizations, so it's worth it to keep this maintained.
; 1724 :   if (bitdepth == 8){

	movzx	eax, BYTE PTR _bitdepth$[ebp]
	cmp	eax, 8
	jne	$LN2@kvz_strate

; 1725 : 
; 1726 :     success &= kvz_strategyselector_register(opaque, "reg_sad", "avx2", 40, &kvz_reg_sad_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _kvz_reg_sad_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_07DDJKLCAH@reg_sad@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN4@kvz_strate
	mov	DWORD PTR tv75[ebp], 0
	jmp	SHORT $LN5@kvz_strate
$LN4@kvz_strate:
	mov	DWORD PTR tv75[ebp], 1
$LN5@kvz_strate:
	mov	cl, BYTE PTR tv75[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1727 :     success &= kvz_strategyselector_register(opaque, "sad_8x8", "avx2", 40, &sad_8bit_8x8_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _sad_8bit_8x8_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_07MEAHLBLP@sad_8x8@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN6@kvz_strate
	mov	DWORD PTR tv86[ebp], 0
	jmp	SHORT $LN7@kvz_strate
$LN6@kvz_strate:
	mov	DWORD PTR tv86[ebp], 1
$LN7@kvz_strate:
	mov	cl, BYTE PTR tv86[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1728 :     success &= kvz_strategyselector_register(opaque, "sad_16x16", "avx2", 40, &sad_8bit_16x16_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _sad_8bit_16x16_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_09CIFGAEGF@sad_16x16@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN8@kvz_strate
	mov	DWORD PTR tv129[ebp], 0
	jmp	SHORT $LN9@kvz_strate
$LN8@kvz_strate:
	mov	DWORD PTR tv129[ebp], 1
$LN9@kvz_strate:
	mov	cl, BYTE PTR tv129[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1729 :     success &= kvz_strategyselector_register(opaque, "sad_32x32", "avx2", 40, &sad_8bit_32x32_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _sad_8bit_32x32_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_09PHPGBCME@sad_32x32@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN10@kvz_strate
	mov	DWORD PTR tv140[ebp], 0
	jmp	SHORT $LN11@kvz_strate
$LN10@kvz_strate:
	mov	DWORD PTR tv140[ebp], 1
$LN11@kvz_strate:
	mov	cl, BYTE PTR tv140[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1730 :     success &= kvz_strategyselector_register(opaque, "sad_64x64", "avx2", 40, &sad_8bit_64x64_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _sad_8bit_64x64_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_09HIOKBDLK@sad_64x64@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN12@kvz_strate
	mov	DWORD PTR tv151[ebp], 0
	jmp	SHORT $LN13@kvz_strate
$LN12@kvz_strate:
	mov	DWORD PTR tv151[ebp], 1
$LN13@kvz_strate:
	mov	cl, BYTE PTR tv151[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1731 : 
; 1732 :     success &= kvz_strategyselector_register(opaque, "satd_4x4", "avx2", 40, &satd_4x4_8bit_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _satd_4x4_8bit_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_08GDBJPAOD@satd_4x4@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN14@kvz_strate
	mov	DWORD PTR tv162[ebp], 0
	jmp	SHORT $LN15@kvz_strate
$LN14@kvz_strate:
	mov	DWORD PTR tv162[ebp], 1
$LN15@kvz_strate:
	mov	cl, BYTE PTR tv162[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1733 :     success &= kvz_strategyselector_register(opaque, "satd_8x8", "avx2", 40, &satd_8x8_8bit_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _satd_8x8_8bit_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_08IFHKAAFH@satd_8x8@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN16@kvz_strate
	mov	DWORD PTR tv173[ebp], 0
	jmp	SHORT $LN17@kvz_strate
$LN16@kvz_strate:
	mov	DWORD PTR tv173[ebp], 1
$LN17@kvz_strate:
	mov	cl, BYTE PTR tv173[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1734 :     success &= kvz_strategyselector_register(opaque, "satd_16x16", "avx2", 40, &satd_16x16_8bit_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _satd_16x16_8bit_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_0L@CJHLPOG@satd_16x16@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN18@kvz_strate
	mov	DWORD PTR tv184[ebp], 0
	jmp	SHORT $LN19@kvz_strate
$LN18@kvz_strate:
	mov	DWORD PTR tv184[ebp], 1
$LN19@kvz_strate:
	mov	cl, BYTE PTR tv184[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1735 :     success &= kvz_strategyselector_register(opaque, "satd_32x32", "avx2", 40, &satd_32x32_8bit_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _satd_32x32_8bit_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_0L@NNDHKJEH@satd_32x32@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN20@kvz_strate
	mov	DWORD PTR tv195[ebp], 0
	jmp	SHORT $LN21@kvz_strate
$LN20@kvz_strate:
	mov	DWORD PTR tv195[ebp], 1
$LN21@kvz_strate:
	mov	cl, BYTE PTR tv195[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1736 :     success &= kvz_strategyselector_register(opaque, "satd_64x64", "avx2", 40, &satd_64x64_8bit_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _satd_64x64_8bit_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_0L@FCCLKIDJ@satd_64x64@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN22@kvz_strate
	mov	DWORD PTR tv206[ebp], 0
	jmp	SHORT $LN23@kvz_strate
$LN22@kvz_strate:
	mov	DWORD PTR tv206[ebp], 1
$LN23@kvz_strate:
	mov	cl, BYTE PTR tv206[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1737 : 
; 1738 :     success &= kvz_strategyselector_register(opaque, "satd_4x4_dual", "avx2", 40, &satd_8bit_4x4_dual_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _satd_8bit_4x4_dual_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_0O@PJODLFIO@satd_4x4_dual@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN24@kvz_strate
	mov	DWORD PTR tv217[ebp], 0
	jmp	SHORT $LN25@kvz_strate
$LN24@kvz_strate:
	mov	DWORD PTR tv217[ebp], 1
$LN25@kvz_strate:
	mov	cl, BYTE PTR tv217[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1739 :     success &= kvz_strategyselector_register(opaque, "satd_8x8_dual", "avx2", 40, &satd_8bit_8x8_dual_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _satd_8bit_8x8_dual_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_0O@NIHHCMOO@satd_8x8_dual@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN26@kvz_strate
	mov	DWORD PTR tv228[ebp], 0
	jmp	SHORT $LN27@kvz_strate
$LN26@kvz_strate:
	mov	DWORD PTR tv228[ebp], 1
$LN27@kvz_strate:
	mov	cl, BYTE PTR tv228[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1740 :     success &= kvz_strategyselector_register(opaque, "satd_16x16_dual", "avx2", 40, &satd_8bit_16x16_dual_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _satd_8bit_16x16_dual_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_0BA@GFDJBOPJ@satd_16x16_dual@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN28@kvz_strate
	mov	DWORD PTR tv239[ebp], 0
	jmp	SHORT $LN29@kvz_strate
$LN28@kvz_strate:
	mov	DWORD PTR tv239[ebp], 1
$LN29@kvz_strate:
	mov	cl, BYTE PTR tv239[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1741 :     success &= kvz_strategyselector_register(opaque, "satd_32x32_dual", "avx2", 40, &satd_8bit_32x32_dual_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _satd_8bit_32x32_dual_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_0BA@ICNECECM@satd_32x32_dual@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN30@kvz_strate
	mov	DWORD PTR tv250[ebp], 0
	jmp	SHORT $LN31@kvz_strate
$LN30@kvz_strate:
	mov	DWORD PTR tv250[ebp], 1
$LN31@kvz_strate:
	mov	cl, BYTE PTR tv250[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1742 :     success &= kvz_strategyselector_register(opaque, "satd_64x64_dual", "avx2", 40, &satd_8bit_64x64_dual_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _satd_8bit_64x64_dual_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_0BA@NFKNMDHH@satd_64x64_dual@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN32@kvz_strate
	mov	DWORD PTR tv261[ebp], 0
	jmp	SHORT $LN33@kvz_strate
$LN32@kvz_strate:
	mov	DWORD PTR tv261[ebp], 1
$LN33@kvz_strate:
	mov	cl, BYTE PTR tv261[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1743 :     success &= kvz_strategyselector_register(opaque, "satd_any_size", "avx2", 40, &satd_any_size_8bit_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _satd_any_size_8bit_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_0O@FHJAMJMP@satd_any_size@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN34@kvz_strate
	mov	DWORD PTR tv272[ebp], 0
	jmp	SHORT $LN35@kvz_strate
$LN34@kvz_strate:
	mov	DWORD PTR tv272[ebp], 1
$LN35@kvz_strate:
	mov	cl, BYTE PTR tv272[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1744 :     success &= kvz_strategyselector_register(opaque, "satd_any_size_quad", "avx2", 40, &satd_any_size_quad_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _satd_any_size_quad_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_0BD@EFOGIBKC@satd_any_size_quad@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN36@kvz_strate
	mov	DWORD PTR tv283[ebp], 0
	jmp	SHORT $LN37@kvz_strate
$LN36@kvz_strate:
	mov	DWORD PTR tv283[ebp], 1
$LN37@kvz_strate:
	mov	cl, BYTE PTR tv283[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1745 : 
; 1746 :     success &= kvz_strategyselector_register(opaque, "pixels_calc_ssd", "avx2", 40, &pixels_calc_ssd_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _pixels_calc_ssd_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_0BA@BPDFDAFM@pixels_calc_ssd@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN38@kvz_strate
	mov	DWORD PTR tv294[ebp], 0
	jmp	SHORT $LN39@kvz_strate
$LN38@kvz_strate:
	mov	DWORD PTR tv294[ebp], 1
$LN39@kvz_strate:
	mov	cl, BYTE PTR tv294[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1747 :     success &= kvz_strategyselector_register(opaque, "bipred_average", "avx2", 40, &bipred_average_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _bipred_average_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_0P@FIKELBGI@bipred_average@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN40@kvz_strate
	mov	DWORD PTR tv305[ebp], 0
	jmp	SHORT $LN41@kvz_strate
$LN40@kvz_strate:
	mov	DWORD PTR tv305[ebp], 1
$LN41@kvz_strate:
	mov	cl, BYTE PTR tv305[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1748 :     success &= kvz_strategyselector_register(opaque, "get_optimized_sad", "avx2", 40, &get_optimized_sad_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _get_optimized_sad_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_0BC@IMILIHON@get_optimized_sad@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN42@kvz_strate
	mov	DWORD PTR tv316[ebp], 0
	jmp	SHORT $LN43@kvz_strate
$LN42@kvz_strate:
	mov	DWORD PTR tv316[ebp], 1
$LN43@kvz_strate:
	mov	cl, BYTE PTR tv316[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1749 :     success &= kvz_strategyselector_register(opaque, "ver_sad", "avx2", 40, &ver_sad_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _ver_sad_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_07OEMLCPNF@ver_sad@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN44@kvz_strate
	mov	DWORD PTR tv327[ebp], 0
	jmp	SHORT $LN45@kvz_strate
$LN44@kvz_strate:
	mov	DWORD PTR tv327[ebp], 1
$LN45@kvz_strate:
	mov	cl, BYTE PTR tv327[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1750 :     success &= kvz_strategyselector_register(opaque, "hor_sad", "avx2", 40, &hor_sad_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _hor_sad_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_07CJIKFDIC@hor_sad@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN46@kvz_strate
	mov	DWORD PTR tv338[ebp], 0
	jmp	SHORT $LN47@kvz_strate
$LN46@kvz_strate:
	mov	DWORD PTR tv338[ebp], 1
$LN47@kvz_strate:
	mov	cl, BYTE PTR tv338[ebp]
	mov	BYTE PTR _success$[ebp], cl

; 1751 : 
; 1752 :     success &= kvz_strategyselector_register(opaque, "pixel_var", "avx2", 40, &pixel_var_avx2);

	movzx	esi, BYTE PTR _success$[ebp]
	push	OFFSET _pixel_var_avx2
	push	40					; 00000028H
	push	OFFSET ??_C@_04GEEJMEMG@avx2@
	push	OFFSET ??_C@_09GDPKBOJB@pixel_var@
	mov	eax, DWORD PTR _opaque$[ebp]
	push	eax
	call	_kvz_strategyselector_register
	add	esp, 20					; 00000014H
	and	esi, eax
	jne	SHORT $LN48@kvz_strate
	mov	DWORD PTR tv349[ebp], 0
	jmp	SHORT $LN49@kvz_strate
$LN48@kvz_strate:
	mov	DWORD PTR tv349[ebp], 1
$LN49@kvz_strate:
	mov	cl, BYTE PTR tv349[ebp]
	mov	BYTE PTR _success$[ebp], cl
$LN2@kvz_strate:

; 1753 : 
; 1754 :   }
; 1755 : #endif // KVZ_BIT_DEPTH == 8
; 1756 : #endif
; 1757 :   return success;

	movzx	eax, BYTE PTR _success$[ebp]

; 1758 : }

	pop	edi
	pop	esi
	pop	ebx
	add	esp, 208				; 000000d0H
	cmp	ebp, esp
	call	__RTC_CheckEsp
	mov	esp, ebp
	pop	ebp
	ret	0
_kvz_strategy_register_picture_avx2 ENDP
_TEXT	ENDS
END
