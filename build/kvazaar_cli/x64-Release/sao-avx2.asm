; Listing generated by Microsoft (R) Optimizing Compiler Version 19.26.28806.0 

include listing.inc

INCLUDELIB OLDNAMES

?sao_eo_idx_to_eo_category@?1??sao_calc_eo_cat@@9@9 DD 01H ; `sao_calc_eo_cat'::`2'::sao_eo_idx_to_eo_category
	DD	02H
	DD	00H
	DD	03H
	DD	04H
cbf_masks DW	01fH
	DW	0fH
	DW	07H
	DW	03H
	DW	01H
	ORG $+6
g_sig_last_scan_16x16 DD 00H
	DD	04H
	DD	01H
	DD	08H
	DD	05H
	DD	02H
	DD	0cH
	DD	09H
	DD	06H
	DD	03H
	DD	0dH
	DD	0aH
	DD	07H
	DD	0eH
	DD	0bH
	DD	0fH
g_group_idx DB	00H
	DB	01H
	DB	02H
	DB	03H
	DB	04H
	DB	04H
	DB	05H
	DB	05H
	DB	06H
	DB	06H
	DB	06H
	DB	06H
	DB	07H
	DB	07H
	DB	07H
	DB	07H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
strategies_to_select DQ FLAT:$SG4294948137
	DQ	FLAT:kvz_array_checksum
	DQ	FLAT:$SG4294948136
	DQ	FLAT:kvz_array_md5
	DQ	FLAT:$SG4294948135
	DQ	FLAT:kvz_reg_sad
	DQ	FLAT:$SG4294948134
	DQ	FLAT:kvz_sad_4x4
	DQ	FLAT:$SG4294948133
	DQ	FLAT:kvz_sad_8x8
	DQ	FLAT:$SG4294948132
	DQ	FLAT:kvz_sad_16x16
	DQ	FLAT:$SG4294948131
	DQ	FLAT:kvz_sad_32x32
	DQ	FLAT:$SG4294948130
	DQ	FLAT:kvz_sad_64x64
	DQ	FLAT:$SG4294948129
	DQ	FLAT:kvz_satd_4x4
	DQ	FLAT:$SG4294948128
	DQ	FLAT:kvz_satd_8x8
	DQ	FLAT:$SG4294948127
	DQ	FLAT:kvz_satd_16x16
	DQ	FLAT:$SG4294948126
	DQ	FLAT:kvz_satd_32x32
	DQ	FLAT:$SG4294948125
	DQ	FLAT:kvz_satd_64x64
	DQ	FLAT:$SG4294948124
	DQ	FLAT:kvz_satd_any_size
	DQ	FLAT:$SG4294948123
	DQ	FLAT:kvz_sad_4x4_dual
	DQ	FLAT:$SG4294948122
	DQ	FLAT:kvz_sad_8x8_dual
	DQ	FLAT:$SG4294948121
	DQ	FLAT:kvz_sad_16x16_dual
	DQ	FLAT:$SG4294948120
	DQ	FLAT:kvz_sad_32x32_dual
	DQ	FLAT:$SG4294948119
	DQ	FLAT:kvz_sad_64x64_dual
	DQ	FLAT:$SG4294948118
	DQ	FLAT:kvz_satd_4x4_dual
	DQ	FLAT:$SG4294948117
	DQ	FLAT:kvz_satd_8x8_dual
	DQ	FLAT:$SG4294948116
	DQ	FLAT:kvz_satd_16x16_dual
	DQ	FLAT:$SG4294948115
	DQ	FLAT:kvz_satd_32x32_dual
	DQ	FLAT:$SG4294948114
	DQ	FLAT:kvz_satd_64x64_dual
	DQ	FLAT:$SG4294948113
	DQ	FLAT:kvz_satd_any_size_quad
	DQ	FLAT:$SG4294948112
	DQ	FLAT:kvz_pixels_calc_ssd
	DQ	FLAT:$SG4294948111
	DQ	FLAT:kvz_bipred_average
	DQ	FLAT:$SG4294948110
	DQ	FLAT:kvz_get_optimized_sad
	DQ	FLAT:$SG4294948109
	DQ	FLAT:kvz_ver_sad
	DQ	FLAT:$SG4294948108
	DQ	FLAT:kvz_hor_sad
	DQ	FLAT:$SG4294948107
	DQ	FLAT:kvz_pixel_var
	DQ	FLAT:$SG4294948106
	DQ	FLAT:kvz_fast_forward_dst_4x4
	DQ	FLAT:$SG4294948105
	DQ	FLAT:kvz_dct_4x4
	DQ	FLAT:$SG4294948104
	DQ	FLAT:kvz_dct_8x8
	DQ	FLAT:$SG4294948103
	DQ	FLAT:kvz_dct_16x16
	DQ	FLAT:$SG4294948102
	DQ	FLAT:kvz_dct_32x32
	DQ	FLAT:$SG4294948101
	DQ	FLAT:kvz_fast_inverse_dst_4x4
	DQ	FLAT:$SG4294948100
	DQ	FLAT:kvz_idct_4x4
	DQ	FLAT:$SG4294948099
	DQ	FLAT:kvz_idct_8x8
	DQ	FLAT:$SG4294948098
	DQ	FLAT:kvz_idct_16x16
	DQ	FLAT:$SG4294948097
	DQ	FLAT:kvz_idct_32x32
	DQ	FLAT:$SG4294948096
	DQ	FLAT:kvz_filter_hpel_blocks_hor_ver_luma
	DQ	FLAT:$SG4294948095
	DQ	FLAT:kvz_filter_hpel_blocks_diag_luma
	DQ	FLAT:$SG4294948094
	DQ	FLAT:kvz_filter_qpel_blocks_hor_ver_luma
	DQ	FLAT:$SG4294948093
	DQ	FLAT:kvz_filter_qpel_blocks_diag_luma
	DQ	FLAT:$SG4294948092
	DQ	FLAT:kvz_sample_quarterpel_luma
	DQ	FLAT:$SG4294948091
	DQ	FLAT:kvz_sample_octpel_chroma
	DQ	FLAT:$SG4294948090
	DQ	FLAT:kvz_sample_quarterpel_luma_hi
	DQ	FLAT:$SG4294948089
	DQ	FLAT:kvz_sample_octpel_chroma_hi
	DQ	FLAT:$SG4294948088
	DQ	FLAT:kvz_get_extended_block
	DQ	FLAT:$SG4294948087
	DQ	FLAT:kvz_quant
	DQ	FLAT:$SG4294948086
	DQ	FLAT:kvz_quantize_residual
	DQ	FLAT:$SG4294948085
	DQ	FLAT:kvz_dequant
	DQ	FLAT:$SG4294948084
	DQ	FLAT:kvz_coeff_abs_sum
	DQ	FLAT:$SG4294948083
	DQ	FLAT:kvz_fast_coeff_cost
	DQ	FLAT:$SG4294948082
	DQ	FLAT:kvz_angular_pred
	DQ	FLAT:$SG4294948081
	DQ	FLAT:kvz_intra_pred_planar
	DQ	FLAT:$SG4294948080
	DQ	FLAT:kvz_intra_pred_filtered_dc
	DQ	FLAT:$SG4294948079
	DQ	FLAT:kvz_sao_edge_ddistortion
	DQ	FLAT:$SG4294948078
	DQ	FLAT:kvz_calc_sao_edge_dir
	DQ	FLAT:$SG4294948077
	DQ	FLAT:kvz_sao_reconstruct_color
	DQ	FLAT:$SG4294948076
	DQ	FLAT:kvz_sao_band_ddistortion
	DQ	FLAT:$SG4294948075
	DQ	FLAT:kvz_encode_coeff_nxn
	DQ	0000000000000000H
	DQ	0000000000000000H
g_sig_last_scan_32x32 DD 00H
	DD	08H
	DD	01H
	DD	010H
	DD	09H
	DD	02H
	DD	018H
	DD	011H
	DD	0aH
	DD	03H
	DD	020H
	DD	019H
	DD	012H
	DD	0bH
	DD	04H
	DD	028H
	DD	021H
	DD	01aH
	DD	013H
	DD	0cH
	DD	05H
	DD	030H
	DD	029H
	DD	022H
	DD	01bH
	DD	014H
	DD	0dH
	DD	06H
	DD	038H
	DD	031H
	DD	02aH
	DD	023H
	DD	01cH
	DD	015H
	DD	0eH
	DD	07H
	DD	039H
	DD	032H
	DD	02bH
	DD	024H
	DD	01dH
	DD	016H
	DD	0fH
	DD	03aH
	DD	033H
	DD	02cH
	DD	025H
	DD	01eH
	DD	017H
	DD	03bH
	DD	034H
	DD	02dH
	DD	026H
	DD	01fH
	DD	03cH
	DD	035H
	DD	02eH
	DD	027H
	DD	03dH
	DD	036H
	DD	02fH
	DD	03eH
	DD	037H
	DD	03fH
g_min_in_group DB 00H
	DB	01H
	DB	02H
	DB	03H
	DB	04H
	DB	06H
	DB	08H
	DB	0cH
	DB	010H
	DB	018H
	ORG $+6
default_fast_coeff_cost_wts DD 03e282e88r	; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e26c094r			; 0.162844
	DD	04081ca43r			; 4.05594
	DD	04064203ar			; 3.56447
	DD	040db915ar			; 6.86149
	DD	03e03d189r			; 0.128729
	DD	04089fbafr			; 4.31197
	DD	0407c5771r			; 3.94284
	DD	040ddeed2r			; 6.9354
	DD	03de33ce6r			; 0.110956
	DD	0408ddcb1r			; 4.43319
	DD	0407c8738r			; 3.94575
	DD	040dc1618r			; 6.8777
	DD	03dc29cfer			; 0.095026
	DD	0408f7938r			; 4.48355
	DD	0408636aar			; 4.19417
	DD	040d90260r			; 6.78154
	DD	03d99b1b8r			; 0.075046
	DD	04094474cr			; 4.6337
	DD	04082b1b6r			; 4.08419
	DD	040d65aeer			; 6.6986
	DD	03d56bca5r			; 0.052426
	DD	0409ef37er			; 4.96722
	DD	04080dee8r			; 4.02721
	DD	040d19306r			; 6.5492
	DD	03d24bcaer			; 0.040219
	DD	040a489car			; 5.14182
	DD	0407ee3bdr			; 3.98265
	DD	040cec513r			; 6.46156
	DD	03d0fba88r			; 0.03509
	DD	040a628e7r			; 5.19249
	DD	040752e49r			; 3.83095
	DD	040cd642ar			; 6.41848
	DD	03cf47d80r			; 0.029845
	DD	040a6c5d0r			; 5.21165
	DD	040743073r			; 3.81546
	DD	040cb0dd8r			; 6.34544
	DD	03cc0b136r			; 0.023522
	DD	040aa4f92r			; 5.32221
	DD	040744224r			; 3.81654
	DD	040cb8aaar			; 6.36068
	DD	03cae87d3r			; 0.021305
	DD	040a73ac3r			; 5.22592
	DD	04075eeccr			; 3.8427
	DD	040ca6cd9r			; 6.32579
	DD	03c821294r			; 0.015878
	DD	040a5dbe0r			; 5.18309
	DD	0407d2f27r			; 3.956
	DD	040ca8cbdr			; 6.32968
	DD	03c2ae297r			; 0.01043
	DD	040a32ce4r			; 5.09923
	DD	04085a85fr			; 4.1768
	DD	040c9c5d6r			; 6.3054
	DD	03c0a2a91r			; 0.008433
	DD	040a0f7der			; 5.03026
	DD	040879a50r			; 4.23759
	DD	040c8a4eer			; 6.27013
	DD	03bd4fdf4r			; 0.0065
	DD	0409f0412r			; 4.96925
	DD	0408adc57r			; 4.3394
	DD	040c6f870r			; 6.21783
	DD	03ba18373r			; 0.004929
	DD	0409d8d50r			; 4.9235
	DD	0408e283fr			; 4.44241
	DD	040c5df6cr			; 6.18352
	DD	03b73775cr			; 0.003715
	DD	0409d4c75r			; 4.91558
	DD	0408dbb1br			; 4.42909
	DD	040c4029fr			; 6.12532
	DD	03b4a70d2r			; 0.003089
	DD	0409c48f7r			; 4.88391
	DD	040920260r			; 4.56279
	DD	040c5019dr			; 6.15645
	DD	03b219c9dr			; 0.002466
	DD	0409c31abr			; 4.88106
	DD	040942800r			; 4.62988
	DD	040c49088r			; 6.14264
	DD	03b0e25c8r			; 0.002169
	DD	0409c3d62r			; 4.88249
	DD	04094ae99r			; 4.64631
	DD	040c415d1r			; 6.12766
	DD	03b26dacbr			; 0.002546
	DD	040996304r			; 4.79334
	DD	0409acc16r			; 4.83741
	DD	040c6606br			; 6.19927
	DD	03aac3a86r			; 0.001314
	DD	04099e220r			; 4.80885
	DD	0409a81bdr			; 4.82834
	DD	040c7ca3cr			; 6.24344
	DD	03a9741d1r			; 0.001154
	DD	0409b9a72r			; 4.8626
	DD	0409b19aar			; 4.84688
	DD	040c693a5r			; 6.20552
	DD	03a80f990r			; 0.000984
	DD	0409bb993r			; 4.8664
	DD	0409b7fa2r			; 4.85933
	DD	040c7b565r			; 6.24089
	DD	03a551f82r			; 0.000813
	DD	0409b698ar			; 4.85663
	DD	0409d95bar			; 4.92453
	DD	040c963a4r			; 6.29341
	DD	03a91c087r			; 0.001112
	DD	04099419er			; 4.78926
	DD	040a050f0r			; 5.00988
	DD	040cddf8fr			; 6.43354
	DD	03a10b418r			; 0.000552
	DD	04098580ar			; 4.76075
	DD	040a2e4f1r			; 5.09045
	DD	040d32e1fr			; 6.59938
	DD	039ccff22r			; 0.000391
	DD	0409ec42cr			; 4.96145
	DD	040a38d95r			; 5.11103
	DD	040d8342fr			; 6.75637
	DD	039ae1049r			; 0.000332
	DD	0409f63f8r			; 4.98095
	DD	040a46b89r			; 5.13813
	DD	040dbc1e8r			; 6.86742
	DD	03952c387r			; 0.000201
	DD	040a5d297r			; 5.18196
	DD	04097af64r			; 4.74016
	DD	040cec07dr			; 6.461
	DD	0397ba882r			; 0.00024
	DD	040a5eeb7r			; 5.18539
	DD	0409bfeb0r			; 4.87484
	DD	040da3603r			; 6.81909
	DD	03908509cr			; 0.00013
	DD	040a8a6b5r			; 5.27035
	DD	040977eacr			; 4.73421
	DD	040da708fr			; 6.82624
	DD	038da1a93r			; 0.000104
	DD	040abe6e8r			; 5.37194
	DD	040930af4r			; 4.59509
	DD	040d5189ar			; 6.65925
	DD	038ae1049r			; 8.3e-05
	DD	040ab9581r			; 5.362
	DD	04093c250r			; 4.61747
	DD	040dacf03r			; 6.83777
	DD	03890b418r			; 6.9e-05
	DD	040a926e3r			; 5.286
	DD	0409828e7r			; 4.75499
	DD	040e516e1r			; 7.15904
	DD	0384d8559r			; 4.9e-05
	DD	040afa18cr			; 5.48847
	DD	0408cace9r			; 4.39611
	DD	040d74682r			; 6.72736
	DD	038734507r			; 5.8e-05
	DD	0409eafa3r			; 4.95894
	DD	040929321r			; 4.58046
	DD	040cf49a5r			; 6.47774
	DD	037eae18br			; 2.8e-05
	DD	040b0ae1br			; 5.52125
	DD	0408e1885r			; 4.44049
	DD	040e68f80r			; 7.20502
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	0379f6230r			; 1.9e-05
	DD	040b9f5d8r			; 5.81126
	DD	0408cc582r			; 4.39911
	DD	040eac30dr			; 7.33631
g_sao_edge_offsets DD 0ffffffffH
	DD	00H
	DD	01H
	DD	00H
	DD	00H
	DD	0ffffffffH
	DD	00H
	DD	01H
	DD	0ffffffffH
	DD	0ffffffffH
	DD	01H
	DD	01H
	DD	01H
	DD	0ffffffffH
	DD	0ffffffffH
	DD	01H
g_sig_last_scan_cg DQ FLAT:g_sig_last_scan_8x8
	DQ	FLAT:g_sig_last_scan_8x8+16
	DQ	FLAT:g_sig_last_scan_8x8+32
	DQ	FLAT:g_sig_last_scan_8x8
	DQ	FLAT:g_sig_last_scan_8x8+16
	DQ	FLAT:g_sig_last_scan_8x8+32
	DQ	FLAT:g_sig_last_scan_16x16
	DQ	0000000000000000H
	DQ	0000000000000000H
	DQ	FLAT:g_sig_last_scan_32x32
	DQ	0000000000000000H
	DQ	0000000000000000H
g_sig_last_scan_8x8 DD 00H
	DD	02H
	DD	01H
	DD	03H
	DD	00H
	DD	01H
	DD	02H
	DD	03H
	DD	00H
	DD	02H
	DD	01H
	DD	03H
	ORG $+3
$SG4294948087 DB 'quant', 00H
	ORG $+7
$SG4294948137 DB 'array_checksum', 00H
	ORG $+1
$SG4294948136 DB 'array_md5', 00H
	ORG $+6
$SG4294948135 DB 'reg_sad', 00H
$SG4294948134 DB 'sad_4x4', 00H
$SG4294948133 DB 'sad_8x8', 00H
$SG4294948132 DB 'sad_16x16', 00H
	ORG $+6
$SG4294948131 DB 'sad_32x32', 00H
	ORG $+6
$SG4294948130 DB 'sad_64x64', 00H
	ORG $+6
$SG4294948129 DB 'satd_4x4', 00H
	ORG $+7
$SG4294948128 DB 'satd_8x8', 00H
	ORG $+7
$SG4294948127 DB 'satd_16x16', 00H
	ORG $+5
$SG4294948126 DB 'satd_32x32', 00H
	ORG $+5
$SG4294948125 DB 'satd_64x64', 00H
	ORG $+5
$SG4294948124 DB 'satd_any_size', 00H
	ORG $+2
$SG4294948123 DB 'sad_4x4_dual', 00H
	ORG $+3
$SG4294948122 DB 'sad_8x8_dual', 00H
	ORG $+3
$SG4294948121 DB 'sad_16x16_dual', 00H
	ORG $+1
$SG4294948120 DB 'sad_32x32_dual', 00H
	ORG $+1
$SG4294948119 DB 'sad_64x64_dual', 00H
	ORG $+1
$SG4294948118 DB 'satd_4x4_dual', 00H
	ORG $+2
$SG4294948117 DB 'satd_8x8_dual', 00H
	ORG $+2
$SG4294948116 DB 'satd_16x16_dual', 00H
$SG4294948115 DB 'satd_32x32_dual', 00H
$SG4294948114 DB 'satd_64x64_dual', 00H
$SG4294948113 DB 'satd_any_size_quad', 00H
	ORG $+5
$SG4294948112 DB 'pixels_calc_ssd', 00H
$SG4294948111 DB 'bipred_average', 00H
	ORG $+1
$SG4294948110 DB 'get_optimized_sad', 00H
	ORG $+6
$SG4294948109 DB 'ver_sad', 00H
$SG4294948108 DB 'hor_sad', 00H
$SG4294948107 DB 'pixel_var', 00H
	ORG $+6
$SG4294948106 DB 'fast_forward_dst_4x4', 00H
	ORG $+3
$SG4294948105 DB 'dct_4x4', 00H
$SG4294948104 DB 'dct_8x8', 00H
$SG4294948103 DB 'dct_16x16', 00H
	ORG $+6
$SG4294948102 DB 'dct_32x32', 00H
	ORG $+6
$SG4294948101 DB 'fast_inverse_dst_4x4', 00H
	ORG $+3
$SG4294948100 DB 'idct_4x4', 00H
	ORG $+7
$SG4294948099 DB 'idct_8x8', 00H
	ORG $+7
$SG4294948098 DB 'idct_16x16', 00H
	ORG $+5
$SG4294948097 DB 'idct_32x32', 00H
	ORG $+5
$SG4294948096 DB 'filter_hpel_blocks_hor_ver_luma', 00H
$SG4294948095 DB 'filter_hpel_blocks_diag_luma', 00H
	ORG $+3
$SG4294948094 DB 'filter_qpel_blocks_hor_ver_luma', 00H
$SG4294948093 DB 'filter_qpel_blocks_diag_luma', 00H
	ORG $+3
$SG4294948092 DB 'sample_quarterpel_luma', 00H
	ORG $+1
$SG4294948091 DB 'sample_octpel_chroma', 00H
	ORG $+3
$SG4294948090 DB 'sample_quarterpel_luma_hi', 00H
	ORG $+6
$SG4294948089 DB 'sample_octpel_chroma_hi', 00H
$SG4294948088 DB 'get_extended_block', 00H
	ORG $+5
$SG4294948086 DB 'quantize_residual', 00H
	ORG $+6
$SG4294948085 DB 'dequant', 00H
$SG4294948084 DB 'coeff_abs_sum', 00H
	ORG $+2
$SG4294948083 DB 'fast_coeff_cost', 00H
$SG4294948082 DB 'angular_pred', 00H
	ORG $+3
$SG4294948081 DB 'intra_pred_planar', 00H
	ORG $+6
$SG4294948080 DB 'intra_pred_filtered_dc', 00H
	ORG $+1
$SG4294948079 DB 'sao_edge_ddistortion', 00H
	ORG $+3
$SG4294948078 DB 'calc_sao_edge_dir', 00H
	ORG $+6
$SG4294948077 DB 'sao_reconstruct_color', 00H
	ORG $+2
$SG4294948076 DB 'sao_band_ddistortion', 00H
	ORG $+3
$SG4294948075 DB 'encode_coeff_nxn', 00H
PUBLIC	kvz_strategy_register_sao_avx2
pdata	SEGMENT
$pdata$sao_band_ddistortion_avx2 DD imagerel sao_band_ddistortion_avx2
	DD	imagerel sao_band_ddistortion_avx2+170
	DD	imagerel $unwind$sao_band_ddistortion_avx2
$pdata$2$sao_band_ddistortion_avx2 DD imagerel sao_band_ddistortion_avx2+170
	DD	imagerel sao_band_ddistortion_avx2+214
	DD	imagerel $chain$2$sao_band_ddistortion_avx2
$pdata$7$sao_band_ddistortion_avx2 DD imagerel sao_band_ddistortion_avx2+214
	DD	imagerel sao_band_ddistortion_avx2+552
	DD	imagerel $chain$7$sao_band_ddistortion_avx2
$pdata$8$sao_band_ddistortion_avx2 DD imagerel sao_band_ddistortion_avx2+552
	DD	imagerel sao_band_ddistortion_avx2+608
	DD	imagerel $chain$8$sao_band_ddistortion_avx2
$pdata$9$sao_band_ddistortion_avx2 DD imagerel sao_band_ddistortion_avx2+608
	DD	imagerel sao_band_ddistortion_avx2+611
	DD	imagerel $chain$9$sao_band_ddistortion_avx2
$pdata$10$sao_band_ddistortion_avx2 DD imagerel sao_band_ddistortion_avx2+611
	DD	imagerel sao_band_ddistortion_avx2+650
	DD	imagerel $chain$10$sao_band_ddistortion_avx2
$pdata$13$sao_band_ddistortion_avx2 DD imagerel sao_band_ddistortion_avx2+650
	DD	imagerel sao_band_ddistortion_avx2+860
	DD	imagerel $chain$13$sao_band_ddistortion_avx2
$pdata$14$sao_band_ddistortion_avx2 DD imagerel sao_band_ddistortion_avx2+860
	DD	imagerel sao_band_ddistortion_avx2+870
	DD	imagerel $chain$14$sao_band_ddistortion_avx2
$pdata$15$sao_band_ddistortion_avx2 DD imagerel sao_band_ddistortion_avx2+870
	DD	imagerel sao_band_ddistortion_avx2+898
	DD	imagerel $chain$15$sao_band_ddistortion_avx2
$pdata$sao_reconstruct_color_avx2 DD imagerel sao_reconstruct_color_avx2
	DD	imagerel sao_reconstruct_color_avx2+2210
	DD	imagerel $unwind$sao_reconstruct_color_avx2
$pdata$reconstruct_color_other DD imagerel reconstruct_color_other
	DD	imagerel reconstruct_color_other+280
	DD	imagerel $unwind$reconstruct_color_other
$pdata$6$reconstruct_color_other DD imagerel reconstruct_color_other+280
	DD	imagerel reconstruct_color_other+1355
	DD	imagerel $chain$6$reconstruct_color_other
$pdata$7$reconstruct_color_other DD imagerel reconstruct_color_other+1355
	DD	imagerel reconstruct_color_other+1399
	DD	imagerel $chain$7$reconstruct_color_other
$pdata$do_one_nonband_ymm DD imagerel do_one_nonband_ymm
	DD	imagerel do_one_nonband_ymm+220
	DD	imagerel $unwind$do_one_nonband_ymm
$pdata$reconstruct_color_band DD imagerel reconstruct_color_band
	DD	imagerel reconstruct_color_band+1007
	DD	imagerel $unwind$reconstruct_color_band
$pdata$lookup_color_band_ymm DD imagerel lookup_color_band_ymm
	DD	imagerel lookup_color_band_ymm+273
	DD	imagerel $unwind$lookup_color_band_ymm
$pdata$calc_sao_offset_array_avx2 DD imagerel calc_sao_offset_array_avx2
	DD	imagerel calc_sao_offset_array_avx2+763
	DD	imagerel $unwind$calc_sao_offset_array_avx2
$pdata$calc_sao_edge_dir_avx2 DD imagerel calc_sao_edge_dir_avx2
	DD	imagerel calc_sao_edge_dir_avx2+1704
	DD	imagerel $unwind$calc_sao_edge_dir_avx2
$pdata$calc_edge_dir_one_ymm DD imagerel calc_edge_dir_one_ymm
	DD	imagerel calc_edge_dir_one_ymm+288
	DD	imagerel $unwind$calc_edge_dir_one_ymm
$pdata$sao_edge_ddistortion_avx2 DD imagerel sao_edge_ddistortion_avx2
	DD	imagerel sao_edge_ddistortion_avx2+2011
	DD	imagerel $unwind$sao_edge_ddistortion_avx2
$pdata$do_one_edge_ymm DD imagerel do_one_edge_ymm
	DD	imagerel do_one_edge_ymm+348
	DD	imagerel $unwind$do_one_edge_ymm
$pdata$calc_diff_off_delta DD imagerel calc_diff_off_delta
	DD	imagerel calc_diff_off_delta+176
	DD	imagerel $unwind$calc_diff_off_delta
$pdata$calc_eo_cat DD imagerel calc_eo_cat
	DD	imagerel calc_eo_cat+115
	DD	imagerel $unwind$calc_eo_cat
$pdata$sao_band_ddistortion_generic DD imagerel sao_band_ddistortion_generic
	DD	imagerel sao_band_ddistortion_generic+51
	DD	imagerel $unwind$sao_band_ddistortion_generic
$pdata$5$sao_band_ddistortion_generic DD imagerel sao_band_ddistortion_generic+51
	DD	imagerel sao_band_ddistortion_generic+253
	DD	imagerel $chain$5$sao_band_ddistortion_generic
$pdata$6$sao_band_ddistortion_generic DD imagerel sao_band_ddistortion_generic+253
	DD	imagerel sao_band_ddistortion_generic+263
	DD	imagerel $chain$6$sao_band_ddistortion_generic
$pdata$sao_edge_ddistortion_generic DD imagerel sao_edge_ddistortion_generic
	DD	imagerel sao_edge_ddistortion_generic+107
	DD	imagerel $unwind$sao_edge_ddistortion_generic
$pdata$3$sao_edge_ddistortion_generic DD imagerel sao_edge_ddistortion_generic+107
	DD	imagerel sao_edge_ddistortion_generic+432
	DD	imagerel $chain$3$sao_edge_ddistortion_generic
$pdata$4$sao_edge_ddistortion_generic DD imagerel sao_edge_ddistortion_generic+432
	DD	imagerel sao_edge_ddistortion_generic+445
	DD	imagerel $chain$4$sao_edge_ddistortion_generic
$pdata$kvz_strategy_register_sao_avx2 DD imagerel $LN5
	DD	imagerel $LN5+206
	DD	imagerel $unwind$kvz_strategy_register_sao_avx2
xdata	SEGMENT
$unwind$sao_band_ddistortion_avx2 DD 096d01H
	DD	01f86dH
	DD	02e83dH
	DD	0180116H
	DD	0c00df00fH
	DD	0600bH
$chain$2$sao_band_ddistortion_avx2 DD 061221H
	DD	03d812H
	DD	04c80cH
	DD	079806H
	DD	imagerel sao_band_ddistortion_avx2
	DD	imagerel sao_band_ddistortion_avx2+170
	DD	imagerel $unwind$sao_band_ddistortion_avx2
$chain$7$sao_band_ddistortion_avx2 DD 0a2c21H
	DD	05b82cH
	DD	06a826H
	DD	088820H
	DD	097814H
	DD	0a6809H
	DD	imagerel sao_band_ddistortion_avx2+170
	DD	imagerel sao_band_ddistortion_avx2+214
	DD	imagerel $chain$2$sao_band_ddistortion_avx2
$chain$8$sao_band_ddistortion_avx2 DD 021H
	DD	imagerel sao_band_ddistortion_avx2+170
	DD	imagerel sao_band_ddistortion_avx2+214
	DD	imagerel $chain$2$sao_band_ddistortion_avx2
$chain$9$sao_band_ddistortion_avx2 DD 021H
	DD	imagerel sao_band_ddistortion_avx2
	DD	imagerel sao_band_ddistortion_avx2+170
	DD	imagerel $unwind$sao_band_ddistortion_avx2
$chain$10$sao_band_ddistortion_avx2 DD 020821H
	DD	01c3408H
	DD	imagerel sao_band_ddistortion_avx2+608
	DD	imagerel sao_band_ddistortion_avx2+611
	DD	imagerel $chain$9$sao_band_ddistortion_avx2
$chain$13$sao_band_ddistortion_avx2 DD 061e21H
	DD	016e41eH
	DD	017d413H
	DD	01f7408H
	DD	imagerel sao_band_ddistortion_avx2+611
	DD	imagerel sao_band_ddistortion_avx2+650
	DD	imagerel $chain$10$sao_band_ddistortion_avx2
$chain$14$sao_band_ddistortion_avx2 DD 021H
	DD	imagerel sao_band_ddistortion_avx2+611
	DD	imagerel sao_band_ddistortion_avx2+650
	DD	imagerel $chain$10$sao_band_ddistortion_avx2
$chain$15$sao_band_ddistortion_avx2 DD 021H
	DD	imagerel sao_band_ddistortion_avx2+608
	DD	imagerel sao_band_ddistortion_avx2+611
	DD	imagerel $chain$9$sao_band_ddistortion_avx2
$unwind$sao_reconstruct_color_avx2 DD 01f6101H
	DD	02ae858H
	DD	02bd850H
	DD	02cc848H
	DD	02db840H
	DD	02ea838H
	DD	02f9833H
	DD	030882eH
	DD	0317829H
	DD	0326824H
	DD	06e741fH
	DD	06d641fH
	DD	06c341fH
	DD	066011fH
	DD	0e016f018H
	DD	0c012d014H
	DD	05010H
$unwind$reconstruct_color_other DD 0f4201H
	DD	04e842H
	DD	05d839H
	DD	022641fH
	DD	021341fH
	DD	01a011fH
	DD	0e016f018H
	DD	0c012d014H
	DD	07010H
$chain$6$reconstruct_color_other DD 0e4321H
	DD	06c843H
	DD	07b839H
	DD	08a82fH
	DD	099824H
	DD	0a881bH
	DD	0b7812H
	DD	0c6809H
	DD	imagerel reconstruct_color_other
	DD	imagerel reconstruct_color_other+280
	DD	imagerel $unwind$reconstruct_color_other
$chain$7$reconstruct_color_other DD 021H
	DD	imagerel reconstruct_color_other
	DD	imagerel reconstruct_color_other+280
	DD	imagerel $unwind$reconstruct_color_other
$unwind$do_one_nonband_ymm DD 0b6801H
	DD	0a868H
	DD	01983cH
	DD	02882fH
	DD	03782aH
	DD	046815H
	DD	0a207H
$unwind$reconstruct_color_band DD 0153c01H
	DD	0289833H
	DD	029882eH
	DD	02a7829H
	DD	02b6824H
	DD	060741fH
	DD	05f641fH
	DD	05e341fH
	DD	058011fH
	DD	0e016f018H
	DD	0c012d014H
	DD	05010H
$unwind$lookup_color_band_ymm DD 051e01H
	DD	0781eH
	DD	016816H
	DD	04204H
$unwind$calc_sao_offset_array_avx2 DD 0174901H
	DD	04f840H
	DD	05e83aH
	DD	06d834H
	DD	07c82eH
	DD	08b829H
	DD	09a824H
	DD	0a981fH
	DD	0b881aH
	DD	0c7815H
	DD	0d6810H
	DD	01c010bH
	DD	05004H
$unwind$calc_sao_edge_dir_avx2 DD 01f6e01H
	DD	012f865H
	DD	013e85dH
	DD	014d855H
	DD	015c84dH
	DD	016b845H
	DD	017a83dH
	DD	0189835H
	DD	0198830H
	DD	01a782bH
	DD	01b6826H
	DD	0413421H
	DD	0380121H
	DD	0e018f01aH
	DD	0c014d016H
	DD	060117012H
	DD	05010H
$unwind$calc_edge_dir_one_ymm DD 098801H
	DD	09888H
	DD	018837H
	DD	027821H
	DD	03681bH
	DD	08204H
$unwind$sao_edge_ddistortion_avx2 DD 01f6e01H
	DD	0ef865H
	DD	0fe85dH
	DD	010d855H
	DD	011c84dH
	DD	012b845H
	DD	013a83dH
	DD	0149835H
	DD	0158830H
	DD	016782bH
	DD	0176826H
	DD	0393421H
	DD	0300121H
	DD	0e018f01aH
	DD	0c014d016H
	DD	060117012H
	DD	05010H
$unwind$do_one_edge_ymm DD 0d9101H
	DD	0b891H
	DD	01a859H
	DD	02984aH
	DD	038844H
	DD	04782aH
	DD	056824H
	DD	0c204H
$unwind$calc_diff_off_delta DD 075e01H
	DD	0885eH
	DD	017815H
	DD	02680fH
	DD	06204H
$unwind$calc_eo_cat DD 051c01H
	DD	0781cH
	DD	016817H
	DD	04204H
$unwind$sao_band_ddistortion_generic DD 031201H
	DD	0f00e4212H
	DD	0700cH
$chain$5$sao_band_ddistortion_generic DD 0c2621H
	DD	0e426H
	DD	01d41fH
	DD	02c416H
	DD	036412H
	DD	04540bH
	DD	0b3404H
	DD	imagerel sao_band_ddistortion_generic
	DD	imagerel sao_band_ddistortion_generic+51
	DD	imagerel $unwind$sao_band_ddistortion_generic
$chain$6$sao_band_ddistortion_generic DD 021H
	DD	imagerel sao_band_ddistortion_generic
	DD	imagerel sao_band_ddistortion_generic+51
	DD	imagerel $unwind$sao_band_ddistortion_generic
$unwind$sao_edge_ddistortion_generic DD 050f01H
	DD	0d00b820fH
	DD	07007c009H
	DD	05006H
$chain$3$sao_edge_ddistortion_generic DD 081821H
	DD	06f418H
	DD	07e413H
	DD	08640eH
	DD	0e3405H
	DD	imagerel sao_edge_ddistortion_generic
	DD	imagerel sao_edge_ddistortion_generic+107
	DD	imagerel $unwind$sao_edge_ddistortion_generic
$chain$4$sao_edge_ddistortion_generic DD 021H
	DD	imagerel sao_edge_ddistortion_generic
	DD	imagerel sao_edge_ddistortion_generic+107
	DD	imagerel $unwind$sao_edge_ddistortion_generic
$unwind$kvz_strategy_register_sao_avx2 DD 060f01H
	DD	09640fH
	DD	08340fH
	DD	0700b520fH
$SG4294948067 DB 'sao_band_ddistortion', 00H
	ORG $+3
$SG4294948068 DB 'avx2', 00H
	ORG $+3
$SG4294948069 DB 'sao_reconstruct_color', 00H
	ORG $+2
$SG4294948070 DB 'avx2', 00H
	ORG $+3
$SG4294948071 DB 'calc_sao_edge_dir', 00H
	ORG $+2
$SG4294948072 DB 'avx2', 00H
	ORG $+7
$SG4294948073 DB 'sao_edge_ddistortion', 00H
	ORG $+3
$SG4294948074 DB 'avx2', 00H
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
opaque$ = 64
bitdepth$dead$ = 72
kvz_strategy_register_sao_avx2 PROC

; 904  : {

$LN5:
	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rsi
	push	rdi
	sub	rsp, 48					; 00000030H

; 905  :   bool success = true;
; 906  : #if COMPILE_INTEL_AVX2
; 907  : #if KVZ_BIT_DEPTH == 8
; 908  :   if (bitdepth == 8) {
; 909  :     success &= kvz_strategyselector_register(opaque, "sao_edge_ddistortion", "avx2", 40, &sao_edge_ddistortion_avx2);

	lea	rax, OFFSET FLAT:sao_edge_ddistortion_avx2
	mov	r9d, 40					; 00000028H
	lea	r8, OFFSET FLAT:$SG4294948074
	mov	QWORD PTR [rsp+32], rax
	lea	rdx, OFFSET FLAT:$SG4294948073
	mov	rsi, rcx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 910  :     success &= kvz_strategyselector_register(opaque, "calc_sao_edge_dir", "avx2", 40, &calc_sao_edge_dir_avx2);

	lea	r8, OFFSET FLAT:$SG4294948072
	lea	rax, OFFSET FLAT:calc_sao_edge_dir_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294948071
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	ebx, 1
	call	kvz_strategyselector_register
	mov	edi, eax

; 911  :     success &= kvz_strategyselector_register(opaque, "sao_reconstruct_color", "avx2", 40, &sao_reconstruct_color_avx2);

	lea	r8, OFFSET FLAT:$SG4294948070
	lea	rax, OFFSET FLAT:sao_reconstruct_color_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294948069
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	edi, ebx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 912  :     success &= kvz_strategyselector_register(opaque, "sao_band_ddistortion", "avx2", 40, &sao_band_ddistortion_avx2);

	lea	r8, OFFSET FLAT:$SG4294948068
	lea	rax, OFFSET FLAT:sao_band_ddistortion_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294948067
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	ebx, edi
	call	kvz_strategyselector_register

; 913  :   }
; 914  : #endif // KVZ_BIT_DEPTH == 8
; 915  : #endif //COMPILE_INTEL_AVX2
; 916  :   return success;
; 917  : }

	mov	rsi, QWORD PTR [rsp+72]
	and	eax, ebx
	mov	rbx, QWORD PTR [rsp+64]
	add	rsp, 48					; 00000030H
	pop	rdi
	ret	0
kvz_strategy_register_sao_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\sao_shared_generics.h
_TEXT	SEGMENT
a$ = 8
b$ = 16
c$ = 24
sao_calc_eo_cat PROC

; 44   :   // Mapping relationships between a, b and c to eo_idx.
; 45   :   static const int sao_eo_idx_to_eo_category[] = { 1, 2, 0, 3, 4 };
; 46   : 
; 47   :   int eo_idx = 2 + SIGN3((int)c - (int)a) + SIGN3((int)c - (int)b);

	movzx	eax, cl
	xor	ecx, ecx
	movzx	r9d, r8b
	mov	r8d, r9d
	sub	r8d, eax
	movzx	eax, dl
	sub	r9d, eax
	mov	edx, ecx
	test	r9d, r9d
	mov	eax, r8d
	setg	dl
	shr	eax, 31
	add	edx, 2
	shr	r9d, 31
	sub	edx, r9d
	sub	edx, eax
	test	r8d, r8d
	setg	cl
	lea	eax, DWORD PTR [rcx+rdx]

; 48   : 
; 49   :   return sao_eo_idx_to_eo_category[eo_idx];

	cdqe
	lea	rcx, OFFSET FLAT:?sao_eo_idx_to_eo_category@?1??sao_calc_eo_cat@@9@9
	mov	eax, DWORD PTR [rcx+rax*4]

; 50   : }

	ret	0
sao_calc_eo_cat ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\sao_shared_generics.h
_TEXT	SEGMENT
tv840 = 0
b_ofs$ = 8
a_ofs$ = 16
b_ofs$1$ = 24
a_ofs$1$ = 32
orig_data$ = 112
rec_data$ = 120
block_width$ = 128
tv839 = 136
block_height$ = 136
y$1$ = 144
eo_class$ = 144
offsets$ = 152
sao_edge_ddistortion_generic PROC

; 58   : {

	mov	DWORD PTR [rsp+24], r8d
	push	rbp
	push	rdi
	push	r12
	push	r13
	sub	rsp, 72					; 00000048H
	mov	r12, rcx

; 59   :   int y, x;
; 60   :   int32_t sum = 0;
; 61   :   vector2d_t a_ofs = g_sao_edge_offsets[eo_class][0];

	lea	rax, OFFSET FLAT:__ImageBase
	movsxd	rcx, DWORD PTR eo_class$[rsp]
	mov	ebp, 1
	add	rcx, rcx
	mov	DWORD PTR y$1$[rsp], ebp
	xor	edi, edi
	mov	r11d, r8d
	mov	r13, rdx
	mov	r10, QWORD PTR g_sao_edge_offsets[rax+rcx*8]

; 62   :   vector2d_t b_ofs = g_sao_edge_offsets[eo_class][1];

	mov	rcx, QWORD PTR g_sao_edge_offsets[rax+rcx*8+8]

; 63   : 
; 64   :   for (y = 1; y < block_height - 1; y++) {

	lea	eax, DWORD PTR [r9-1]
	mov	QWORD PTR a_ofs$1$[rsp], r10
	mov	QWORD PTR a_ofs$[rsp], r10
	mov	QWORD PTR b_ofs$1$[rsp], rcx
	mov	QWORD PTR b_ofs$[rsp], rcx
	mov	DWORD PTR tv840[rsp], eax
	cmp	eax, ebp
	jle	$LN21@sao_edge_d
	mov	QWORD PTR [rsp+112], rbx
	lea	r9d, DWORD PTR [r8-1]
	mov	QWORD PTR [rsp+64], rsi
	mov	QWORD PTR [rsp+56], r14
	mov	QWORD PTR [rsp+48], r15
	mov	DWORD PTR tv839[rsp], r9d
	npad	5
$LL4@sao_edge_d:

; 65   :     for (x = 1; x < block_width - 1; x++) {

	cmp	r9d, 1
	jle	$LN2@sao_edge_d
	mov	r8d, DWORD PTR b_ofs$[rsp+4]
	lea	ebx, DWORD PTR [r9-1]
	mov	eax, DWORD PTR a_ofs$[rsp+4]
	add	r8d, ebp
	sub	eax, r8d
	mov	r14d, ebp
	sub	r14d, r8d
	add	eax, ebp
	imul	r14d, r11d
	lea	rbp, OFFSET FLAT:__ImageBase
	mov	edx, r8d
	imul	eax, r11d
	imul	edx, r11d
	sub	r14d, ecx
	inc	edx
	add	edx, ecx
	sub	eax, ecx
	lea	ecx, DWORD PTR [rax+r10]
	mov	r10, QWORD PTR offsets$[rsp]
$LL7@sao_edge_d:

; 47   :   int eo_idx = 2 + SIGN3((int)c - (int)a) + SIGN3((int)c - (int)b);

	movzx	r8d, BYTE PTR [rdx+r13]

; 71   :       uint8_t   b    =  rec_data[b_pos];
; 72   :       uint8_t   c    =  rec_data[c_pos];

	lea	eax, DWORD PTR [r14+rdx]

; 47   :   int eo_idx = 2 + SIGN3((int)c - (int)a) + SIGN3((int)c - (int)b);

	movzx	esi, BYTE PTR [rax+r13]
	xor	r9d, r9d

; 71   :       uint8_t   b    =  rec_data[b_pos];
; 72   :       uint8_t   c    =  rec_data[c_pos];

	mov	r15d, eax

; 47   :   int eo_idx = 2 + SIGN3((int)c - (int)a) + SIGN3((int)c - (int)b);

	mov	r11d, esi
	sub	r11d, r8d

; 66   :       uint32_t c_pos =  y            * block_width + x;
; 67   :       uint32_t a_pos = (y + a_ofs.y) * block_width + x + a_ofs.x;
; 68   :       uint32_t b_pos = (y + b_ofs.y) * block_width + x + b_ofs.x;
; 69   : 
; 70   :       uint8_t   a    =  rec_data[a_pos];

	lea	eax, DWORD PTR [rcx+rdx]

; 47   :   int eo_idx = 2 + SIGN3((int)c - (int)a) + SIGN3((int)c - (int)b);

	movzx	eax, BYTE PTR [rax+r13]
	mov	r8d, esi
	sub	r8d, eax
	mov	eax, r11d
	test	r8d, r8d
	setg	r9b
	shr	eax, 31
	add	r9d, 2
	shr	r8d, 31
	sub	r9d, r8d
	sub	r9d, eax
	xor	eax, eax
	test	r11d, r11d
	setg	al
	add	eax, r9d

; 48   : 
; 49   :   return sao_eo_idx_to_eo_category[eo_idx];

	cdqe

; 73   :       uint8_t   orig = orig_data[c_pos];
; 74   : 
; 75   :       int32_t eo_cat = sao_calc_eo_cat(a, b, c);
; 76   :       int32_t offset = offsets[eo_cat];

	movsxd	rax, DWORD PTR ?sao_eo_idx_to_eo_category@?1??sao_calc_eo_cat@@9@9[rbp+rax*4]
	mov	r9d, DWORD PTR [r10+rax*4]

; 77   : 
; 78   :       if (offset != 0) {

	test	r9d, r9d
	je	SHORT $LN5@sao_edge_d

; 79   :         int32_t diff   = orig - c;

	movzx	eax, BYTE PTR [r15+r12]
	sub	eax, esi

; 80   :         int32_t delta  = diff - offset;

	mov	r8d, eax

; 81   :         int32_t curr   = delta * delta - diff * diff;

	imul	eax, eax
	sub	r8d, r9d
	imul	r8d, r8d

; 82   : 
; 83   :         sum += curr;

	sub	r8d, eax
	add	edi, r8d
$LN5@sao_edge_d:

; 65   :     for (x = 1; x < block_width - 1; x++) {

	inc	edx
	sub	rbx, 1
	jne	$LL7@sao_edge_d
	mov	rcx, QWORD PTR b_ofs$1$[rsp]
	mov	r10, QWORD PTR a_ofs$1$[rsp]
	mov	ebp, DWORD PTR y$1$[rsp]
	mov	r11d, DWORD PTR block_width$[rsp]
	mov	r9d, DWORD PTR tv839[rsp]
	mov	eax, DWORD PTR tv840[rsp]
$LN2@sao_edge_d:

; 63   : 
; 64   :   for (y = 1; y < block_height - 1; y++) {

	inc	ebp
	mov	DWORD PTR y$1$[rsp], ebp
	cmp	ebp, eax
	jl	$LL4@sao_edge_d

; 84   :       }
; 85   :     }
; 86   :   }
; 87   :   return sum;

	mov	r15, QWORD PTR [rsp+48]
	mov	r14, QWORD PTR [rsp+56]
	mov	rsi, QWORD PTR [rsp+64]
	mov	rbx, QWORD PTR [rsp+112]
$LN21@sao_edge_d:

; 88   : }

	mov	eax, edi
	add	rsp, 72					; 00000048H
	pop	r13
	pop	r12
	pop	rdi
	pop	rbp
	ret	0
sao_edge_ddistortion_generic ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\sao_shared_generics.h
_TEXT	SEGMENT
tv487 = 64
state$ = 64
orig_data$ = 72
rec_data$ = 80
block_width$ = 88
block_height$ = 96
band_pos$ = 104
sao_bands$ = 112
sao_band_ddistortion_generic PROC

; 97   : {

	mov	r11, rsp
	mov	QWORD PTR [r11+24], r8
	mov	QWORD PTR [r11+16], rdx
	push	rdi
	push	r15
	sub	rsp, 40					; 00000028H

; 98   :   int y, x;
; 99   :   int shift = state->encoder_control->bitdepth-5;

	mov	rax, QWORD PTR [rcx]

; 100  :   int sum = 0;

	xor	edi, edi
	movsxd	r15, r9d
	movzx	r9d, BYTE PTR [rax+2592]

; 101  :   for (y = 0; y < block_height; ++y) {

	mov	eax, DWORD PTR block_height$[rsp]
	mov	BYTE PTR tv487[rsp], r9b
	test	eax, eax
	jle	$LN18@sao_band_d

; 98   :   int y, x;
; 99   :   int shift = state->encoder_control->bitdepth-5;

	mov	QWORD PTR [r11+32], rbx
	mov	rcx, r15
	mov	QWORD PTR [r11-24], rbp
	mov	rbp, rdx
	mov	QWORD PTR [r11-32], rsi
	mov	QWORD PTR [r11-40], r12
	mov	r12d, DWORD PTR band_pos$[rsp]
	mov	QWORD PTR [r11-48], r13
	mov	r13d, eax
	mov	QWORD PTR [rsp], r14
	mov	r14, QWORD PTR sao_bands$[rsp]
	npad	2
$LL4@sao_band_d:

; 102  :     for (x = 0; x < block_width; ++x) {

	test	r15d, r15d
	jle	SHORT $LN2@sao_band_d
	lea	eax, DWORD PTR [r9-5]
	mov	ebx, r15d
	mov	r11, r8
	movzx	esi, al
	sub	r11, rdx
	mov	r9, rbp
	npad	8
$LL7@sao_band_d:

; 103  :       const int32_t curr_pos = y * block_width + x;
; 104  : 
; 105  :       kvz_pixel rec  =  rec_data[curr_pos];
; 106  :       kvz_pixel orig = orig_data[curr_pos];
; 107  : 
; 108  :       int32_t band = (rec >> shift) - band_pos;

	movzx	r10d, BYTE PTR [r11+r9]

; 109  :       int32_t offset = 0;

	xor	r8d, r8d
	shrx	eax, r10d, esi
	sub	eax, r12d

; 110  :       if (band >= 0 && band <= 3) {

	cmp	eax, 3
	ja	SHORT $LN8@sao_band_d

; 111  :         offset = sao_bands[band];

	movsxd	rcx, eax
	mov	r8d, DWORD PTR [r14+rcx*4]
$LN8@sao_band_d:

; 112  :       }
; 113  :       // Offset is applied to reconstruction, so it is subtracted from diff.
; 114  : 
; 115  :       int32_t diff  = orig - rec;

	movzx	edx, BYTE PTR [r9]
	sub	edx, r10d

; 116  :       int32_t delta = diff - offset;

	mov	ecx, edx
	sub	ecx, r8d

; 117  : 
; 118  :       int32_t dmask = (offset == 0) ? -1 : 0;
; 119  :       diff  &= ~dmask;

	neg	r8d
	sbb	eax, eax
	inc	r9

; 120  :       delta &= ~dmask;

	and	ecx, eax
	and	edx, eax

; 121  : 
; 122  :       sum += delta * delta - diff * diff;

	imul	ecx, ecx
	imul	edx, edx
	sub	ecx, edx
	add	edi, ecx
	sub	rbx, 1
	jne	SHORT $LL7@sao_band_d
	mov	rdx, QWORD PTR orig_data$[rsp]
	mov	rcx, r15
	mov	r8, QWORD PTR rec_data$[rsp]
	movzx	r9d, BYTE PTR tv487[rsp]
$LN2@sao_band_d:

; 101  :   for (y = 0; y < block_height; ++y) {

	add	rbp, rcx
	sub	r13, 1
	jne	SHORT $LL4@sao_band_d

; 123  :     }
; 124  :   }
; 125  : 
; 126  :   return sum;

	mov	r14, QWORD PTR [rsp]
	mov	r13, QWORD PTR [rsp+8]
	mov	r12, QWORD PTR [rsp+16]
	mov	rsi, QWORD PTR [rsp+24]
	mov	rbp, QWORD PTR [rsp+32]
	mov	rbx, QWORD PTR [rsp+88]
$LN18@sao_band_d:

; 127  : }

	mov	eax, edi
	add	rsp, 40					; 00000028H
	pop	r15
	pop	rdi
	ret	0
sao_band_ddistortion_generic ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
_TEXT	SEGMENT
v$ = 8
hsum_8x32b PROC

; 172  :   __m256i sum1 = v;
; 173  :   __m256i sum2 = _mm256_permute4x64_epi64(sum1, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm0, YMMWORD PTR [rcx], 78		; 0000004eH

; 174  :   __m256i sum3 = _mm256_add_epi32        (sum1, sum2);

	vpaddd	ymm2, ymm0, YMMWORD PTR [rcx]

; 175  :   __m256i sum4 = _mm256_shuffle_epi32    (sum3, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	ymm1, ymm2, 78				; 0000004eH

; 176  :   __m256i sum5 = _mm256_add_epi32        (sum3, sum4);

	vpaddd	ymm3, ymm1, ymm2

; 177  :   __m256i sum6 = _mm256_shuffle_epi32    (sum5, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm0, ymm3, 177				; 000000b1H

; 178  :   __m256i sum7 = _mm256_add_epi32        (sum5, sum6);

	vpaddd	ymm1, ymm0, ymm3

; 179  : 
; 180  :   __m128i sum8 = _mm256_castsi256_si128  (sum7);
; 181  :   int32_t sum9 = _mm_cvtsi128_si32       (sum8);

	vmovd	eax, xmm1
	vzeroupper

; 182  :   return  sum9;
; 183  : }

	ret	0
hsum_8x32b ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
a$ = 8
b$ = 16
sign3_diff_epu8 PROC

; 58   :   // Subtract 0x80 from unsigneds to compare them as signed
; 59   :   const __m256i epu2epi = _mm256_set1_epi8  (0x80);
; 60   :   const __m256i ones    = _mm256_set1_epi8  (0x01);
; 61   : 
; 62   :   __m256i a_signed      = _mm256_sub_epi8   (a,        epu2epi);

	vmovdqu	ymm0, YMMWORD PTR [rcx]
	vpsubb	ymm3, ymm0, YMMWORD PTR __ymm@8080808080808080808080808080808080808080808080808080808080808080

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vmovdqu	ymm1, YMMWORD PTR [rdx]
	vpsubb	ymm2, ymm1, YMMWORD PTR __ymm@8080808080808080808080808080808080808080808080808080808080808080
	vmovdqu	ymm0, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101

; 64   : 
; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm3, ymm3, ymm2

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm0, ymm0, ymm3

; 67   : }

	ret	0
sign3_diff_epu8 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
a$ = 48
b$ = 56
c$ = 64
calc_eo_cat PROC

; 73   : {

	sub	rsp, 40					; 00000028H

; 62   :   __m256i a_signed      = _mm256_sub_epi8   (a,        epu2epi);

	vmovdqu	ymm0, YMMWORD PTR [r8]
	vmovdqu	ymm3, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101
	vmovaps	XMMWORD PTR [rsp+16], xmm6
	vmovaps	XMMWORD PTR [rsp], xmm7

; 59   :   const __m256i epu2epi = _mm256_set1_epi8  (0x80);

	vmovdqu	ymm7, YMMWORD PTR __ymm@8080808080808080808080808080808080808080808080808080808080808080

; 62   :   __m256i a_signed      = _mm256_sub_epi8   (a,        epu2epi);

	vpsubb	ymm6, ymm0, ymm7

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vmovdqu	ymm0, YMMWORD PTR [rcx]
	vpsubb	ymm1, ymm0, ymm7
	vmovdqu	ymm0, YMMWORD PTR [rdx]

; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm2, ymm6, ymm1

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vpsubb	ymm1, ymm0, ymm7

; 85   : }

	vmovaps	xmm7, XMMWORD PTR [rsp]

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm4, ymm3, ymm2

; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm2, ymm6, ymm1

; 74   :   const __m256i twos       = _mm256_set1_epi8  (0x02);
; 75   :   const __m256i idx_to_cat = _mm256_setr_epi64x(0x0403000201, 0,

	vmovdqu	ymm1, YMMWORD PTR __ymm@0000000000000000000000040300020100000000000000000000000403000201

; 85   : }

	vmovaps	xmm6, XMMWORD PTR [rsp+16]

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm3, ymm3, ymm2

; 76   :                                                 0x0403000201, 0);
; 77   : 
; 78   :   __m256i c_a_sign         = sign3_diff_epu8    (c, a);
; 79   :   __m256i c_b_sign         = sign3_diff_epu8    (c, b);
; 80   : 
; 81   :   __m256i signsum          = _mm256_add_epi8    (c_a_sign,   c_b_sign);

	vpaddb	ymm0, ymm3, ymm4

; 82   :   __m256i eo_idx           = _mm256_add_epi8    (signsum,    twos);

	vpaddb	ymm4, ymm0, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202

; 83   : 
; 84   :   return                     _mm256_shuffle_epi8(idx_to_cat, eo_idx);

	vpshufb	ymm0, ymm1, ymm4

; 85   : }

	add	rsp, 40					; 00000028H
	ret	0
calc_eo_cat ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
v$ = 8
shift$dead$ = 16
srli_epi8 PROC

; 90   :   const uint8_t hibit_mask     = 0xff >> shift;
; 91   :   const __m256i hibit_mask_256 = _mm256_set1_epi8(hibit_mask);
; 92   : 
; 93   :   __m256i v_shifted = _mm256_srli_epi32(v,         shift);

	vmovdqu	ymm0, YMMWORD PTR [rcx]
	vpsrld	ymm1, ymm0, 3

; 94   :   __m256i v_masked  = _mm256_and_si256 (v_shifted, hibit_mask_256);

	vpand	ymm0, ymm1, YMMWORD PTR __ymm@1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f

; 95   : 
; 96   :   return v_masked;
; 97   : }

	ret	0
srli_epi8 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
v$ = 8
res_lo$ = 16
res_hi$ = 24
cvt_epu8_epi16 PROC

; 103  :   const __m256i zero  = _mm256_setzero_si256();
; 104  :              *res_lo  = _mm256_unpacklo_epi8(v, zero);

	vmovdqu	ymm1, YMMWORD PTR [rcx]
	vpxor	xmm2, xmm2, xmm2
	vpunpcklbw ymm0, ymm1, ymm2

; 105  :              *res_hi  = _mm256_unpackhi_epi8(v, zero);

	vpunpckhbw ymm1, ymm1, ymm2
	vmovdqu	YMMWORD PTR [r8], ymm1
	vmovdqu	YMMWORD PTR [rdx], ymm0
	vzeroupper

; 106  : }

	ret	0
cvt_epu8_epi16 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
v$ = 8
res_lo$ = 16
res_hi$ = 24
cvt_epi8_epi16 PROC

; 112  :   const __m256i zero  = _mm256_setzero_si256();
; 113  :         __m256i signs = _mm256_cmpgt_epi8   (zero, v);

	vmovdqu	ymm3, YMMWORD PTR [rcx]
	vpxor	xmm0, xmm0, xmm0
	vpcmpgtb ymm2, ymm0, ymm3

; 114  :              *res_lo  = _mm256_unpacklo_epi8(v,    signs);

	vpunpcklbw ymm1, ymm3, ymm2

; 115  :              *res_hi  = _mm256_unpackhi_epi8(v,    signs);

	vpunpckhbw ymm0, ymm3, ymm2
	vmovdqu	YMMWORD PTR [rdx], ymm1
	vmovdqu	YMMWORD PTR [r8], ymm0
	vzeroupper

; 116  : }

	ret	0
cvt_epi8_epi16 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
a$ = 8
b$ = 16
res_lo$ = 24
res_hi$ = 32
diff_epi8_epi16 PROC

; 123  :   const __m256i invmask = _mm256_set1_epi16(0xff01);
; 124  : 
; 125  :   __m256i composite_lo  = _mm256_unpacklo_epi8(a, b);

	vmovdqu	ymm3, YMMWORD PTR [rcx]
	vpunpcklbw ymm0, ymm3, YMMWORD PTR [rdx]

; 126  :   __m256i composite_hi  = _mm256_unpackhi_epi8(a, b);
; 127  : 
; 128  :          *res_lo        = _mm256_maddubs_epi16(composite_lo, invmask);

	vpmaddubsw ymm1, ymm0, YMMWORD PTR __ymm@ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01
	vpunpckhbw ymm2, ymm3, YMMWORD PTR [rdx]

; 129  :          *res_hi        = _mm256_maddubs_epi16(composite_hi, invmask);

	vpmaddubsw ymm0, ymm2, YMMWORD PTR __ymm@ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01
	vmovdqu	YMMWORD PTR [r9], ymm0
	vmovdqu	YMMWORD PTR [r8], ymm1
	vzeroupper

; 130  : }

	ret	0
diff_epi8_epi16 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
v$ = 8
res_lo$ = 16
res_hi$ = 24
cvt_shufmask_epi8_epi16 PROC

; 138  : {

	vpxor	xmm1, xmm1, xmm1

; 139  :   const __m256i zero = _mm256_setzero_si256();
; 140  :   const __m256i ones = _mm256_set1_epi8(1);
; 141  : 
; 142  :   // There's no 8-bit shift, so highest bit could bleed into neighboring byte
; 143  :   // if set. To avoid it, reset all sign bits with max. The only valid input
; 144  :   // values for v are [0, 7] anyway and invalid places should be masked out by
; 145  :   // caller, so it doesn't matter that we turn negative bytes into garbage.
; 146  :   __m256i v_nonnegs  = _mm256_max_epi8  (zero,      v);

	vpmaxsb	ymm1, ymm1, YMMWORD PTR [rcx]

; 147  :   __m256i v_lobytes  = _mm256_slli_epi32(v_nonnegs, 1);

	vpslld	ymm3, ymm1, 1

; 148  :   __m256i v_hibytes  = _mm256_add_epi8  (v_lobytes, ones);

	vpaddb	ymm2, ymm3, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101

; 149  : 
; 150  :           *res_lo    = _mm256_unpacklo_epi8(v_lobytes, v_hibytes);

	vpunpcklbw ymm1, ymm3, ymm2

; 151  :           *res_hi    = _mm256_unpackhi_epi8(v_lobytes, v_hibytes);

	vpunpckhbw ymm0, ymm3, ymm2
	vmovdqu	YMMWORD PTR [rdx], ymm1
	vmovdqu	YMMWORD PTR [r8], ymm0
	vzeroupper

; 152  : }

	ret	0
cvt_shufmask_epi8_epi16 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
v$ = 8
epi32v_fits_in_epi8s PROC

; 158  :   // Compare most significant 25 bits of SAO bands to the sign bit to assert
; 159  :   // that the i32's are between -128 and 127 (only comparing 24 would fail to
; 160  :   // detect values of 128...255)
; 161  :   __m128i  v_ms25b = _mm_srai_epi32   (v,  7);
; 162  :   __m128i  v_signs = _mm_srai_epi32   (v, 31);

	vmovdqu	xmm0, XMMWORD PTR [rcx]
	vpsrad	xmm1, xmm0, 31
	vpsrad	xmm0, xmm0, 7

; 163  :   __m128i  ok_i32s = _mm_cmpeq_epi32  (v_ms25b, v_signs);

	vpcmpeqd xmm1, xmm1, xmm0

; 164  :   return             _mm_movemask_epi8(ok_i32s);

	vpmovmskb eax, xmm1

; 165  : }

	ret	0
epi32v_fits_in_epi8s ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
v$ = 8
truncate_epi32_epi8 PROC

; 169  :   // LSBs of each dword, the values values must fit in 8 bits anyway for
; 170  :   // what this intended for (use epi32v_fits_in_epi8s to check if needed)
; 171  :   const __m128i trunc_shufmask = _mm_set1_epi32  (0x0c080400);
; 172  :         __m128i sbs_8          = _mm_shuffle_epi8(v, trunc_shufmask);

	vmovdqu	xmm0, XMMWORD PTR [rcx]
	vpshufb	xmm0, xmm0, XMMWORD PTR __xmm@0c0804000c0804000c0804000c080400

; 173  :   return        sbs_8;
; 174  : }

	ret	0
truncate_epi32_epi8 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
buf$ = 8
start_pos$ = 16
width_rest$ = 24
load_border_bytes PROC

; 181  :   uint32_t last_dword = 0;

	xor	eax, eax

; 182  :   for (int32_t i = 0; i < width_rest; i++) {

	test	r8d, r8d
	jle	SHORT $LN12@load_borde
	movsxd	r10, edx
	mov	r9d, eax
	add	r10, rcx
	mov	r8d, r8d
	npad	13
$LL8@load_borde:

; 183  :     uint8_t  currb = buf[start_pos + i];
; 184  :     uint32_t currd = ((uint32_t)currb) << (i * 8);

	movzx	ecx, BYTE PTR [r10]
	lea	r10, QWORD PTR [r10+1]
	shlx	edx, ecx, r9d

; 185  :     last_dword |= currd;

	or	eax, edx
	add	r9d, 8
	sub	r8, 1
	jne	SHORT $LL8@load_borde
$LN12@load_borde:

; 186  :   }
; 187  :   return last_dword;
; 188  : }

	ret	0
load_border_bytes ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
buf$ = 8
start_pos$ = 16
width_rest$ = 24
data$ = 32
store_border_bytes PROC

; 195  :   for (uint32_t i = 0; i < width_rest; i++) {

	xor	eax, eax
	test	r8d, r8d
	je	SHORT $LN3@store_bord
	npad	9
$LL8@store_bord:

; 196  :     uint8_t currb = data & 0xff;
; 197  :     buf[start_pos + i] = currb;

	lea	r10d, DWORD PTR [rax+rdx]
	inc	eax
	mov	BYTE PTR [r10+rcx], r9b

; 198  :     data >>= 8;

	shr	r9d, 8
	cmp	eax, r8d
	jb	SHORT $LL8@store_bord
$LN3@store_bord:

; 199  :   }
; 200  : }

	ret	0
store_border_bytes ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
db4_mask$ = 8
width_rest$ = 16
gen_badbyte_mask PROC

; 207  :   const __m256i zero    = _mm256_setzero_si256();
; 208  : 
; 209  :   uint32_t last_badbytes = 0xffffffff << (width_rest << 3);

	lea	eax, DWORD PTR [rdx*8]
	mov	edx, -1					; ffffffffH
	shlx	eax, edx, eax

; 210  :   __m256i  badbyte_mask  = _mm256_cmpeq_epi8  (db4_mask,     zero);
; 211  :   return                   _mm256_insert_epi32(badbyte_mask, last_badbytes, 7);

	vmovd	xmm2, eax
	vpxor	xmm1, xmm1, xmm1
	vpcmpeqb ymm1, ymm1, YMMWORD PTR [rcx]
	vpbroadcastd ymm2, xmm2
	vpblendd ymm0, ymm1, ymm2, 128			; 00000080H

; 212  : }

	ret	0
gen_badbyte_mask ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
v$ = 8
broadcast_xmm2ymm PROC

; 218  :   __m256i res = _mm256_castsi128_si256 (v);

	vmovdqu	xmm0, XMMWORD PTR [rcx]

; 219  :   return        _mm256_inserti128_si256(res, v, 1);

	vinserti128 ymm0, ymm0, xmm0, 1

; 220  : }

	ret	0
broadcast_xmm2ymm ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
diff_lo$ = 64
diff_hi$ = 72
offsets$ = 80
orig$dead$ = 88
calc_diff_off_delta PROC

; 227  : {

	sub	rsp, 56					; 00000038H

; 228  :   const __m256i zero          = _mm256_setzero_si256();
; 229  :   const __m256i negate_hiword = _mm256_set1_epi32(0xffff0001);
; 230  : 
; 231  :   __m256i orig_lo, orig_hi, offsets_lo, offsets_hi;
; 232  : 
; 233  :   cvt_epu8_epi16(orig,    &orig_lo,    &orig_hi);
; 234  :   cvt_epi8_epi16(offsets, &offsets_lo, &offsets_hi);

	vmovdqu	ymm1, YMMWORD PTR [r8]
	vmovaps	XMMWORD PTR [rsp+32], xmm6
	vmovaps	XMMWORD PTR [rsp+16], xmm7
	vpxor	xmm3, xmm3, xmm3

; 113  :         __m256i signs = _mm256_cmpgt_epi8   (zero, v);

	vpcmpgtb ymm0, ymm3, ymm1

; 114  :              *res_lo  = _mm256_unpacklo_epi8(v,    signs);

	vpunpcklbw ymm5, ymm1, ymm0

; 115  :              *res_hi  = _mm256_unpackhi_epi8(v,    signs);

	vpunpckhbw ymm6, ymm1, ymm0

; 235  : 
; 236  :   __m256i offsets_0_lo = _mm256_cmpeq_epi16   (offsets_lo,   zero);
; 237  :   __m256i offsets_0_hi = _mm256_cmpeq_epi16   (offsets_hi,   zero);
; 238  : 
; 239  :   __m256i delta_lo     = _mm256_sub_epi16     (diff_lo,      offsets_lo);
; 240  :   __m256i delta_hi     = _mm256_sub_epi16     (diff_hi,      offsets_hi);
; 241  : 
; 242  :   __m256i diff_lo_m    = _mm256_andnot_si256  (offsets_0_lo, diff_lo);

	vmovdqu	ymm0, YMMWORD PTR [rcx]

; 243  :   __m256i diff_hi_m    = _mm256_andnot_si256  (offsets_0_hi, diff_hi);

	vmovdqu	ymm1, YMMWORD PTR [rdx]
	vpcmpeqw ymm2, ymm5, ymm3
	vpandn	ymm4, ymm2, ymm0
	vpsubw	ymm0, ymm0, ymm5

; 244  :   __m256i delta_lo_m   = _mm256_andnot_si256  (offsets_0_lo, delta_lo);

	vpandn	ymm2, ymm2, ymm0
	vpcmpeqw ymm3, ymm6, ymm3
	vpandn	ymm7, ymm3, ymm1
	vpsubw	ymm1, ymm1, ymm6

; 245  :   __m256i delta_hi_m   = _mm256_andnot_si256  (offsets_0_hi, delta_hi);

	vpandn	ymm0, ymm3, ymm1

; 246  : 
; 247  :   __m256i dd0_lo       = _mm256_unpacklo_epi16(delta_lo_m,   diff_lo_m);
; 248  :   __m256i dd0_hi       = _mm256_unpackhi_epi16(delta_lo_m,   diff_lo_m);
; 249  :   __m256i dd1_lo       = _mm256_unpacklo_epi16(delta_hi_m,   diff_hi_m);

	vpunpcklwd ymm6, ymm0, ymm7

; 250  :   __m256i dd1_hi       = _mm256_unpackhi_epi16(delta_hi_m,   diff_hi_m);

	vpunpckhwd ymm7, ymm0, ymm7
	vpunpcklwd ymm3, ymm2, ymm4
	vmovaps	XMMWORD PTR [rsp], xmm8
	vmovdqu	ymm8, YMMWORD PTR __ymm@ffff0001ffff0001ffff0001ffff0001ffff0001ffff0001ffff0001ffff0001
	vpunpckhwd ymm4, ymm2, ymm4

; 251  : 
; 252  :   __m256i dd0_lo_n     = _mm256_sign_epi16    (dd0_lo,       negate_hiword);
; 253  :   __m256i dd0_hi_n     = _mm256_sign_epi16    (dd0_hi,       negate_hiword);

	vpsignw	ymm1, ymm4, ymm8

; 254  :   __m256i dd1_lo_n     = _mm256_sign_epi16    (dd1_lo,       negate_hiword);
; 255  :   __m256i dd1_hi_n     = _mm256_sign_epi16    (dd1_hi,       negate_hiword);
; 256  : 
; 257  :   __m256i sum0_lo      = _mm256_madd_epi16    (dd0_lo,       dd0_lo_n);
; 258  :   __m256i sum0_hi      = _mm256_madd_epi16    (dd0_hi,       dd0_hi_n);

	vpmaddwd ymm2, ymm1, ymm4
	vpsignw	ymm0, ymm3, ymm8
	vpmaddwd ymm3, ymm0, ymm3
	vpsignw	ymm0, ymm6, ymm8
	vpsignw	ymm1, ymm7, ymm8

; 259  :   __m256i sum1_lo      = _mm256_madd_epi16    (dd1_lo,       dd1_lo_n);
; 260  :   __m256i sum1_hi      = _mm256_madd_epi16    (dd1_hi,       dd1_hi_n);
; 261  : 
; 262  :   __m256i sum0         = _mm256_add_epi32     (sum0_lo,      sum0_hi);
; 263  :   __m256i sum1         = _mm256_add_epi32     (sum1_lo,      sum1_hi);
; 264  :   return                 _mm256_add_epi32     (sum0,         sum1);
; 265  : }

	vmovaps	xmm8, XMMWORD PTR [rsp]
	vpmaddwd ymm4, ymm0, ymm6
	vmovaps	xmm6, XMMWORD PTR [rsp+32]
	vpaddd	ymm5, ymm2, ymm3
	vpmaddwd ymm2, ymm1, ymm7
	vmovaps	xmm7, XMMWORD PTR [rsp+16]
	vpaddd	ymm0, ymm2, ymm4
	vpaddd	ymm0, ymm0, ymm5
	add	rsp, 56					; 00000038H
	ret	0
calc_diff_off_delta ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
a$ = 112
b$ = 120
c$ = 128
orig$ = 136
badbyte_mask$ = 144
offsets_256$ = 152
do_one_edge_ymm PROC

; 273  : {

	sub	rsp, 104				; 00000068H

; 274  :   __m256i eo_cat = calc_eo_cat(a, b, c);

	vmovdqu	ymm3, YMMWORD PTR [r8]

; 277  : 
; 278  :   __m256i offset_lo, offset_hi;
; 279  :   cvt_epi8_epi16(offset, &offset_lo, &offset_hi);
; 280  : 
; 281  :   __m256i diff_lo, diff_hi;
; 282  :   diff_epi8_epi16(orig, c, &diff_lo, &diff_hi);

	vmovdqu	ymm1, YMMWORD PTR [r9]
	mov	rax, QWORD PTR badbyte_mask$[rsp]

; 126  :   __m256i composite_hi  = _mm256_unpackhi_epi8(a, b);

	vpunpckhbw ymm0, ymm1, ymm3
	vpunpcklbw ymm1, ymm1, ymm3
	vmovaps	XMMWORD PTR [rsp+80], xmm6
	vmovaps	XMMWORD PTR [rsp+64], xmm7

; 59   :   const __m256i epu2epi = _mm256_set1_epi8  (0x80);

	vmovdqu	ymm7, YMMWORD PTR __ymm@8080808080808080808080808080808080808080808080808080808080808080

; 60   :   const __m256i ones    = _mm256_set1_epi8  (0x01);
; 61   : 
; 62   :   __m256i a_signed      = _mm256_sub_epi8   (a,        epu2epi);

	vpsubb	ymm6, ymm3, ymm7
	vmovdqu	ymm3, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101
	vmovaps	XMMWORD PTR [rsp+48], xmm8
	vmovaps	XMMWORD PTR [rsp+32], xmm9

; 128  :          *res_lo        = _mm256_maddubs_epi16(composite_lo, invmask);

	vpmaddubsw ymm9, ymm1, YMMWORD PTR __ymm@ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01
	vmovaps	XMMWORD PTR [rsp+16], xmm10

; 129  :          *res_hi        = _mm256_maddubs_epi16(composite_hi, invmask);

	vpmaddubsw ymm10, ymm0, YMMWORD PTR __ymm@ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vmovdqu	ymm0, YMMWORD PTR [rdx]
	vpsubb	ymm1, ymm0, ymm7
	vmovdqu	ymm0, YMMWORD PTR [rcx]

; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm2, ymm6, ymm1

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vpsubb	ymm1, ymm0, ymm7

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm4, ymm3, ymm2

; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm2, ymm6, ymm1

; 75   :   const __m256i idx_to_cat = _mm256_setr_epi64x(0x0403000201, 0,

	vmovdqu	ymm1, YMMWORD PTR __ymm@0000000000000000000000040300020100000000000000000000000403000201

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm3, ymm3, ymm2
	vmovaps	XMMWORD PTR [rsp], xmm11

; 229  :   const __m256i negate_hiword = _mm256_set1_epi32(0xffff0001);

	vmovdqu	ymm11, YMMWORD PTR __ymm@ffff0001ffff0001ffff0001ffff0001ffff0001ffff0001ffff0001ffff0001

; 81   :   __m256i signsum          = _mm256_add_epi8    (c_a_sign,   c_b_sign);

	vpaddb	ymm0, ymm3, ymm4

; 82   :   __m256i eo_idx           = _mm256_add_epi8    (signsum,    twos);

	vpaddb	ymm4, ymm0, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202

; 83   : 
; 84   :   return                     _mm256_shuffle_epi8(idx_to_cat, eo_idx);

	vpshufb	ymm2, ymm1, ymm4

; 275  :           eo_cat = _mm256_or_si256    (eo_cat,      badbyte_mask);

	vpor	ymm3, ymm2, YMMWORD PTR [rax]

; 276  :   __m256i offset = _mm256_shuffle_epi8(offsets_256, eo_cat);

	mov	rax, QWORD PTR offsets_256$[rsp]
	vpxor	xmm8, xmm8, xmm8
	vmovdqu	ymm0, YMMWORD PTR [rax]
	vpshufb	ymm2, ymm0, ymm3

; 113  :         __m256i signs = _mm256_cmpgt_epi8   (zero, v);

	vpcmpgtb ymm1, ymm8, ymm2

; 114  :              *res_lo  = _mm256_unpacklo_epi8(v,    signs);

	vpunpcklbw ymm0, ymm2, ymm1

; 236  :   __m256i offsets_0_lo = _mm256_cmpeq_epi16   (offsets_lo,   zero);

	vpcmpeqw ymm3, ymm0, ymm8

; 115  :              *res_hi  = _mm256_unpackhi_epi8(v,    signs);

	vpunpckhbw ymm1, ymm2, ymm1

; 237  :   __m256i offsets_0_hi = _mm256_cmpeq_epi16   (offsets_hi,   zero);

	vpcmpeqw ymm4, ymm1, ymm8

; 283  : 
; 284  :   return calc_diff_off_delta(diff_lo, diff_hi, offset, orig);
; 285  : }

	vmovaps	xmm8, XMMWORD PTR [rsp+48]

; 239  :   __m256i delta_lo     = _mm256_sub_epi16     (diff_lo,      offsets_lo);

	vpsubw	ymm0, ymm9, ymm0

; 244  :   __m256i delta_lo_m   = _mm256_andnot_si256  (offsets_0_lo, delta_lo);

	vpandn	ymm2, ymm3, ymm0
	vpandn	ymm5, ymm3, ymm9

; 283  : 
; 284  :   return calc_diff_off_delta(diff_lo, diff_hi, offset, orig);
; 285  : }

	vmovaps	xmm9, XMMWORD PTR [rsp+32]

; 243  :   __m256i diff_hi_m    = _mm256_andnot_si256  (offsets_0_hi, diff_hi);

	vpandn	ymm7, ymm4, ymm10

; 246  : 
; 247  :   __m256i dd0_lo       = _mm256_unpacklo_epi16(delta_lo_m,   diff_lo_m);

	vpunpcklwd ymm3, ymm2, ymm5
	vpsubw	ymm1, ymm10, ymm1

; 283  : 
; 284  :   return calc_diff_off_delta(diff_lo, diff_hi, offset, orig);
; 285  : }

	vmovaps	xmm10, XMMWORD PTR [rsp+16]

; 245  :   __m256i delta_hi_m   = _mm256_andnot_si256  (offsets_0_hi, delta_hi);

	vpandn	ymm0, ymm4, ymm1

; 248  :   __m256i dd0_hi       = _mm256_unpackhi_epi16(delta_lo_m,   diff_lo_m);
; 249  :   __m256i dd1_lo       = _mm256_unpacklo_epi16(delta_hi_m,   diff_hi_m);

	vpunpcklwd ymm6, ymm0, ymm7

; 250  :   __m256i dd1_hi       = _mm256_unpackhi_epi16(delta_hi_m,   diff_hi_m);

	vpunpckhwd ymm7, ymm0, ymm7
	vpunpckhwd ymm4, ymm2, ymm5

; 251  : 
; 252  :   __m256i dd0_lo_n     = _mm256_sign_epi16    (dd0_lo,       negate_hiword);
; 253  :   __m256i dd0_hi_n     = _mm256_sign_epi16    (dd0_hi,       negate_hiword);

	vpsignw	ymm1, ymm4, ymm11

; 254  :   __m256i dd1_lo_n     = _mm256_sign_epi16    (dd1_lo,       negate_hiword);
; 255  :   __m256i dd1_hi_n     = _mm256_sign_epi16    (dd1_hi,       negate_hiword);
; 256  : 
; 257  :   __m256i sum0_lo      = _mm256_madd_epi16    (dd0_lo,       dd0_lo_n);
; 258  :   __m256i sum0_hi      = _mm256_madd_epi16    (dd0_hi,       dd0_hi_n);

	vpmaddwd ymm2, ymm1, ymm4
	vpsignw	ymm0, ymm3, ymm11
	vpmaddwd ymm3, ymm0, ymm3
	vpsignw	ymm0, ymm6, ymm11
	vpsignw	ymm1, ymm7, ymm11

; 283  : 
; 284  :   return calc_diff_off_delta(diff_lo, diff_hi, offset, orig);
; 285  : }

	vmovaps	xmm11, XMMWORD PTR [rsp]

; 259  :   __m256i sum1_lo      = _mm256_madd_epi16    (dd1_lo,       dd1_lo_n);

	vpmaddwd ymm4, ymm0, ymm6

; 283  : 
; 284  :   return calc_diff_off_delta(diff_lo, diff_hi, offset, orig);
; 285  : }

	vmovaps	xmm6, XMMWORD PTR [rsp+80]

; 262  :   __m256i sum0         = _mm256_add_epi32     (sum0_lo,      sum0_hi);

	vpaddd	ymm5, ymm2, ymm3
	vpmaddwd ymm2, ymm1, ymm7

; 283  : 
; 284  :   return calc_diff_off_delta(diff_lo, diff_hi, offset, orig);
; 285  : }

	vmovaps	xmm7, XMMWORD PTR [rsp+64]

; 263  :   __m256i sum1         = _mm256_add_epi32     (sum1_lo,      sum1_hi);

	vpaddd	ymm0, ymm2, ymm4

; 264  :   return                 _mm256_add_epi32     (sum0,         sum1);

	vpaddd	ymm0, ymm0, ymm5

; 283  : 
; 284  :   return calc_diff_off_delta(diff_lo, diff_hi, offset, orig);
; 285  : }

	add	rsp, 104				; 00000068H
	ret	0
do_one_edge_ymm ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\sao_shared_generics.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
y$1$ = 0
width_rest$1$ = 0
tv3112 = 4
tv3095 = 4
tv3152 = 8
y$1$ = 8
tv3101 = 12
tv3132 = 16
b_ofs$1 = 24
b_ofs$ = 24
width_db32$1$ = 32
width_db4$1$ = 36
curr_cpos$1$ = 40
tv3149 = 44
curr_apos$1$ = 48
b_ofs$1$ = 56
curr_bpos$1$ = 56
tv3148 = 64
a_ofs$2 = 64
a_ofs$1$ = 72
a_ofs$ = 80
sum$1$ = 96
offsets_256$1$ = 128
db4_mask$1$ = 160
orig_data$ = 448
rec_data$ = 456
block_width$ = 464
block_height$ = 472
eo_class$ = 480
offsets$ = 488
sao_edge_ddistortion_avx2 PROC

; 293  : {

	mov	rax, rsp
	mov	QWORD PTR [rax+16], rbx
	mov	DWORD PTR [rax+24], r8d
	mov	QWORD PTR [rax+8], rcx
	push	rbp
	push	rsi
	push	rdi
	push	r12
	push	r13
	push	r14
	push	r15
	sub	rsp, 384				; 00000180H
	vmovaps	XMMWORD PTR [rax-72], xmm6
	vmovaps	XMMWORD PTR [rax-88], xmm7
	vmovaps	XMMWORD PTR [rax-104], xmm8
	vmovaps	XMMWORD PTR [rax-120], xmm9
	vmovaps	XMMWORD PTR [rax-136], xmm10
	vmovaps	XMMWORD PTR [rax-152], xmm11
	vmovaps	XMMWORD PTR [rax-168], xmm12
	vmovaps	XMMWORD PTR [rax-184], xmm13
	vmovaps	XMMWORD PTR [rax-200], xmm14
	vmovaps	XMMWORD PTR [rax-216], xmm15
	lea	rbp, QWORD PTR [rsp+32]
	and	rbp, -32				; ffffffffffffffe0H
	mov	r13d, r8d

; 294  :   int32_t y, x;
; 295  :   vector2d_t a_ofs = g_sao_edge_offsets[eo_class][0];

	lea	rax, OFFSET FLAT:__ImageBase
	mov	rsi, rcx

; 296  :   vector2d_t b_ofs = g_sao_edge_offsets[eo_class][1];
; 297  : 
; 298  :    int32_t scan_width = block_width -   2;
; 299  :   uint32_t width_db32 = scan_width  & ~31;
; 300  :   uint32_t width_db4  = scan_width  &  ~3;
; 301  :   uint32_t width_rest = scan_width  &   3;
; 302  : 
; 303  :   // Form the load&store mask
; 304  :   const __m256i wdb4_256      = _mm256_set1_epi32 (width_db4 & 31);
; 305  :   const __m256i indexes       = _mm256_setr_epi32 (3, 7, 11, 15, 19, 23, 27, 31);
; 306  :   const __m256i db4_mask      = _mm256_cmpgt_epi32(wdb4_256, indexes);
; 307  : 
; 308  :   const __m256i zero          = _mm256_setzero_si256();
; 309  : 
; 310  :   __m128i offsets03 = _mm_loadu_si128((const __m128i *)offsets);
; 311  :   __m128i offsets4  = _mm_cvtsi32_si128(offsets[4]);
; 312  : 
; 313  :   uint16_t offsets_ok = epi32v_fits_in_epi8s(offsets03) &

	lea	ebx, DWORD PTR [r9-1]
	movsxd	rcx, DWORD PTR eo_class$[rsp]
	mov	rdi, rdx
	add	rcx, rcx
	mov	DWORD PTR tv3132[rbp], ebx
	lea	r15d, DWORD PTR [r13-2]
	and	r15d, -32				; ffffffe0H
	lea	r11d, DWORD PTR [r13-2]
	and	r11d, 3
	mov	DWORD PTR width_db32$1$[rbp], r15d
	mov	r10, QWORD PTR g_sao_edge_offsets[rax+rcx*8]
	mov	rcx, QWORD PTR g_sao_edge_offsets[rax+rcx*8+8]
	mov	r8, r10
	lea	eax, DWORD PTR [r13-2]
	shr	r8, 32					; 00000020H
	and	eax, -4					; fffffffcH
	mov	QWORD PTR a_ofs$1$[rbp], r10
	mov	DWORD PTR width_db4$1$[rbp], eax
	and	eax, 31
	vmovd	xmm0, eax
	mov	rax, QWORD PTR offsets$[rsp]
	vpbroadcastd ymm0, xmm0
	vpcmpgtd ymm14, ymm0, YMMWORD PTR __ymm@0000001f0000001b00000017000000130000000f0000000b0000000700000003
	mov	QWORD PTR a_ofs$[rbp], r10
	vmovd	xmm4, DWORD PTR [rax+16]
	vmovdqu	xmm3, XMMWORD PTR [rax]

; 161  :   __m128i  v_ms25b = _mm_srai_epi32   (v,  7);

	vpsrad	xmm0, xmm4, 7

; 162  :   __m128i  v_signs = _mm_srai_epi32   (v, 31);

	vpsrad	xmm1, xmm4, 31

; 163  :   __m128i  ok_i32s = _mm_cmpeq_epi32  (v_ms25b, v_signs);

	vpcmpeqd xmm1, xmm1, xmm0

; 164  :   return             _mm_movemask_epi8(ok_i32s);

	vpmovmskb edx, xmm1

; 161  :   __m128i  v_ms25b = _mm_srai_epi32   (v,  7);

	vpsrad	xmm0, xmm3, 7

; 162  :   __m128i  v_signs = _mm_srai_epi32   (v, 31);

	vpsrad	xmm2, xmm3, 31

; 163  :   __m128i  ok_i32s = _mm_cmpeq_epi32  (v_ms25b, v_signs);

	vpcmpeqd xmm1, xmm2, xmm0

; 164  :   return             _mm_movemask_epi8(ok_i32s);

	vpmovmskb eax, xmm1

; 296  :   vector2d_t b_ofs = g_sao_edge_offsets[eo_class][1];
; 297  : 
; 298  :    int32_t scan_width = block_width -   2;
; 299  :   uint32_t width_db32 = scan_width  & ~31;
; 300  :   uint32_t width_db4  = scan_width  &  ~3;
; 301  :   uint32_t width_rest = scan_width  &   3;
; 302  : 
; 303  :   // Form the load&store mask
; 304  :   const __m256i wdb4_256      = _mm256_set1_epi32 (width_db4 & 31);
; 305  :   const __m256i indexes       = _mm256_setr_epi32 (3, 7, 11, 15, 19, 23, 27, 31);
; 306  :   const __m256i db4_mask      = _mm256_cmpgt_epi32(wdb4_256, indexes);
; 307  : 
; 308  :   const __m256i zero          = _mm256_setzero_si256();
; 309  : 
; 310  :   __m128i offsets03 = _mm_loadu_si128((const __m128i *)offsets);
; 311  :   __m128i offsets4  = _mm_cvtsi32_si128(offsets[4]);
; 312  : 
; 313  :   uint16_t offsets_ok = epi32v_fits_in_epi8s(offsets03) &

	and	dx, ax
	mov	QWORD PTR b_ofs$1$[rbp], rcx

; 314  :                         epi32v_fits_in_epi8s(offsets4);
; 315  : 
; 316  :   assert(NUM_SAO_EDGE_CATEGORIES == 5);
; 317  : 
; 318  :   if (offsets_ok != 0xffff) {

	mov	eax, 65535				; 0000ffffH
	mov	QWORD PTR b_ofs$[rbp], rcx
	mov	DWORD PTR width_rest$1$[rbp], r11d
	vmovdqu	YMMWORD PTR db4_mask$1$[rbp], ymm14
	vpxor	xmm13, xmm13, xmm13
	cmp	dx, ax
	je	$LN8@sao_edge_d
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\sao_shared_generics.h

; 60   :   int32_t sum = 0;

	mov	r9d, 1

; 61   :   vector2d_t a_ofs = g_sao_edge_offsets[eo_class][0];

	mov	QWORD PTR a_ofs$2[rbp], r10
	xor	r12d, r12d

; 62   :   vector2d_t b_ofs = g_sao_edge_offsets[eo_class][1];

	mov	QWORD PTR b_ofs$1[rbp], rcx
	mov	DWORD PTR y$1$[rbp], r9d

; 63   : 
; 64   :   for (y = 1; y < block_height - 1; y++) {

	cmp	ebx, r9d
	jle	$LN17@sao_edge_d
	lea	r11d, DWORD PTR [r13-1]
	mov	DWORD PTR tv3152[rbp], r11d
$LL18@sao_edge_d:

; 65   :     for (x = 1; x < block_width - 1; x++) {

	cmp	r11d, 1
	jle	$LN16@sao_edge_d
	mov	r8d, DWORD PTR b_ofs$1[rbp+4]
	lea	r15d, DWORD PTR [r11-1]
	mov	eax, DWORD PTR a_ofs$2[rbp+4]
	add	r8d, r9d
	sub	eax, r8d
	mov	r14d, r9d
	sub	r14d, r8d
	add	eax, r9d
	imul	r14d, r13d
	mov	edx, r8d
	imul	edx, r13d
	imul	r13d, eax
	sub	r14d, ecx
	inc	edx
	add	edx, ecx
	sub	r13d, ecx
	mov	rcx, QWORD PTR orig_data$[rsp]
	add	r13d, r10d
	mov	r10, QWORD PTR offsets$[rsp]
	npad	10
$LL21@sao_edge_d:

; 47   :   int eo_idx = 2 + SIGN3((int)c - (int)a) + SIGN3((int)c - (int)b);

	movzx	r8d, BYTE PTR [rdx+rdi]

; 72   :       uint8_t   c    =  rec_data[c_pos];

	lea	eax, DWORD PTR [r14+rdx]

; 47   :   int eo_idx = 2 + SIGN3((int)c - (int)a) + SIGN3((int)c - (int)b);

	movzx	ebx, BYTE PTR [rax+rdi]
	xor	r9d, r9d

; 72   :       uint8_t   c    =  rec_data[c_pos];

	mov	esi, eax

; 47   :   int eo_idx = 2 + SIGN3((int)c - (int)a) + SIGN3((int)c - (int)b);

	mov	r11d, ebx
	sub	r11d, r8d

; 70   :       uint8_t   a    =  rec_data[a_pos];

	lea	eax, DWORD PTR [rdx+r13]

; 47   :   int eo_idx = 2 + SIGN3((int)c - (int)a) + SIGN3((int)c - (int)b);

	movzx	eax, BYTE PTR [rax+rdi]
	mov	r8d, ebx
	sub	r8d, eax
	mov	eax, r11d
	test	r8d, r8d
	setg	r9b
	shr	eax, 31
	add	r9d, 2
	shr	r8d, 31
	sub	r9d, r8d

; 76   :       int32_t offset = offsets[eo_cat];

	lea	r8, OFFSET FLAT:__ImageBase

; 47   :   int eo_idx = 2 + SIGN3((int)c - (int)a) + SIGN3((int)c - (int)b);

	sub	r9d, eax
	xor	eax, eax
	test	r11d, r11d
	setg	al
	add	eax, r9d

; 48   : 
; 49   :   return sao_eo_idx_to_eo_category[eo_idx];

	cdqe

; 76   :       int32_t offset = offsets[eo_cat];

	movsxd	rax, DWORD PTR ?sao_eo_idx_to_eo_category@?1??sao_calc_eo_cat@@9@9[r8+rax*4]
	mov	r9d, DWORD PTR [r10+rax*4]

; 77   : 
; 78   :       if (offset != 0) {

	test	r9d, r9d
	je	SHORT $LN19@sao_edge_d

; 79   :         int32_t diff   = orig - c;

	movzx	r8d, BYTE PTR [rsi+rcx]
	sub	r8d, ebx

; 80   :         int32_t delta  = diff - offset;

	mov	eax, r8d

; 81   :         int32_t curr   = delta * delta - diff * diff;

	imul	r8d, r8d
	sub	eax, r9d
	imul	eax, eax

; 82   : 
; 83   :         sum += curr;

	sub	eax, r8d
	add	r12d, eax
$LN19@sao_edge_d:

; 65   :     for (x = 1; x < block_width - 1; x++) {

	inc	edx
	sub	r15, 1
	jne	$LL21@sao_edge_d
	mov	rcx, QWORD PTR b_ofs$1$[rbp]
	mov	r10, QWORD PTR a_ofs$1$[rbp]
	mov	r13d, DWORD PTR block_width$[rsp]
	mov	r9d, DWORD PTR y$1$[rbp]
	mov	r11d, DWORD PTR tv3152[rbp]
	mov	ebx, DWORD PTR tv3132[rbp]
$LN16@sao_edge_d:

; 63   : 
; 64   :   for (y = 1; y < block_height - 1; y++) {

	inc	r9d
	mov	DWORD PTR y$1$[rbp], r9d
	cmp	r9d, ebx
	jl	$LL18@sao_edge_d
$LN17@sao_edge_d:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c

; 319  :     return sao_edge_ddistortion_generic(orig_data,

	mov	eax, r12d
	jmp	$LN1@sao_edge_d
$LN8@sao_edge_d:

; 172  :         __m128i sbs_8          = _mm_shuffle_epi8(v, trunc_shufmask);

	vpshufb	xmm1, xmm3, XMMWORD PTR __xmm@0c0804000c0804000c0804000c080400
	vpshufb	xmm0, xmm4, XMMWORD PTR __xmm@0c0804000c0804000c0804000c080400

; 320  :                                         rec_data,
; 321  :                                         block_width,
; 322  :                                         block_height,
; 323  :                                         eo_class,
; 324  :                                         offsets);
; 325  :   }
; 326  : 
; 327  :   __m128i offsets03_8b = truncate_epi32_epi8(offsets03);
; 328  :   __m128i offsets4_8b  = truncate_epi32_epi8(offsets4);
; 329  :   __m128i offsets_8b   = _mm_unpacklo_epi32 (offsets03_8b, offsets4_8b);

	vpunpckldq xmm2, xmm1, xmm0

; 218  :   __m256i res = _mm256_castsi128_si256 (v);

	vmovups	xmm1, xmm2

; 171  :   const __m128i trunc_shufmask = _mm_set1_epi32  (0x0c080400);

	mov	r12d, 1
	mov	DWORD PTR y$1$[rbp], r12d
	mov	r14d, r12d

; 219  :   return        _mm256_inserti128_si256(res, v, 1);

	vinserti128 ymm5, ymm1, xmm2, 1
	vpxor	xmm1, xmm2, xmm2

; 330  :   __m256i offsets_256  = broadcast_xmm2ymm  (offsets_8b);
; 331  : 
; 332  :   __m256i sum = _mm256_setzero_si256();

	vmovdqu	YMMWORD PTR sum$1$[rbp], ymm1

; 219  :   return        _mm256_inserti128_si256(res, v, 1);

	vmovdqu	YMMWORD PTR offsets_256$1$[rbp], ymm5

; 333  :   for (y = 1; y < block_height - 1; y++) {

	cmp	ebx, r12d
	jle	$LN3@sao_edge_d
	inc	r15d

; 209  :   uint32_t last_badbytes = 0xffffffff << (width_rest << 3);

	lea	r9d, DWORD PTR [r8+1]
	imul	r9d, r13d
	lea	eax, DWORD PTR [r11*8]
	mov	DWORD PTR tv3149[rbp], eax
	mov	DWORD PTR tv3148[rbp], r15d
	add	r9d, r10d
	mov	DWORD PTR tv3101[rbp], r9d
	npad	8
$LL4@sao_edge_d:

; 334  :     for (x = 1; x < width_db32 + 1; x += 32) {

	mov	ebx, r13d
	mov	r8d, r12d
	imul	ebx, r14d
	cmp	r15d, r12d
	jbe	$LN125@sao_edge_d
	mov	edx, DWORD PTR b_ofs$[rbp+4]
	lea	r8d, DWORD PTR [r15-2]
	vmovdqu	ymm10, YMMWORD PTR __ymm@8080808080808080808080808080808080808080808080808080808080808080
	vmovdqu	ymm11, YMMWORD PTR __ymm@ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01
	vmovdqu	ymm9, YMMWORD PTR __ymm@ffff0001ffff0001ffff0001ffff0001ffff0001ffff0001ffff0001ffff0001
	vmovdqu	ymm12, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101
	vmovdqu	ymm14, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202
	vmovdqu	ymm15, YMMWORD PTR __ymm@0000000000000000000000040300020100000000000000000000000403000201
	add	edx, r14d
	shr	r8d, 5
	inc	r8d
	lea	r11d, DWORD PTR [rcx+1]
	mov	r15d, r8d
	mov	eax, edx
	imul	eax, r13d
	mov	r9d, r14d
	sub	r9d, edx
	shl	r8d, 5
	imul	r9d, r13d
	add	r11d, eax
	mov	eax, DWORD PTR a_ofs$[rbp+4]
	sub	eax, edx
	add	eax, r14d
	sub	r9d, ecx
	imul	eax, r13d
	sub	eax, ecx
	add	eax, r10d
	inc	r8d
	mov	r10d, eax
	npad	4
$LL7@sao_edge_d:

; 340  :       __m256i b      = _mm256_loadu_si256((const __m256i *)(rec_data  + b_pos));
; 341  :       __m256i c      = _mm256_loadu_si256((const __m256i *)(rec_data  + c_pos));

	lea	eax, DWORD PTR [r9+r11]

; 342  :       __m256i orig   = _mm256_loadu_si256((const __m256i *)(orig_data + c_pos));
; 343  : 
; 344  :       __m256i curr   = do_one_edge_ymm(a, b, c, orig, zero, offsets_256);

	vmovdqu	ymm2, YMMWORD PTR [rax+rdi]

; 282  :   diff_epi8_epi16(orig, c, &diff_lo, &diff_hi);

	vmovdqu	ymm1, YMMWORD PTR [rax+rsi]

; 335  :       uint32_t c_pos =  y            * block_width + x;
; 336  :       uint32_t a_pos = (y + a_ofs.y) * block_width + x + a_ofs.x;
; 337  :       uint32_t b_pos = (y + b_ofs.y) * block_width + x + b_ofs.x;
; 338  : 
; 339  :       __m256i a      = _mm256_loadu_si256((const __m256i *)(rec_data  + a_pos));

	lea	eax, DWORD PTR [r10+r11]

; 126  :   __m256i composite_hi  = _mm256_unpackhi_epi8(a, b);

	vpunpckhbw ymm0, ymm1, ymm2
	vpunpcklbw ymm1, ymm1, ymm2

; 129  :          *res_hi        = _mm256_maddubs_epi16(composite_hi, invmask);

	vpmaddubsw ymm8, ymm0, ymm11

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vmovdqu	ymm0, YMMWORD PTR [r11+rdi]

; 334  :     for (x = 1; x < width_db32 + 1; x += 32) {

	lea	r11d, DWORD PTR [r11+32]

; 128  :          *res_lo        = _mm256_maddubs_epi16(composite_lo, invmask);

	vpmaddubsw ymm6, ymm1, ymm11

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vpsubb	ymm1, ymm0, ymm10
	vmovdqu	ymm0, YMMWORD PTR [rax+rdi]

; 62   :   __m256i a_signed      = _mm256_sub_epi8   (a,        epu2epi);

	vpsubb	ymm3, ymm2, ymm10

; 64   : 
; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm2, ymm3, ymm1

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vpsubb	ymm1, ymm0, ymm10

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm4, ymm12, ymm2

; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm2, ymm3, ymm1

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm3, ymm12, ymm2

; 81   :   __m256i signsum          = _mm256_add_epi8    (c_a_sign,   c_b_sign);

	vpaddb	ymm0, ymm3, ymm4

; 82   :   __m256i eo_idx           = _mm256_add_epi8    (signsum,    twos);

	vpaddb	ymm1, ymm0, ymm14

; 83   : 
; 84   :   return                     _mm256_shuffle_epi8(idx_to_cat, eo_idx);

	vpshufb	ymm2, ymm15, ymm1

; 275  :           eo_cat = _mm256_or_si256    (eo_cat,      badbyte_mask);

	vpor	ymm3, ymm2, ymm13

; 276  :   __m256i offset = _mm256_shuffle_epi8(offsets_256, eo_cat);

	vpshufb	ymm4, ymm5, ymm3

; 113  :         __m256i signs = _mm256_cmpgt_epi8   (zero, v);

	vpcmpgtb ymm0, ymm13, ymm4

; 115  :              *res_hi  = _mm256_unpackhi_epi8(v,    signs);

	vpunpckhbw ymm5, ymm4, ymm0
	vpunpcklbw ymm2, ymm4, ymm0

; 236  :   __m256i offsets_0_lo = _mm256_cmpeq_epi16   (offsets_lo,   zero);

	vpcmpeqw ymm1, ymm2, ymm13

; 237  :   __m256i offsets_0_hi = _mm256_cmpeq_epi16   (offsets_hi,   zero);
; 238  : 
; 239  :   __m256i delta_lo     = _mm256_sub_epi16     (diff_lo,      offsets_lo);
; 240  :   __m256i delta_hi     = _mm256_sub_epi16     (diff_hi,      offsets_hi);
; 241  : 
; 242  :   __m256i diff_lo_m    = _mm256_andnot_si256  (offsets_0_lo, diff_lo);

	vpandn	ymm4, ymm1, ymm6
	vpsubw	ymm0, ymm6, ymm2

; 243  :   __m256i diff_hi_m    = _mm256_andnot_si256  (offsets_0_hi, diff_hi);
; 244  :   __m256i delta_lo_m   = _mm256_andnot_si256  (offsets_0_lo, delta_lo);

	vpandn	ymm2, ymm1, ymm0
	vpcmpeqw ymm3, ymm5, ymm13
	vpandn	ymm7, ymm3, ymm8
	vpsubw	ymm1, ymm8, ymm5

; 245  :   __m256i delta_hi_m   = _mm256_andnot_si256  (offsets_0_hi, delta_hi);

	vpandn	ymm0, ymm3, ymm1

; 246  : 
; 247  :   __m256i dd0_lo       = _mm256_unpacklo_epi16(delta_lo_m,   diff_lo_m);

	vpunpcklwd ymm3, ymm2, ymm4

; 248  :   __m256i dd0_hi       = _mm256_unpackhi_epi16(delta_lo_m,   diff_lo_m);
; 249  :   __m256i dd1_lo       = _mm256_unpacklo_epi16(delta_hi_m,   diff_hi_m);

	vpunpcklwd ymm6, ymm0, ymm7

; 250  :   __m256i dd1_hi       = _mm256_unpackhi_epi16(delta_hi_m,   diff_hi_m);

	vpunpckhwd ymm7, ymm0, ymm7
	vpunpckhwd ymm4, ymm2, ymm4

; 251  : 
; 252  :   __m256i dd0_lo_n     = _mm256_sign_epi16    (dd0_lo,       negate_hiword);

	vpsignw	ymm0, ymm3, ymm9

; 253  :   __m256i dd0_hi_n     = _mm256_sign_epi16    (dd0_hi,       negate_hiword);
; 254  :   __m256i dd1_lo_n     = _mm256_sign_epi16    (dd1_lo,       negate_hiword);
; 255  :   __m256i dd1_hi_n     = _mm256_sign_epi16    (dd1_hi,       negate_hiword);
; 256  : 
; 257  :   __m256i sum0_lo      = _mm256_madd_epi16    (dd0_lo,       dd0_lo_n);

	vpmaddwd ymm3, ymm0, ymm3
	vpsignw	ymm1, ymm4, ymm9

; 258  :   __m256i sum0_hi      = _mm256_madd_epi16    (dd0_hi,       dd0_hi_n);

	vpmaddwd ymm2, ymm1, ymm4

; 259  :   __m256i sum1_lo      = _mm256_madd_epi16    (dd1_lo,       dd1_lo_n);
; 260  :   __m256i sum1_hi      = _mm256_madd_epi16    (dd1_hi,       dd1_hi_n);
; 261  : 
; 262  :   __m256i sum0         = _mm256_add_epi32     (sum0_lo,      sum0_hi);

	vpaddd	ymm5, ymm2, ymm3
	vpsignw	ymm0, ymm6, ymm9
	vpsignw	ymm1, ymm7, ymm9
	vpmaddwd ymm2, ymm1, ymm7
	vpmaddwd ymm4, ymm0, ymm6

; 263  :   __m256i sum1         = _mm256_add_epi32     (sum1_lo,      sum1_hi);

	vpaddd	ymm0, ymm2, ymm4

; 264  :   return                 _mm256_add_epi32     (sum0,         sum1);

	vpaddd	ymm3, ymm0, ymm5

; 345  :               sum    = _mm256_add_epi32(sum, curr);

	vpaddd	ymm1, ymm3, YMMWORD PTR sum$1$[rbp]
	vmovdqu	ymm5, YMMWORD PTR offsets_256$1$[rbp]
	vmovdqu	YMMWORD PTR sum$1$[rbp], ymm1
	sub	r15, r12
	jne	$LL7@sao_edge_d
	mov	r10, QWORD PTR a_ofs$1$[rbp]
	vmovdqu	ymm14, YMMWORD PTR db4_mask$1$[rbp]
	mov	r11d, DWORD PTR width_rest$1$[rbp]
	mov	r9d, DWORD PTR tv3101[rbp]
$LN125@sao_edge_d:

; 346  :     }
; 347  :     if (scan_width > width_db32) {

	lea	eax, DWORD PTR [r13-2]
	cmp	eax, DWORD PTR width_db32$1$[rbp]
	jbe	$LN2@sao_edge_d

; 348  :       const uint32_t curr_cpos   =  y            * block_width + x;
; 349  :       const uint32_t rest_cpos   =  y            * block_width + width_db4 + 1;

	mov	esi, DWORD PTR width_db4$1$[rbp]
	lea	eax, DWORD PTR [rbx+r8]
	mov	DWORD PTR curr_cpos$1$[rbp], eax

; 181  :   uint32_t last_dword = 0;

	xor	r15d, r15d

; 348  :       const uint32_t curr_cpos   =  y            * block_width + x;
; 349  :       const uint32_t rest_cpos   =  y            * block_width + width_db4 + 1;

	lea	eax, DWORD PTR [rsi+1]
	add	eax, ebx

; 350  : 
; 351  :       const  int32_t curr_apos   = (y + a_ofs.y) * block_width + x + a_ofs.x;
; 352  :       const  int32_t rest_apos   = (y + a_ofs.y) * block_width + width_db4 + a_ofs.x + 1;
; 353  : 
; 354  :       const  int32_t curr_bpos   = (y + b_ofs.y) * block_width + x + b_ofs.x;

	mov	ebx, DWORD PTR b_ofs$[rbp+4]
	add	ebx, r14d
	mov	DWORD PTR tv3095[rbp], eax
	imul	ebx, r13d
	lea	eax, DWORD PTR [r9+r8]
	mov	DWORD PTR curr_apos$1$[rbp], eax
	add	ebx, ecx
	lea	eax, DWORD PTR [rbx+r8]
	mov	DWORD PTR curr_bpos$1$[rbp], eax

; 182  :   for (int32_t i = 0; i < width_rest; i++) {

	test	r11d, r11d
	je	$LN62@sao_edge_d
	lea	eax, DWORD PTR [r9+rsi]
	xor	r8d, r8d
	movsxd	rdx, eax
	lea	r9, QWORD PTR [rdi+1]
	add	r9, rdx
$LL108@sao_edge_d:

; 183  :     uint8_t  currb = buf[start_pos + i];
; 184  :     uint32_t currd = ((uint32_t)currb) << (i * 8);

	movzx	eax, BYTE PTR [r9]
	lea	r9, QWORD PTR [r9+1]
	shlx	edx, eax, r8d

; 185  :     last_dword |= currd;

	or	r15d, edx
	add	r8d, 8
	sub	r11, r12
	jne	SHORT $LL108@sao_edge_d
	mov	r14d, DWORD PTR width_rest$1$[rbp]
	lea	eax, DWORD PTR [rbx+rsi]
	movsxd	rdx, eax
	lea	r9, QWORD PTR [rdi+1]
	add	r9, rdx
	xor	r8d, r8d
	xor	esi, esi
	mov	r11d, r14d
	npad	14
$LL110@sao_edge_d:

; 184  :     uint32_t currd = ((uint32_t)currb) << (i * 8);

	movzx	eax, BYTE PTR [r9]
	lea	r9, QWORD PTR [r9+1]
	shlx	edx, eax, r8d

; 185  :     last_dword |= currd;

	or	esi, edx
	add	r8d, 8
	sub	r11, r12
	jne	SHORT $LL110@sao_edge_d
	movsxd	r9, DWORD PTR tv3095[rbp]
	xor	r8d, r8d
	add	r9, rdi
	mov	r11, r14
	xor	ebx, ebx
	npad	9
$LL112@sao_edge_d:

; 184  :     uint32_t currd = ((uint32_t)currb) << (i * 8);

	movzx	eax, BYTE PTR [r9]
	lea	r9, QWORD PTR [r9+1]
	shlx	edx, eax, r8d

; 185  :     last_dword |= currd;

	or	ebx, edx
	add	r8d, 8
	sub	r11, r12
	jne	SHORT $LL112@sao_edge_d
	movsxd	r9, DWORD PTR tv3095[rbp]
	xor	r8d, r8d
	add	r9, QWORD PTR orig_data$[rsp]
	npad	9
$LL114@sao_edge_d:

; 184  :     uint32_t currd = ((uint32_t)currb) << (i * 8);

	movzx	eax, BYTE PTR [r9]
	lea	r9, QWORD PTR [r9+1]
	shlx	edx, eax, r8d

; 185  :     last_dword |= currd;

	or	r11d, edx
	add	r8d, 8
	sub	r14, r12
	jne	SHORT $LL114@sao_edge_d

; 182  :   for (int32_t i = 0; i < width_rest; i++) {

	mov	r14d, DWORD PTR y$1$[rbp]
	mov	r9d, DWORD PTR tv3101[rbp]
	jmp	SHORT $LN67@sao_edge_d
$LN62@sao_edge_d:

; 173  :   return        sbs_8;
; 174  : }
; 175  : 
; 176  : // Read 0-3 bytes (pixels) into uint32_t
; 177  : static INLINE uint32_t load_border_bytes(const uint8_t *buf,
; 178  :                                          const int32_t  start_pos,
; 179  :                                          const int32_t  width_rest)
; 180  : {
; 181  :   uint32_t last_dword = 0;

	xor	r11d, r11d
	xor	esi, esi
	xor	ebx, ebx
$LN67@sao_edge_d:

; 364  :       const int32_t *b_ptr    = (const int32_t *)(rec_data  + curr_bpos);
; 365  :       const int32_t *c_ptr    = (const int32_t *)(rec_data  + curr_cpos);

	mov	eax, DWORD PTR curr_cpos$1$[rbp]

; 59   :   const __m256i epu2epi = _mm256_set1_epi8  (0x80);

	vmovdqu	ymm9, YMMWORD PTR __ymm@8080808080808080808080808080808080808080808080808080808080808080

; 229  :   const __m256i negate_hiword = _mm256_set1_epi32(0xffff0001);

	vmovdqu	ymm12, YMMWORD PTR __ymm@ffff0001ffff0001ffff0001ffff0001ffff0001ffff0001ffff0001ffff0001

; 60   :   const __m256i ones    = _mm256_set1_epi8  (0x01);

	vmovdqu	ymm7, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101

; 369  :       __m256i b    = _mm256_maskload_epi32(b_ptr,    db4_mask);
; 370  :       __m256i c    = _mm256_maskload_epi32(c_ptr,    db4_mask);

	vpmaskmovd ymm0, ymm14, YMMWORD PTR [rax+rdi]

; 371  :       __m256i orig = _mm256_maskload_epi32(orig_ptr, db4_mask);

	mov	rdx, QWORD PTR orig_data$[rsp]

; 375  :               c    = _mm256_insert_epi32  (c,        c_last,    7);

	vmovd	xmm1, ebx
	vpbroadcastd ymm1, xmm1
	vpblendd ymm4, ymm0, ymm1, 128			; 00000080H
	vpmaskmovd ymm0, ymm14, YMMWORD PTR [rax+rdx]

; 209  :   uint32_t last_badbytes = 0xffffffff << (width_rest << 3);

	mov	eax, DWORD PTR tv3149[rbp]
	mov	edx, -1					; ffffffffH

; 62   :   __m256i a_signed      = _mm256_sub_epi8   (a,        epu2epi);

	vpsubb	ymm8, ymm4, ymm9

; 209  :   uint32_t last_badbytes = 0xffffffff << (width_rest << 3);

	shlx	eax, edx, eax

; 376  :               orig = _mm256_insert_epi32  (orig,     orig_last, 7);

	vmovd	xmm1, r11d
	vpbroadcastd ymm1, xmm1
	vpblendd ymm3, ymm0, ymm1, 128			; 00000080H

; 125  :   __m256i composite_lo  = _mm256_unpacklo_epi8(a, b);

	vpunpcklbw ymm1, ymm3, ymm4

; 127  : 
; 128  :          *res_lo        = _mm256_maddubs_epi16(composite_lo, invmask);

	vpmaddubsw ymm10, ymm1, YMMWORD PTR __ymm@ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01
	vpunpckhbw ymm0, ymm3, ymm4

; 129  :          *res_hi        = _mm256_maddubs_epi16(composite_hi, invmask);

	vpmaddubsw ymm11, ymm0, YMMWORD PTR __ymm@ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01

; 211  :   return                   _mm256_insert_epi32(badbyte_mask, last_badbytes, 7);

	vmovd	xmm1, eax

; 355  :       const  int32_t rest_bpos   = (y + b_ofs.y) * block_width + width_db4 + b_ofs.x + 1;
; 356  : 
; 357  :       // Same trick to read a narrow line as there is in the band SAO routine
; 358  :       uint32_t a_last         = load_border_bytes(rec_data,  rest_apos, width_rest);
; 359  :       uint32_t b_last         = load_border_bytes(rec_data,  rest_bpos, width_rest);
; 360  :       uint32_t c_last         = load_border_bytes(rec_data,  rest_cpos, width_rest);
; 361  :       uint32_t orig_last      = load_border_bytes(orig_data, rest_cpos, width_rest);
; 362  : 
; 363  :       const int32_t *a_ptr    = (const int32_t *)(rec_data  + curr_apos);

	movsxd	rax, DWORD PTR curr_apos$1$[rbp]

; 211  :   return                   _mm256_insert_epi32(badbyte_mask, last_badbytes, 7);

	vpbroadcastd ymm1, xmm1
	vpcmpeqb ymm0, ymm13, ymm14
	vpblendd ymm6, ymm0, ymm1, 128			; 00000080H

; 366  :       const int32_t *orig_ptr = (const int32_t *)(orig_data + curr_cpos);
; 367  : 
; 368  :       __m256i a    = _mm256_maskload_epi32(a_ptr,    db4_mask);

	vpmaskmovd ymm0, ymm14, YMMWORD PTR [rax+rdi]
	movsxd	rax, DWORD PTR curr_bpos$1$[rbp]

; 372  : 
; 373  :               a    = _mm256_insert_epi32  (a,        a_last,    7);

	vmovd	xmm2, r15d
	vpbroadcastd ymm2, xmm2
	vpblendd ymm1, ymm0, ymm2, 128			; 00000080H
	vpmaskmovd ymm0, ymm14, YMMWORD PTR [rax+rdi]

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vpsubb	ymm2, ymm1, ymm9

; 64   : 
; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm3, ymm8, ymm2

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm4, ymm7, ymm3

; 374  :               b    = _mm256_insert_epi32  (b,        b_last,    7);

	vmovd	xmm1, esi
	mov	rsi, QWORD PTR orig_data$[rsp]
	vpbroadcastd ymm1, xmm1
	vpblendd ymm1, ymm0, ymm1, 128			; 00000080H

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vpsubb	ymm2, ymm1, ymm9

; 64   : 
; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm3, ymm8, ymm2

; 75   :   const __m256i idx_to_cat = _mm256_setr_epi64x(0x0403000201, 0,

	vmovdqu	ymm2, YMMWORD PTR __ymm@0000000000000000000000040300020100000000000000000000000403000201

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm0, ymm7, ymm3

; 276  :   __m256i offset = _mm256_shuffle_epi8(offsets_256, eo_cat);

	vmovdqu	ymm3, YMMWORD PTR offsets_256$1$[rbp]

; 81   :   __m256i signsum          = _mm256_add_epi8    (c_a_sign,   c_b_sign);

	vpaddb	ymm1, ymm0, ymm4

; 82   :   __m256i eo_idx           = _mm256_add_epi8    (signsum,    twos);

	vpaddb	ymm4, ymm1, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202

; 83   : 
; 84   :   return                     _mm256_shuffle_epi8(idx_to_cat, eo_idx);

	vpshufb	ymm0, ymm2, ymm4

; 275  :           eo_cat = _mm256_or_si256    (eo_cat,      badbyte_mask);

	vpor	ymm1, ymm0, ymm6

; 276  :   __m256i offset = _mm256_shuffle_epi8(offsets_256, eo_cat);

	vpshufb	ymm3, ymm3, ymm1

; 113  :         __m256i signs = _mm256_cmpgt_epi8   (zero, v);

	vpcmpgtb ymm0, ymm13, ymm3

; 114  :              *res_lo  = _mm256_unpacklo_epi8(v,    signs);

	vpunpcklbw ymm2, ymm3, ymm0

; 115  :              *res_hi  = _mm256_unpackhi_epi8(v,    signs);

	vpunpckhbw ymm5, ymm3, ymm0

; 236  :   __m256i offsets_0_lo = _mm256_cmpeq_epi16   (offsets_lo,   zero);

	vpcmpeqw ymm1, ymm2, ymm13

; 237  :   __m256i offsets_0_hi = _mm256_cmpeq_epi16   (offsets_hi,   zero);
; 238  : 
; 239  :   __m256i delta_lo     = _mm256_sub_epi16     (diff_lo,      offsets_lo);
; 240  :   __m256i delta_hi     = _mm256_sub_epi16     (diff_hi,      offsets_hi);
; 241  : 
; 242  :   __m256i diff_lo_m    = _mm256_andnot_si256  (offsets_0_lo, diff_lo);

	vpandn	ymm4, ymm1, ymm10
	vpsubw	ymm0, ymm10, ymm2

; 243  :   __m256i diff_hi_m    = _mm256_andnot_si256  (offsets_0_hi, diff_hi);
; 244  :   __m256i delta_lo_m   = _mm256_andnot_si256  (offsets_0_lo, delta_lo);

	vpandn	ymm2, ymm1, ymm0
	vpcmpeqw ymm3, ymm5, ymm13
	vpandn	ymm7, ymm3, ymm11
	vpsubw	ymm1, ymm11, ymm5

; 245  :   __m256i delta_hi_m   = _mm256_andnot_si256  (offsets_0_hi, delta_hi);

	vpandn	ymm0, ymm3, ymm1

; 246  : 
; 247  :   __m256i dd0_lo       = _mm256_unpacklo_epi16(delta_lo_m,   diff_lo_m);

	vpunpcklwd ymm3, ymm2, ymm4

; 248  :   __m256i dd0_hi       = _mm256_unpackhi_epi16(delta_lo_m,   diff_lo_m);
; 249  :   __m256i dd1_lo       = _mm256_unpacklo_epi16(delta_hi_m,   diff_hi_m);

	vpunpcklwd ymm6, ymm0, ymm7

; 250  :   __m256i dd1_hi       = _mm256_unpackhi_epi16(delta_hi_m,   diff_hi_m);

	vpunpckhwd ymm7, ymm0, ymm7
	vpunpckhwd ymm4, ymm2, ymm4

; 251  : 
; 252  :   __m256i dd0_lo_n     = _mm256_sign_epi16    (dd0_lo,       negate_hiword);

	vpsignw	ymm0, ymm3, ymm12

; 253  :   __m256i dd0_hi_n     = _mm256_sign_epi16    (dd0_hi,       negate_hiword);
; 254  :   __m256i dd1_lo_n     = _mm256_sign_epi16    (dd1_lo,       negate_hiword);
; 255  :   __m256i dd1_hi_n     = _mm256_sign_epi16    (dd1_hi,       negate_hiword);
; 256  : 
; 257  :   __m256i sum0_lo      = _mm256_madd_epi16    (dd0_lo,       dd0_lo_n);

	vpmaddwd ymm3, ymm0, ymm3
	vpsignw	ymm1, ymm4, ymm12

; 258  :   __m256i sum0_hi      = _mm256_madd_epi16    (dd0_hi,       dd0_hi_n);

	vpmaddwd ymm2, ymm1, ymm4
	vpsignw	ymm0, ymm6, ymm12
	vpsignw	ymm1, ymm7, ymm12

; 259  :   __m256i sum1_lo      = _mm256_madd_epi16    (dd1_lo,       dd1_lo_n);
; 260  :   __m256i sum1_hi      = _mm256_madd_epi16    (dd1_hi,       dd1_hi_n);
; 261  : 
; 262  :   __m256i sum0         = _mm256_add_epi32     (sum0_lo,      sum0_hi);

	vpaddd	ymm5, ymm2, ymm3
	vpmaddwd ymm2, ymm1, ymm7
	vpmaddwd ymm4, ymm0, ymm6

; 263  :   __m256i sum1         = _mm256_add_epi32     (sum1_lo,      sum1_hi);

	vpaddd	ymm0, ymm2, ymm4

; 264  :   return                 _mm256_add_epi32     (sum0,         sum1);

	vpaddd	ymm3, ymm0, ymm5

; 377  : 
; 378  :       // Mask all unused bytes to 0xFF, so they won't count anywhere
; 379  :       __m256i badbyte_mask = gen_badbyte_mask(db4_mask, width_rest);
; 380  : 
; 381  :       __m256i curr  = do_one_edge_ymm(a, b, c, orig, badbyte_mask, offsets_256);
; 382  :               sum   = _mm256_add_epi32(sum, curr);

	vpaddd	ymm1, ymm3, YMMWORD PTR sum$1$[rbp]
	vmovdqu	YMMWORD PTR sum$1$[rbp], ymm1
$LN2@sao_edge_d:

; 333  :   for (y = 1; y < block_height - 1; y++) {

	vmovdqu	ymm5, YMMWORD PTR offsets_256$1$[rbp]
	mov	r11d, DWORD PTR width_rest$1$[rbp]
	inc	r14d
	mov	r15d, DWORD PTR tv3148[rbp]
	add	r9d, r13d
	mov	DWORD PTR y$1$[rbp], r14d
	mov	DWORD PTR tv3101[rbp], r9d
	cmp	r14d, DWORD PTR tv3132[rbp]
	jl	$LL4@sao_edge_d
$LN3@sao_edge_d:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 173  :   __m256i sum2 = _mm256_permute4x64_epi64(sum1, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm0, ymm1, 78				; 0000004eH

; 174  :   __m256i sum3 = _mm256_add_epi32        (sum1, sum2);

	vpaddd	ymm2, ymm0, ymm1

; 175  :   __m256i sum4 = _mm256_shuffle_epi32    (sum3, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	ymm1, ymm2, 78				; 0000004eH

; 176  :   __m256i sum5 = _mm256_add_epi32        (sum3, sum4);

	vpaddd	ymm3, ymm1, ymm2

; 177  :   __m256i sum6 = _mm256_shuffle_epi32    (sum5, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm0, ymm3, 177				; 000000b1H

; 178  :   __m256i sum7 = _mm256_add_epi32        (sum5, sum6);

	vpaddd	ymm1, ymm0, ymm3

; 179  : 
; 180  :   __m128i sum8 = _mm256_castsi256_si128  (sum7);
; 181  :   int32_t sum9 = _mm_cvtsi128_si32       (sum8);

	vmovd	eax, xmm1
$LN1@sao_edge_d:
	vzeroupper
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c

; 386  : }

	lea	r11, QWORD PTR [rsp+384]
	mov	rbx, QWORD PTR [r11+72]
	vmovaps	xmm6, XMMWORD PTR [r11-16]
	vmovaps	xmm7, XMMWORD PTR [r11-32]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm9, XMMWORD PTR [r11-64]
	vmovaps	xmm10, XMMWORD PTR [r11-80]
	vmovaps	xmm11, XMMWORD PTR [r11-96]
	vmovaps	xmm12, XMMWORD PTR [r11-112]
	vmovaps	xmm13, XMMWORD PTR [r11-128]
	vmovaps	xmm14, XMMWORD PTR [r11-144]
	vmovaps	xmm15, XMMWORD PTR [r11-160]
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r13
	pop	r12
	pop	rdi
	pop	rsi
	pop	rbp
	ret	0
sao_edge_ddistortion_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
a$ = 80
b$ = 88
c$ = 96
orig$ = 104
badbyte_mask$ = 112
diff_accum$ = 120
hit_cnt$ = 128
calc_edge_dir_one_ymm PROC

; 395  : {

	sub	rsp, 72					; 00000048H

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vmovdqu	ymm0, YMMWORD PTR [rdx]
	vmovdqu	ymm3, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101

; 398  :                 eo_cat  = _mm256_or_si256  (eo_cat, badbyte_mask);

	mov	rax, QWORD PTR badbyte_mask$[rsp]
	vmovaps	XMMWORD PTR [rsp+48], xmm6
	vmovaps	XMMWORD PTR [rsp+32], xmm7

; 59   :   const __m256i epu2epi = _mm256_set1_epi8  (0x80);

	vmovdqu	ymm7, YMMWORD PTR __ymm@8080808080808080808080808080808080808080808080808080808080808080

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vpsubb	ymm1, ymm0, ymm7
	vmovdqu	ymm0, YMMWORD PTR [rcx]
	vmovaps	XMMWORD PTR [rsp+16], xmm8

; 396  :   const __m256i ones_16 = _mm256_set1_epi16(1);
; 397  :         __m256i eo_cat  = calc_eo_cat      (a, b, c);

	vmovdqu	ymm8, YMMWORD PTR [r8]

; 62   :   __m256i a_signed      = _mm256_sub_epi8   (a,        epu2epi);

	vpsubb	ymm6, ymm8, ymm7

; 64   : 
; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm2, ymm6, ymm1

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vpsubb	ymm1, ymm0, ymm7

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm4, ymm3, ymm2

; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm2, ymm6, ymm1

; 75   :   const __m256i idx_to_cat = _mm256_setr_epi64x(0x0403000201, 0,

	vmovdqu	ymm1, YMMWORD PTR __ymm@0000000000000000000000040300020100000000000000000000000403000201

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm3, ymm3, ymm2

; 81   :   __m256i signsum          = _mm256_add_epi8    (c_a_sign,   c_b_sign);

	vpaddb	ymm0, ymm3, ymm4

; 82   :   __m256i eo_idx           = _mm256_add_epi8    (signsum,    twos);

	vpaddb	ymm4, ymm0, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202

; 83   : 
; 84   :   return                     _mm256_shuffle_epi8(idx_to_cat, eo_idx);

	vpshufb	ymm2, ymm1, ymm4

; 399  : 
; 400  :   __m256i diffs_lo, diffs_hi;
; 401  :   diff_epi8_epi16(orig, c, &diffs_lo, &diffs_hi);

	vmovdqu	ymm1, YMMWORD PTR [r9]
	vpor	ymm5, ymm2, YMMWORD PTR [rax]
	mov	r9, QWORD PTR hit_cnt$[rsp]

; 402  : 
; 403  :   for (uint32_t i = 0; i < 5; i++) {

	xor	r8d, r8d
	vmovaps	XMMWORD PTR [rsp], xmm9
	vmovdqu	ymm9, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	mov	rax, QWORD PTR diff_accum$[rsp]

; 125  :   __m256i composite_lo  = _mm256_unpacklo_epi8(a, b);

	vpunpcklbw ymm0, ymm1, ymm8

; 126  :   __m256i composite_hi  = _mm256_unpackhi_epi8(a, b);
; 127  : 
; 128  :          *res_lo        = _mm256_maddubs_epi16(composite_lo, invmask);

	vpmaddubsw ymm6, ymm0, YMMWORD PTR __ymm@ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01
	vpunpckhbw ymm1, ymm1, ymm8

; 129  :          *res_hi        = _mm256_maddubs_epi16(composite_hi, invmask);

	vpmaddubsw ymm7, ymm1, YMMWORD PTR __ymm@ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01
$LL4@calc_edge_:

; 404  :     __m256i  curr_id       = _mm256_set1_epi8    (i);

	movsx	ecx, r8b
	lea	rax, QWORD PTR [rax+32]
	vmovd	xmm0, ecx
	vpbroadcastb ymm0, xmm0

; 405  :     __m256i  eoc_mask      = _mm256_cmpeq_epi8   (eo_cat, curr_id);

	vpcmpeqb ymm4, ymm0, ymm5

; 406  :     uint32_t eoc_bits      = _mm256_movemask_epi8(eoc_mask);
; 407  :     uint32_t eoc_hits      = _mm_popcnt_u32      (eoc_bits);
; 408  : 
; 409  :     __m256i  eoc_mask_lo   = _mm256_unpacklo_epi8(eoc_mask,      eoc_mask);
; 410  :     __m256i  eoc_mask_hi   = _mm256_unpackhi_epi8(eoc_mask,      eoc_mask);

	vpunpckhbw ymm0, ymm4, ymm4
	vpunpcklbw ymm1, ymm4, ymm4

; 411  : 
; 412  :     __m256i  eoc_diffs_lo  = _mm256_and_si256    (diffs_lo,      eoc_mask_lo);
; 413  :     __m256i  eoc_diffs_hi  = _mm256_and_si256    (diffs_hi,      eoc_mask_hi);

	vpand	ymm3, ymm0, ymm7
	vpand	ymm2, ymm1, ymm6

; 414  : 
; 415  :     __m256i  eoc_diffs_16  = _mm256_add_epi16    (eoc_diffs_lo,  eoc_diffs_hi);

	vpaddw	ymm0, ymm2, ymm3

; 416  :     __m256i  eoc_diffs_32  = _mm256_madd_epi16   (eoc_diffs_16,  ones_16);

	vpmaddwd ymm3, ymm0, ymm9

; 417  : 
; 418  :              diff_accum[i] = _mm256_add_epi32    (diff_accum[i], eoc_diffs_32);

	vpaddd	ymm1, ymm3, YMMWORD PTR [rax-32]
	vmovdqu	YMMWORD PTR [rax-32], ymm1
	vpmovmskb ecx, ymm4
	popcnt	edx, ecx

; 419  :              hit_cnt[i]   += eoc_hits;

	add	DWORD PTR [r9], edx
	lea	r9, QWORD PTR [r9+4]
	inc	r8d
	cmp	r8d, 5
	jb	SHORT $LL4@calc_edge_

; 420  :   }
; 421  : }

	vzeroupper
	vmovaps	xmm6, XMMWORD PTR [rsp+48]
	vmovaps	xmm7, XMMWORD PTR [rsp+32]
	vmovaps	xmm8, XMMWORD PTR [rsp+16]
	vmovaps	xmm9, XMMWORD PTR [rsp]
	add	rsp, 72					; 00000048H
	ret	0
calc_edge_dir_one_ymm ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
tv2721 = 0
width_db4$1$ = 4
width_rest$1$ = 8
width_db32$1$ = 12
curr_cpos$1$ = 16
curr_apos$1$ = 20
curr_bpos$1$ = 24
tv2760 = 28
tv2758 = 32
b_ofs$ = 40
hit_cnt$1$ = 48
a_ofs$ = 56
b_ofs$1$ = 64
a_ofs$1$ = 72
diff_accum$ = 96
orig_data$ = 512
rec_data$ = 520
eo_class$ = 528
block_width$ = 536
block_height$ = 544
cat_sum_cnt$ = 552
calc_sao_edge_dir_avx2 PROC

; 429  : {

	mov	rax, rsp
	mov	QWORD PTR [rax+16], rbx
	mov	DWORD PTR [rax+32], r9d
	mov	QWORD PTR [rax+8], rcx
	push	rbp
	push	rsi
	push	rdi
	push	r12
	push	r13
	push	r14
	push	r15
	sub	rsp, 448				; 000001c0H
	vmovaps	XMMWORD PTR [rax-72], xmm6
	vmovaps	XMMWORD PTR [rax-88], xmm7
	vmovaps	XMMWORD PTR [rax-104], xmm8
	vmovaps	XMMWORD PTR [rax-120], xmm9
	vmovaps	XMMWORD PTR [rax-136], xmm10
	vmovaps	XMMWORD PTR [rax-152], xmm11
	vmovaps	XMMWORD PTR [rax-168], xmm12
	vmovaps	XMMWORD PTR [rax-184], xmm13
	vmovaps	XMMWORD PTR [rax-200], xmm14
	vmovaps	XMMWORD PTR [rax-216], xmm15
	lea	rbp, QWORD PTR [rsp+32]
	and	rbp, -32				; ffffffffffffffe0H

; 430  :   vector2d_t a_ofs = g_sao_edge_offsets[eo_class][0];

	lea	rcx, OFFSET FLAT:g_sao_edge_offsets
	movsxd	rax, r8d
	add	rax, rax

; 431  :   vector2d_t b_ofs = g_sao_edge_offsets[eo_class][1];
; 432  : 
; 433  :   int32_t *diff_sum   = cat_sum_cnt[0];
; 434  :   int32_t *hit_cnt    = cat_sum_cnt[1];
; 435  : 
; 436  :   int32_t scan_width  = block_width -   2;
; 437  :   int32_t width_db32  = scan_width  & ~31;
; 438  :   int32_t width_db4   = scan_width  &  ~3;

	lea	r11d, DWORD PTR [r9-2]
	and	r11d, -4
	lea	r10d, DWORD PTR [r9-2]
	and	r10d, -32				; ffffffffffffffe0H
	mov	DWORD PTR width_db4$1$[rbp], r11d
	mov	r15, rdx
	mov	DWORD PTR width_db32$1$[rbp], r10d
	mov	r8, QWORD PTR [rcx+rax*8]

; 439  :   int32_t width_rest  = scan_width  &   3;

	lea	r13d, DWORD PTR [r9-2]
	mov	rax, QWORD PTR [rcx+rax*8+8]
	and	r13d, 3
	mov	rcx, QWORD PTR cat_sum_cnt$[rsp]
	mov	rdx, r8
	add	rcx, 20
	shr	rdx, 32					; 00000020H
	mov	QWORD PTR hit_cnt$1$[rbp], rcx
	mov	r12d, 1

; 440  : 
; 441  :   const __m256i zero          = _mm256_setzero_si256();
; 442  : 
; 443  :   // Form the load&store mask
; 444  :   const __m256i wdb4_256      = _mm256_set1_epi32 (width_db4 & 31);

	mov	ecx, r11d
	mov	QWORD PTR a_ofs$1$[rbp], r8
	and	ecx, 31
	mov	QWORD PTR a_ofs$[rbp], r8
	vmovd	xmm0, ecx

; 445  :   const __m256i indexes       = _mm256_setr_epi32 (3, 7, 11, 15, 19, 23, 27, 31);
; 446  :   const __m256i db4_mask      = _mm256_cmpgt_epi32(wdb4_256, indexes);
; 447  : 
; 448  :   __m256i diff_accum[5] = { _mm256_setzero_si256() };
; 449  : 
; 450  :   int32_t y, x;
; 451  :   for (y = 1; y < block_height - 1; y++) {

	mov	ecx, DWORD PTR block_height$[rsp]
	dec	ecx
	mov	QWORD PTR b_ofs$1$[rbp], rax
	mov	QWORD PTR b_ofs$[rbp], rax
	mov	DWORD PTR width_rest$1$[rbp], r13d
	mov	DWORD PTR tv2760[rbp], ecx
	vpxor	xmm14, xmm14, xmm14
	vpbroadcastd ymm0, xmm0
	vpcmpgtd ymm15, ymm0, YMMWORD PTR __ymm@0000001f0000001b00000017000000130000000f0000000b0000000700000003
	vpxor	xmm4, xmm4, xmm4
	vxorps	xmm5, xmm5, xmm5
	vxorps	xmm6, xmm6, xmm6
	vxorps	xmm7, xmm7, xmm7
	vmovdqu	YMMWORD PTR diff_accum$[rbp], ymm14
	vmovdqu	YMMWORD PTR diff_accum$[rbp+32], ymm4
	vmovdqu	YMMWORD PTR diff_accum$[rbp+64], ymm5
	vmovdqu	YMMWORD PTR diff_accum$[rbp+96], ymm6
	vmovdqu	YMMWORD PTR diff_accum$[rbp+128], ymm7
	cmp	ecx, r12d
	jle	$LL3@calc_sao_e
	inc	r10d

; 209  :   uint32_t last_badbytes = 0xffffffff << (width_rest << 3);

	inc	edx
	mov	DWORD PTR tv2758[rbp], r10d
	imul	edx, r9d
	add	edx, r8d
	mov	DWORD PTR tv2721[rbp], edx
$LL4@calc_sao_e:

; 452  :     for (x = 1; x < width_db32 + 1; x += 32) {

	mov	esi, 1
	cmp	r10d, esi
	jle	$LN6@calc_sao_e
	mov	edx, DWORD PTR b_ofs$[rbp+4]
	lea	ebx, DWORD PTR [rax+1]
	mov	r13d, DWORD PTR a_ofs$[rbp+4]
	lea	esi, DWORD PTR [r10-2]
	vmovdqu	ymm8, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vmovdqu	ymm9, YMMWORD PTR __ymm@8080808080808080808080808080808080808080808080808080808080808080
	vmovdqu	ymm10, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101
	vmovdqu	ymm12, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202
	vmovdqu	ymm13, YMMWORD PTR __ymm@0000000000000000000000040300020100000000000000000000000403000201
	vmovdqu	ymm11, YMMWORD PTR __ymm@ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01
	add	edx, r12d
	shr	esi, 5
	sub	r13d, edx
	inc	esi
	add	r13d, r12d
	mov	edi, esi
	imul	r13d, r9d
	mov	r14d, r12d
	sub	r14d, edx
	shl	esi, 5
	imul	r14d, r9d
	mov	ecx, edx
	imul	ecx, r9d
	sub	r13d, eax
	add	r13d, r8d
	mov	r8, QWORD PTR orig_data$[rsp]
	sub	r14d, eax
	mov	rax, QWORD PTR hit_cnt$1$[rbp]
	add	ebx, ecx
	inc	esi
	npad	3
$LL7@calc_sao_e:

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vmovdqu	ymm0, YMMWORD PTR [rbx+r15]

; 458  :       __m256i b      = _mm256_loadu_si256((const __m256i *)(rec_data  + b_off));
; 459  :       __m256i c      = _mm256_loadu_si256((const __m256i *)(rec_data  + c_off));

	lea	ecx, DWORD PTR [r14+rbx]

; 403  :   for (uint32_t i = 0; i < 5; i++) {

	xor	r10d, r10d

; 460  :       __m256i orig   = _mm256_loadu_si256((const __m256i *)(orig_data + c_off));
; 461  : 
; 462  :       calc_edge_dir_one_ymm(a, b, c, orig, zero, diff_accum, hit_cnt);

	vmovdqu	ymm5, YMMWORD PTR [rcx+r15]

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vpsubb	ymm1, ymm0, ymm9

; 62   :   __m256i a_signed      = _mm256_sub_epi8   (a,        epu2epi);

	vpsubb	ymm3, ymm5, ymm9

; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm2, ymm3, ymm1

; 458  :       __m256i b      = _mm256_loadu_si256((const __m256i *)(rec_data  + b_off));
; 459  :       __m256i c      = _mm256_loadu_si256((const __m256i *)(rec_data  + c_off));

	mov	r9d, ecx
	mov	r11, rax

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm4, ymm10, ymm2

; 456  : 
; 457  :       __m256i a      = _mm256_loadu_si256((const __m256i *)(rec_data  + a_off));

	lea	ecx, DWORD PTR [rbx+r13]

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vmovdqu	ymm0, YMMWORD PTR [rcx+r15]
	vpsubb	ymm1, ymm0, ymm9

; 64   : 
; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm2, ymm3, ymm1

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm3, ymm10, ymm2

; 81   :   __m256i signsum          = _mm256_add_epi8    (c_a_sign,   c_b_sign);

	vpaddb	ymm0, ymm3, ymm4

; 82   :   __m256i eo_idx           = _mm256_add_epi8    (signsum,    twos);

	vpaddb	ymm1, ymm0, ymm12

; 83   : 
; 84   :   return                     _mm256_shuffle_epi8(idx_to_cat, eo_idx);

	vpshufb	ymm2, ymm13, ymm1

; 401  :   diff_epi8_epi16(orig, c, &diffs_lo, &diffs_hi);

	vmovdqu	ymm1, YMMWORD PTR [r9+r8]

; 125  :   __m256i composite_lo  = _mm256_unpacklo_epi8(a, b);

	vpunpcklbw ymm0, ymm1, ymm5

; 126  :   __m256i composite_hi  = _mm256_unpackhi_epi8(a, b);

	vpunpckhbw ymm1, ymm1, ymm5

; 129  :          *res_hi        = _mm256_maddubs_epi16(composite_hi, invmask);

	vpmaddubsw ymm5, ymm1, ymm11

; 398  :                 eo_cat  = _mm256_or_si256  (eo_cat, badbyte_mask);

	vpor	ymm6, ymm2, ymm14

; 128  :          *res_lo        = _mm256_maddubs_epi16(composite_lo, invmask);

	vpmaddubsw ymm7, ymm0, ymm11

; 453  :       const uint32_t a_off = (y + a_ofs.y) * block_width + x + a_ofs.x;
; 454  :       const uint32_t b_off = (y + b_ofs.y) * block_width + x + b_ofs.x;
; 455  :       const uint32_t c_off =  y            * block_width + x;

	lea	rcx, QWORD PTR diff_accum$[rbp]
$LL16@calc_sao_e:

; 404  :     __m256i  curr_id       = _mm256_set1_epi8    (i);

	movsx	edx, r10b
	lea	rcx, QWORD PTR [rcx+32]
	vmovd	xmm0, edx
	vpbroadcastb ymm0, xmm0

; 405  :     __m256i  eoc_mask      = _mm256_cmpeq_epi8   (eo_cat, curr_id);

	vpcmpeqb ymm4, ymm0, ymm6

; 406  :     uint32_t eoc_bits      = _mm256_movemask_epi8(eoc_mask);
; 407  :     uint32_t eoc_hits      = _mm_popcnt_u32      (eoc_bits);
; 408  : 
; 409  :     __m256i  eoc_mask_lo   = _mm256_unpacklo_epi8(eoc_mask,      eoc_mask);
; 410  :     __m256i  eoc_mask_hi   = _mm256_unpackhi_epi8(eoc_mask,      eoc_mask);

	vpunpckhbw ymm0, ymm4, ymm4
	vpunpcklbw ymm1, ymm4, ymm4

; 411  : 
; 412  :     __m256i  eoc_diffs_lo  = _mm256_and_si256    (diffs_lo,      eoc_mask_lo);
; 413  :     __m256i  eoc_diffs_hi  = _mm256_and_si256    (diffs_hi,      eoc_mask_hi);

	vpand	ymm3, ymm0, ymm5
	vpand	ymm2, ymm1, ymm7

; 414  : 
; 415  :     __m256i  eoc_diffs_16  = _mm256_add_epi16    (eoc_diffs_lo,  eoc_diffs_hi);

	vpaddw	ymm0, ymm2, ymm3
	vpmovmskb edx, ymm4
	popcnt	r9d, edx

; 416  :     __m256i  eoc_diffs_32  = _mm256_madd_epi16   (eoc_diffs_16,  ones_16);
; 417  : 
; 418  :              diff_accum[i] = _mm256_add_epi32    (diff_accum[i], eoc_diffs_32);
; 419  :              hit_cnt[i]   += eoc_hits;

	add	DWORD PTR [r11], r9d
	lea	r11, QWORD PTR [r11+4]
	vpmaddwd ymm3, ymm0, ymm8
	vpaddd	ymm1, ymm3, YMMWORD PTR [rcx-32]
	inc	r10d
	vmovdqu	YMMWORD PTR [rcx-32], ymm1
	cmp	r10d, 5
	jb	SHORT $LL16@calc_sao_e

; 452  :     for (x = 1; x < width_db32 + 1; x += 32) {

	add	ebx, 32					; 00000020H
	sub	rdi, 1
	jne	$LL7@calc_sao_e
	mov	rax, QWORD PTR b_ofs$1$[rbp]
	mov	r8, QWORD PTR a_ofs$1$[rbp]
	mov	r13d, DWORD PTR width_rest$1$[rbp]
	mov	r9d, DWORD PTR block_width$[rsp]
	mov	edx, DWORD PTR tv2721[rbp]
	mov	r11d, DWORD PTR width_db4$1$[rbp]
$LN6@calc_sao_e:

; 463  :     }
; 464  :     if (scan_width > width_db32) {

	lea	ecx, DWORD PTR [r9-2]
	cmp	ecx, DWORD PTR width_db32$1$[rbp]
	jle	$LN2@calc_sao_e

; 465  :       const uint32_t curr_cpos   =  y            * block_width + x;
; 466  :       const uint32_t rest_cpos   =  y            * block_width + width_db4 + 1;
; 467  : 
; 468  :       const  int32_t curr_apos   = (y + a_ofs.y) * block_width + x + a_ofs.x;
; 469  :       const  int32_t rest_apos   = (y + a_ofs.y) * block_width + width_db4 + a_ofs.x + 1;
; 470  : 
; 471  :       const  int32_t curr_bpos   = (y + b_ofs.y) * block_width + x + b_ofs.x;

	mov	ebx, DWORD PTR b_ofs$[rbp+4]
	mov	ecx, r12d
	imul	ecx, r9d
	add	ebx, r12d
	imul	ebx, r9d

; 181  :   uint32_t last_dword = 0;

	xor	r14d, r14d

; 465  :       const uint32_t curr_cpos   =  y            * block_width + x;
; 466  :       const uint32_t rest_cpos   =  y            * block_width + width_db4 + 1;
; 467  : 
; 468  :       const  int32_t curr_apos   = (y + a_ofs.y) * block_width + x + a_ofs.x;
; 469  :       const  int32_t rest_apos   = (y + a_ofs.y) * block_width + width_db4 + a_ofs.x + 1;
; 470  : 
; 471  :       const  int32_t curr_bpos   = (y + b_ofs.y) * block_width + x + b_ofs.x;

	add	ebx, eax
	lea	r10d, DWORD PTR [rcx+rsi]
	lea	edi, DWORD PTR [rcx+1]
	mov	DWORD PTR curr_cpos$1$[rbp], r10d
	lea	ecx, DWORD PTR [rdx+rsi]
	add	edi, r11d
	mov	DWORD PTR curr_apos$1$[rbp], ecx
	lea	ecx, DWORD PTR [rbx+rsi]
	mov	DWORD PTR curr_bpos$1$[rbp], ecx

; 182  :   for (int32_t i = 0; i < width_rest; i++) {

	test	r13d, r13d
	je	$LN38@calc_sao_e
	lea	ecx, DWORD PTR [rdx+r11]
	xor	r9d, r9d
	movsxd	rdx, ecx
	lea	r10, QWORD PTR [r15+1]
	add	r10, rdx
	mov	r11d, r13d
	npad	10
$LL81@calc_sao_e:

; 183  :     uint8_t  currb = buf[start_pos + i];
; 184  :     uint32_t currd = ((uint32_t)currb) << (i * 8);

	movzx	ecx, BYTE PTR [r10]
	lea	r10, QWORD PTR [r10+1]
	shlx	edx, ecx, r9d

; 185  :     last_dword |= currd;

	or	r14d, edx
	add	r9d, 8
	sub	r11, 1
	jne	SHORT $LL81@calc_sao_e
	mov	ecx, DWORD PTR width_db4$1$[rbp]
	lea	r10, QWORD PTR [r15+1]
	add	ecx, ebx
	mov	r11d, r13d
	movsxd	rdx, ecx
	xor	r9d, r9d
	add	r10, rdx
	xor	esi, esi
$LL83@calc_sao_e:

; 184  :     uint32_t currd = ((uint32_t)currb) << (i * 8);

	movzx	ecx, BYTE PTR [r10]
	lea	r10, QWORD PTR [r10+1]
	shlx	edx, ecx, r9d

; 185  :     last_dword |= currd;

	or	esi, edx
	add	r9d, 8
	sub	r11, 1
	jne	SHORT $LL83@calc_sao_e
	movsxd	rbx, edi
	xor	r9d, r9d
	mov	r11d, r13d
	xor	edi, edi
	lea	r10, QWORD PTR [rbx+r15]
	npad	7
$LL85@calc_sao_e:

; 184  :     uint32_t currd = ((uint32_t)currb) << (i * 8);

	movzx	ecx, BYTE PTR [r10]
	lea	r10, QWORD PTR [r10+1]
	shlx	edx, ecx, r9d

; 185  :     last_dword |= currd;

	or	edi, edx
	add	r9d, 8
	sub	r11, 1
	jne	SHORT $LL85@calc_sao_e
	mov	r10, QWORD PTR orig_data$[rsp]
	xor	r9d, r9d
	add	r10, rbx
	mov	ebx, r13d
	npad	6
$LL87@calc_sao_e:

; 184  :     uint32_t currd = ((uint32_t)currb) << (i * 8);

	movzx	ecx, BYTE PTR [r10]
	lea	r10, QWORD PTR [r10+1]
	shlx	edx, ecx, r9d

; 185  :     last_dword |= currd;

	or	r11d, edx
	add	r9d, 8
	sub	rbx, 1
	jne	SHORT $LL87@calc_sao_e

; 182  :   for (int32_t i = 0; i < width_rest; i++) {

	mov	r10d, DWORD PTR curr_cpos$1$[rbp]
	jmp	SHORT $LN43@calc_sao_e
$LN38@calc_sao_e:

; 130  : }
; 131  : 
; 132  : // Convert a byte-addressed mask for VPSHUFB into two word-addressed ones, for
; 133  : // example:
; 134  : // 7 3 6 2 5 1 4 0 => e f 6 7 c d 4 5 a b 2 3 8 9 0 1
; 135  : static INLINE void cvt_shufmask_epi8_epi16(const __m256i  v,
; 136  :                                                  __m256i *res_lo,
; 137  :                                                  __m256i *res_hi)
; 138  : {
; 139  :   const __m256i zero = _mm256_setzero_si256();
; 140  :   const __m256i ones = _mm256_set1_epi8(1);
; 141  : 
; 142  :   // There's no 8-bit shift, so highest bit could bleed into neighboring byte
; 143  :   // if set. To avoid it, reset all sign bits with max. The only valid input
; 144  :   // values for v are [0, 7] anyway and invalid places should be masked out by
; 145  :   // caller, so it doesn't matter that we turn negative bytes into garbage.
; 146  :   __m256i v_nonnegs  = _mm256_max_epi8  (zero,      v);
; 147  :   __m256i v_lobytes  = _mm256_slli_epi32(v_nonnegs, 1);
; 148  :   __m256i v_hibytes  = _mm256_add_epi8  (v_lobytes, ones);
; 149  : 
; 150  :           *res_lo    = _mm256_unpacklo_epi8(v_lobytes, v_hibytes);
; 151  :           *res_hi    = _mm256_unpackhi_epi8(v_lobytes, v_hibytes);
; 152  : }
; 153  : 
; 154  : // Check if all 4 dwords of v are in [-128, 127] and can be truncated to
; 155  : // 8 bits each. Returns -1 if everything is fine
; 156  : static INLINE uint16_t epi32v_fits_in_epi8s(const __m128i v)
; 157  : {
; 158  :   // Compare most significant 25 bits of SAO bands to the sign bit to assert
; 159  :   // that the i32's are between -128 and 127 (only comparing 24 would fail to
; 160  :   // detect values of 128...255)
; 161  :   __m128i  v_ms25b = _mm_srai_epi32   (v,  7);
; 162  :   __m128i  v_signs = _mm_srai_epi32   (v, 31);
; 163  :   __m128i  ok_i32s = _mm_cmpeq_epi32  (v_ms25b, v_signs);
; 164  :   return             _mm_movemask_epi8(ok_i32s);
; 165  : }
; 166  : 
; 167  : static INLINE __m128i truncate_epi32_epi8(const __m128i v)
; 168  : {
; 169  :   // LSBs of each dword, the values values must fit in 8 bits anyway for
; 170  :   // what this intended for (use epi32v_fits_in_epi8s to check if needed)
; 171  :   const __m128i trunc_shufmask = _mm_set1_epi32  (0x0c080400);
; 172  :         __m128i sbs_8          = _mm_shuffle_epi8(v, trunc_shufmask);
; 173  :   return        sbs_8;
; 174  : }
; 175  : 
; 176  : // Read 0-3 bytes (pixels) into uint32_t
; 177  : static INLINE uint32_t load_border_bytes(const uint8_t *buf,
; 178  :                                          const int32_t  start_pos,
; 179  :                                          const int32_t  width_rest)
; 180  : {
; 181  :   uint32_t last_dword = 0;

	xor	r11d, r11d
	xor	esi, esi
	xor	edi, edi
$LN43@calc_sao_e:

; 59   :   const __m256i epu2epi = _mm256_set1_epi8  (0x80);

	vmovdqu	ymm9, YMMWORD PTR __ymm@8080808080808080808080808080808080808080808080808080808080808080

; 60   :   const __m256i ones    = _mm256_set1_epi8  (0x01);

	vmovdqu	ymm7, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101

; 396  :   const __m256i ones_16 = _mm256_set1_epi16(1);

	vmovdqu	ymm11, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 480  :       const int32_t *b_ptr       = (const int32_t *)(rec_data  + curr_bpos);
; 481  :       const int32_t *c_ptr       = (const int32_t *)(rec_data  + curr_cpos);

	mov	edx, r10d

; 209  :   uint32_t last_badbytes = 0xffffffff << (width_rest << 3);

	lea	ecx, DWORD PTR [r13*8]

; 491  :               c    = _mm256_insert_epi32  (c,        c_last,    7);

	mov	r10, QWORD PTR hit_cnt$1$[rbp]

; 209  :   uint32_t last_badbytes = 0xffffffff << (width_rest << 3);

	mov	r9d, -1					; ffffffffH
	shlx	ecx, r9d, ecx

; 491  :               c    = _mm256_insert_epi32  (c,        c_last,    7);

	vmovd	xmm1, edi
	vpmaskmovd ymm0, ymm15, YMMWORD PTR [rdx+r15]
	vpbroadcastd ymm1, xmm1
	vpblendd ymm10, ymm0, ymm1, 128			; 00000080H

; 211  :   return                   _mm256_insert_epi32(badbyte_mask, last_badbytes, 7);

	vmovd	xmm1, ecx

; 472  :       const  int32_t rest_bpos   = (y + b_ofs.y) * block_width + width_db4 + b_ofs.x + 1;
; 473  : 
; 474  :             uint32_t a_last      = load_border_bytes(rec_data,  rest_apos, width_rest);
; 475  :             uint32_t b_last      = load_border_bytes(rec_data,  rest_bpos, width_rest);
; 476  :             uint32_t c_last      = load_border_bytes(rec_data,  rest_cpos, width_rest);
; 477  :             uint32_t orig_last   = load_border_bytes(orig_data, rest_cpos, width_rest);
; 478  : 
; 479  :       const int32_t *a_ptr       = (const int32_t *)(rec_data  + curr_apos);

	movsxd	rcx, DWORD PTR curr_apos$1$[rbp]

; 403  :   for (uint32_t i = 0; i < 5; i++) {

	xor	r9d, r9d

; 211  :   return                   _mm256_insert_epi32(badbyte_mask, last_badbytes, 7);

	vpbroadcastd ymm1, xmm1
	vpcmpeqb ymm0, ymm14, ymm15
	vpblendd ymm6, ymm0, ymm1, 128			; 00000080H

; 482  :       const int32_t *orig_ptr    = (const int32_t *)(orig_data + curr_cpos);
; 483  : 
; 484  :       __m256i a    = _mm256_maskload_epi32(a_ptr,    db4_mask);

	vpmaskmovd ymm0, ymm15, YMMWORD PTR [rcx+r15]
	movsxd	rcx, DWORD PTR curr_bpos$1$[rbp]

; 62   :   __m256i a_signed      = _mm256_sub_epi8   (a,        epu2epi);

	vpsubb	ymm8, ymm10, ymm9

; 488  : 
; 489  :               a    = _mm256_insert_epi32  (a,        a_last,    7);

	vmovd	xmm2, r14d
	vpbroadcastd ymm2, xmm2
	vpblendd ymm1, ymm0, ymm2, 128			; 00000080H
	vpmaskmovd ymm0, ymm15, YMMWORD PTR [rcx+r15]
	mov	rcx, QWORD PTR orig_data$[rsp]

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vpsubb	ymm2, ymm1, ymm9

; 64   : 
; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm3, ymm8, ymm2

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm4, ymm7, ymm3

; 490  :               b    = _mm256_insert_epi32  (b,        b_last,    7);

	vmovd	xmm1, esi
	vpbroadcastd ymm1, xmm1
	vpblendd ymm1, ymm0, ymm1, 128			; 00000080H

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vpsubb	ymm2, ymm1, ymm9

; 64   : 
; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm3, ymm8, ymm2

; 75   :   const __m256i idx_to_cat = _mm256_setr_epi64x(0x0403000201, 0,

	vmovdqu	ymm2, YMMWORD PTR __ymm@0000000000000000000000040300020100000000000000000000000403000201

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm0, ymm7, ymm3

; 81   :   __m256i signsum          = _mm256_add_epi8    (c_a_sign,   c_b_sign);

	vpaddb	ymm1, ymm0, ymm4

; 82   :   __m256i eo_idx           = _mm256_add_epi8    (signsum,    twos);

	vpaddb	ymm4, ymm1, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202

; 83   : 
; 84   :   return                     _mm256_shuffle_epi8(idx_to_cat, eo_idx);

	vpshufb	ymm0, ymm2, ymm4

; 398  :                 eo_cat  = _mm256_or_si256  (eo_cat, badbyte_mask);

	vpor	ymm5, ymm0, ymm6

; 485  :       __m256i b    = _mm256_maskload_epi32(b_ptr,    db4_mask);
; 486  :       __m256i c    = _mm256_maskload_epi32(c_ptr,    db4_mask);
; 487  :       __m256i orig = _mm256_maskload_epi32(orig_ptr, db4_mask);

	vpmaskmovd ymm0, ymm15, YMMWORD PTR [rdx+rcx]

; 492  :               orig = _mm256_insert_epi32  (orig,     orig_last, 7);

	vmovd	xmm1, r11d
	vpbroadcastd ymm1, xmm1
	vpblendd ymm3, ymm0, ymm1, 128			; 00000080H

; 125  :   __m256i composite_lo  = _mm256_unpacklo_epi8(a, b);

	vpunpcklbw ymm0, ymm3, ymm10

; 127  : 
; 128  :          *res_lo        = _mm256_maddubs_epi16(composite_lo, invmask);

	vpmaddubsw ymm6, ymm0, YMMWORD PTR __ymm@ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01
	vpunpckhbw ymm1, ymm3, ymm10

; 129  :          *res_hi        = _mm256_maddubs_epi16(composite_hi, invmask);

	vpmaddubsw ymm7, ymm1, YMMWORD PTR __ymm@ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01ff01

; 491  :               c    = _mm256_insert_epi32  (c,        c_last,    7);

	lea	r11, QWORD PTR diff_accum$[rbp]
$LL51@calc_sao_e:

; 404  :     __m256i  curr_id       = _mm256_set1_epi8    (i);

	movsx	ecx, r9b
	lea	r11, QWORD PTR [r11+32]
	vmovd	xmm0, ecx
	vpbroadcastb ymm0, xmm0

; 405  :     __m256i  eoc_mask      = _mm256_cmpeq_epi8   (eo_cat, curr_id);

	vpcmpeqb ymm4, ymm0, ymm5

; 406  :     uint32_t eoc_bits      = _mm256_movemask_epi8(eoc_mask);
; 407  :     uint32_t eoc_hits      = _mm_popcnt_u32      (eoc_bits);
; 408  : 
; 409  :     __m256i  eoc_mask_lo   = _mm256_unpacklo_epi8(eoc_mask,      eoc_mask);
; 410  :     __m256i  eoc_mask_hi   = _mm256_unpackhi_epi8(eoc_mask,      eoc_mask);

	vpunpckhbw ymm0, ymm4, ymm4
	vpunpcklbw ymm1, ymm4, ymm4

; 411  : 
; 412  :     __m256i  eoc_diffs_lo  = _mm256_and_si256    (diffs_lo,      eoc_mask_lo);
; 413  :     __m256i  eoc_diffs_hi  = _mm256_and_si256    (diffs_hi,      eoc_mask_hi);

	vpand	ymm3, ymm0, ymm7
	vpand	ymm2, ymm1, ymm6

; 414  : 
; 415  :     __m256i  eoc_diffs_16  = _mm256_add_epi16    (eoc_diffs_lo,  eoc_diffs_hi);

	vpaddw	ymm0, ymm2, ymm3
	vpmovmskb ecx, ymm4
	popcnt	edx, ecx

; 416  :     __m256i  eoc_diffs_32  = _mm256_madd_epi16   (eoc_diffs_16,  ones_16);
; 417  : 
; 418  :              diff_accum[i] = _mm256_add_epi32    (diff_accum[i], eoc_diffs_32);
; 419  :              hit_cnt[i]   += eoc_hits;

	add	DWORD PTR [r10], edx
	lea	r10, QWORD PTR [r10+4]
	vpmaddwd ymm3, ymm0, ymm11
	vpaddd	ymm1, ymm3, YMMWORD PTR [r11-32]
	inc	r9d
	vmovdqu	YMMWORD PTR [r11-32], ymm1
	cmp	r9d, 5
	jb	SHORT $LL51@calc_sao_e
	mov	r9d, DWORD PTR block_width$[rsp]
	mov	edx, DWORD PTR tv2721[rbp]
$LN2@calc_sao_e:

; 445  :   const __m256i indexes       = _mm256_setr_epi32 (3, 7, 11, 15, 19, 23, 27, 31);
; 446  :   const __m256i db4_mask      = _mm256_cmpgt_epi32(wdb4_256, indexes);
; 447  : 
; 448  :   __m256i diff_accum[5] = { _mm256_setzero_si256() };
; 449  : 
; 450  :   int32_t y, x;
; 451  :   for (y = 1; y < block_height - 1; y++) {

	mov	r11d, DWORD PTR width_db4$1$[rbp]
	add	edx, r9d
	mov	r10d, DWORD PTR tv2758[rbp]
	inc	r12d
	mov	DWORD PTR tv2721[rbp], edx
	cmp	r12d, DWORD PTR tv2760[rbp]
	jl	$LL4@calc_sao_e
	vmovdqu	ymm7, YMMWORD PTR diff_accum$[rbp+128]
	vmovdqu	ymm6, YMMWORD PTR diff_accum$[rbp+96]
	vmovdqu	ymm5, YMMWORD PTR diff_accum$[rbp+64]
	vmovdqu	ymm4, YMMWORD PTR diff_accum$[rbp+32]
	vmovdqu	ymm14, YMMWORD PTR diff_accum$[rbp]
$LL3@calc_sao_e:

; 501  :     diff_sum[i] += sum;

	mov	rcx, QWORD PTR cat_sum_cnt$[rsp]
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 173  :   __m256i sum2 = _mm256_permute4x64_epi64(sum1, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm0, ymm14, 78				; 0000004eH

; 174  :   __m256i sum3 = _mm256_add_epi32        (sum1, sum2);

	vpaddd	ymm2, ymm0, ymm14

; 175  :   __m256i sum4 = _mm256_shuffle_epi32    (sum3, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	ymm1, ymm2, 78				; 0000004eH

; 176  :   __m256i sum5 = _mm256_add_epi32        (sum3, sum4);

	vpaddd	ymm3, ymm1, ymm2

; 177  :   __m256i sum6 = _mm256_shuffle_epi32    (sum5, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm0, ymm3, 177				; 000000b1H

; 178  :   __m256i sum7 = _mm256_add_epi32        (sum5, sum6);

	vpaddd	ymm1, ymm0, ymm3

; 179  : 
; 180  :   __m128i sum8 = _mm256_castsi256_si128  (sum7);
; 181  :   int32_t sum9 = _mm_cvtsi128_si32       (sum8);

	vmovd	eax, xmm1
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c

; 501  :     diff_sum[i] += sum;

	add	DWORD PTR [rcx], eax
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 173  :   __m256i sum2 = _mm256_permute4x64_epi64(sum1, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm0, ymm4, 78				; 0000004eH

; 174  :   __m256i sum3 = _mm256_add_epi32        (sum1, sum2);

	vpaddd	ymm2, ymm0, ymm4

; 175  :   __m256i sum4 = _mm256_shuffle_epi32    (sum3, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	ymm1, ymm2, 78				; 0000004eH

; 176  :   __m256i sum5 = _mm256_add_epi32        (sum3, sum4);

	vpaddd	ymm3, ymm1, ymm2

; 177  :   __m256i sum6 = _mm256_shuffle_epi32    (sum5, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm0, ymm3, 177				; 000000b1H

; 178  :   __m256i sum7 = _mm256_add_epi32        (sum5, sum6);

	vpaddd	ymm1, ymm0, ymm3

; 179  : 
; 180  :   __m128i sum8 = _mm256_castsi256_si128  (sum7);
; 181  :   int32_t sum9 = _mm_cvtsi128_si32       (sum8);

	vmovd	eax, xmm1
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c

; 501  :     diff_sum[i] += sum;

	add	DWORD PTR [rcx+4], eax
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 173  :   __m256i sum2 = _mm256_permute4x64_epi64(sum1, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm0, ymm5, 78				; 0000004eH

; 174  :   __m256i sum3 = _mm256_add_epi32        (sum1, sum2);

	vpaddd	ymm2, ymm0, ymm5

; 175  :   __m256i sum4 = _mm256_shuffle_epi32    (sum3, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	ymm1, ymm2, 78				; 0000004eH

; 176  :   __m256i sum5 = _mm256_add_epi32        (sum3, sum4);

	vpaddd	ymm3, ymm1, ymm2

; 177  :   __m256i sum6 = _mm256_shuffle_epi32    (sum5, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm0, ymm3, 177				; 000000b1H

; 178  :   __m256i sum7 = _mm256_add_epi32        (sum5, sum6);

	vpaddd	ymm1, ymm0, ymm3

; 179  : 
; 180  :   __m128i sum8 = _mm256_castsi256_si128  (sum7);
; 181  :   int32_t sum9 = _mm_cvtsi128_si32       (sum8);

	vmovd	eax, xmm1
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c

; 501  :     diff_sum[i] += sum;

	add	DWORD PTR [rcx+8], eax
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 173  :   __m256i sum2 = _mm256_permute4x64_epi64(sum1, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm0, ymm6, 78				; 0000004eH

; 174  :   __m256i sum3 = _mm256_add_epi32        (sum1, sum2);

	vpaddd	ymm2, ymm0, ymm6

; 175  :   __m256i sum4 = _mm256_shuffle_epi32    (sum3, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	ymm1, ymm2, 78				; 0000004eH

; 176  :   __m256i sum5 = _mm256_add_epi32        (sum3, sum4);

	vpaddd	ymm3, ymm1, ymm2

; 177  :   __m256i sum6 = _mm256_shuffle_epi32    (sum5, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm0, ymm3, 177				; 000000b1H

; 178  :   __m256i sum7 = _mm256_add_epi32        (sum5, sum6);

	vpaddd	ymm1, ymm0, ymm3

; 179  : 
; 180  :   __m128i sum8 = _mm256_castsi256_si128  (sum7);
; 181  :   int32_t sum9 = _mm_cvtsi128_si32       (sum8);

	vmovd	eax, xmm1
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c

; 501  :     diff_sum[i] += sum;

	add	DWORD PTR [rcx+12], eax
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 173  :   __m256i sum2 = _mm256_permute4x64_epi64(sum1, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm0, ymm7, 78				; 0000004eH

; 174  :   __m256i sum3 = _mm256_add_epi32        (sum1, sum2);

	vpaddd	ymm2, ymm0, ymm7

; 175  :   __m256i sum4 = _mm256_shuffle_epi32    (sum3, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	ymm1, ymm2, 78				; 0000004eH

; 176  :   __m256i sum5 = _mm256_add_epi32        (sum3, sum4);

	vpaddd	ymm3, ymm1, ymm2

; 177  :   __m256i sum6 = _mm256_shuffle_epi32    (sum5, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm0, ymm3, 177				; 000000b1H

; 178  :   __m256i sum7 = _mm256_add_epi32        (sum5, sum6);

	vpaddd	ymm1, ymm0, ymm3

; 179  : 
; 180  :   __m128i sum8 = _mm256_castsi256_si128  (sum7);
; 181  :   int32_t sum9 = _mm_cvtsi128_si32       (sum8);

	vmovd	eax, xmm1
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c

; 501  :     diff_sum[i] += sum;

	add	DWORD PTR [rcx+16], eax
	vzeroupper

; 502  :   }
; 503  : }

	lea	r11, QWORD PTR [rsp+448]
	mov	rbx, QWORD PTR [r11+72]
	vmovaps	xmm6, XMMWORD PTR [r11-16]
	vmovaps	xmm7, XMMWORD PTR [r11-32]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm9, XMMWORD PTR [r11-64]
	vmovaps	xmm10, XMMWORD PTR [r11-80]
	vmovaps	xmm11, XMMWORD PTR [r11-96]
	vmovaps	xmm12, XMMWORD PTR [r11-112]
	vmovaps	xmm13, XMMWORD PTR [r11-128]
	vmovaps	xmm14, XMMWORD PTR [r11-144]
	vmovaps	xmm15, XMMWORD PTR [r11-160]
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r13
	pop	r12
	pop	rdi
	pop	rsi
	pop	rbp
	ret	0
calc_sao_edge_dir_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
bands$4$ = 0
encoder$dead$ = 240
sao$ = 248
offsets$ = 256
color_i$ = 264
calc_sao_offset_array_avx2 PROC

; 516  : {

	mov	rax, rsp
	push	rbp
	sub	rsp, 224				; 000000e0H
	vmovaps	XMMWORD PTR [rax-24], xmm6
	vmovaps	XMMWORD PTR [rax-40], xmm7
	vmovaps	XMMWORD PTR [rax-56], xmm8
	vmovaps	XMMWORD PTR [rax-72], xmm9
	vmovaps	XMMWORD PTR [rax-88], xmm10
	vmovaps	XMMWORD PTR [rax-104], xmm11
	vmovaps	XMMWORD PTR [rax-120], xmm12
	vmovaps	XMMWORD PTR [rsp+96], xmm13
	vmovaps	XMMWORD PTR [rsp+80], xmm14
	vmovaps	XMMWORD PTR [rsp+64], xmm15
	lea	rbp, QWORD PTR [rsp+32]
	and	rbp, -32				; ffffffffffffffe0H

; 533  :                                                 0, 0, 0x01010101, 0x01010101);
; 534  : 
; 535  :   // We'll only ever address SAO offsets 1, 2, 3, 4, 6, 7, 8, 9, so only load
; 536  :   // them and truncate into signed 16 bits (anything out of that range will
; 537  :   // anyway saturate anything they're used to do)
; 538  :   __m128i sao_offs_lo  = _mm_loadu_si128((const __m128i *)(sao->offsets + 1));
; 539  :   __m128i sao_offs_hi  = _mm_loadu_si128((const __m128i *)(sao->offsets + 6));
; 540  : 
; 541  :   __m128i sao_offs_xmm = _mm_packs_epi32  (sao_offs_lo, sao_offs_hi);

	vmovdqu	xmm0, XMMWORD PTR [rdx+32]
	vpackssdw xmm2, xmm0, XMMWORD PTR [rdx+52]
	vmovdqu	ymm4, YMMWORD PTR __ymm@0101010101010101000000000000000001010101010101010000000000000000
	vmovdqu	ymm14, YMMWORD PTR __ymm@0303030303030303030303030303030303030303030303030303030303030303
	vmovdqu	ymm15, YMMWORD PTR __ymm@1010101010101010101010101010101010101010101010101010101010101010
	vmovdqu	ymm9, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202
	vmovdqu	ymm8, YMMWORD PTR __ymm@0f0e0d0c0b0a090807060504030201000f0e0d0c0b0a09080706050403020100
	vmovdqu	ymm3, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101
	xor	eax, eax
	mov	ecx, 4
	cmp	r9d, 2
	mov	r10d, 24

; 218  :   __m256i res = _mm256_castsi128_si256 (v);

	vmovups	xmm1, xmm2

; 517  :   const uint32_t band_pos   = (color_i == COLOR_V) ? 1 : 0;
; 518  :   const  int32_t cur_bp     = sao->band_position[band_pos];
; 519  : 
; 520  :   const __m256i  zero       = _mm256_setzero_si256();
; 521  :   const __m256i  threes     = _mm256_set1_epi8  (  3);
; 522  : 
; 523  :   const __m256i  band_pos_v = _mm256_set1_epi8  (band_pos << 2);

	cmove	eax, ecx
	movsx	eax, al
	vmovd	xmm11, eax
	mov	eax, 20
	cmove	eax, r10d
	add	r8, 64					; 00000040H
	vpxor	xmm10, xmm10, xmm10
	vpbroadcastb ymm11, xmm11

; 524  :   const __m256i  cur_bp_v   = _mm256_set1_epi8  (cur_bp);
; 525  :   const __m256i  val_incr   = _mm256_set1_epi8  (16);
; 526  :   const __m256i  band_incr  = _mm256_set1_epi8  ( 2);
; 527  :         __m256i  vals       = _mm256_setr_epi8  ( 0,  1,  2,  3,  4,  5,  6,  7,
; 528  :                                                   8,  9, 10, 11, 12, 13, 14, 15,
; 529  :                                                   0,  1,  2,  3,  4,  5,  6,  7,
; 530  :                                                   8,  9, 10, 11, 12, 13, 14, 15);
; 531  : 
; 532  :         __m256i  bands     = _mm256_setr_epi32 (0, 0, 0x01010101, 0x01010101,

	vmovdqu	YMMWORD PTR bands$4$[rbp], ymm4
	movsx	eax, BYTE PTR [rax+rdx]
	vmovd	xmm12, eax
	vpbroadcastb ymm12, xmm12

; 219  :   return        _mm256_inserti128_si256(res, v, 1);

	vinserti128 ymm13, ymm1, xmm2, 1
$LL4@calc_sao_o:

; 575  :             bands   = _mm256_add_epi8    (bands,    band_incr);

	vpaddb	ymm9, ymm9, YMMWORD PTR bands$4$[rbp]
	vpsubb	ymm2, ymm4, ymm12
	vpaddb	ymm0, ymm2, ymm11

; 146  :   __m256i v_nonnegs  = _mm256_max_epi8  (zero,      v);

	vpmaxsb	ymm1, ymm0, ymm10

; 147  :   __m256i v_lobytes  = _mm256_slli_epi32(v_nonnegs, 1);

	vpslld	ymm6, ymm1, 1

; 542  :   __m256i sao_offs     = broadcast_xmm2ymm(sao_offs_xmm);
; 543  : 
; 544  :   for (uint32_t i = 0; i < 16; i++) {
; 545  :     // bands will always be in [0, 31], and cur_bp in [0, 27], so no overflow
; 546  :     // can occur
; 547  :     __m256i band_m_bp = _mm256_sub_epi8    (bands,  cur_bp_v);
; 548  : 
; 549  :     // If (x & ~3) != 0 for any signed x, then x < 0 or x > 3
; 550  :     __m256i bmbp_bads = _mm256_andnot_si256(threes,    band_m_bp);

	vpandn	ymm0, ymm14, ymm2

; 551  :     __m256i in_band   = _mm256_cmpeq_epi8  (zero,      bmbp_bads);

	vpcmpeqb ymm7, ymm0, ymm10

; 148  :   __m256i v_hibytes  = _mm256_add_epi8  (v_lobytes, ones);

	vpaddb	ymm5, ymm3, ymm6

; 151  :           *res_hi    = _mm256_unpackhi_epi8(v_lobytes, v_hibytes);

	vpunpckhbw ymm1, ymm6, ymm5

; 562  :     __m256i offs_hi = _mm256_shuffle_epi8(sao_offs, offid_hi);

	vpshufb	ymm0, ymm13, ymm1

; 104  :              *res_lo  = _mm256_unpacklo_epi8(v, zero);

	vpunpcklbw ymm3, ymm8, ymm10

; 105  :              *res_hi  = _mm256_unpackhi_epi8(v, zero);

	vpunpckhbw ymm2, ymm8, ymm10

; 565  :     __m256i sums_hi = _mm256_adds_epi16  (val_hi,   offs_hi);

	vpaddsw	ymm2, ymm0, ymm2

; 568  :             sums_hi = _mm256_max_epi16   (sums_hi,  zero);

	vpmaxsw	ymm4, ymm2, ymm10

; 150  :           *res_lo    = _mm256_unpacklo_epi8(v_lobytes, v_hibytes);

	vpunpcklbw ymm0, ymm6, ymm5

; 552  : 
; 553  :     __m256i offset_id = _mm256_add_epi8    (band_m_bp, band_pos_v);
; 554  : 
; 555  :     __m256i val_lo, val_hi;
; 556  :     cvt_epu8_epi16(vals, &val_lo, &val_hi);
; 557  : 
; 558  :     __m256i offid_lo, offid_hi;
; 559  :     cvt_shufmask_epi8_epi16(offset_id, &offid_lo, &offid_hi);
; 560  : 
; 561  :     __m256i offs_lo = _mm256_shuffle_epi8(sao_offs, offid_lo);

	vpshufb	ymm1, ymm13, ymm0

; 563  : 
; 564  :     __m256i sums_lo = _mm256_adds_epi16  (val_lo,   offs_lo);

	vpaddsw	ymm2, ymm1, ymm3

; 566  : 
; 567  :             sums_lo = _mm256_max_epi16   (sums_lo,  zero);

	vpmaxsw	ymm3, ymm2, ymm10

; 569  : 
; 570  :     __m256i offs    = _mm256_packus_epi16(sums_lo,  sums_hi);

	vpackuswb ymm0, ymm3, ymm4

; 571  : 
; 572  :     offsets[i]      = _mm256_blendv_epi8 (vals,     offs, in_band);

	vpblendvb ymm1, ymm8, ymm0, ymm7
	vmovdqu	YMMWORD PTR [r8-64], ymm1
	vpsubb	ymm2, ymm9, ymm12

; 575  :             bands   = _mm256_add_epi8    (bands,    band_incr);

	vpaddb	ymm9, ymm9, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202
	vpaddb	ymm0, ymm2, ymm11

; 146  :   __m256i v_nonnegs  = _mm256_max_epi8  (zero,      v);

	vpmaxsb	ymm1, ymm0, ymm10

; 147  :   __m256i v_lobytes  = _mm256_slli_epi32(v_nonnegs, 1);

	vpslld	ymm6, ymm1, 1

; 148  :   __m256i v_hibytes  = _mm256_add_epi8  (v_lobytes, ones);

	vpaddb	ymm5, ymm6, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101

; 542  :   __m256i sao_offs     = broadcast_xmm2ymm(sao_offs_xmm);
; 543  : 
; 544  :   for (uint32_t i = 0; i < 16; i++) {
; 545  :     // bands will always be in [0, 31], and cur_bp in [0, 27], so no overflow
; 546  :     // can occur
; 547  :     __m256i band_m_bp = _mm256_sub_epi8    (bands,  cur_bp_v);
; 548  : 
; 549  :     // If (x & ~3) != 0 for any signed x, then x < 0 or x > 3
; 550  :     __m256i bmbp_bads = _mm256_andnot_si256(threes,    band_m_bp);

	vpandn	ymm0, ymm14, ymm2

; 551  :     __m256i in_band   = _mm256_cmpeq_epi8  (zero,      bmbp_bads);

	vpcmpeqb ymm7, ymm0, ymm10

; 151  :           *res_hi    = _mm256_unpackhi_epi8(v_lobytes, v_hibytes);

	vpunpckhbw ymm1, ymm6, ymm5

; 562  :     __m256i offs_hi = _mm256_shuffle_epi8(sao_offs, offid_hi);

	vpshufb	ymm0, ymm13, ymm1

; 573  : 
; 574  :             vals    = _mm256_add_epi8    (vals,     val_incr);

	vpaddb	ymm8, ymm8, ymm15

; 104  :              *res_lo  = _mm256_unpacklo_epi8(v, zero);

	vpunpcklbw ymm3, ymm8, ymm10

; 105  :              *res_hi  = _mm256_unpackhi_epi8(v, zero);

	vpunpckhbw ymm2, ymm8, ymm10

; 565  :     __m256i sums_hi = _mm256_adds_epi16  (val_hi,   offs_hi);

	vpaddsw	ymm2, ymm0, ymm2

; 568  :             sums_hi = _mm256_max_epi16   (sums_hi,  zero);

	vpmaxsw	ymm4, ymm2, ymm10

; 150  :           *res_lo    = _mm256_unpacklo_epi8(v_lobytes, v_hibytes);

	vpunpcklbw ymm0, ymm6, ymm5

; 552  : 
; 553  :     __m256i offset_id = _mm256_add_epi8    (band_m_bp, band_pos_v);
; 554  : 
; 555  :     __m256i val_lo, val_hi;
; 556  :     cvt_epu8_epi16(vals, &val_lo, &val_hi);
; 557  : 
; 558  :     __m256i offid_lo, offid_hi;
; 559  :     cvt_shufmask_epi8_epi16(offset_id, &offid_lo, &offid_hi);
; 560  : 
; 561  :     __m256i offs_lo = _mm256_shuffle_epi8(sao_offs, offid_lo);

	vpshufb	ymm1, ymm13, ymm0

; 563  : 
; 564  :     __m256i sums_lo = _mm256_adds_epi16  (val_lo,   offs_lo);

	vpaddsw	ymm2, ymm1, ymm3

; 566  : 
; 567  :             sums_lo = _mm256_max_epi16   (sums_lo,  zero);

	vpmaxsw	ymm3, ymm2, ymm10

; 569  : 
; 570  :     __m256i offs    = _mm256_packus_epi16(sums_lo,  sums_hi);

	vpackuswb ymm0, ymm3, ymm4

; 571  : 
; 572  :     offsets[i]      = _mm256_blendv_epi8 (vals,     offs, in_band);

	vpblendvb ymm1, ymm8, ymm0, ymm7
	vmovdqu	YMMWORD PTR [r8-32], ymm1
	vpsubb	ymm2, ymm9, ymm12

; 575  :             bands   = _mm256_add_epi8    (bands,    band_incr);

	vpaddb	ymm9, ymm9, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202
	vpaddb	ymm0, ymm2, ymm11

; 146  :   __m256i v_nonnegs  = _mm256_max_epi8  (zero,      v);

	vpmaxsb	ymm1, ymm0, ymm10

; 147  :   __m256i v_lobytes  = _mm256_slli_epi32(v_nonnegs, 1);

	vpslld	ymm6, ymm1, 1

; 148  :   __m256i v_hibytes  = _mm256_add_epi8  (v_lobytes, ones);

	vpaddb	ymm5, ymm6, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101

; 542  :   __m256i sao_offs     = broadcast_xmm2ymm(sao_offs_xmm);
; 543  : 
; 544  :   for (uint32_t i = 0; i < 16; i++) {
; 545  :     // bands will always be in [0, 31], and cur_bp in [0, 27], so no overflow
; 546  :     // can occur
; 547  :     __m256i band_m_bp = _mm256_sub_epi8    (bands,  cur_bp_v);
; 548  : 
; 549  :     // If (x & ~3) != 0 for any signed x, then x < 0 or x > 3
; 550  :     __m256i bmbp_bads = _mm256_andnot_si256(threes,    band_m_bp);

	vpandn	ymm0, ymm14, ymm2

; 551  :     __m256i in_band   = _mm256_cmpeq_epi8  (zero,      bmbp_bads);

	vpcmpeqb ymm7, ymm0, ymm10

; 151  :           *res_hi    = _mm256_unpackhi_epi8(v_lobytes, v_hibytes);

	vpunpckhbw ymm1, ymm6, ymm5

; 562  :     __m256i offs_hi = _mm256_shuffle_epi8(sao_offs, offid_hi);

	vpshufb	ymm0, ymm13, ymm1

; 573  : 
; 574  :             vals    = _mm256_add_epi8    (vals,     val_incr);

	vpaddb	ymm8, ymm8, ymm15

; 104  :              *res_lo  = _mm256_unpacklo_epi8(v, zero);

	vpunpcklbw ymm3, ymm8, ymm10

; 105  :              *res_hi  = _mm256_unpackhi_epi8(v, zero);

	vpunpckhbw ymm2, ymm8, ymm10

; 565  :     __m256i sums_hi = _mm256_adds_epi16  (val_hi,   offs_hi);

	vpaddsw	ymm2, ymm0, ymm2

; 568  :             sums_hi = _mm256_max_epi16   (sums_hi,  zero);

	vpmaxsw	ymm4, ymm2, ymm10

; 150  :           *res_lo    = _mm256_unpacklo_epi8(v_lobytes, v_hibytes);

	vpunpcklbw ymm0, ymm6, ymm5

; 552  : 
; 553  :     __m256i offset_id = _mm256_add_epi8    (band_m_bp, band_pos_v);
; 554  : 
; 555  :     __m256i val_lo, val_hi;
; 556  :     cvt_epu8_epi16(vals, &val_lo, &val_hi);
; 557  : 
; 558  :     __m256i offid_lo, offid_hi;
; 559  :     cvt_shufmask_epi8_epi16(offset_id, &offid_lo, &offid_hi);
; 560  : 
; 561  :     __m256i offs_lo = _mm256_shuffle_epi8(sao_offs, offid_lo);

	vpshufb	ymm1, ymm13, ymm0

; 563  : 
; 564  :     __m256i sums_lo = _mm256_adds_epi16  (val_lo,   offs_lo);

	vpaddsw	ymm2, ymm1, ymm3

; 566  : 
; 567  :             sums_lo = _mm256_max_epi16   (sums_lo,  zero);

	vpmaxsw	ymm3, ymm2, ymm10

; 569  : 
; 570  :     __m256i offs    = _mm256_packus_epi16(sums_lo,  sums_hi);

	vpackuswb ymm0, ymm3, ymm4

; 571  : 
; 572  :     offsets[i]      = _mm256_blendv_epi8 (vals,     offs, in_band);

	vpblendvb ymm1, ymm8, ymm0, ymm7
	vpsubb	ymm2, ymm9, ymm12
	vpaddb	ymm0, ymm2, ymm11
	vmovdqu	YMMWORD PTR [r8], ymm1

; 146  :   __m256i v_nonnegs  = _mm256_max_epi8  (zero,      v);

	vpmaxsb	ymm1, ymm0, ymm10

; 147  :   __m256i v_lobytes  = _mm256_slli_epi32(v_nonnegs, 1);

	vpslld	ymm6, ymm1, 1

; 148  :   __m256i v_hibytes  = _mm256_add_epi8  (v_lobytes, ones);

	vpaddb	ymm5, ymm6, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101

; 542  :   __m256i sao_offs     = broadcast_xmm2ymm(sao_offs_xmm);
; 543  : 
; 544  :   for (uint32_t i = 0; i < 16; i++) {
; 545  :     // bands will always be in [0, 31], and cur_bp in [0, 27], so no overflow
; 546  :     // can occur
; 547  :     __m256i band_m_bp = _mm256_sub_epi8    (bands,  cur_bp_v);
; 548  : 
; 549  :     // If (x & ~3) != 0 for any signed x, then x < 0 or x > 3
; 550  :     __m256i bmbp_bads = _mm256_andnot_si256(threes,    band_m_bp);

	vpandn	ymm0, ymm14, ymm2

; 151  :           *res_hi    = _mm256_unpackhi_epi8(v_lobytes, v_hibytes);

	vpunpckhbw ymm1, ymm6, ymm5

; 573  : 
; 574  :             vals    = _mm256_add_epi8    (vals,     val_incr);

	vpaddb	ymm8, ymm8, ymm15
	vpcmpeqb ymm7, ymm0, ymm10
	vpshufb	ymm0, ymm13, ymm1

; 105  :              *res_hi  = _mm256_unpackhi_epi8(v, zero);

	vpunpckhbw ymm2, ymm8, ymm10

; 565  :     __m256i sums_hi = _mm256_adds_epi16  (val_hi,   offs_hi);

	vpaddsw	ymm2, ymm0, ymm2

; 150  :           *res_lo    = _mm256_unpacklo_epi8(v_lobytes, v_hibytes);

	vpunpcklbw ymm0, ymm6, ymm5

; 552  : 
; 553  :     __m256i offset_id = _mm256_add_epi8    (band_m_bp, band_pos_v);
; 554  : 
; 555  :     __m256i val_lo, val_hi;
; 556  :     cvt_epu8_epi16(vals, &val_lo, &val_hi);
; 557  : 
; 558  :     __m256i offid_lo, offid_hi;
; 559  :     cvt_shufmask_epi8_epi16(offset_id, &offid_lo, &offid_hi);
; 560  : 
; 561  :     __m256i offs_lo = _mm256_shuffle_epi8(sao_offs, offid_lo);

	vpshufb	ymm1, ymm13, ymm0

; 568  :             sums_hi = _mm256_max_epi16   (sums_hi,  zero);

	vpmaxsw	ymm4, ymm2, ymm10

; 104  :              *res_lo  = _mm256_unpacklo_epi8(v, zero);

	vpunpcklbw ymm3, ymm8, ymm10

; 563  : 
; 564  :     __m256i sums_lo = _mm256_adds_epi16  (val_lo,   offs_lo);

	lea	r8, QWORD PTR [r8+128]
	vpaddsw	ymm2, ymm1, ymm3

; 566  : 
; 567  :             sums_lo = _mm256_max_epi16   (sums_lo,  zero);

	vpmaxsw	ymm3, ymm2, ymm10

; 569  : 
; 570  :     __m256i offs    = _mm256_packus_epi16(sums_lo,  sums_hi);

	vpackuswb ymm0, ymm3, ymm4

; 575  :             bands   = _mm256_add_epi8    (bands,    band_incr);

	vpaddb	ymm4, ymm9, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202
	vmovdqu	ymm9, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202
	vmovdqu	ymm3, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101
	vpblendvb ymm1, ymm8, ymm0, ymm7
	vmovdqu	YMMWORD PTR [r8-96], ymm1
	vpaddb	ymm8, ymm8, ymm15
	vmovdqu	YMMWORD PTR bands$4$[rbp], ymm4
	sub	rcx, 1
	jne	$LL4@calc_sao_o
	vzeroupper

; 576  :   }
; 577  : }

	lea	r11, QWORD PTR [rsp+224]
	vmovaps	xmm6, XMMWORD PTR [r11-16]
	vmovaps	xmm7, XMMWORD PTR [r11-32]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm9, XMMWORD PTR [r11-64]
	vmovaps	xmm10, XMMWORD PTR [r11-80]
	vmovaps	xmm11, XMMWORD PTR [r11-96]
	vmovaps	xmm12, XMMWORD PTR [r11-112]
	vmovaps	xmm13, XMMWORD PTR [r11-128]
	vmovaps	xmm14, XMMWORD PTR [rsp+80]
	vmovaps	xmm15, XMMWORD PTR [rsp+64]
	mov	rsp, r11
	pop	rbp
	ret	0
calc_sao_offset_array_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
curr_row$ = 48
offsets$ = 56
lookup_color_band_ymm PROC

; 581  : {

	sub	rsp, 40					; 00000028H

; 582  :   const __m256i select_nibble = _mm256_set1_epi8   (0x0f);

	vmovdqu	ymm1, YMMWORD PTR __ymm@0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f

; 583  :   const __m256i lo_nibbles    = _mm256_and_si256   (select_nibble, curr_row);

	vmovdqu	ymm0, YMMWORD PTR [rcx]
	vmovaps	XMMWORD PTR [rsp+16], xmm6

; 584  :   const __m256i hi_nibbles    = _mm256_andnot_si256(select_nibble, curr_row);
; 585  : 
; 586  :   // Loop through the offset vectors, the 0xi'th one always holding
; 587  :   // offsets 0xi0...0xif. Use shuffle to do a lookup on the current
; 588  :   // offset vector, then check which pixels actually should be looked
; 589  :   // up from this vector (ie. whether their values are 0xi0...0xif) and
; 590  :   // mask out any but correct ones.
; 591  :   __m256i result_row = _mm256_setzero_si256();
; 592  :   for (uint8_t i = 0; i < 16; i += 4) {

	xor	r8b, r8b
	vmovaps	XMMWORD PTR [rsp], xmm7
	mov	r9b, 32					; 00000020H
	vpand	ymm6, ymm0, ymm1
	vpandn	ymm7, ymm1, ymm0
	vpxor	xmm5, xmm5, xmm5
	npad	3
$LL4@lookup_col:

; 593  : 
; 594  :     __m256i curr_hinib0   = _mm256_set1_epi8   ((i + 0) << 4);
; 595  :     __m256i curr_hinib1   = _mm256_set1_epi8   ((i + 1) << 4);
; 596  :     __m256i curr_hinib2   = _mm256_set1_epi8   ((i + 2) << 4);
; 597  :     __m256i curr_hinib3   = _mm256_set1_epi8   ((i + 3) << 4);
; 598  : 
; 599  :     __m256i hinib_select0 = _mm256_cmpeq_epi8  (curr_hinib0,    hi_nibbles);
; 600  :     __m256i hinib_select1 = _mm256_cmpeq_epi8  (curr_hinib1,    hi_nibbles);
; 601  :     __m256i hinib_select2 = _mm256_cmpeq_epi8  (curr_hinib2,    hi_nibbles);
; 602  :     __m256i hinib_select3 = _mm256_cmpeq_epi8  (curr_hinib3,    hi_nibbles);
; 603  : 
; 604  :     __m256i lonib_lookup0 = _mm256_shuffle_epi8(offsets[i + 0], lo_nibbles);

	movzx	ecx, r8b

; 605  :     __m256i lonib_lookup1 = _mm256_shuffle_epi8(offsets[i + 1], lo_nibbles);
; 606  :     __m256i lonib_lookup2 = _mm256_shuffle_epi8(offsets[i + 2], lo_nibbles);

	lea	rax, QWORD PTR [rcx+2]
	shl	rax, 5
	vmovdqu	ymm0, YMMWORD PTR [rax+rdx]
	vpshufb	ymm2, ymm0, ymm6
	movsx	eax, r9b
	vmovd	xmm1, eax
	vpbroadcastb ymm1, xmm1
	vpcmpeqb ymm0, ymm1, ymm7

; 607  :     __m256i lonib_lookup3 = _mm256_shuffle_epi8(offsets[i + 3], lo_nibbles);
; 608  : 
; 609  :     __m256i lookup_mskd0  = _mm256_and_si256   (hinib_select0,  lonib_lookup0);
; 610  :     __m256i lookup_mskd1  = _mm256_and_si256   (hinib_select1,  lonib_lookup1);
; 611  :     __m256i lookup_mskd2  = _mm256_and_si256   (hinib_select2,  lonib_lookup2);

	vpand	ymm3, ymm0, ymm2
	lea	rax, QWORD PTR [rcx+3]
	shl	rcx, 5
	shl	rax, 5
	vmovdqu	ymm0, YMMWORD PTR [rax+rdx]
	vpshufb	ymm1, ymm0, ymm6
	lea	eax, DWORD PTR [r9+16]
	movsx	eax, al
	vmovd	xmm0, eax
	vpbroadcastb ymm0, xmm0
	vpcmpeqb ymm0, ymm0, ymm7

; 612  :     __m256i lookup_mskd3  = _mm256_and_si256   (hinib_select3,  lonib_lookup3);

	vpand	ymm1, ymm0, ymm1
	vmovdqu	ymm0, YMMWORD PTR [rcx+rdx]

; 613  : 
; 614  :     __m256i lookup_mskd01 = _mm256_or_si256    (lookup_mskd0,   lookup_mskd1);
; 615  :     __m256i lookup_mskd23 = _mm256_or_si256    (lookup_mskd2,   lookup_mskd3);

	vpor	ymm4, ymm1, ymm3
	vpshufb	ymm1, ymm0, ymm6
	movzx	eax, r8b
	shl	al, 4
	movsx	eax, al
	vmovd	xmm0, eax
	movzx	eax, r8b
	add	r8b, 4
	inc	rax
	shl	rax, 5
	vpbroadcastb ymm0, xmm0
	vpcmpeqb ymm0, ymm0, ymm7
	vpand	ymm3, ymm0, ymm1
	vmovdqu	ymm0, YMMWORD PTR [rax+rdx]
	lea	eax, DWORD PTR [r9-16]
	add	r9b, 64					; 00000040H
	movsx	eax, al
	vmovd	xmm1, eax
	vpbroadcastb ymm1, xmm1
	vpcmpeqb ymm1, ymm1, ymm7
	vpshufb	ymm2, ymm0, ymm6
	vpand	ymm2, ymm1, ymm2
	vpor	ymm3, ymm2, ymm3

; 616  :     __m256i lookup_res    = _mm256_or_si256    (lookup_mskd01,  lookup_mskd23);

	vpor	ymm4, ymm3, ymm4

; 617  : 
; 618  :             result_row    = _mm256_or_si256    (result_row,     lookup_res);

	vpor	ymm5, ymm4, ymm5
	cmp	r8b, 16
	jb	$LL4@lookup_col

; 619  :   }
; 620  :   return result_row;
; 621  : }

	vmovaps	xmm6, XMMWORD PTR [rsp+16]
	vmovaps	xmm7, XMMWORD PTR [rsp]
	vmovdqu	ymm0, ymm5
	add	rsp, 40					; 00000028H
	ret	0
lookup_color_band_ymm ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
width_db4$1$ = 0
db4_mask$1$ = 32
offsets$ = 64
encoder$dead$ = 752
rec_data$ = 760
new_rec_data$ = 768
sao$ = 776
stride$ = 784
new_stride$ = 792
block_width$ = 800
block_height$ = 808
color_i$ = 816
reconstruct_color_band PROC

; 632  : {

	mov	rax, rsp
	mov	QWORD PTR [rax+8], rbx
	mov	QWORD PTR [rax+16], rsi
	mov	QWORD PTR [rax+24], rdi
	push	rbp
	push	r12
	push	r13
	push	r14
	push	r15
	sub	rsp, 704				; 000002c0H
	vmovaps	XMMWORD PTR [rax-56], xmm6
	vmovaps	XMMWORD PTR [rax-72], xmm7
	vmovaps	XMMWORD PTR [rax-88], xmm8
	vmovaps	XMMWORD PTR [rax-104], xmm9
	lea	rbp, QWORD PTR [rsp+64]
	and	rbp, -32				; ffffffffffffffe0H

; 633  :   const uint32_t width_db32 = block_width & ~31;

	mov	ecx, DWORD PTR block_width$[rsp]
	mov	r10, r9

; 634  :   const uint32_t width_db4  = block_width &  ~3;
; 635  :   const uint32_t width_rest = block_width &   3;
; 636  : 
; 637  :   // Form the load&store mask
; 638  :   const __m256i wdb4_256      = _mm256_set1_epi32 (width_db4 & 31);
; 639  :   const __m256i indexes       = _mm256_setr_epi32 (3, 7, 11, 15, 19, 23, 27, 31);
; 640  :   const __m256i db4_mask      = _mm256_cmpgt_epi32(wdb4_256, indexes);
; 641  : 
; 642  :   // Each of the 256 offsets is a byte, but only 16 are held in one YMM since
; 643  :   // lanes must be duplicated to use shuffle.
; 644  :   __m256i offsets[16];
; 645  :   calc_sao_offset_array_avx2(encoder, sao, offsets, color_i);

	mov	r9d, DWORD PTR color_i$[rsp]
	mov	eax, ecx
	and	eax, -4					; fffffffcH
	mov	r13d, ecx
	mov	DWORD PTR width_db4$1$[rbp], eax
	mov	r14d, ecx
	and	eax, 31
	mov	rdi, r8
	vmovd	xmm0, eax
	vpbroadcastd ymm0, xmm0
	vpcmpgtd ymm2, ymm0, YMMWORD PTR __ymm@0000001f0000001b00000017000000130000000f0000000b0000000700000003
	mov	r12, rdx
	lea	r8, QWORD PTR offsets$[rbp]
	vmovdqu	YMMWORD PTR db4_mask$1$[rbp], ymm2
	and	r13d, -32				; ffffffe0H
	and	r14d, 3
	mov	rdx, r10
	vzeroupper
	call	calc_sao_offset_array_avx2
	xor	r15d, r15d

; 646  : 
; 647  :   for (uint32_t y = 0; y < block_height; y++) {

	cmp	DWORD PTR block_height$[rsp], r15d
	jbe	$LN3@reconstruc
	mov	r8d, DWORD PTR new_stride$[rsp]
	mov	eax, DWORD PTR stride$[rsp]
$LL4@reconstruc:

; 648  :     uint32_t x = 0;

	xor	esi, esi

; 649  :     for (; x < width_db32; x += 32) {

	test	r13d, r13d
	je	$LN6@reconstruc

; 648  :     uint32_t x = 0;

	vmovdqu	ymm8, YMMWORD PTR __ymm@0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f
	lea	esi, DWORD PTR [r13-1]

; 582  :   const __m256i select_nibble = _mm256_set1_epi8   (0x0f);

	mov	r11d, r8d

; 648  :     uint32_t x = 0;

	shr	esi, 5

; 582  :   const __m256i select_nibble = _mm256_set1_epi8   (0x0f);

	sub	r11d, eax

; 648  :     uint32_t x = 0;

	inc	esi
	imul	r11d, r15d
	mov	edx, r15d
	mov	r10d, esi
	imul	edx, eax
	shl	esi, 5
	vpxor	xmm9, xmm9, xmm9
	npad	7
$LL7@reconstruc:

; 583  :   const __m256i lo_nibbles    = _mm256_and_si256   (select_nibble, curr_row);

	vpand	ymm6, ymm8, YMMWORD PTR [rdx+r12]

; 584  :   const __m256i hi_nibbles    = _mm256_andnot_si256(select_nibble, curr_row);

	vpandn	ymm7, ymm8, YMMWORD PTR [rdx+r12]

; 650  :       const uint32_t curr_srcpos = y *     stride + x;
; 651  :       const uint32_t curr_dstpos = y * new_stride + x;

	lea	ebx, DWORD PTR [r11+rdx]

; 592  :   for (uint8_t i = 0; i < 16; i += 4) {

	xor	r8b, r8b
	vmovdqu	ymm5, ymm9

; 650  :       const uint32_t curr_srcpos = y *     stride + x;
; 651  :       const uint32_t curr_dstpos = y * new_stride + x;

	mov	r9b, 32					; 00000020H
	npad	5
$LL13@reconstruc:

; 604  :     __m256i lonib_lookup0 = _mm256_shuffle_epi8(offsets[i + 0], lo_nibbles);

	movzx	ecx, r8b

; 605  :     __m256i lonib_lookup1 = _mm256_shuffle_epi8(offsets[i + 1], lo_nibbles);
; 606  :     __m256i lonib_lookup2 = _mm256_shuffle_epi8(offsets[i + 2], lo_nibbles);

	mov	eax, ecx
	shl	rax, 5
	vmovdqu	ymm0, YMMWORD PTR offsets$[rbp+rax+64]
	vpshufb	ymm2, ymm0, ymm6
	movsx	eax, r9b
	vmovd	xmm1, eax
	vpbroadcastb ymm1, xmm1
	vpcmpeqb ymm0, ymm1, ymm7

; 607  :     __m256i lonib_lookup3 = _mm256_shuffle_epi8(offsets[i + 3], lo_nibbles);
; 608  : 
; 609  :     __m256i lookup_mskd0  = _mm256_and_si256   (hinib_select0,  lonib_lookup0);
; 610  :     __m256i lookup_mskd1  = _mm256_and_si256   (hinib_select1,  lonib_lookup1);
; 611  :     __m256i lookup_mskd2  = _mm256_and_si256   (hinib_select2,  lonib_lookup2);

	vpand	ymm3, ymm0, ymm2
	mov	eax, ecx
	shl	rcx, 5
	shl	rax, 5
	vmovdqu	ymm0, YMMWORD PTR offsets$[rbp+rax+96]
	vpshufb	ymm1, ymm0, ymm6
	lea	eax, DWORD PTR [r9+16]
	movsx	eax, al
	vmovd	xmm0, eax
	vpbroadcastb ymm0, xmm0
	vpcmpeqb ymm0, ymm0, ymm7

; 612  :     __m256i lookup_mskd3  = _mm256_and_si256   (hinib_select3,  lonib_lookup3);

	vpand	ymm1, ymm0, ymm1
	vmovdqu	ymm0, YMMWORD PTR offsets$[rbp+rcx]
	movzx	eax, r8b
	shl	al, 4
	movsx	eax, al

; 613  : 
; 614  :     __m256i lookup_mskd01 = _mm256_or_si256    (lookup_mskd0,   lookup_mskd1);
; 615  :     __m256i lookup_mskd23 = _mm256_or_si256    (lookup_mskd2,   lookup_mskd3);

	vpor	ymm4, ymm1, ymm3
	vpshufb	ymm1, ymm0, ymm6
	vmovd	xmm0, eax
	vpbroadcastb ymm0, xmm0
	vpcmpeqb ymm0, ymm0, ymm7
	vpand	ymm2, ymm0, ymm1
	movzx	eax, r8b
	add	r8b, 4

; 616  :     __m256i lookup_res    = _mm256_or_si256    (lookup_mskd01,  lookup_mskd23);
; 617  : 
; 618  :             result_row    = _mm256_or_si256    (result_row,     lookup_res);

	shl	rax, 5
	vmovdqu	ymm0, YMMWORD PTR offsets$[rbp+rax+32]
	vpshufb	ymm1, ymm0, ymm6
	lea	eax, DWORD PTR [r9-16]
	add	r9b, 64					; 00000040H
	movsx	eax, al
	vmovd	xmm0, eax
	vpbroadcastb ymm0, xmm0
	vpcmpeqb ymm0, ymm0, ymm7
	vpand	ymm1, ymm0, ymm1
	vpor	ymm2, ymm1, ymm2
	vpor	ymm3, ymm2, ymm4
	vpor	ymm5, ymm3, ymm5
	cmp	r8b, 16
	jb	$LL13@reconstruc

; 652  : 
; 653  :       __m256i curr_row = _mm256_loadu_si256((const __m256i *)(rec_data + curr_srcpos));
; 654  :       __m256i result   = lookup_color_band_ymm(curr_row, offsets);
; 655  :       _mm256_storeu_si256((__m256i *)(new_rec_data + curr_dstpos), result);

	add	edx, 32					; 00000020H
	vmovdqu	YMMWORD PTR [rbx+rdi], ymm5
	sub	r10, 1
	jne	$LL7@reconstruc
	mov	eax, DWORD PTR stride$[rsp]
	mov	r8d, DWORD PTR new_stride$[rsp]
$LN6@reconstruc:

; 656  :     }
; 657  :     if (block_width > width_db32) {

	cmp	DWORD PTR block_width$[rsp], r13d
	jbe	$LN2@reconstruc

; 658  :       const uint32_t curr_srcpos = y *     stride + x;

	mov	ecx, r15d

; 181  :   uint32_t last_dword = 0;

	xor	r9d, r9d

; 658  :       const uint32_t curr_srcpos = y *     stride + x;

	imul	ecx, eax

; 659  :       const uint32_t curr_dstpos = y * new_stride + x;

	mov	eax, r15d
	imul	eax, r8d

; 661  :       const uint32_t rest_dstpos = y * new_stride + width_db4;

	mov	r8d, DWORD PTR width_db4$1$[rbp]
	lea	ebx, DWORD PTR [rcx+rsi]
	add	esi, eax
	lea	r11d, DWORD PTR [rax+r8]

; 182  :   for (int32_t i = 0; i < width_rest; i++) {

	test	r14d, r14d
	je	SHORT $LN17@reconstruc

; 660  :       const uint32_t rest_srcpos = y *     stride + width_db4;

	lea	eax, DWORD PTR [rcx+r8]
	mov	r10d, r14d
	movsxd	r8, eax
	xor	edx, edx
	add	r8, r12
$LL42@reconstruc:

; 184  :     uint32_t currd = ((uint32_t)currb) << (i * 8);

	movzx	eax, BYTE PTR [r8]
	lea	r8, QWORD PTR [r8+1]
	shlx	ecx, eax, edx

; 185  :     last_dword |= currd;

	or	r9d, ecx
	add	edx, 8
	sub	r10, 1
	jne	SHORT $LL42@reconstruc
$LN17@reconstruc:

; 672  : 
; 673  :       __m256i curr_row = _mm256_maskload_epi32(src_ptr,  db4_mask);

	vmovdqu	ymm8, YMMWORD PTR db4_mask$1$[rbp]
	vpmaskmovd ymm0, ymm8, YMMWORD PTR [rbx+r12]

; 674  :               curr_row = _mm256_insert_epi32  (curr_row, last_dword, 7);

	vmovd	xmm1, r9d
	vpbroadcastd ymm1, xmm1
	vpblendd ymm2, ymm0, ymm1, 128			; 00000080H

; 582  :   const __m256i select_nibble = _mm256_set1_epi8   (0x0f);

	vmovdqu	ymm1, YMMWORD PTR __ymm@0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f

; 583  :   const __m256i lo_nibbles    = _mm256_and_si256   (select_nibble, curr_row);

	vpand	ymm6, ymm2, ymm1

; 584  :   const __m256i hi_nibbles    = _mm256_andnot_si256(select_nibble, curr_row);

	vpandn	ymm7, ymm1, ymm2
	vpxor	xmm5, xmm5, xmm5

; 585  : 
; 586  :   // Loop through the offset vectors, the 0xi'th one always holding
; 587  :   // offsets 0xi0...0xif. Use shuffle to do a lookup on the current
; 588  :   // offset vector, then check which pixels actually should be looked
; 589  :   // up from this vector (ie. whether their values are 0xi0...0xif) and
; 590  :   // mask out any but correct ones.
; 591  :   __m256i result_row = _mm256_setzero_si256();
; 592  :   for (uint8_t i = 0; i < 16; i += 4) {

	xor	dl, dl

; 662  : 
; 663  :       // Read the very last pixels byte by byte and pack them into one dword.
; 664  :       // Piggyback said dword as the highest dword of the row vector variable,
; 665  :       // that particular place can never be loaded into by the maskmove
; 666  :       // (otherwise that vector would go through the divisible-by-32 code
; 667  :       // path).
; 668  :       uint32_t last_dword = load_border_bytes(rec_data, rest_srcpos, width_rest);
; 669  : 
; 670  :       const int32_t *src_ptr = (const int32_t *)(    rec_data + curr_srcpos);
; 671  :             int32_t *dst_ptr = (      int32_t *)(new_rec_data + curr_dstpos);

	mov	r8b, 32					; 00000020H
	npad	3
$LL23@reconstruc:

; 604  :     __m256i lonib_lookup0 = _mm256_shuffle_epi8(offsets[i + 0], lo_nibbles);

	movzx	ecx, dl

; 605  :     __m256i lonib_lookup1 = _mm256_shuffle_epi8(offsets[i + 1], lo_nibbles);
; 606  :     __m256i lonib_lookup2 = _mm256_shuffle_epi8(offsets[i + 2], lo_nibbles);

	mov	eax, ecx
	shl	rax, 5
	vmovdqu	ymm0, YMMWORD PTR offsets$[rbp+rax+64]
	vpshufb	ymm2, ymm0, ymm6
	movsx	eax, r8b
	vmovd	xmm1, eax
	vpbroadcastb ymm1, xmm1
	vpcmpeqb ymm0, ymm1, ymm7

; 607  :     __m256i lonib_lookup3 = _mm256_shuffle_epi8(offsets[i + 3], lo_nibbles);
; 608  : 
; 609  :     __m256i lookup_mskd0  = _mm256_and_si256   (hinib_select0,  lonib_lookup0);
; 610  :     __m256i lookup_mskd1  = _mm256_and_si256   (hinib_select1,  lonib_lookup1);
; 611  :     __m256i lookup_mskd2  = _mm256_and_si256   (hinib_select2,  lonib_lookup2);

	vpand	ymm3, ymm0, ymm2
	mov	eax, ecx
	shl	rcx, 5
	shl	rax, 5
	vmovdqu	ymm0, YMMWORD PTR offsets$[rbp+rax+96]
	vpshufb	ymm1, ymm0, ymm6
	lea	eax, DWORD PTR [r8+16]
	movsx	eax, al
	vmovd	xmm0, eax
	vpbroadcastb ymm0, xmm0
	vpcmpeqb ymm0, ymm0, ymm7

; 612  :     __m256i lookup_mskd3  = _mm256_and_si256   (hinib_select3,  lonib_lookup3);

	vpand	ymm1, ymm0, ymm1
	vmovdqu	ymm0, YMMWORD PTR offsets$[rbp+rcx]
	movzx	eax, dl
	shl	al, 4
	movsx	eax, al

; 613  : 
; 614  :     __m256i lookup_mskd01 = _mm256_or_si256    (lookup_mskd0,   lookup_mskd1);
; 615  :     __m256i lookup_mskd23 = _mm256_or_si256    (lookup_mskd2,   lookup_mskd3);

	vpor	ymm4, ymm1, ymm3
	vpshufb	ymm1, ymm0, ymm6
	vmovd	xmm0, eax
	vpbroadcastb ymm0, xmm0
	vpcmpeqb ymm0, ymm0, ymm7
	vpand	ymm2, ymm0, ymm1
	movzx	eax, dl
	add	dl, 4

; 616  :     __m256i lookup_res    = _mm256_or_si256    (lookup_mskd01,  lookup_mskd23);
; 617  : 
; 618  :             result_row    = _mm256_or_si256    (result_row,     lookup_res);

	shl	rax, 5
	vmovdqu	ymm0, YMMWORD PTR offsets$[rbp+rax+32]
	vpshufb	ymm1, ymm0, ymm6
	lea	eax, DWORD PTR [r8-16]
	add	r8b, 64					; 00000040H
	movsx	eax, al
	vmovd	xmm0, eax
	vpbroadcastb ymm0, xmm0
	vpcmpeqb ymm0, ymm0, ymm7
	vpand	ymm1, ymm0, ymm1
	vpor	ymm2, ymm1, ymm2
	vpor	ymm3, ymm2, ymm4
	vpor	ymm5, ymm3, ymm5
	cmp	dl, 16
	jb	$LL23@reconstruc

; 675  :       __m256i result   = lookup_color_band_ymm(curr_row, offsets);
; 676  : 
; 677  :       _mm256_maskstore_epi32(dst_ptr, db4_mask, result);
; 678  :       uint32_t last_dword_dst = _mm256_extract_epi32(result, 7);

	xor	eax, eax
	vextracti128 xmm0, ymm5, 1
	vpmaskmovd YMMWORD PTR [rsi+rdi], ymm8, ymm5
	vpextrd	r8d, xmm0, 3

; 195  :   for (uint32_t i = 0; i < width_rest; i++) {

	test	r14d, r14d
	je	SHORT $LN2@reconstruc
	npad	12
$LL44@reconstruc:

; 196  :     uint8_t currb = data & 0xff;
; 197  :     buf[start_pos + i] = currb;

	lea	ecx, DWORD PTR [rax+r11]
	inc	eax
	mov	BYTE PTR [rcx+rdi], r8b

; 198  :     data >>= 8;

	shr	r8d, 8
	cmp	eax, r14d
	jb	SHORT $LL44@reconstruc
$LN2@reconstruc:

; 646  : 
; 647  :   for (uint32_t y = 0; y < block_height; y++) {

	mov	eax, DWORD PTR stride$[rsp]
	inc	r15d
	mov	r8d, DWORD PTR new_stride$[rsp]
	cmp	r15d, DWORD PTR block_height$[rsp]
	jb	$LL4@reconstruc
$LN3@reconstruc:
	vzeroupper

; 679  : 
; 680  :       store_border_bytes(new_rec_data, rest_dstpos, width_rest, last_dword_dst);
; 681  :     }
; 682  :   }
; 683  : }

	lea	r11, QWORD PTR [rsp+704]
	mov	rbx, QWORD PTR [r11+48]
	mov	rsi, QWORD PTR [r11+56]
	mov	rdi, QWORD PTR [r11+64]
	vmovaps	xmm6, XMMWORD PTR [r11-16]
	vmovaps	xmm7, XMMWORD PTR [r11-32]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm9, XMMWORD PTR [r11-64]
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r13
	pop	r12
	pop	rbp
	ret	0
reconstruct_color_band ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
a$ = 96
b$ = 104
c$ = 112
sao_offs$ = 120
do_one_nonband_ymm PROC

; 689  : {

	mov	rax, rsp
	sub	rsp, 88					; 00000058H

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vmovdqu	ymm0, YMMWORD PTR [rdx]

; 693  :   __m256i eo_cat_lo, eo_cat_hi, c_lo, c_hi;
; 694  :   cvt_shufmask_epi8_epi16(eo_cat, &eo_cat_lo, &eo_cat_hi);
; 695  :   cvt_epu8_epi16         (c,      &c_lo,      &c_hi);
; 696  : 
; 697  :   __m256i offs_lo = _mm256_shuffle_epi8(sao_offs, eo_cat_lo);
; 698  :   __m256i offs_hi = _mm256_shuffle_epi8(sao_offs, eo_cat_hi);

	vmovdqu	ymm5, YMMWORD PTR [r9]
	vmovaps	XMMWORD PTR [rax-24], xmm6

; 59   :   const __m256i epu2epi = _mm256_set1_epi8  (0x80);

	vmovdqu	ymm6, YMMWORD PTR __ymm@8080808080808080808080808080808080808080808080808080808080808080

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vpsubb	ymm1, ymm0, ymm6
	vmovdqu	ymm0, YMMWORD PTR [rcx]
	vmovaps	XMMWORD PTR [rax-40], xmm7
	vmovaps	XMMWORD PTR [rax-56], xmm8

; 60   :   const __m256i ones    = _mm256_set1_epi8  (0x01);

	vmovdqu	ymm8, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101
	vmovaps	XMMWORD PTR [rax-72], xmm9

; 690  :   const __m256i zero = _mm256_setzero_si256();
; 691  : 
; 692  :   __m256i eo_cat = calc_eo_cat(a, b, c);

	vmovdqu	ymm9, YMMWORD PTR [r8]

; 62   :   __m256i a_signed      = _mm256_sub_epi8   (a,        epu2epi);

	vpsubb	ymm3, ymm9, ymm6

; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm2, ymm3, ymm1

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vpsubb	ymm1, ymm0, ymm6

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm4, ymm8, ymm2

; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm2, ymm3, ymm1

; 75   :   const __m256i idx_to_cat = _mm256_setr_epi64x(0x0403000201, 0,

	vmovdqu	ymm1, YMMWORD PTR __ymm@0000000000000000000000040300020100000000000000000000000403000201

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm3, ymm8, ymm2
	vmovaps	XMMWORD PTR [rsp], xmm10
	vpxor	xmm10, xmm10, xmm10

; 81   :   __m256i signsum          = _mm256_add_epi8    (c_a_sign,   c_b_sign);

	vpaddb	ymm0, ymm3, ymm4

; 82   :   __m256i eo_idx           = _mm256_add_epi8    (signsum,    twos);

	vpaddb	ymm4, ymm0, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202

; 83   : 
; 84   :   return                     _mm256_shuffle_epi8(idx_to_cat, eo_idx);

	vpshufb	ymm2, ymm1, ymm4

; 146  :   __m256i v_nonnegs  = _mm256_max_epi8  (zero,      v);

	vpmaxsb	ymm0, ymm2, ymm10

; 147  :   __m256i v_lobytes  = _mm256_slli_epi32(v_nonnegs, 1);

	vpslld	ymm7, ymm0, 1

; 148  :   __m256i v_hibytes  = _mm256_add_epi8  (v_lobytes, ones);

	vpaddb	ymm6, ymm8, ymm7

; 699  : 
; 700  :   __m256i res_lo  = _mm256_adds_epi16  (offs_lo,  c_lo);
; 701  :   __m256i res_hi  = _mm256_adds_epi16  (offs_hi,  c_hi);
; 702  : 
; 703  :           res_lo  = _mm256_max_epi16   (res_lo,   zero);
; 704  :           res_hi  = _mm256_max_epi16   (res_hi,   zero);
; 705  : 
; 706  :   __m256i res     = _mm256_packus_epi16(res_lo,   res_hi);
; 707  :   return res;
; 708  : }

	vmovaps	xmm8, XMMWORD PTR [rax-56]

; 151  :           *res_hi    = _mm256_unpackhi_epi8(v_lobytes, v_hibytes);

	vpunpckhbw ymm0, ymm7, ymm6

; 693  :   __m256i eo_cat_lo, eo_cat_hi, c_lo, c_hi;
; 694  :   cvt_shufmask_epi8_epi16(eo_cat, &eo_cat_lo, &eo_cat_hi);
; 695  :   cvt_epu8_epi16         (c,      &c_lo,      &c_hi);
; 696  : 
; 697  :   __m256i offs_lo = _mm256_shuffle_epi8(sao_offs, eo_cat_lo);
; 698  :   __m256i offs_hi = _mm256_shuffle_epi8(sao_offs, eo_cat_hi);

	vpshufb	ymm1, ymm5, ymm0

; 150  :           *res_lo    = _mm256_unpacklo_epi8(v_lobytes, v_hibytes);

	vpunpcklbw ymm0, ymm7, ymm6

; 699  : 
; 700  :   __m256i res_lo  = _mm256_adds_epi16  (offs_lo,  c_lo);
; 701  :   __m256i res_hi  = _mm256_adds_epi16  (offs_hi,  c_hi);
; 702  : 
; 703  :           res_lo  = _mm256_max_epi16   (res_lo,   zero);
; 704  :           res_hi  = _mm256_max_epi16   (res_hi,   zero);
; 705  : 
; 706  :   __m256i res     = _mm256_packus_epi16(res_lo,   res_hi);
; 707  :   return res;
; 708  : }

	vmovaps	xmm6, XMMWORD PTR [rax-24]
	vmovaps	xmm7, XMMWORD PTR [rax-40]

; 104  :              *res_lo  = _mm256_unpacklo_epi8(v, zero);

	vpunpcklbw ymm3, ymm9, ymm10

; 105  :              *res_hi  = _mm256_unpackhi_epi8(v, zero);

	vpunpckhbw ymm2, ymm9, ymm10

; 699  : 
; 700  :   __m256i res_lo  = _mm256_adds_epi16  (offs_lo,  c_lo);
; 701  :   __m256i res_hi  = _mm256_adds_epi16  (offs_hi,  c_hi);
; 702  : 
; 703  :           res_lo  = _mm256_max_epi16   (res_lo,   zero);
; 704  :           res_hi  = _mm256_max_epi16   (res_hi,   zero);
; 705  : 
; 706  :   __m256i res     = _mm256_packus_epi16(res_lo,   res_hi);
; 707  :   return res;
; 708  : }

	vmovaps	xmm9, XMMWORD PTR [rax-72]
	vpaddsw	ymm2, ymm1, ymm2
	vpmaxsw	ymm4, ymm2, ymm10
	vpshufb	ymm1, ymm5, ymm0
	vpaddsw	ymm2, ymm1, ymm3
	vpmaxsw	ymm3, ymm2, ymm10
	vmovaps	xmm10, XMMWORD PTR [rsp]
	vpackuswb ymm0, ymm3, ymm4
	add	rsp, 88					; 00000058H
	ret	0
do_one_nonband_ymm ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
tv1930 = 0
tv1927 = 0
rest_srcpos$1$ = 8
curr_srcpos$1$ = 12
curr_bpos$1$ = 16
curr_apos$1$ = 20
curr_dstpos$1$ = 24
rest_dstpos$1$ = 32
tv1926 = 32
b_ofs$ = 40
a_ofs$1$ = 48
width_db32$1$ = 256
encoder$dead$ = 256
rec_data$ = 264
new_rec_data$ = 272
width_db4$1$ = 280
sao$ = 280
stride$ = 288
new_stride$ = 296
block_width$ = 304
block_height$ = 312
tv1846 = 320
color_i$ = 320
reconstruct_color_other PROC

; 719  : {

	mov	QWORD PTR [rsp+16], rbx
	mov	QWORD PTR [rsp+24], rsi
	mov	QWORD PTR [rsp+8], rcx
	push	rdi
	push	r12
	push	r13
	push	r14
	push	r15
	sub	rsp, 208				; 000000d0H

; 720  :   const uint32_t   offset_v    = color_i == COLOR_V ? 5 : 0;
; 721  :   const vector2d_t a_ofs       = g_sao_edge_offsets[sao->eo_class][0];

	movsxd	rax, DWORD PTR [r9+4]
	mov	r12, rdx
	mov	ebx, DWORD PTR stride$[rsp]
	add	rax, rax
	mov	r10, r9
	vmovaps	XMMWORD PTR [rsp+80], xmm13
	mov	r13, r8
	vmovaps	XMMWORD PTR [rsp+64], xmm14
	lea	r8, OFFSET FLAT:g_sao_edge_offsets

; 722  :   const vector2d_t b_ofs       = g_sao_edge_offsets[sao->eo_class][1];
; 723  : 
; 724  :   const uint32_t   width_db32  = block_width & ~31;
; 725  :   const uint32_t   width_db4   = block_width &  ~3;
; 726  :   const uint32_t   width_rest  = block_width &   3;
; 727  : 
; 728  :   // Form the load&store mask
; 729  :   const __m256i    wdb4_256    = _mm256_set1_epi32 (width_db4 & 31);
; 730  :   const __m256i    indexes     = _mm256_setr_epi32 (3, 7, 11, 15, 19, 23, 27, 31);
; 731  :   const __m256i    db4_mask    = _mm256_cmpgt_epi32(wdb4_256, indexes);
; 732  : 
; 733  :   // Again, saturate offsets to signed 16 bits, because anything outside of
; 734  :   // [-255, 255] will saturate anything these are used with
; 735  :   const __m128i    sao_offs_lo = _mm_loadu_si128  ((const __m128i *)(sao->offsets + offset_v + 0));

	mov	r9d, 28
	mov	rcx, QWORD PTR [r8+rax*8]

; 736  :   const __m128i    sao_offs_hi = _mm_cvtsi32_si128(sao->offsets[offset_v + 4]);

	mov	edi, 64					; 00000040H
	mov	rax, QWORD PTR [r8+rax*8+8]
	mov	rdx, rcx
	mov	r8d, DWORD PTR block_width$[rsp]
	mov	rsi, rcx
	shr	rdx, 32					; 00000020H
	mov	r11d, r8d
	mov	QWORD PTR tv1927[rsp], rdx
	and	r11d, -4				; fffffffcH
	shr	rsi, 32					; 00000020H
	mov	rdx, rax
	shr	rdx, 32					; 00000020H
	mov	r14d, r8d
	mov	QWORD PTR tv1926[rsp], rdx
	and	r14d, 3
	mov	edx, r8d
	mov	QWORD PTR a_ofs$1$[rsp], rcx
	and	edx, -32				; ffffffe0H
	mov	QWORD PTR b_ofs$[rsp], rax
	mov	DWORD PTR width_db32$1$[rsp], edx
	mov	edx, r11d
	and	edx, 31
	mov	DWORD PTR width_db4$1$[rsp], r11d
	cmp	DWORD PTR color_i$[rsp], 2
	vmovd	xmm0, edx
	mov	edx, 48					; 00000030H
	cmove	r9d, edx
	mov	edx, 44					; 0000002cH
	cmove	edx, edi
	xor	r15d, r15d
	vpbroadcastd ymm0, xmm0
	vpcmpgtd ymm14, ymm0, YMMWORD PTR __ymm@0000001f0000001b00000017000000130000000f0000000b0000000700000003

; 737  :   const __m128i    sao_offs_16 = _mm_packs_epi32  (sao_offs_lo, sao_offs_hi);

	vmovdqu	xmm0, XMMWORD PTR [r9+r10]
	vmovd	xmm1, DWORD PTR [rdx+r10]
	vpackssdw xmm2, xmm0, xmm1

; 218  :   __m256i res = _mm256_castsi128_si256 (v);

	vmovups	xmm1, xmm2

; 219  :   return        _mm256_inserti128_si256(res, v, 1);

	vinserti128 ymm13, ymm1, xmm2, 1

; 738  : 
; 739  :   const __m256i    sao_offs    = broadcast_xmm2ymm(sao_offs_16);
; 740  : 
; 741  :   for (uint32_t y = 0; y < block_height; y++) {

	cmp	DWORD PTR block_height$[rsp], r15d
	jbe	$LN3@reconstruc

; 720  :   const uint32_t   offset_v    = color_i == COLOR_V ? 5 : 0;
; 721  :   const vector2d_t a_ofs       = g_sao_edge_offsets[sao->eo_class][0];

	mov	edx, DWORD PTR width_db32$1$[rsp]
	mov	edi, ebx
	imul	edi, DWORD PTR tv1927[rsp]
	vmovaps	XMMWORD PTR [rsp+192], xmm6
	vmovaps	XMMWORD PTR [rsp+176], xmm7
	vmovaps	XMMWORD PTR [rsp+160], xmm8
	vmovaps	XMMWORD PTR [rsp+144], xmm9
	add	edi, ecx
	vmovaps	XMMWORD PTR [rsp+128], xmm10
	sub	esi, DWORD PTR tv1926[rsp]
	vmovaps	XMMWORD PTR [rsp+112], xmm11
	mov	QWORD PTR tv1930[rsp], rsi
	vmovaps	XMMWORD PTR [rsp+96], xmm12
	mov	DWORD PTR tv1846[rsp], edi
$LL4@reconstruc:

; 742  :     uint32_t x;
; 743  :     for (x = 0; x < width_db32; x += 32) {

	xor	r10d, r10d
	test	edx, edx
	je	$LN6@reconstruc

; 744  :       const uint32_t  src_pos = y *     stride + x;
; 745  :       const uint32_t  dst_pos = y * new_stride + x;

	mov	r8d, DWORD PTR b_ofs$[rsp+4]
	add	esi, r15d
	mov	r10d, DWORD PTR width_db32$1$[rsp]
	add	r8d, r15d
	vmovdqu	ymm9, YMMWORD PTR __ymm@8080808080808080808080808080808080808080808080808080808080808080
	vmovdqu	ymm10, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101
	vmovdqu	ymm11, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202
	vmovdqu	ymm12, YMMWORD PTR __ymm@0000000000000000000000040300020100000000000000000000000403000201
	imul	r8d, ebx
	mov	r11d, r15d
	imul	r11d, ebx
	imul	esi, ebx
	mov	ebx, r15d
	imul	ebx, DWORD PTR new_stride$[rsp]
	vpxor	xmm8, xmm8, xmm8
	sub	r11d, r8d
	sub	esi, eax
	sub	r11d, eax
	sub	ebx, r8d
	lea	r9d, DWORD PTR [r8+rax]
	sub	ebx, eax
	add	esi, ecx
	dec	r10d
	shr	r10d, 5
	inc	r10d
	mov	edi, r10d
	shl	r10d, 5
$LL7@reconstruc:

; 753  :       __m256i b = _mm256_loadu_si256((const __m256i *)(rec_data + b_pos));
; 754  :       __m256i c = _mm256_loadu_si256((const __m256i *)(rec_data + src_pos));

	lea	edx, DWORD PTR [r11+r9]

; 755  : 
; 756  :       __m256i res = do_one_nonband_ymm(a, b, c, sao_offs);

	vmovdqu	ymm7, YMMWORD PTR [rdx+r12]
	movsxd	rdx, r9d

; 62   :   __m256i a_signed      = _mm256_sub_epi8   (a,        epu2epi);

	vpsubb	ymm3, ymm7, ymm9

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vmovdqu	ymm0, YMMWORD PTR [rdx+r12]
	vpsubb	ymm1, ymm0, ymm9

; 64   : 
; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm2, ymm3, ymm1

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm4, ymm10, ymm2

; 746  : 
; 747  :       // TODO: these will go negative, but that's a defect of the original
; 748  :       // code already since 2013 (98f2a1aedc5f4933c2729ae15412549dea9e5549)
; 749  :       const int32_t   a_pos   = (y + a_ofs.y) * stride + x + a_ofs.x;
; 750  :       const int32_t   b_pos   = (y + b_ofs.y) * stride + x + b_ofs.x;
; 751  : 
; 752  :       __m256i a = _mm256_loadu_si256((const __m256i *)(rec_data + a_pos));

	lea	edx, DWORD PTR [rsi+r9]
	movsxd	r8, edx

; 757  :       _mm256_storeu_si256((__m256i *)(new_rec_data + dst_pos), res);

	lea	edx, DWORD PTR [rbx+r9]
	add	r9d, 32					; 00000020H

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vmovdqu	ymm0, YMMWORD PTR [r8+r12]
	vpsubb	ymm1, ymm0, ymm9

; 64   : 
; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm2, ymm3, ymm1

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm3, ymm10, ymm2

; 81   :   __m256i signsum          = _mm256_add_epi8    (c_a_sign,   c_b_sign);

	vpaddb	ymm0, ymm3, ymm4

; 82   :   __m256i eo_idx           = _mm256_add_epi8    (signsum,    twos);

	vpaddb	ymm1, ymm0, ymm11

; 83   : 
; 84   :   return                     _mm256_shuffle_epi8(idx_to_cat, eo_idx);

	vpshufb	ymm2, ymm12, ymm1

; 146  :   __m256i v_nonnegs  = _mm256_max_epi8  (zero,      v);

	vpmaxsb	ymm3, ymm2, ymm8

; 147  :   __m256i v_lobytes  = _mm256_slli_epi32(v_nonnegs, 1);

	vpslld	ymm6, ymm3, 1

; 148  :   __m256i v_hibytes  = _mm256_add_epi8  (v_lobytes, ones);

	vpaddb	ymm5, ymm10, ymm6

; 151  :           *res_hi    = _mm256_unpackhi_epi8(v_lobytes, v_hibytes);

	vpunpckhbw ymm0, ymm6, ymm5

; 698  :   __m256i offs_hi = _mm256_shuffle_epi8(sao_offs, eo_cat_hi);

	vpshufb	ymm1, ymm13, ymm0

; 150  :           *res_lo    = _mm256_unpacklo_epi8(v_lobytes, v_hibytes);

	vpunpcklbw ymm0, ymm6, ymm5

; 104  :              *res_lo  = _mm256_unpacklo_epi8(v, zero);

	vpunpcklbw ymm3, ymm7, ymm8

; 105  :              *res_hi  = _mm256_unpackhi_epi8(v, zero);

	vpunpckhbw ymm2, ymm7, ymm8

; 701  :   __m256i res_hi  = _mm256_adds_epi16  (offs_hi,  c_hi);

	vpaddsw	ymm2, ymm1, ymm2
	vpshufb	ymm1, ymm13, ymm0

; 702  : 
; 703  :           res_lo  = _mm256_max_epi16   (res_lo,   zero);
; 704  :           res_hi  = _mm256_max_epi16   (res_hi,   zero);

	vpmaxsw	ymm4, ymm2, ymm8
	vpaddsw	ymm2, ymm1, ymm3
	vpmaxsw	ymm3, ymm2, ymm8

; 705  : 
; 706  :   __m256i res     = _mm256_packus_epi16(res_lo,   res_hi);

	vpackuswb ymm0, ymm3, ymm4

; 757  :       _mm256_storeu_si256((__m256i *)(new_rec_data + dst_pos), res);

	vmovdqu	YMMWORD PTR [rdx+r13], ymm0
	sub	rdi, 1
	jne	$LL7@reconstruc
	mov	edi, DWORD PTR tv1846[rsp]
	mov	ebx, DWORD PTR stride$[rsp]
	mov	r11d, DWORD PTR width_db4$1$[rsp]
	mov	rsi, QWORD PTR tv1930[rsp]
	mov	edx, DWORD PTR width_db32$1$[rsp]
$LN6@reconstruc:

; 758  :     }
; 759  :     if (block_width > width_db32) {

	cmp	DWORD PTR block_width$[rsp], edx
	jbe	$LN2@reconstruc

; 760  :       const uint32_t curr_srcpos =  y            * stride + x;

	mov	edx, r15d

; 181  :   uint32_t last_dword = 0;

	xor	esi, esi

; 760  :       const uint32_t curr_srcpos =  y            * stride + x;

	imul	edx, ebx
	lea	r8d, DWORD PTR [rdx+r10]

; 761  :       const uint32_t rest_srcpos =  y            * stride + width_db4;

	add	edx, r11d
	mov	DWORD PTR curr_srcpos$1$[rsp], r8d

; 765  : 
; 766  :       const  int32_t curr_bpos   = (y + b_ofs.y) * stride + b_ofs.x + x;

	mov	r8d, DWORD PTR b_ofs$[rsp+4]
	add	r8d, r15d
	mov	DWORD PTR rest_srcpos$1$[rsp], edx
	imul	r8d, ebx
	lea	edx, DWORD PTR [rdi+r10]
	mov	DWORD PTR curr_apos$1$[rsp], edx
	add	r8d, eax
	lea	edx, DWORD PTR [r8+r10]
	mov	DWORD PTR curr_bpos$1$[rsp], edx

; 767  :       const  int32_t rest_bpos   = (y + b_ofs.y) * stride + b_ofs.x + width_db4;

	lea	ebx, DWORD PTR [r8+r11]

; 768  : 
; 769  :       const uint32_t curr_dstpos = y * new_stride + x;

	mov	edx, r15d
	imul	edx, DWORD PTR new_stride$[rsp]
	lea	r8d, DWORD PTR [rdx+r10]
	mov	DWORD PTR curr_dstpos$1$[rsp], r8d

; 770  :       const uint32_t rest_dstpos = y * new_stride + width_db4;

	lea	r8d, DWORD PTR [rdx+r11]
	mov	DWORD PTR rest_dstpos$1$[rsp], r8d

; 182  :   for (int32_t i = 0; i < width_rest; i++) {

	test	r14d, r14d
	je	$LN31@reconstruc

; 762  : 
; 763  :       const  int32_t curr_apos   = (y + a_ofs.y) * stride + a_ofs.x + x;
; 764  :       const  int32_t rest_apos   = (y + a_ofs.y) * stride + a_ofs.x + width_db4;

	lea	edx, DWORD PTR [rdi+r11]
	xor	r9d, r9d
	movsxd	r10, edx
	add	r10, r12
	mov	r11d, r14d
	npad	8
$LL68@reconstruc:

; 184  :     uint32_t currd = ((uint32_t)currb) << (i * 8);

	movzx	edx, BYTE PTR [r10]
	lea	r10, QWORD PTR [r10+1]
	shlx	r8d, edx, r9d

; 185  :     last_dword |= currd;

	or	esi, r8d
	add	r9d, 8
	sub	r11, 1
	jne	SHORT $LL68@reconstruc
	movsxd	r10, ebx
	xor	r9d, r9d
	add	r10, r12
	mov	r11d, r14d
	xor	ebx, ebx
	npad	8
$LL70@reconstruc:

; 184  :     uint32_t currd = ((uint32_t)currb) << (i * 8);

	movzx	edx, BYTE PTR [r10]
	lea	r10, QWORD PTR [r10+1]
	shlx	r8d, edx, r9d

; 185  :     last_dword |= currd;

	or	ebx, r8d
	add	r9d, 8
	sub	r11, 1
	jne	SHORT $LL70@reconstruc
	movsxd	r10, DWORD PTR rest_srcpos$1$[rsp]
	xor	r9d, r9d
	add	r10, r12
	mov	edi, r14d
	npad	8
$LL72@reconstruc:

; 184  :     uint32_t currd = ((uint32_t)currb) << (i * 8);

	movzx	edx, BYTE PTR [r10]
	lea	r10, QWORD PTR [r10+1]
	shlx	r8d, edx, r9d

; 185  :     last_dword |= currd;

	or	r11d, r8d
	add	r9d, 8
	sub	rdi, 1
	jne	SHORT $LL72@reconstruc

; 182  :   for (int32_t i = 0; i < width_rest; i++) {

	mov	edi, DWORD PTR tv1846[rsp]
	jmp	SHORT $LN36@reconstruc
$LN31@reconstruc:

; 152  : }
; 153  : 
; 154  : // Check if all 4 dwords of v are in [-128, 127] and can be truncated to
; 155  : // 8 bits each. Returns -1 if everything is fine
; 156  : static INLINE uint16_t epi32v_fits_in_epi8s(const __m128i v)
; 157  : {
; 158  :   // Compare most significant 25 bits of SAO bands to the sign bit to assert
; 159  :   // that the i32's are between -128 and 127 (only comparing 24 would fail to
; 160  :   // detect values of 128...255)
; 161  :   __m128i  v_ms25b = _mm_srai_epi32   (v,  7);
; 162  :   __m128i  v_signs = _mm_srai_epi32   (v, 31);
; 163  :   __m128i  ok_i32s = _mm_cmpeq_epi32  (v_ms25b, v_signs);
; 164  :   return             _mm_movemask_epi8(ok_i32s);
; 165  : }
; 166  : 
; 167  : static INLINE __m128i truncate_epi32_epi8(const __m128i v)
; 168  : {
; 169  :   // LSBs of each dword, the values values must fit in 8 bits anyway for
; 170  :   // what this intended for (use epi32v_fits_in_epi8s to check if needed)
; 171  :   const __m128i trunc_shufmask = _mm_set1_epi32  (0x0c080400);
; 172  :         __m128i sbs_8          = _mm_shuffle_epi8(v, trunc_shufmask);
; 173  :   return        sbs_8;
; 174  : }
; 175  : 
; 176  : // Read 0-3 bytes (pixels) into uint32_t
; 177  : static INLINE uint32_t load_border_bytes(const uint8_t *buf,
; 178  :                                          const int32_t  start_pos,
; 179  :                                          const int32_t  width_rest)
; 180  : {
; 181  :   uint32_t last_dword = 0;

	xor	r11d, r11d
	xor	ebx, ebx
$LN36@reconstruc:

; 59   :   const __m256i epu2epi = _mm256_set1_epi8  (0x80);

	vmovdqu	ymm7, YMMWORD PTR __ymm@8080808080808080808080808080808080808080808080808080808080808080

; 60   :   const __m256i ones    = _mm256_set1_epi8  (0x01);

	vmovdqu	ymm8, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101

; 771  : 
; 772  :       uint32_t a_last        = load_border_bytes(rec_data, rest_apos,   width_rest);
; 773  :       uint32_t b_last        = load_border_bytes(rec_data, rest_bpos,   width_rest);
; 774  :       uint32_t c_last        = load_border_bytes(rec_data, rest_srcpos, width_rest);
; 775  : 
; 776  :       const int32_t   *a_ptr = (const int32_t *)(    rec_data + curr_apos);
; 777  :       const int32_t   *b_ptr = (const int32_t *)(    rec_data + curr_bpos);
; 778  :       const int32_t   *c_ptr = (const int32_t *)(    rec_data + curr_srcpos);

	mov	edx, DWORD PTR curr_srcpos$1$[rsp]
	vpxor	xmm10, xmm10, xmm10

; 787  :               c = _mm256_insert_epi32  (c, c_last, 7);

	vmovd	xmm1, r11d
	vpbroadcastd ymm1, xmm1
	vpmaskmovd ymm0, ymm14, YMMWORD PTR [rdx+r12]
	movsxd	rdx, DWORD PTR curr_bpos$1$[rsp]
	vpblendd ymm9, ymm0, ymm1, 128			; 00000080H

; 62   :   __m256i a_signed      = _mm256_sub_epi8   (a,        epu2epi);

	vpsubb	ymm6, ymm9, ymm7

; 786  :               b = _mm256_insert_epi32  (b, b_last, 7);

	vmovd	xmm1, ebx
	vpmaskmovd ymm0, ymm14, YMMWORD PTR [rdx+r12]
	movsxd	rdx, DWORD PTR curr_apos$1$[rsp]
	vpbroadcastd ymm1, xmm1
	vpblendd ymm1, ymm0, ymm1, 128			; 00000080H

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vpsubb	ymm2, ymm1, ymm7

; 779  :             int32_t *dst_ptr = (      int32_t *)(new_rec_data + curr_dstpos);
; 780  : 
; 781  :       __m256i a = _mm256_maskload_epi32(a_ptr, db4_mask);

	vpmaskmovd ymm0, ymm14, YMMWORD PTR [rdx+r12]
	mov	edx, DWORD PTR curr_dstpos$1$[rsp]

; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm3, ymm6, ymm2

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm4, ymm8, ymm3

; 782  :       __m256i b = _mm256_maskload_epi32(b_ptr, db4_mask);
; 783  :       __m256i c = _mm256_maskload_epi32(c_ptr, db4_mask);
; 784  : 
; 785  :               a = _mm256_insert_epi32  (a, a_last, 7);

	vmovd	xmm1, esi
	vpbroadcastd ymm1, xmm1
	vpblendd ymm1, ymm0, ymm1, 128			; 00000080H

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vpsubb	ymm2, ymm1, ymm7

; 64   : 
; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm3, ymm6, ymm2

; 75   :   const __m256i idx_to_cat = _mm256_setr_epi64x(0x0403000201, 0,

	vmovdqu	ymm2, YMMWORD PTR __ymm@0000000000000000000000040300020100000000000000000000000403000201

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm0, ymm8, ymm3

; 81   :   __m256i signsum          = _mm256_add_epi8    (c_a_sign,   c_b_sign);

	vpaddb	ymm1, ymm0, ymm4

; 82   :   __m256i eo_idx           = _mm256_add_epi8    (signsum,    twos);

	vpaddb	ymm4, ymm1, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202

; 83   : 
; 84   :   return                     _mm256_shuffle_epi8(idx_to_cat, eo_idx);

	vpshufb	ymm0, ymm2, ymm4

; 146  :   __m256i v_nonnegs  = _mm256_max_epi8  (zero,      v);

	vpmaxsb	ymm1, ymm0, ymm10

; 147  :   __m256i v_lobytes  = _mm256_slli_epi32(v_nonnegs, 1);

	vpslld	ymm6, ymm1, 1

; 148  :   __m256i v_hibytes  = _mm256_add_epi8  (v_lobytes, ones);

	vpaddb	ymm5, ymm8, ymm6

; 151  :           *res_hi    = _mm256_unpackhi_epi8(v_lobytes, v_hibytes);

	vpunpckhbw ymm0, ymm6, ymm5

; 698  :   __m256i offs_hi = _mm256_shuffle_epi8(sao_offs, eo_cat_hi);

	vpshufb	ymm1, ymm13, ymm0

; 150  :           *res_lo    = _mm256_unpacklo_epi8(v_lobytes, v_hibytes);

	vpunpcklbw ymm0, ymm6, ymm5

; 104  :              *res_lo  = _mm256_unpacklo_epi8(v, zero);

	vpunpcklbw ymm3, ymm9, ymm10

; 105  :              *res_hi  = _mm256_unpackhi_epi8(v, zero);

	vpunpckhbw ymm2, ymm9, ymm10

; 701  :   __m256i res_hi  = _mm256_adds_epi16  (offs_hi,  c_hi);

	vpaddsw	ymm2, ymm1, ymm2
	vpshufb	ymm1, ymm13, ymm0

; 702  : 
; 703  :           res_lo  = _mm256_max_epi16   (res_lo,   zero);
; 704  :           res_hi  = _mm256_max_epi16   (res_hi,   zero);

	vpmaxsw	ymm4, ymm2, ymm10
	vpaddsw	ymm2, ymm1, ymm3
	vpmaxsw	ymm3, ymm2, ymm10

; 705  : 
; 706  :   __m256i res     = _mm256_packus_epi16(res_lo,   res_hi);

	vpackuswb ymm0, ymm3, ymm4

; 788  : 
; 789  :       __m256i res = do_one_nonband_ymm(a, b, c, sao_offs);
; 790  :       _mm256_maskstore_epi32(dst_ptr, db4_mask, res);

	vpmaskmovd YMMWORD PTR [rdx+r13], ymm14, ymm0
	xor	edx, edx

; 791  : 
; 792  :       uint32_t last_dword = _mm256_extract_epi32(res, 7);

	vextracti128 xmm0, ymm0, 1
	vpextrd	r10d, xmm0, 3

; 195  :   for (uint32_t i = 0; i < width_rest; i++) {

	test	r14d, r14d
	je	SHORT $LN90@reconstruc
	mov	ecx, DWORD PTR rest_dstpos$1$[rsp]
	npad	13
$LL74@reconstruc:

; 196  :     uint8_t currb = data & 0xff;
; 197  :     buf[start_pos + i] = currb;

	lea	r8d, DWORD PTR [rdx+rcx]
	inc	edx
	mov	BYTE PTR [r8+r13], r10b

; 198  :     data >>= 8;

	shr	r10d, 8
	cmp	edx, r14d
	jb	SHORT $LL74@reconstruc
	mov	rcx, QWORD PTR a_ofs$1$[rsp]
$LN90@reconstruc:

; 738  : 
; 739  :   const __m256i    sao_offs    = broadcast_xmm2ymm(sao_offs_16);
; 740  : 
; 741  :   for (uint32_t y = 0; y < block_height; y++) {

	mov	edx, DWORD PTR width_db32$1$[rsp]
	mov	rsi, QWORD PTR tv1930[rsp]
	mov	ebx, DWORD PTR stride$[rsp]
$LN2@reconstruc:
	mov	r11d, DWORD PTR width_db4$1$[rsp]
	dec	esi
	add	edi, ebx
	mov	QWORD PTR tv1930[rsp], rsi
	inc	r15d
	mov	DWORD PTR tv1846[rsp], edi
	cmp	r15d, DWORD PTR block_height$[rsp]
	jb	$LL4@reconstruc
	vmovaps	xmm12, XMMWORD PTR [rsp+96]
	vmovaps	xmm11, XMMWORD PTR [rsp+112]
	vmovaps	xmm10, XMMWORD PTR [rsp+128]
	vmovaps	xmm9, XMMWORD PTR [rsp+144]
	vmovaps	xmm8, XMMWORD PTR [rsp+160]
	vmovaps	xmm7, XMMWORD PTR [rsp+176]
	vmovaps	xmm6, XMMWORD PTR [rsp+192]
$LN3@reconstruc:

; 793  : 
; 794  :       store_border_bytes(new_rec_data, rest_dstpos, width_rest, last_dword);
; 795  :     }
; 796  :   }
; 797  : }

	vzeroupper
	vmovaps	xmm14, XMMWORD PTR [rsp+64]
	lea	r11, QWORD PTR [rsp+208]
	mov	rbx, QWORD PTR [r11+56]
	mov	rsi, QWORD PTR [r11+64]
	vmovaps	xmm13, XMMWORD PTR [r11-128]
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r13
	pop	r12
	pop	rdi
	ret	0
reconstruct_color_other ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
width_db32$1$ = 0
tv3160 = 4
tv3256 = 8
tv3254 = 8
width_db4$1$ = 16
rest_srcpos$1$ = 20
curr_srcpos$1$ = 24
curr_apos$1$ = 28
curr_bpos$1$ = 32
curr_dstpos$1$ = 36
rest_dstpos$1$ = 40
a_ofs$1 = 48
b_ofs$1$ = 56
db4_mask$1$ = 64
offsets$2 = 96
encoder$ = 864
rec_data$ = 872
new_rec_data$ = 880
sao$ = 888
stride$ = 896
new_stride$ = 904
block_width$ = 912
block_height$ = 920
color_i$ = 928
sao_reconstruct_color_avx2 PROC

; 808  : {

	mov	rax, rsp
	mov	QWORD PTR [rax+8], rbx
	mov	QWORD PTR [rax+16], rsi
	mov	QWORD PTR [rax+24], rdi
	push	rbp
	push	r12
	push	r13
	push	r14
	push	r15
	sub	rsp, 816				; 00000330H
	vmovaps	XMMWORD PTR [rax-56], xmm6
	vmovaps	XMMWORD PTR [rax-72], xmm7
	vmovaps	XMMWORD PTR [rax-88], xmm8
	vmovaps	XMMWORD PTR [rax-104], xmm9
	vmovaps	XMMWORD PTR [rax-120], xmm10
	vmovaps	XMMWORD PTR [rax-136], xmm11
	vmovaps	XMMWORD PTR [rax-152], xmm12
	vmovaps	XMMWORD PTR [rax-168], xmm13
	vmovaps	XMMWORD PTR [rax-184], xmm14
	lea	rbp, QWORD PTR [rsp+64]
	and	rbp, -32				; ffffffffffffffe0H

; 809  :   if (sao->type == SAO_TYPE_BAND) {

	mov	eax, DWORD PTR block_width$[rsp]
	mov	rdi, r8
	mov	esi, DWORD PTR stride$[rsp]
	mov	r13d, eax
	vmovdqu	ymm1, YMMWORD PTR __ymm@0000001f0000001b00000017000000130000000f0000000b0000000700000003
	and	r13d, -32				; ffffffe0H
	mov	r8d, eax
	and	r8d, -4					; fffffffcH
	mov	DWORD PTR width_db32$1$[rbp], r13d
	mov	ebx, eax
	mov	DWORD PTR width_db4$1$[rbp], r8d
	and	ebx, 3
	mov	r10, r9
	cmp	DWORD PTR [r9], 1
	mov	r12, rdx
	jne	$LN2@sao_recons

; 645  :   calc_sao_offset_array_avx2(encoder, sao, offsets, color_i);

	mov	r9d, DWORD PTR color_i$[rsp]
	mov	eax, r8d
	and	eax, 31
	lea	r8, QWORD PTR offsets$2[rbp]
	vmovd	xmm0, eax
	vpbroadcastd ymm0, xmm0
	vpcmpgtd ymm2, ymm0, ymm1
	vmovdqu	YMMWORD PTR db4_mask$1$[rbp], ymm2
	mov	rdx, r10
	vzeroupper
	call	calc_sao_offset_array_avx2

; 646  : 
; 647  :   for (uint32_t y = 0; y < block_height; y++) {

	mov	eax, DWORD PTR block_height$[rsp]
	xor	r15d, r15d
	test	eax, eax
	je	$LN36@sao_recons
	mov	edx, DWORD PTR new_stride$[rsp]
	npad	3
$LL8@sao_recons:

; 648  :     uint32_t x = 0;

	xor	r14d, r14d

; 649  :     for (; x < width_db32; x += 32) {

	test	r13d, r13d
	je	$LN10@sao_recons
	vmovdqu	ymm8, YMMWORD PTR __ymm@0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f

; 648  :     uint32_t x = 0;

	lea	r14d, DWORD PTR [r13-1]

; 582  :   const __m256i select_nibble = _mm256_set1_epi8   (0x0f);

	mov	r11d, edx

; 648  :     uint32_t x = 0;

	shr	r14d, 5

; 582  :   const __m256i select_nibble = _mm256_set1_epi8   (0x0f);

	sub	r11d, esi

; 648  :     uint32_t x = 0;

	inc	r14d
	imul	r11d, r15d
	mov	r9d, r15d
	mov	r10d, r14d
	imul	r9d, esi
	shl	r14d, 5
	vpxor	xmm9, xmm9, xmm9
	npad	4
$LL11@sao_recons:

; 583  :   const __m256i lo_nibbles    = _mm256_and_si256   (select_nibble, curr_row);

	vpand	ymm5, ymm8, YMMWORD PTR [r9+r12]

; 584  :   const __m256i hi_nibbles    = _mm256_andnot_si256(select_nibble, curr_row);

	vpandn	ymm6, ymm8, YMMWORD PTR [r9+r12]

; 651  :       const uint32_t curr_dstpos = y * new_stride + x;

	lea	esi, DWORD PTR [r11+r9]

; 592  :   for (uint8_t i = 0; i < 16; i += 4) {

	xor	al, al
	vmovdqu	ymm7, ymm9

; 651  :       const uint32_t curr_dstpos = y * new_stride + x;

	mov	r8b, 32					; 00000020H
	npad	6
$LL17@sao_recons:

; 604  :     __m256i lonib_lookup0 = _mm256_shuffle_epi8(offsets[i + 0], lo_nibbles);

	movzx	edx, al

; 605  :     __m256i lonib_lookup1 = _mm256_shuffle_epi8(offsets[i + 1], lo_nibbles);
; 606  :     __m256i lonib_lookup2 = _mm256_shuffle_epi8(offsets[i + 2], lo_nibbles);

	mov	ecx, edx
	shl	rcx, 5
	vmovdqu	ymm0, YMMWORD PTR offsets$2[rbp+rcx+64]
	vpshufb	ymm2, ymm0, ymm5
	movsx	ecx, r8b
	vmovd	xmm1, ecx
	vpbroadcastb ymm1, xmm1
	vpcmpeqb ymm0, ymm1, ymm6

; 607  :     __m256i lonib_lookup3 = _mm256_shuffle_epi8(offsets[i + 3], lo_nibbles);
; 608  : 
; 609  :     __m256i lookup_mskd0  = _mm256_and_si256   (hinib_select0,  lonib_lookup0);
; 610  :     __m256i lookup_mskd1  = _mm256_and_si256   (hinib_select1,  lonib_lookup1);
; 611  :     __m256i lookup_mskd2  = _mm256_and_si256   (hinib_select2,  lonib_lookup2);

	vpand	ymm3, ymm0, ymm2
	mov	ecx, edx
	shl	rdx, 5
	shl	rcx, 5
	vmovdqu	ymm0, YMMWORD PTR offsets$2[rbp+rcx+96]
	vpshufb	ymm1, ymm0, ymm5
	lea	ecx, DWORD PTR [r8+16]
	movsx	ecx, cl
	vmovd	xmm0, ecx
	vpbroadcastb ymm0, xmm0
	vpcmpeqb ymm0, ymm0, ymm6

; 612  :     __m256i lookup_mskd3  = _mm256_and_si256   (hinib_select3,  lonib_lookup3);

	vpand	ymm1, ymm0, ymm1
	vmovdqu	ymm0, YMMWORD PTR offsets$2[rbp+rdx]
	movzx	ecx, al
	shl	cl, 4
	movsx	ecx, cl

; 613  : 
; 614  :     __m256i lookup_mskd01 = _mm256_or_si256    (lookup_mskd0,   lookup_mskd1);
; 615  :     __m256i lookup_mskd23 = _mm256_or_si256    (lookup_mskd2,   lookup_mskd3);

	vpor	ymm4, ymm1, ymm3
	vpshufb	ymm1, ymm0, ymm5
	vmovd	xmm0, ecx
	vpbroadcastb ymm0, xmm0
	vpcmpeqb ymm0, ymm0, ymm6
	vpand	ymm2, ymm0, ymm1
	movzx	ecx, al
	add	al, 4

; 616  :     __m256i lookup_res    = _mm256_or_si256    (lookup_mskd01,  lookup_mskd23);
; 617  : 
; 618  :             result_row    = _mm256_or_si256    (result_row,     lookup_res);

	shl	rcx, 5
	vmovdqu	ymm0, YMMWORD PTR offsets$2[rbp+rcx+32]
	vpshufb	ymm1, ymm0, ymm5
	lea	ecx, DWORD PTR [r8-16]
	add	r8b, 64					; 00000040H
	movsx	ecx, cl
	vmovd	xmm0, ecx
	vpbroadcastb ymm0, xmm0
	vpcmpeqb ymm0, ymm0, ymm6
	vpand	ymm1, ymm0, ymm1
	vpor	ymm2, ymm1, ymm2
	vpor	ymm3, ymm2, ymm4
	vpor	ymm7, ymm3, ymm7
	cmp	al, 16
	jb	$LL17@sao_recons

; 655  :       _mm256_storeu_si256((__m256i *)(new_rec_data + curr_dstpos), result);

	add	r9d, 32					; 00000020H
	vmovdqu	YMMWORD PTR [rsi+rdi], ymm7
	sub	r10, 1
	jne	$LL11@sao_recons
	mov	esi, DWORD PTR stride$[rsp]
	mov	edx, DWORD PTR new_stride$[rsp]
	mov	eax, DWORD PTR block_height$[rsp]
$LN10@sao_recons:

; 656  :     }
; 657  :     if (block_width > width_db32) {

	cmp	DWORD PTR block_width$[rsp], r13d
	jbe	$LN6@sao_recons

; 661  :       const uint32_t rest_dstpos = y * new_stride + width_db4;

	mov	r8d, DWORD PTR width_db4$1$[rbp]
	mov	ecx, r15d
	imul	ecx, esi
	mov	eax, r15d
	imul	eax, edx

; 181  :   uint32_t last_dword = 0;

	xor	r9d, r9d

; 658  :       const uint32_t curr_srcpos = y *     stride + x;

	lea	esi, DWORD PTR [rcx+r14]

; 659  :       const uint32_t curr_dstpos = y * new_stride + x;

	add	r14d, eax

; 661  :       const uint32_t rest_dstpos = y * new_stride + width_db4;

	lea	r11d, DWORD PTR [rax+r8]

; 182  :   for (int32_t i = 0; i < width_rest; i++) {

	test	ebx, ebx
	je	SHORT $LN21@sao_recons

; 660  :       const uint32_t rest_srcpos = y *     stride + width_db4;

	lea	eax, DWORD PTR [rcx+r8]
	mov	r10d, ebx
	movsxd	r8, eax
	xor	edx, edx
	add	r8, r12
	npad	13
$LL113@sao_recons:

; 184  :     uint32_t currd = ((uint32_t)currb) << (i * 8);

	movzx	eax, BYTE PTR [r8]
	lea	r8, QWORD PTR [r8+1]
	shlx	ecx, eax, edx

; 185  :     last_dword |= currd;

	or	r9d, ecx
	add	edx, 8
	sub	r10, 1
	jne	SHORT $LL113@sao_recons
$LN21@sao_recons:

; 673  :       __m256i curr_row = _mm256_maskload_epi32(src_ptr,  db4_mask);

	vmovdqu	ymm8, YMMWORD PTR db4_mask$1$[rbp]
	vpmaskmovd ymm0, ymm8, YMMWORD PTR [rsi+r12]

; 674  :               curr_row = _mm256_insert_epi32  (curr_row, last_dword, 7);

	vmovd	xmm1, r9d
	vpbroadcastd ymm1, xmm1
	vpblendd ymm2, ymm0, ymm1, 128			; 00000080H

; 582  :   const __m256i select_nibble = _mm256_set1_epi8   (0x0f);

	vmovdqu	ymm1, YMMWORD PTR __ymm@0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f0f

; 583  :   const __m256i lo_nibbles    = _mm256_and_si256   (select_nibble, curr_row);

	vpand	ymm5, ymm2, ymm1

; 584  :   const __m256i hi_nibbles    = _mm256_andnot_si256(select_nibble, curr_row);

	vpandn	ymm6, ymm1, ymm2
	vpxor	xmm7, xmm7, xmm7

; 585  : 
; 586  :   // Loop through the offset vectors, the 0xi'th one always holding
; 587  :   // offsets 0xi0...0xif. Use shuffle to do a lookup on the current
; 588  :   // offset vector, then check which pixels actually should be looked
; 589  :   // up from this vector (ie. whether their values are 0xi0...0xif) and
; 590  :   // mask out any but correct ones.
; 591  :   __m256i result_row = _mm256_setzero_si256();
; 592  :   for (uint8_t i = 0; i < 16; i += 4) {

	xor	dl, dl

; 671  :             int32_t *dst_ptr = (      int32_t *)(new_rec_data + curr_dstpos);

	mov	r8b, 32					; 00000020H
	npad	3
$LL27@sao_recons:

; 604  :     __m256i lonib_lookup0 = _mm256_shuffle_epi8(offsets[i + 0], lo_nibbles);

	movzx	ecx, dl

; 605  :     __m256i lonib_lookup1 = _mm256_shuffle_epi8(offsets[i + 1], lo_nibbles);
; 606  :     __m256i lonib_lookup2 = _mm256_shuffle_epi8(offsets[i + 2], lo_nibbles);

	mov	eax, ecx
	shl	rax, 5
	vmovdqu	ymm0, YMMWORD PTR offsets$2[rbp+rax+64]
	vpshufb	ymm2, ymm0, ymm5
	movsx	eax, r8b
	vmovd	xmm1, eax
	vpbroadcastb ymm1, xmm1
	vpcmpeqb ymm0, ymm1, ymm6

; 607  :     __m256i lonib_lookup3 = _mm256_shuffle_epi8(offsets[i + 3], lo_nibbles);
; 608  : 
; 609  :     __m256i lookup_mskd0  = _mm256_and_si256   (hinib_select0,  lonib_lookup0);
; 610  :     __m256i lookup_mskd1  = _mm256_and_si256   (hinib_select1,  lonib_lookup1);
; 611  :     __m256i lookup_mskd2  = _mm256_and_si256   (hinib_select2,  lonib_lookup2);

	vpand	ymm3, ymm0, ymm2
	mov	eax, ecx
	shl	rcx, 5
	shl	rax, 5
	vmovdqu	ymm0, YMMWORD PTR offsets$2[rbp+rax+96]
	vpshufb	ymm1, ymm0, ymm5
	lea	eax, DWORD PTR [r8+16]
	movsx	eax, al
	vmovd	xmm0, eax
	vpbroadcastb ymm0, xmm0
	vpcmpeqb ymm0, ymm0, ymm6

; 612  :     __m256i lookup_mskd3  = _mm256_and_si256   (hinib_select3,  lonib_lookup3);

	vpand	ymm1, ymm0, ymm1
	vmovdqu	ymm0, YMMWORD PTR offsets$2[rbp+rcx]
	movzx	eax, dl
	shl	al, 4
	movsx	eax, al

; 613  : 
; 614  :     __m256i lookup_mskd01 = _mm256_or_si256    (lookup_mskd0,   lookup_mskd1);
; 615  :     __m256i lookup_mskd23 = _mm256_or_si256    (lookup_mskd2,   lookup_mskd3);

	vpor	ymm4, ymm1, ymm3
	vpshufb	ymm1, ymm0, ymm5
	vmovd	xmm0, eax
	vpbroadcastb ymm0, xmm0
	vpcmpeqb ymm0, ymm0, ymm6
	vpand	ymm2, ymm0, ymm1
	movzx	eax, dl
	add	dl, 4

; 616  :     __m256i lookup_res    = _mm256_or_si256    (lookup_mskd01,  lookup_mskd23);
; 617  : 
; 618  :             result_row    = _mm256_or_si256    (result_row,     lookup_res);

	shl	rax, 5
	vmovdqu	ymm0, YMMWORD PTR offsets$2[rbp+rax+32]
	vpshufb	ymm1, ymm0, ymm5
	lea	eax, DWORD PTR [r8-16]
	add	r8b, 64					; 00000040H
	movsx	eax, al
	vmovd	xmm0, eax
	vpbroadcastb ymm0, xmm0
	vpcmpeqb ymm0, ymm0, ymm6
	vpand	ymm1, ymm0, ymm1
	vpor	ymm2, ymm1, ymm2
	vpor	ymm3, ymm2, ymm4
	vpor	ymm7, ymm3, ymm7
	cmp	dl, 16
	jb	$LL27@sao_recons

; 678  :       uint32_t last_dword_dst = _mm256_extract_epi32(result, 7);

	xor	edx, edx
	vextracti128 xmm0, ymm7, 1
	vpmaskmovd YMMWORD PTR [r14+rdi], ymm8, ymm7
	vpextrd	r8d, xmm0, 3

; 195  :   for (uint32_t i = 0; i < width_rest; i++) {

	test	ebx, ebx
	je	SHORT $LN152@sao_recons
	npad	10
$LL115@sao_recons:

; 196  :     uint8_t currb = data & 0xff;
; 197  :     buf[start_pos + i] = currb;

	lea	eax, DWORD PTR [rdx+r11]
	inc	edx
	mov	BYTE PTR [rax+rdi], r8b

; 198  :     data >>= 8;

	shr	r8d, 8
	cmp	edx, ebx
	jb	SHORT $LL115@sao_recons
$LN152@sao_recons:

; 647  :   for (uint32_t y = 0; y < block_height; y++) {

	mov	eax, DWORD PTR block_height$[rsp]
$LN6@sao_recons:
	mov	esi, DWORD PTR stride$[rsp]
	inc	r15d
	mov	edx, DWORD PTR new_stride$[rsp]
	cmp	r15d, eax
	jb	$LL8@sao_recons

; 810  :     reconstruct_color_band (encoder, rec_data, new_rec_data, sao, stride, new_stride, block_width, block_height, color_i);
; 811  :   } else {

	jmp	$LN36@sao_recons
$LN2@sao_recons:

; 721  :   const vector2d_t a_ofs       = g_sao_edge_offsets[sao->eo_class][0];

	movsxd	rcx, DWORD PTR [r9+4]
	lea	rdx, OFFSET FLAT:g_sao_edge_offsets
	add	rcx, rcx

; 722  :   const vector2d_t b_ofs       = g_sao_edge_offsets[sao->eo_class][1];
; 723  : 
; 724  :   const uint32_t   width_db32  = block_width & ~31;
; 725  :   const uint32_t   width_db4   = block_width &  ~3;
; 726  :   const uint32_t   width_rest  = block_width &   3;
; 727  : 
; 728  :   // Form the load&store mask
; 729  :   const __m256i    wdb4_256    = _mm256_set1_epi32 (width_db4 & 31);
; 730  :   const __m256i    indexes     = _mm256_setr_epi32 (3, 7, 11, 15, 19, 23, 27, 31);
; 731  :   const __m256i    db4_mask    = _mm256_cmpgt_epi32(wdb4_256, indexes);
; 732  : 
; 733  :   // Again, saturate offsets to signed 16 bits, because anything outside of
; 734  :   // [-255, 255] will saturate anything these are used with
; 735  :   const __m128i    sao_offs_lo = _mm_loadu_si128  ((const __m128i *)(sao->offsets + offset_v + 0));

	mov	r9d, 28

; 736  :   const __m128i    sao_offs_hi = _mm_cvtsi32_si128(sao->offsets[offset_v + 4]);

	mov	r14d, 64				; 00000040H
	mov	rax, QWORD PTR [rdx+rcx*8]
	mov	rcx, QWORD PTR [rdx+rcx*8+8]
	mov	r11, rax
	shr	r11, 32					; 00000020H
	mov	rdx, rcx
	shr	rdx, 32					; 00000020H
	mov	r15, rcx
	mov	QWORD PTR tv3254[rbp], rdx
	mov	edx, r8d
	and	edx, 31
	shr	r15, 32					; 00000020H
	cmp	DWORD PTR color_i$[rsp], 2
	vmovd	xmm0, edx
	mov	edx, 48					; 00000030H
	mov	QWORD PTR a_ofs$1[rbp], rax
	cmove	r9d, edx
	mov	QWORD PTR b_ofs$1$[rbp], rcx
	vpbroadcastd ymm0, xmm0
	vpcmpgtd ymm14, ymm0, ymm1
	mov	edx, 44					; 0000002cH

; 737  :   const __m128i    sao_offs_16 = _mm_packs_epi32  (sao_offs_lo, sao_offs_hi);

	vmovdqu	xmm0, XMMWORD PTR [r9+r10]
	cmove	edx, r14d
	xor	r13d, r13d
	vmovd	xmm1, DWORD PTR [rdx+r10]
	vpackssdw xmm2, xmm0, xmm1

; 218  :   __m256i res = _mm256_castsi128_si256 (v);

	vmovups	xmm1, xmm2

; 219  :   return        _mm256_inserti128_si256(res, v, 1);

	vinserti128 ymm13, ymm1, xmm2, 1

; 741  :   for (uint32_t y = 0; y < block_height; y++) {

	cmp	DWORD PTR block_height$[rsp], r13d
	jbe	$LN36@sao_recons
	mov	edx, DWORD PTR width_db32$1$[rbp]

; 707  :   return res;
; 708  : }
; 709  : 
; 710  : static INLINE void reconstruct_color_other(const encoder_control_t *encoder,
; 711  :                                            const uint8_t           *rec_data,
; 712  :                                                  uint8_t           *new_rec_data,
; 713  :                                            const sao_info_t        *sao,
; 714  :                                                  int32_t            stride,
; 715  :                                                  int32_t            new_stride,
; 716  :                                                  int32_t            block_width,
; 717  :                                                  int32_t            block_height,
; 718  :                                                  color_t            color_i)
; 719  : {
; 720  :   const uint32_t   offset_v    = color_i == COLOR_V ? 5 : 0;
; 721  :   const vector2d_t a_ofs       = g_sao_edge_offsets[sao->eo_class][0];

	mov	r14d, esi
	imul	r14d, DWORD PTR tv3254[rbp]
	add	r14d, ecx
	sub	r15d, r11d
	mov	DWORD PTR tv3160[rbp], r14d
	mov	QWORD PTR tv3256[rbp], r15
	npad	3
$LL37@sao_recons:

; 742  :     uint32_t x;
; 743  :     for (x = 0; x < width_db32; x += 32) {

	xor	r10d, r10d
	test	edx, edx
	je	$LN39@sao_recons

; 744  :       const uint32_t  src_pos = y *     stride + x;
; 745  :       const uint32_t  dst_pos = y * new_stride + x;

	mov	r8d, DWORD PTR a_ofs$1[rbp+4]
	add	r15d, r13d
	mov	r10d, DWORD PTR width_db32$1$[rbp]
	add	r8d, r13d
	vmovdqu	ymm9, YMMWORD PTR __ymm@8080808080808080808080808080808080808080808080808080808080808080
	vmovdqu	ymm10, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101
	vmovdqu	ymm11, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202
	vmovdqu	ymm12, YMMWORD PTR __ymm@0000000000000000000000040300020100000000000000000000000403000201
	imul	r8d, esi
	mov	r11d, r13d
	imul	r11d, esi
	imul	r15d, esi
	mov	esi, r13d
	imul	esi, DWORD PTR new_stride$[rsp]
	vpxor	xmm8, xmm8, xmm8
	sub	r11d, r8d
	sub	r15d, eax
	sub	r11d, eax
	sub	esi, r8d
	lea	r9d, DWORD PTR [r8+rax]
	sub	esi, eax
	add	r15d, ecx
	dec	r10d
	shr	r10d, 5
	inc	r10d
	mov	r14d, r10d
	shl	r10d, 5
	npad	2
$LL40@sao_recons:

; 754  :       __m256i c = _mm256_loadu_si256((const __m256i *)(rec_data + src_pos));

	lea	edx, DWORD PTR [r11+r9]

; 755  : 
; 756  :       __m256i res = do_one_nonband_ymm(a, b, c, sao_offs);

	vmovdqu	ymm7, YMMWORD PTR [rdx+r12]
	movsxd	rdx, r9d

; 62   :   __m256i a_signed      = _mm256_sub_epi8   (a,        epu2epi);

	vpsubb	ymm3, ymm7, ymm9

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vmovdqu	ymm0, YMMWORD PTR [rdx+r12]
	vpsubb	ymm1, ymm0, ymm9

; 64   : 
; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm2, ymm3, ymm1

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm4, ymm10, ymm2

; 753  :       __m256i b = _mm256_loadu_si256((const __m256i *)(rec_data + b_pos));

	lea	edx, DWORD PTR [r9+r15]
	movsxd	r8, edx

; 757  :       _mm256_storeu_si256((__m256i *)(new_rec_data + dst_pos), res);

	lea	edx, DWORD PTR [rsi+r9]
	add	r9d, 32					; 00000020H

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vmovdqu	ymm0, YMMWORD PTR [r8+r12]
	vpsubb	ymm1, ymm0, ymm9

; 64   : 
; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm2, ymm3, ymm1

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm3, ymm10, ymm2

; 81   :   __m256i signsum          = _mm256_add_epi8    (c_a_sign,   c_b_sign);

	vpaddb	ymm0, ymm3, ymm4

; 82   :   __m256i eo_idx           = _mm256_add_epi8    (signsum,    twos);

	vpaddb	ymm1, ymm0, ymm11

; 83   : 
; 84   :   return                     _mm256_shuffle_epi8(idx_to_cat, eo_idx);

	vpshufb	ymm2, ymm12, ymm1

; 146  :   __m256i v_nonnegs  = _mm256_max_epi8  (zero,      v);

	vpmaxsb	ymm3, ymm2, ymm8

; 147  :   __m256i v_lobytes  = _mm256_slli_epi32(v_nonnegs, 1);

	vpslld	ymm6, ymm3, 1

; 148  :   __m256i v_hibytes  = _mm256_add_epi8  (v_lobytes, ones);

	vpaddb	ymm5, ymm10, ymm6

; 151  :           *res_hi    = _mm256_unpackhi_epi8(v_lobytes, v_hibytes);

	vpunpckhbw ymm0, ymm6, ymm5

; 698  :   __m256i offs_hi = _mm256_shuffle_epi8(sao_offs, eo_cat_hi);

	vpshufb	ymm1, ymm13, ymm0

; 150  :           *res_lo    = _mm256_unpacklo_epi8(v_lobytes, v_hibytes);

	vpunpcklbw ymm0, ymm6, ymm5

; 104  :              *res_lo  = _mm256_unpacklo_epi8(v, zero);

	vpunpcklbw ymm3, ymm7, ymm8

; 105  :              *res_hi  = _mm256_unpackhi_epi8(v, zero);

	vpunpckhbw ymm2, ymm7, ymm8

; 701  :   __m256i res_hi  = _mm256_adds_epi16  (offs_hi,  c_hi);

	vpaddsw	ymm2, ymm1, ymm2
	vpshufb	ymm1, ymm13, ymm0

; 702  : 
; 703  :           res_lo  = _mm256_max_epi16   (res_lo,   zero);
; 704  :           res_hi  = _mm256_max_epi16   (res_hi,   zero);

	vpmaxsw	ymm4, ymm2, ymm8
	vpaddsw	ymm2, ymm1, ymm3
	vpmaxsw	ymm3, ymm2, ymm8

; 705  : 
; 706  :   __m256i res     = _mm256_packus_epi16(res_lo,   res_hi);

	vpackuswb ymm0, ymm3, ymm4

; 757  :       _mm256_storeu_si256((__m256i *)(new_rec_data + dst_pos), res);

	vmovdqu	YMMWORD PTR [rdx+rdi], ymm0
	sub	r14, 1
	jne	$LL40@sao_recons
	mov	r14d, DWORD PTR tv3160[rbp]
	mov	esi, DWORD PTR stride$[rsp]
	mov	r15, QWORD PTR tv3256[rbp]
	mov	edx, DWORD PTR width_db32$1$[rbp]
$LN39@sao_recons:

; 758  :     }
; 759  :     if (block_width > width_db32) {

	cmp	DWORD PTR block_width$[rsp], edx
	jbe	$LN35@sao_recons

; 761  :       const uint32_t rest_srcpos =  y            * stride + width_db4;

	mov	r11d, DWORD PTR width_db4$1$[rbp]
	mov	edx, r13d

; 762  : 
; 763  :       const  int32_t curr_apos   = (y + a_ofs.y) * stride + a_ofs.x + x;

	mov	r9d, DWORD PTR a_ofs$1[rbp+4]

; 181  :   uint32_t last_dword = 0;

	xor	r15d, r15d

; 760  :       const uint32_t curr_srcpos =  y            * stride + x;

	imul	edx, esi

; 762  : 
; 763  :       const  int32_t curr_apos   = (y + a_ofs.y) * stride + a_ofs.x + x;

	add	r9d, r13d
	imul	r9d, esi

; 765  : 
; 766  :       const  int32_t curr_bpos   = (y + b_ofs.y) * stride + b_ofs.x + x;
; 767  :       const  int32_t rest_bpos   = (y + b_ofs.y) * stride + b_ofs.x + width_db4;

	lea	esi, DWORD PTR [r11+r14]
	add	r9d, eax
	lea	r8d, DWORD PTR [rdx+r10]
	add	edx, r11d
	mov	DWORD PTR rest_srcpos$1$[rbp], edx
	lea	edx, DWORD PTR [r9+r10]
	mov	DWORD PTR curr_srcpos$1$[rbp], r8d
	mov	DWORD PTR curr_apos$1$[rbp], edx
	lea	edx, DWORD PTR [r10+r14]
	mov	DWORD PTR curr_bpos$1$[rbp], edx

; 768  : 
; 769  :       const uint32_t curr_dstpos = y * new_stride + x;

	mov	edx, r13d
	imul	edx, DWORD PTR new_stride$[rsp]
	lea	r8d, DWORD PTR [rdx+r10]
	mov	DWORD PTR curr_dstpos$1$[rbp], r8d

; 770  :       const uint32_t rest_dstpos = y * new_stride + width_db4;

	lea	r8d, DWORD PTR [rdx+r11]
	mov	DWORD PTR rest_dstpos$1$[rbp], r8d

; 182  :   for (int32_t i = 0; i < width_rest; i++) {

	test	ebx, ebx
	je	$LN64@sao_recons

; 764  :       const  int32_t rest_apos   = (y + a_ofs.y) * stride + a_ofs.x + width_db4;

	lea	edx, DWORD PTR [r9+r11]
	xor	r9d, r9d
	movsxd	r10, edx
	add	r10, r12
	mov	r11d, ebx
	npad	13
$LL117@sao_recons:

; 184  :     uint32_t currd = ((uint32_t)currb) << (i * 8);

	movzx	edx, BYTE PTR [r10]
	lea	r10, QWORD PTR [r10+1]
	shlx	r8d, edx, r9d

; 185  :     last_dword |= currd;

	or	r15d, r8d
	add	r9d, 8
	sub	r11, 1
	jne	SHORT $LL117@sao_recons
	movsxd	r10, esi
	xor	r9d, r9d
	add	r10, r12
	mov	r11d, ebx
	xor	esi, esi
	npad	8
$LL119@sao_recons:

; 184  :     uint32_t currd = ((uint32_t)currb) << (i * 8);

	movzx	edx, BYTE PTR [r10]
	lea	r10, QWORD PTR [r10+1]
	shlx	r8d, edx, r9d

; 185  :     last_dword |= currd;

	or	esi, r8d
	add	r9d, 8
	sub	r11, 1
	jne	SHORT $LL119@sao_recons
	movsxd	r10, DWORD PTR rest_srcpos$1$[rbp]
	xor	r9d, r9d
	add	r10, r12
	mov	r14d, ebx
	npad	9
$LL121@sao_recons:

; 184  :     uint32_t currd = ((uint32_t)currb) << (i * 8);

	movzx	edx, BYTE PTR [r10]
	lea	r10, QWORD PTR [r10+1]
	shlx	r8d, edx, r9d

; 185  :     last_dword |= currd;

	or	r11d, r8d
	add	r9d, 8
	sub	r14, 1
	jne	SHORT $LL121@sao_recons

; 182  :   for (int32_t i = 0; i < width_rest; i++) {

	mov	r14d, DWORD PTR tv3160[rbp]
	jmp	SHORT $LN69@sao_recons
$LN64@sao_recons:

; 152  : }
; 153  : 
; 154  : // Check if all 4 dwords of v are in [-128, 127] and can be truncated to
; 155  : // 8 bits each. Returns -1 if everything is fine
; 156  : static INLINE uint16_t epi32v_fits_in_epi8s(const __m128i v)
; 157  : {
; 158  :   // Compare most significant 25 bits of SAO bands to the sign bit to assert
; 159  :   // that the i32's are between -128 and 127 (only comparing 24 would fail to
; 160  :   // detect values of 128...255)
; 161  :   __m128i  v_ms25b = _mm_srai_epi32   (v,  7);
; 162  :   __m128i  v_signs = _mm_srai_epi32   (v, 31);
; 163  :   __m128i  ok_i32s = _mm_cmpeq_epi32  (v_ms25b, v_signs);
; 164  :   return             _mm_movemask_epi8(ok_i32s);
; 165  : }
; 166  : 
; 167  : static INLINE __m128i truncate_epi32_epi8(const __m128i v)
; 168  : {
; 169  :   // LSBs of each dword, the values values must fit in 8 bits anyway for
; 170  :   // what this intended for (use epi32v_fits_in_epi8s to check if needed)
; 171  :   const __m128i trunc_shufmask = _mm_set1_epi32  (0x0c080400);
; 172  :         __m128i sbs_8          = _mm_shuffle_epi8(v, trunc_shufmask);
; 173  :   return        sbs_8;
; 174  : }
; 175  : 
; 176  : // Read 0-3 bytes (pixels) into uint32_t
; 177  : static INLINE uint32_t load_border_bytes(const uint8_t *buf,
; 178  :                                          const int32_t  start_pos,
; 179  :                                          const int32_t  width_rest)
; 180  : {
; 181  :   uint32_t last_dword = 0;

	xor	r11d, r11d
	xor	esi, esi
$LN69@sao_recons:

; 59   :   const __m256i epu2epi = _mm256_set1_epi8  (0x80);

	vmovdqu	ymm7, YMMWORD PTR __ymm@8080808080808080808080808080808080808080808080808080808080808080

; 60   :   const __m256i ones    = _mm256_set1_epi8  (0x01);

	vmovdqu	ymm8, YMMWORD PTR __ymm@0101010101010101010101010101010101010101010101010101010101010101

; 778  :       const int32_t   *c_ptr = (const int32_t *)(    rec_data + curr_srcpos);

	mov	edx, DWORD PTR curr_srcpos$1$[rbp]

; 788  : 
; 789  :       __m256i res = do_one_nonband_ymm(a, b, c, sao_offs);

	xor	r9d, r9d
	vpxor	xmm10, xmm10, xmm10

; 787  :               c = _mm256_insert_epi32  (c, c_last, 7);

	vmovd	xmm1, r11d
	vpbroadcastd ymm1, xmm1
	vpmaskmovd ymm0, ymm14, YMMWORD PTR [rdx+r12]
	movsxd	rdx, DWORD PTR curr_apos$1$[rbp]
	vpblendd ymm9, ymm0, ymm1, 128			; 00000080H

; 62   :   __m256i a_signed      = _mm256_sub_epi8   (a,        epu2epi);

	vpsubb	ymm6, ymm9, ymm7

; 785  :               a = _mm256_insert_epi32  (a, a_last, 7);

	vmovd	xmm1, r15d
	vpmaskmovd ymm0, ymm14, YMMWORD PTR [rdx+r12]
	movsxd	rdx, DWORD PTR curr_bpos$1$[rbp]
	vpbroadcastd ymm1, xmm1
	vpblendd ymm1, ymm0, ymm1, 128			; 00000080H

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vpsubb	ymm2, ymm1, ymm7

; 782  :       __m256i b = _mm256_maskload_epi32(b_ptr, db4_mask);

	vpmaskmovd ymm0, ymm14, YMMWORD PTR [rdx+r12]
	mov	edx, DWORD PTR curr_dstpos$1$[rbp]

; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm3, ymm6, ymm2

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm4, ymm8, ymm3

; 786  :               b = _mm256_insert_epi32  (b, b_last, 7);

	vmovd	xmm1, esi
	vpbroadcastd ymm1, xmm1
	vpblendd ymm1, ymm0, ymm1, 128			; 00000080H

; 63   :   __m256i b_signed      = _mm256_sub_epi8   (b,        epu2epi);

	vpsubb	ymm2, ymm1, ymm7

; 64   : 
; 65   :   __m256i diff          = _mm256_subs_epi8  (a_signed, b_signed);

	vpsubsb	ymm3, ymm6, ymm2

; 75   :   const __m256i idx_to_cat = _mm256_setr_epi64x(0x0403000201, 0,

	vmovdqu	ymm2, YMMWORD PTR __ymm@0000000000000000000000040300020100000000000000000000000403000201

; 66   :   return                  _mm256_sign_epi8  (ones,     diff);

	vpsignb	ymm0, ymm8, ymm3

; 81   :   __m256i signsum          = _mm256_add_epi8    (c_a_sign,   c_b_sign);

	vpaddb	ymm1, ymm0, ymm4

; 82   :   __m256i eo_idx           = _mm256_add_epi8    (signsum,    twos);

	vpaddb	ymm4, ymm1, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202

; 83   : 
; 84   :   return                     _mm256_shuffle_epi8(idx_to_cat, eo_idx);

	vpshufb	ymm0, ymm2, ymm4

; 146  :   __m256i v_nonnegs  = _mm256_max_epi8  (zero,      v);

	vpmaxsb	ymm1, ymm0, ymm10

; 147  :   __m256i v_lobytes  = _mm256_slli_epi32(v_nonnegs, 1);

	vpslld	ymm6, ymm1, 1

; 148  :   __m256i v_hibytes  = _mm256_add_epi8  (v_lobytes, ones);

	vpaddb	ymm5, ymm8, ymm6

; 151  :           *res_hi    = _mm256_unpackhi_epi8(v_lobytes, v_hibytes);

	vpunpckhbw ymm0, ymm6, ymm5

; 698  :   __m256i offs_hi = _mm256_shuffle_epi8(sao_offs, eo_cat_hi);

	vpshufb	ymm1, ymm13, ymm0

; 150  :           *res_lo    = _mm256_unpacklo_epi8(v_lobytes, v_hibytes);

	vpunpcklbw ymm0, ymm6, ymm5

; 104  :              *res_lo  = _mm256_unpacklo_epi8(v, zero);

	vpunpcklbw ymm3, ymm9, ymm10

; 105  :              *res_hi  = _mm256_unpackhi_epi8(v, zero);

	vpunpckhbw ymm2, ymm9, ymm10

; 701  :   __m256i res_hi  = _mm256_adds_epi16  (offs_hi,  c_hi);

	vpaddsw	ymm2, ymm1, ymm2
	vpshufb	ymm1, ymm13, ymm0

; 702  : 
; 703  :           res_lo  = _mm256_max_epi16   (res_lo,   zero);
; 704  :           res_hi  = _mm256_max_epi16   (res_hi,   zero);

	vpmaxsw	ymm4, ymm2, ymm10
	vpaddsw	ymm2, ymm1, ymm3
	vpmaxsw	ymm3, ymm2, ymm10

; 705  : 
; 706  :   __m256i res     = _mm256_packus_epi16(res_lo,   res_hi);

	vpackuswb ymm0, ymm3, ymm4

; 790  :       _mm256_maskstore_epi32(dst_ptr, db4_mask, res);

	vpmaskmovd YMMWORD PTR [rdx+rdi], ymm14, ymm0

; 791  : 
; 792  :       uint32_t last_dword = _mm256_extract_epi32(res, 7);

	vextracti128 xmm0, ymm0, 1
	vpextrd	r10d, xmm0, 3

; 195  :   for (uint32_t i = 0; i < width_rest; i++) {

	test	ebx, ebx
	je	SHORT $LN151@sao_recons
	mov	ecx, DWORD PTR rest_dstpos$1$[rbp]
	npad	4
$LL123@sao_recons:

; 196  :     uint8_t currb = data & 0xff;
; 197  :     buf[start_pos + i] = currb;

	lea	edx, DWORD PTR [r9+rcx]
	inc	r9d
	mov	BYTE PTR [rdx+rdi], r10b

; 198  :     data >>= 8;

	shr	r10d, 8
	cmp	r9d, ebx
	jb	SHORT $LL123@sao_recons
	mov	rcx, QWORD PTR b_ofs$1$[rbp]
$LN151@sao_recons:

; 741  :   for (uint32_t y = 0; y < block_height; y++) {

	mov	edx, DWORD PTR width_db32$1$[rbp]
	mov	r15, QWORD PTR tv3256[rbp]
	mov	esi, DWORD PTR stride$[rsp]
$LN35@sao_recons:
	dec	r15d
	add	r14d, esi
	inc	r13d
	mov	QWORD PTR tv3256[rbp], r15
	mov	DWORD PTR tv3160[rbp], r14d
	cmp	r13d, DWORD PTR block_height$[rsp]
	jb	$LL37@sao_recons
$LN36@sao_recons:
	vzeroupper

; 812  :     reconstruct_color_other(encoder, rec_data, new_rec_data, sao, stride, new_stride, block_width, block_height, color_i);
; 813  :   }
; 814  : }

	lea	r11, QWORD PTR [rsp+816]
	mov	rbx, QWORD PTR [r11+48]
	mov	rsi, QWORD PTR [r11+56]
	mov	rdi, QWORD PTR [r11+64]
	vmovaps	xmm6, XMMWORD PTR [r11-16]
	vmovaps	xmm7, XMMWORD PTR [r11-32]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm9, XMMWORD PTR [r11-64]
	vmovaps	xmm10, XMMWORD PTR [r11-80]
	vmovaps	xmm11, XMMWORD PTR [r11-96]
	vmovaps	xmm12, XMMWORD PTR [r11-112]
	vmovaps	xmm13, XMMWORD PTR [r11-128]
	vmovaps	xmm14, XMMWORD PTR [r11-144]
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r13
	pop	r12
	pop	rbp
	ret	0
sao_reconstruct_color_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\sao_shared_generics.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c
_TEXT	SEGMENT
tv956 = 0
state$ = 224
orig_data$ = 232
rec_data$ = 240
block_width$ = 248
block_height$ = 256
band_pos$ = 264
tv1052 = 272
sao_bands$ = 272
sao_band_ddistortion_avx2 PROC

; 823  : {

	mov	QWORD PTR [rsp+24], r8
	mov	QWORD PTR [rsp+16], rdx
	push	rsi
	push	r12
	push	r15
	sub	rsp, 192				; 000000c0H

; 824  :   const uint32_t bitdepth = 8;
; 825  :   const uint32_t shift    = bitdepth - 5;
; 826  : 
; 827  :   // Clamp band_pos to 32 from above. It'll be subtracted from the shifted
; 828  :   // rec_data values, which in 8-bit depth will always be clamped to [0, 31],
; 829  :   // so if it ever exceeds 32, all the band values will be negative and
; 830  :   // ignored. Ditto for less than -4.
; 831  :   __m128i bp_128   = _mm_cvtsi32_si128    (band_pos);

	mov	r12d, DWORD PTR band_pos$[rsp]

; 832  :   __m128i hilimit  = _mm_cvtsi32_si128    (32);
; 833  :   __m128i lolimit  = _mm_cvtsi32_si128    (-4);

	mov	eax, -4

; 834  : 
; 835  :           bp_128   = _mm_min_epi8         (bp_128, hilimit);
; 836  :           bp_128   = _mm_max_epi8         (bp_128, lolimit);
; 837  : 
; 838  :   __m256i bp_256  = _mm256_broadcastb_epi8(bp_128);
; 839  : 
; 840  :   __m128i sbs_32   = _mm_loadu_si128((const __m128i *)sao_bands);

	mov	r15, QWORD PTR sao_bands$[rsp]
	mov	r10, rdx
	vmovd	xmm2, eax
	mov	eax, 32					; 00000020H
	vmovaps	XMMWORD PTR [rsp+32], xmm14
	vmovd	xmm1, eax
	vmovdqu	xmm3, XMMWORD PTR [r15]
	vmovd	xmm0, r12d
	vpminsb	xmm1, xmm0, xmm1
	vpmaxsb	xmm2, xmm1, xmm2
	vpbroadcastb ymm14, xmm2

; 172  :         __m128i sbs_8          = _mm_shuffle_epi8(v, trunc_shufmask);

	vpshufb	xmm2, xmm3, XMMWORD PTR __xmm@0c0804000c0804000c0804000c080400

; 218  :   __m256i res = _mm256_castsi128_si256 (v);

	vmovups	xmm1, xmm2

; 823  : {

	vmovaps	XMMWORD PTR [rsp+16], xmm15

; 841  :   __m128i sbs_8    = truncate_epi32_epi8(sbs_32);
; 842  :   __m256i sb_256   = broadcast_xmm2ymm  (sbs_8);
; 843  : 
; 844  :   // These should trigger like, never, at least the later condition of block
; 845  :   // not being a multiple of 32 wide. Rather safe than sorry though, huge SAO
; 846  :   // bands are more tricky of these two because the algorithm needs a complete
; 847  :   // reimplementation to work on 16-bit values.
; 848  :   if (epi32v_fits_in_epi8s(sbs_32) != 0xffff)

	mov	edx, 65535				; 0000ffffH

; 219  :   return        _mm256_inserti128_si256(res, v, 1);

	vinserti128 ymm15, ymm1, xmm2, 1

; 162  :   __m128i  v_signs = _mm_srai_epi32   (v, 31);

	vpsrad	xmm2, xmm3, 31
	vpsrad	xmm0, xmm3, 7

; 163  :   __m128i  ok_i32s = _mm_cmpeq_epi32  (v_ms25b, v_signs);

	vpcmpeqd xmm1, xmm2, xmm0

; 164  :   return             _mm_movemask_epi8(ok_i32s);

	vpmovmskb eax, xmm1

; 823  : {

	movsxd	rsi, r9d
	mov	r11, r8

; 841  :   __m128i sbs_8    = truncate_epi32_epi8(sbs_32);
; 842  :   __m256i sb_256   = broadcast_xmm2ymm  (sbs_8);
; 843  : 
; 844  :   // These should trigger like, never, at least the later condition of block
; 845  :   // not being a multiple of 32 wide. Rather safe than sorry though, huge SAO
; 846  :   // bands are more tricky of these two because the algorithm needs a complete
; 847  :   // reimplementation to work on 16-bit values.
; 848  :   if (epi32v_fits_in_epi8s(sbs_32) != 0xffff)

	cmp	ax, dx
	jne	$use_generic$57

; 849  :     goto use_generic;
; 850  : 
; 851  :   // If VVC or something will start using SAO on blocks with width a multiple
; 852  :   // of 16, feel free to implement a XMM variant of this algorithm
; 853  :   if ((block_width & 31) != 0)

	test	sil, 31
	jne	$use_generic$57

; 857  :   const __m256i threes        = _mm256_set1_epi8 (3);
; 858  : 
; 859  :   __m256i sum = _mm256_setzero_si256();
; 860  :   for (uint32_t y = 0; y < block_height; y++) {

	mov	eax, DWORD PTR block_height$[rsp]
	vmovaps	XMMWORD PTR [rsp+112], xmm9
	vmovaps	XMMWORD PTR [rsp+64], xmm12
	vmovaps	XMMWORD PTR [rsp+48], xmm13
	vmovdqu	ymm13, YMMWORD PTR __ymm@0303030303030303030303030303030303030303030303030303030303030303
	vpxor	xmm9, xmm9, xmm9
	vpxor	xmm12, xmm12, xmm12
	test	eax, eax
	je	$LN3@sao_band_d

; 854  :     goto use_generic;
; 855  : 
; 856  :   const __m256i zero          = _mm256_setzero_si256();

	vmovaps	XMMWORD PTR [rsp+160], xmm6
	xor	edx, edx
	vmovaps	XMMWORD PTR [rsp+144], xmm7
	mov	r9d, eax
	vmovaps	XMMWORD PTR [rsp+128], xmm8
	vmovaps	XMMWORD PTR [rsp+96], xmm10
	vmovaps	XMMWORD PTR [rsp+80], xmm11
$LL4@sao_band_d:

; 861  :     for (uint32_t x = 0; x < block_width; x += 32) {

	xor	r8d, r8d
	test	esi, esi
	je	$LN2@sao_band_d
	vmovdqu	ymm11, YMMWORD PTR __ymm@1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f1f
	vmovdqu	ymm10, YMMWORD PTR __ymm@ffff0001ffff0001ffff0001ffff0001ffff0001ffff0001ffff0001ffff0001
	npad	3
$LL7@sao_band_d:

; 862  :       const int32_t curr_pos = y * block_width + x;
; 863  : 
; 864  :       __m256i   rd = _mm256_loadu_si256((const __m256i *)( rec_data + curr_pos));

	lea	eax, DWORD PTR [rdx+r8]
	add	r8d, 32					; 00000020H
	movsxd	rcx, eax
	vmovdqu	ymm4, YMMWORD PTR [rcx+r11]

; 865  :       __m256i orig = _mm256_loadu_si256((const __m256i *)(orig_data + curr_pos));
; 866  : 
; 867  :       __m256i orig_lo, orig_hi, rd_lo, rd_hi;
; 868  :       cvt_epu8_epi16(orig, &orig_lo, &orig_hi);

	vmovdqu	ymm3, YMMWORD PTR [rcx+r10]

; 105  :              *res_hi  = _mm256_unpackhi_epi8(v, zero);

	vpunpckhbw ymm2, ymm3, ymm9

; 93   :   __m256i v_shifted = _mm256_srli_epi32(v,         shift);

	vpsrld	ymm0, ymm4, 3

; 94   :   __m256i v_masked  = _mm256_and_si256 (v_shifted, hibit_mask_256);

	vpand	ymm1, ymm0, ymm11

; 873  : 
; 874  :       // The shift will clamp band to 0...31; band_pos on the other
; 875  :       // hand is always between 0...32, so band will be -1...31. Anything
; 876  :       // below zero is ignored, so we can clamp band_pos to 32.
; 877  :       __m256i rd_divd      = srli_epi8           (rd,            shift);
; 878  :       __m256i band         = _mm256_sub_epi8     (rd_divd,       bp_256);

	vpsubb	ymm5, ymm1, ymm14

; 104  :              *res_lo  = _mm256_unpacklo_epi8(v, zero);

	vpunpcklbw ymm1, ymm3, ymm9

; 105  :              *res_hi  = _mm256_unpackhi_epi8(v, zero);

	vpunpckhbw ymm0, ymm4, ymm9

; 872  :       __m256i diff_hi      = _mm256_sub_epi16     (orig_hi,      rd_hi);

	vpsubw	ymm8, ymm2, ymm0

; 104  :              *res_lo  = _mm256_unpacklo_epi8(v, zero);

	vpunpcklbw ymm0, ymm4, ymm9

; 869  :       cvt_epu8_epi16(rd,   &rd_lo,   &rd_hi);
; 870  : 
; 871  :       __m256i diff_lo      = _mm256_sub_epi16     (orig_lo,      rd_lo);

	vpsubw	ymm6, ymm1, ymm0

; 879  : 
; 880  :       // Force all <0 or >3 bands to 0xff, which will zero the shuffle result
; 881  :       __m256i band_lt_0    = _mm256_cmpgt_epi8   (zero,          band);

	vpcmpgtb ymm0, ymm9, ymm5

; 882  :       __m256i band_gt_3    = _mm256_cmpgt_epi8   (band,          threes);

	vpcmpgtb ymm2, ymm5, ymm13

; 883  :       __m256i band_inv     = _mm256_or_si256     (band_lt_0,     band_gt_3);

	vpor	ymm1, ymm0, ymm2

; 884  : 
; 885  :               band         = _mm256_or_si256     (band,          band_inv);

	vpor	ymm2, ymm1, ymm5

; 886  : 
; 887  :       __m256i offsets      = _mm256_shuffle_epi8 (sb_256,        band);

	vpshufb	ymm3, ymm15, ymm2

; 113  :         __m256i signs = _mm256_cmpgt_epi8   (zero, v);

	vpcmpgtb ymm0, ymm9, ymm3

; 114  :              *res_lo  = _mm256_unpacklo_epi8(v,    signs);

	vpunpcklbw ymm2, ymm3, ymm0

; 115  :              *res_hi  = _mm256_unpackhi_epi8(v,    signs);

	vpunpckhbw ymm5, ymm3, ymm0

; 236  :   __m256i offsets_0_lo = _mm256_cmpeq_epi16   (offsets_lo,   zero);

	vpcmpeqw ymm1, ymm2, ymm9

; 237  :   __m256i offsets_0_hi = _mm256_cmpeq_epi16   (offsets_hi,   zero);
; 238  : 
; 239  :   __m256i delta_lo     = _mm256_sub_epi16     (diff_lo,      offsets_lo);
; 240  :   __m256i delta_hi     = _mm256_sub_epi16     (diff_hi,      offsets_hi);
; 241  : 
; 242  :   __m256i diff_lo_m    = _mm256_andnot_si256  (offsets_0_lo, diff_lo);

	vpandn	ymm4, ymm1, ymm6
	vpsubw	ymm0, ymm6, ymm2

; 243  :   __m256i diff_hi_m    = _mm256_andnot_si256  (offsets_0_hi, diff_hi);
; 244  :   __m256i delta_lo_m   = _mm256_andnot_si256  (offsets_0_lo, delta_lo);

	vpandn	ymm2, ymm1, ymm0
	vpcmpeqw ymm3, ymm5, ymm9
	vpandn	ymm7, ymm3, ymm8
	vpsubw	ymm1, ymm8, ymm5

; 245  :   __m256i delta_hi_m   = _mm256_andnot_si256  (offsets_0_hi, delta_hi);

	vpandn	ymm0, ymm3, ymm1

; 246  : 
; 247  :   __m256i dd0_lo       = _mm256_unpacklo_epi16(delta_lo_m,   diff_lo_m);

	vpunpcklwd ymm3, ymm2, ymm4

; 248  :   __m256i dd0_hi       = _mm256_unpackhi_epi16(delta_lo_m,   diff_lo_m);
; 249  :   __m256i dd1_lo       = _mm256_unpacklo_epi16(delta_hi_m,   diff_hi_m);

	vpunpcklwd ymm6, ymm0, ymm7

; 250  :   __m256i dd1_hi       = _mm256_unpackhi_epi16(delta_hi_m,   diff_hi_m);

	vpunpckhwd ymm7, ymm0, ymm7
	vpunpckhwd ymm4, ymm2, ymm4

; 251  : 
; 252  :   __m256i dd0_lo_n     = _mm256_sign_epi16    (dd0_lo,       negate_hiword);

	vpsignw	ymm0, ymm3, ymm10

; 253  :   __m256i dd0_hi_n     = _mm256_sign_epi16    (dd0_hi,       negate_hiword);
; 254  :   __m256i dd1_lo_n     = _mm256_sign_epi16    (dd1_lo,       negate_hiword);
; 255  :   __m256i dd1_hi_n     = _mm256_sign_epi16    (dd1_hi,       negate_hiword);
; 256  : 
; 257  :   __m256i sum0_lo      = _mm256_madd_epi16    (dd0_lo,       dd0_lo_n);

	vpmaddwd ymm3, ymm0, ymm3
	vpsignw	ymm1, ymm4, ymm10

; 258  :   __m256i sum0_hi      = _mm256_madd_epi16    (dd0_hi,       dd0_hi_n);

	vpmaddwd ymm2, ymm1, ymm4
	vpsignw	ymm0, ymm6, ymm10

; 259  :   __m256i sum1_lo      = _mm256_madd_epi16    (dd1_lo,       dd1_lo_n);
; 260  :   __m256i sum1_hi      = _mm256_madd_epi16    (dd1_hi,       dd1_hi_n);
; 261  : 
; 262  :   __m256i sum0         = _mm256_add_epi32     (sum0_lo,      sum0_hi);

	vpaddd	ymm5, ymm2, ymm3
	vpsignw	ymm1, ymm7, ymm10
	vpmaddwd ymm4, ymm0, ymm6
	vpmaddwd ymm2, ymm1, ymm7

; 263  :   __m256i sum1         = _mm256_add_epi32     (sum1_lo,      sum1_hi);

	vpaddd	ymm0, ymm2, ymm4

; 264  :   return                 _mm256_add_epi32     (sum0,         sum1);

	vpaddd	ymm3, ymm0, ymm5

; 888  : 
; 889  :       __m256i curr_sum     = calc_diff_off_delta (diff_lo, diff_hi, offsets, orig);
; 890  :               sum          = _mm256_add_epi32    (sum,          curr_sum);

	vpaddd	ymm12, ymm3, ymm12
	cmp	r8d, esi
	jb	$LL7@sao_band_d
$LN2@sao_band_d:

; 857  :   const __m256i threes        = _mm256_set1_epi8 (3);
; 858  : 
; 859  :   __m256i sum = _mm256_setzero_si256();
; 860  :   for (uint32_t y = 0; y < block_height; y++) {

	add	edx, esi
	sub	r9, 1
	jne	$LL4@sao_band_d
	vmovaps	xmm11, XMMWORD PTR [rsp+80]
	vmovaps	xmm10, XMMWORD PTR [rsp+96]
	vmovaps	xmm8, XMMWORD PTR [rsp+128]
	vmovaps	xmm7, XMMWORD PTR [rsp+144]
	vmovaps	xmm6, XMMWORD PTR [rsp+160]
$LN3@sao_band_d:

; 893  :   return hsum_8x32b(sum);

	vmovaps	xmm13, XMMWORD PTR [rsp+48]
	vmovaps	xmm9, XMMWORD PTR [rsp+112]
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 173  :   __m256i sum2 = _mm256_permute4x64_epi64(sum1, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm0, ymm12, 78				; 0000004eH

; 174  :   __m256i sum3 = _mm256_add_epi32        (sum1, sum2);

	vpaddd	ymm2, ymm0, ymm12
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c

; 893  :   return hsum_8x32b(sum);

	vmovaps	xmm12, XMMWORD PTR [rsp+64]
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 175  :   __m256i sum4 = _mm256_shuffle_epi32    (sum3, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	ymm1, ymm2, 78				; 0000004eH

; 176  :   __m256i sum5 = _mm256_add_epi32        (sum3, sum4);

	vpaddd	ymm3, ymm1, ymm2

; 177  :   __m256i sum6 = _mm256_shuffle_epi32    (sum5, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm0, ymm3, 177				; 000000b1H

; 178  :   __m256i sum7 = _mm256_add_epi32        (sum5, sum6);

	vpaddd	ymm1, ymm0, ymm3

; 179  : 
; 180  :   __m128i sum8 = _mm256_castsi256_si128  (sum7);
; 181  :   int32_t sum9 = _mm_cvtsi128_si32       (sum8);

	vmovd	eax, xmm1
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c

; 893  :   return hsum_8x32b(sum);

	jmp	$LN1@sao_band_d
$use_generic$57:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\sao_shared_generics.h

; 99   :   int shift = state->encoder_control->bitdepth-5;

	mov	rax, QWORD PTR [rcx]
	mov	QWORD PTR [rsp+224], rbx

; 100  :   int sum = 0;

	xor	ebx, ebx
	movzx	edx, BYTE PTR [rax+2592]

; 101  :   for (y = 0; y < block_height; ++y) {

	mov	eax, DWORD PTR block_height$[rsp]
	mov	BYTE PTR tv1052[rsp], dl
	test	eax, eax
	jle	$LN33@sao_band_d

; 99   :   int shift = state->encoder_control->bitdepth-5;

	mov	QWORD PTR [rsp+248], rdi
	mov	rcx, rsi
	mov	QWORD PTR [rsp+184], r13
	mov	r13, r10
	mov	QWORD PTR [rsp+176], r14
	mov	QWORD PTR tv956[rsp], rax
	npad	4
$LL34@sao_band_d:

; 102  :     for (x = 0; x < block_width; ++x) {

	test	esi, esi
	jle	SHORT $LN32@sao_band_d
	mov	rdi, r11
	lea	eax, DWORD PTR [rdx-5]
	movzx	r14d, al
	sub	rdi, r10
	mov	r9, r13
	mov	r11d, esi
	npad	9
$LL37@sao_band_d:

; 103  :       const int32_t curr_pos = y * block_width + x;
; 104  : 
; 105  :       kvz_pixel rec  =  rec_data[curr_pos];
; 106  :       kvz_pixel orig = orig_data[curr_pos];
; 107  : 
; 108  :       int32_t band = (rec >> shift) - band_pos;

	movzx	r10d, BYTE PTR [rdi+r9]

; 109  :       int32_t offset = 0;

	xor	r8d, r8d
	shrx	eax, r10d, r14d
	sub	eax, r12d

; 110  :       if (band >= 0 && band <= 3) {

	cmp	eax, 3
	ja	SHORT $LN38@sao_band_d

; 111  :         offset = sao_bands[band];

	movsxd	rcx, eax
	mov	r8d, DWORD PTR [r15+rcx*4]
$LN38@sao_band_d:

; 112  :       }
; 113  :       // Offset is applied to reconstruction, so it is subtracted from diff.
; 114  : 
; 115  :       int32_t diff  = orig - rec;

	movzx	edx, BYTE PTR [r9]
	sub	edx, r10d

; 116  :       int32_t delta = diff - offset;

	mov	ecx, edx
	sub	ecx, r8d

; 117  : 
; 118  :       int32_t dmask = (offset == 0) ? -1 : 0;
; 119  :       diff  &= ~dmask;

	neg	r8d
	sbb	eax, eax
	inc	r9

; 120  :       delta &= ~dmask;

	and	ecx, eax
	and	edx, eax

; 121  : 
; 122  :       sum += delta * delta - diff * diff;

	imul	ecx, ecx
	imul	edx, edx
	sub	ecx, edx
	add	ebx, ecx
	sub	r11, 1
	jne	SHORT $LL37@sao_band_d
	mov	rax, QWORD PTR tv956[rsp]
	mov	rcx, rsi
	mov	r10, QWORD PTR orig_data$[rsp]
	movzx	edx, BYTE PTR tv1052[rsp]
	mov	r11, QWORD PTR rec_data$[rsp]
$LN32@sao_band_d:

; 101  :   for (y = 0; y < block_height; ++y) {

	add	r13, rcx
	sub	rax, 1
	mov	QWORD PTR tv956[rsp], rax
	jne	$LL34@sao_band_d
	mov	r14, QWORD PTR [rsp+176]
	mov	r13, QWORD PTR [rsp+184]
	mov	rdi, QWORD PTR [rsp+248]
$LN33@sao_band_d:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\sao-avx2.c

; 896  :   return sao_band_ddistortion_generic(state, orig_data, rec_data, block_width,

	mov	eax, ebx
	mov	rbx, QWORD PTR [rsp+224]
$LN1@sao_band_d:

; 897  :       block_height, band_pos, sao_bands);
; 898  : }

	vzeroupper
	vmovaps	xmm14, XMMWORD PTR [rsp+32]
	vmovaps	xmm15, XMMWORD PTR [rsp+16]
	add	rsp, 192				; 000000c0H
	pop	r15
	pop	r12
	pop	rsi
	ret	0
sao_band_ddistortion_avx2 ENDP
_TEXT	ENDS
END
