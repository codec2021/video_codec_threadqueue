; Listing generated by Microsoft (R) Optimizing Compiler Version 19.26.28806.0 

include listing.inc

INCLUDELIB OLDNAMES

?modedisp2invsampledisp@?1??kvz_angular_pred_avx2@@9@9 DW 00H ; `kvz_angular_pred_avx2'::`2'::modedisp2invsampledisp
	DW	01000H
	DW	0666H
	DW	038eH
	DW	0276H
	DW	01e2H
	DW	0186H
	DW	013bH
	DW	0100H
cbf_masks DW	01fH
	DW	0fH
	DW	07H
	DW	03H
	DW	01H
	ORG $+1
?modedisp2sampledisp@?1??kvz_angular_pred_avx2@@9@9 DB 00H ; `kvz_angular_pred_avx2'::`2'::modedisp2sampledisp
	DB	02H
	DB	05H
	DB	09H
	DB	0dH
	DB	011H
	DB	015H
	DB	01aH
	DB	020H
	ORG $+7
g_sig_last_scan_16x16 DD 00H
	DD	04H
	DD	01H
	DD	08H
	DD	05H
	DD	02H
	DD	0cH
	DD	09H
	DD	06H
	DD	03H
	DD	0dH
	DD	0aH
	DD	07H
	DD	0eH
	DD	0bH
	DD	0fH
g_group_idx DB	00H
	DB	01H
	DB	02H
	DB	03H
	DB	04H
	DB	04H
	DB	05H
	DB	05H
	DB	06H
	DB	06H
	DB	06H
	DB	06H
	DB	07H
	DB	07H
	DB	07H
	DB	07H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
strategies_to_select DQ FLAT:$SG4294947294
	DQ	FLAT:kvz_array_checksum
	DQ	FLAT:$SG4294947293
	DQ	FLAT:kvz_array_md5
	DQ	FLAT:$SG4294947292
	DQ	FLAT:kvz_reg_sad
	DQ	FLAT:$SG4294947291
	DQ	FLAT:kvz_sad_4x4
	DQ	FLAT:$SG4294947290
	DQ	FLAT:kvz_sad_8x8
	DQ	FLAT:$SG4294947289
	DQ	FLAT:kvz_sad_16x16
	DQ	FLAT:$SG4294947288
	DQ	FLAT:kvz_sad_32x32
	DQ	FLAT:$SG4294947287
	DQ	FLAT:kvz_sad_64x64
	DQ	FLAT:$SG4294947286
	DQ	FLAT:kvz_satd_4x4
	DQ	FLAT:$SG4294947285
	DQ	FLAT:kvz_satd_8x8
	DQ	FLAT:$SG4294947284
	DQ	FLAT:kvz_satd_16x16
	DQ	FLAT:$SG4294947283
	DQ	FLAT:kvz_satd_32x32
	DQ	FLAT:$SG4294947282
	DQ	FLAT:kvz_satd_64x64
	DQ	FLAT:$SG4294947281
	DQ	FLAT:kvz_satd_any_size
	DQ	FLAT:$SG4294947280
	DQ	FLAT:kvz_sad_4x4_dual
	DQ	FLAT:$SG4294947279
	DQ	FLAT:kvz_sad_8x8_dual
	DQ	FLAT:$SG4294947278
	DQ	FLAT:kvz_sad_16x16_dual
	DQ	FLAT:$SG4294947277
	DQ	FLAT:kvz_sad_32x32_dual
	DQ	FLAT:$SG4294947276
	DQ	FLAT:kvz_sad_64x64_dual
	DQ	FLAT:$SG4294947275
	DQ	FLAT:kvz_satd_4x4_dual
	DQ	FLAT:$SG4294947274
	DQ	FLAT:kvz_satd_8x8_dual
	DQ	FLAT:$SG4294947273
	DQ	FLAT:kvz_satd_16x16_dual
	DQ	FLAT:$SG4294947272
	DQ	FLAT:kvz_satd_32x32_dual
	DQ	FLAT:$SG4294947271
	DQ	FLAT:kvz_satd_64x64_dual
	DQ	FLAT:$SG4294947270
	DQ	FLAT:kvz_satd_any_size_quad
	DQ	FLAT:$SG4294947269
	DQ	FLAT:kvz_pixels_calc_ssd
	DQ	FLAT:$SG4294947268
	DQ	FLAT:kvz_bipred_average
	DQ	FLAT:$SG4294947267
	DQ	FLAT:kvz_get_optimized_sad
	DQ	FLAT:$SG4294947266
	DQ	FLAT:kvz_ver_sad
	DQ	FLAT:$SG4294947265
	DQ	FLAT:kvz_hor_sad
	DQ	FLAT:$SG4294947264
	DQ	FLAT:kvz_pixel_var
	DQ	FLAT:$SG4294947263
	DQ	FLAT:kvz_fast_forward_dst_4x4
	DQ	FLAT:$SG4294947262
	DQ	FLAT:kvz_dct_4x4
	DQ	FLAT:$SG4294947261
	DQ	FLAT:kvz_dct_8x8
	DQ	FLAT:$SG4294947260
	DQ	FLAT:kvz_dct_16x16
	DQ	FLAT:$SG4294947259
	DQ	FLAT:kvz_dct_32x32
	DQ	FLAT:$SG4294947258
	DQ	FLAT:kvz_fast_inverse_dst_4x4
	DQ	FLAT:$SG4294947257
	DQ	FLAT:kvz_idct_4x4
	DQ	FLAT:$SG4294947256
	DQ	FLAT:kvz_idct_8x8
	DQ	FLAT:$SG4294947255
	DQ	FLAT:kvz_idct_16x16
	DQ	FLAT:$SG4294947254
	DQ	FLAT:kvz_idct_32x32
	DQ	FLAT:$SG4294947253
	DQ	FLAT:kvz_filter_hpel_blocks_hor_ver_luma
	DQ	FLAT:$SG4294947252
	DQ	FLAT:kvz_filter_hpel_blocks_diag_luma
	DQ	FLAT:$SG4294947251
	DQ	FLAT:kvz_filter_qpel_blocks_hor_ver_luma
	DQ	FLAT:$SG4294947250
	DQ	FLAT:kvz_filter_qpel_blocks_diag_luma
	DQ	FLAT:$SG4294947249
	DQ	FLAT:kvz_sample_quarterpel_luma
	DQ	FLAT:$SG4294947248
	DQ	FLAT:kvz_sample_octpel_chroma
	DQ	FLAT:$SG4294947247
	DQ	FLAT:kvz_sample_quarterpel_luma_hi
	DQ	FLAT:$SG4294947246
	DQ	FLAT:kvz_sample_octpel_chroma_hi
	DQ	FLAT:$SG4294947245
	DQ	FLAT:kvz_get_extended_block
	DQ	FLAT:$SG4294947244
	DQ	FLAT:kvz_quant
	DQ	FLAT:$SG4294947243
	DQ	FLAT:kvz_quantize_residual
	DQ	FLAT:$SG4294947242
	DQ	FLAT:kvz_dequant
	DQ	FLAT:$SG4294947241
	DQ	FLAT:kvz_coeff_abs_sum
	DQ	FLAT:$SG4294947240
	DQ	FLAT:kvz_fast_coeff_cost
	DQ	FLAT:$SG4294947239
	DQ	FLAT:kvz_angular_pred
	DQ	FLAT:$SG4294947238
	DQ	FLAT:kvz_intra_pred_planar
	DQ	FLAT:$SG4294947237
	DQ	FLAT:kvz_intra_pred_filtered_dc
	DQ	FLAT:$SG4294947236
	DQ	FLAT:kvz_sao_edge_ddistortion
	DQ	FLAT:$SG4294947235
	DQ	FLAT:kvz_calc_sao_edge_dir
	DQ	FLAT:$SG4294947234
	DQ	FLAT:kvz_sao_reconstruct_color
	DQ	FLAT:$SG4294947233
	DQ	FLAT:kvz_sao_band_ddistortion
	DQ	FLAT:$SG4294947232
	DQ	FLAT:kvz_encode_coeff_nxn
	DQ	0000000000000000H
	DQ	0000000000000000H
g_sig_last_scan_32x32 DD 00H
	DD	08H
	DD	01H
	DD	010H
	DD	09H
	DD	02H
	DD	018H
	DD	011H
	DD	0aH
	DD	03H
	DD	020H
	DD	019H
	DD	012H
	DD	0bH
	DD	04H
	DD	028H
	DD	021H
	DD	01aH
	DD	013H
	DD	0cH
	DD	05H
	DD	030H
	DD	029H
	DD	022H
	DD	01bH
	DD	014H
	DD	0dH
	DD	06H
	DD	038H
	DD	031H
	DD	02aH
	DD	023H
	DD	01cH
	DD	015H
	DD	0eH
	DD	07H
	DD	039H
	DD	032H
	DD	02bH
	DD	024H
	DD	01dH
	DD	016H
	DD	0fH
	DD	03aH
	DD	033H
	DD	02cH
	DD	025H
	DD	01eH
	DD	017H
	DD	03bH
	DD	034H
	DD	02dH
	DD	026H
	DD	01fH
	DD	03cH
	DD	035H
	DD	02eH
	DD	027H
	DD	03dH
	DD	036H
	DD	02fH
	DD	03eH
	DD	037H
	DD	03fH
g_min_in_group DB 00H
	DB	01H
	DB	02H
	DB	03H
	DB	04H
	DB	06H
	DB	08H
	DB	0cH
	DB	010H
	DB	018H
	ORG $+6
default_fast_coeff_cost_wts DD 03e282e88r	; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e26c094r			; 0.162844
	DD	04081ca43r			; 4.05594
	DD	04064203ar			; 3.56447
	DD	040db915ar			; 6.86149
	DD	03e03d189r			; 0.128729
	DD	04089fbafr			; 4.31197
	DD	0407c5771r			; 3.94284
	DD	040ddeed2r			; 6.9354
	DD	03de33ce6r			; 0.110956
	DD	0408ddcb1r			; 4.43319
	DD	0407c8738r			; 3.94575
	DD	040dc1618r			; 6.8777
	DD	03dc29cfer			; 0.095026
	DD	0408f7938r			; 4.48355
	DD	0408636aar			; 4.19417
	DD	040d90260r			; 6.78154
	DD	03d99b1b8r			; 0.075046
	DD	04094474cr			; 4.6337
	DD	04082b1b6r			; 4.08419
	DD	040d65aeer			; 6.6986
	DD	03d56bca5r			; 0.052426
	DD	0409ef37er			; 4.96722
	DD	04080dee8r			; 4.02721
	DD	040d19306r			; 6.5492
	DD	03d24bcaer			; 0.040219
	DD	040a489car			; 5.14182
	DD	0407ee3bdr			; 3.98265
	DD	040cec513r			; 6.46156
	DD	03d0fba88r			; 0.03509
	DD	040a628e7r			; 5.19249
	DD	040752e49r			; 3.83095
	DD	040cd642ar			; 6.41848
	DD	03cf47d80r			; 0.029845
	DD	040a6c5d0r			; 5.21165
	DD	040743073r			; 3.81546
	DD	040cb0dd8r			; 6.34544
	DD	03cc0b136r			; 0.023522
	DD	040aa4f92r			; 5.32221
	DD	040744224r			; 3.81654
	DD	040cb8aaar			; 6.36068
	DD	03cae87d3r			; 0.021305
	DD	040a73ac3r			; 5.22592
	DD	04075eeccr			; 3.8427
	DD	040ca6cd9r			; 6.32579
	DD	03c821294r			; 0.015878
	DD	040a5dbe0r			; 5.18309
	DD	0407d2f27r			; 3.956
	DD	040ca8cbdr			; 6.32968
	DD	03c2ae297r			; 0.01043
	DD	040a32ce4r			; 5.09923
	DD	04085a85fr			; 4.1768
	DD	040c9c5d6r			; 6.3054
	DD	03c0a2a91r			; 0.008433
	DD	040a0f7der			; 5.03026
	DD	040879a50r			; 4.23759
	DD	040c8a4eer			; 6.27013
	DD	03bd4fdf4r			; 0.0065
	DD	0409f0412r			; 4.96925
	DD	0408adc57r			; 4.3394
	DD	040c6f870r			; 6.21783
	DD	03ba18373r			; 0.004929
	DD	0409d8d50r			; 4.9235
	DD	0408e283fr			; 4.44241
	DD	040c5df6cr			; 6.18352
	DD	03b73775cr			; 0.003715
	DD	0409d4c75r			; 4.91558
	DD	0408dbb1br			; 4.42909
	DD	040c4029fr			; 6.12532
	DD	03b4a70d2r			; 0.003089
	DD	0409c48f7r			; 4.88391
	DD	040920260r			; 4.56279
	DD	040c5019dr			; 6.15645
	DD	03b219c9dr			; 0.002466
	DD	0409c31abr			; 4.88106
	DD	040942800r			; 4.62988
	DD	040c49088r			; 6.14264
	DD	03b0e25c8r			; 0.002169
	DD	0409c3d62r			; 4.88249
	DD	04094ae99r			; 4.64631
	DD	040c415d1r			; 6.12766
	DD	03b26dacbr			; 0.002546
	DD	040996304r			; 4.79334
	DD	0409acc16r			; 4.83741
	DD	040c6606br			; 6.19927
	DD	03aac3a86r			; 0.001314
	DD	04099e220r			; 4.80885
	DD	0409a81bdr			; 4.82834
	DD	040c7ca3cr			; 6.24344
	DD	03a9741d1r			; 0.001154
	DD	0409b9a72r			; 4.8626
	DD	0409b19aar			; 4.84688
	DD	040c693a5r			; 6.20552
	DD	03a80f990r			; 0.000984
	DD	0409bb993r			; 4.8664
	DD	0409b7fa2r			; 4.85933
	DD	040c7b565r			; 6.24089
	DD	03a551f82r			; 0.000813
	DD	0409b698ar			; 4.85663
	DD	0409d95bar			; 4.92453
	DD	040c963a4r			; 6.29341
	DD	03a91c087r			; 0.001112
	DD	04099419er			; 4.78926
	DD	040a050f0r			; 5.00988
	DD	040cddf8fr			; 6.43354
	DD	03a10b418r			; 0.000552
	DD	04098580ar			; 4.76075
	DD	040a2e4f1r			; 5.09045
	DD	040d32e1fr			; 6.59938
	DD	039ccff22r			; 0.000391
	DD	0409ec42cr			; 4.96145
	DD	040a38d95r			; 5.11103
	DD	040d8342fr			; 6.75637
	DD	039ae1049r			; 0.000332
	DD	0409f63f8r			; 4.98095
	DD	040a46b89r			; 5.13813
	DD	040dbc1e8r			; 6.86742
	DD	03952c387r			; 0.000201
	DD	040a5d297r			; 5.18196
	DD	04097af64r			; 4.74016
	DD	040cec07dr			; 6.461
	DD	0397ba882r			; 0.00024
	DD	040a5eeb7r			; 5.18539
	DD	0409bfeb0r			; 4.87484
	DD	040da3603r			; 6.81909
	DD	03908509cr			; 0.00013
	DD	040a8a6b5r			; 5.27035
	DD	040977eacr			; 4.73421
	DD	040da708fr			; 6.82624
	DD	038da1a93r			; 0.000104
	DD	040abe6e8r			; 5.37194
	DD	040930af4r			; 4.59509
	DD	040d5189ar			; 6.65925
	DD	038ae1049r			; 8.3e-05
	DD	040ab9581r			; 5.362
	DD	04093c250r			; 4.61747
	DD	040dacf03r			; 6.83777
	DD	03890b418r			; 6.9e-05
	DD	040a926e3r			; 5.286
	DD	0409828e7r			; 4.75499
	DD	040e516e1r			; 7.15904
	DD	0384d8559r			; 4.9e-05
	DD	040afa18cr			; 5.48847
	DD	0408cace9r			; 4.39611
	DD	040d74682r			; 6.72736
	DD	038734507r			; 5.8e-05
	DD	0409eafa3r			; 4.95894
	DD	040929321r			; 4.58046
	DD	040cf49a5r			; 6.47774
	DD	037eae18br			; 2.8e-05
	DD	040b0ae1br			; 5.52125
	DD	0408e1885r			; 4.44049
	DD	040e68f80r			; 7.20502
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	0379f6230r			; 1.9e-05
	DD	040b9f5d8r			; 5.81126
	DD	0408cc582r			; 4.39911
	DD	040eac30dr			; 7.33631
g_sao_edge_offsets DD 0ffffffffH
	DD	00H
	DD	01H
	DD	00H
	DD	00H
	DD	0ffffffffH
	DD	00H
	DD	01H
	DD	0ffffffffH
	DD	0ffffffffH
	DD	01H
	DD	01H
	DD	01H
	DD	0ffffffffH
	DD	0ffffffffH
	DD	01H
g_sig_last_scan_cg DQ FLAT:g_sig_last_scan_8x8
	DQ	FLAT:g_sig_last_scan_8x8+16
	DQ	FLAT:g_sig_last_scan_8x8+32
	DQ	FLAT:g_sig_last_scan_8x8
	DQ	FLAT:g_sig_last_scan_8x8+16
	DQ	FLAT:g_sig_last_scan_8x8+32
	DQ	FLAT:g_sig_last_scan_16x16
	DQ	0000000000000000H
	DQ	0000000000000000H
	DQ	FLAT:g_sig_last_scan_32x32
	DQ	0000000000000000H
	DQ	0000000000000000H
g_sig_last_scan_8x8 DD 00H
	DD	02H
	DD	01H
	DD	03H
	DD	00H
	DD	01H
	DD	02H
	DD	03H
	DD	00H
	DD	02H
	DD	01H
	DD	03H
	ORG $+3
$SG4294947244 DB 'quant', 00H
	ORG $+7
$SG4294947294 DB 'array_checksum', 00H
	ORG $+1
$SG4294947293 DB 'array_md5', 00H
	ORG $+6
$SG4294947292 DB 'reg_sad', 00H
$SG4294947291 DB 'sad_4x4', 00H
$SG4294947290 DB 'sad_8x8', 00H
$SG4294947289 DB 'sad_16x16', 00H
	ORG $+6
$SG4294947288 DB 'sad_32x32', 00H
	ORG $+6
$SG4294947287 DB 'sad_64x64', 00H
	ORG $+6
$SG4294947286 DB 'satd_4x4', 00H
	ORG $+7
$SG4294947285 DB 'satd_8x8', 00H
	ORG $+7
$SG4294947284 DB 'satd_16x16', 00H
	ORG $+5
$SG4294947283 DB 'satd_32x32', 00H
	ORG $+5
$SG4294947282 DB 'satd_64x64', 00H
	ORG $+5
$SG4294947281 DB 'satd_any_size', 00H
	ORG $+2
$SG4294947280 DB 'sad_4x4_dual', 00H
	ORG $+3
$SG4294947279 DB 'sad_8x8_dual', 00H
	ORG $+3
$SG4294947278 DB 'sad_16x16_dual', 00H
	ORG $+1
$SG4294947277 DB 'sad_32x32_dual', 00H
	ORG $+1
$SG4294947276 DB 'sad_64x64_dual', 00H
	ORG $+1
$SG4294947275 DB 'satd_4x4_dual', 00H
	ORG $+2
$SG4294947274 DB 'satd_8x8_dual', 00H
	ORG $+2
$SG4294947273 DB 'satd_16x16_dual', 00H
$SG4294947272 DB 'satd_32x32_dual', 00H
$SG4294947271 DB 'satd_64x64_dual', 00H
$SG4294947270 DB 'satd_any_size_quad', 00H
	ORG $+5
$SG4294947269 DB 'pixels_calc_ssd', 00H
$SG4294947268 DB 'bipred_average', 00H
	ORG $+1
$SG4294947267 DB 'get_optimized_sad', 00H
	ORG $+6
$SG4294947266 DB 'ver_sad', 00H
$SG4294947265 DB 'hor_sad', 00H
$SG4294947264 DB 'pixel_var', 00H
	ORG $+6
$SG4294947263 DB 'fast_forward_dst_4x4', 00H
	ORG $+3
$SG4294947262 DB 'dct_4x4', 00H
$SG4294947261 DB 'dct_8x8', 00H
$SG4294947260 DB 'dct_16x16', 00H
	ORG $+6
$SG4294947259 DB 'dct_32x32', 00H
	ORG $+6
$SG4294947258 DB 'fast_inverse_dst_4x4', 00H
	ORG $+3
$SG4294947257 DB 'idct_4x4', 00H
	ORG $+7
$SG4294947256 DB 'idct_8x8', 00H
	ORG $+7
$SG4294947255 DB 'idct_16x16', 00H
	ORG $+5
$SG4294947254 DB 'idct_32x32', 00H
	ORG $+5
$SG4294947253 DB 'filter_hpel_blocks_hor_ver_luma', 00H
$SG4294947252 DB 'filter_hpel_blocks_diag_luma', 00H
	ORG $+3
$SG4294947251 DB 'filter_qpel_blocks_hor_ver_luma', 00H
$SG4294947250 DB 'filter_qpel_blocks_diag_luma', 00H
	ORG $+3
$SG4294947249 DB 'sample_quarterpel_luma', 00H
	ORG $+1
$SG4294947248 DB 'sample_octpel_chroma', 00H
	ORG $+3
$SG4294947247 DB 'sample_quarterpel_luma_hi', 00H
	ORG $+6
$SG4294947246 DB 'sample_octpel_chroma_hi', 00H
$SG4294947245 DB 'get_extended_block', 00H
	ORG $+5
$SG4294947243 DB 'quantize_residual', 00H
	ORG $+6
$SG4294947242 DB 'dequant', 00H
$SG4294947241 DB 'coeff_abs_sum', 00H
	ORG $+2
$SG4294947240 DB 'fast_coeff_cost', 00H
$SG4294947239 DB 'angular_pred', 00H
	ORG $+3
$SG4294947238 DB 'intra_pred_planar', 00H
	ORG $+6
$SG4294947237 DB 'intra_pred_filtered_dc', 00H
	ORG $+1
$SG4294947236 DB 'sao_edge_ddistortion', 00H
	ORG $+3
$SG4294947235 DB 'calc_sao_edge_dir', 00H
	ORG $+6
$SG4294947234 DB 'sao_reconstruct_color', 00H
	ORG $+2
$SG4294947233 DB 'sao_band_ddistortion', 00H
	ORG $+3
$SG4294947232 DB 'encode_coeff_nxn', 00H
PUBLIC	kvz_strategy_register_intra_avx2
pdata	SEGMENT
$pdata$kvz_intra_pred_filtered_dc_avx2 DD imagerel kvz_intra_pred_filtered_dc_avx2
	DD	imagerel kvz_intra_pred_filtered_dc_avx2+79
	DD	imagerel $unwind$kvz_intra_pred_filtered_dc_avx2
$pdata$3$kvz_intra_pred_filtered_dc_avx2 DD imagerel kvz_intra_pred_filtered_dc_avx2+79
	DD	imagerel kvz_intra_pred_filtered_dc_avx2+265
	DD	imagerel $chain$3$kvz_intra_pred_filtered_dc_avx2
$pdata$4$kvz_intra_pred_filtered_dc_avx2 DD imagerel kvz_intra_pred_filtered_dc_avx2+265
	DD	imagerel kvz_intra_pred_filtered_dc_avx2+350
	DD	imagerel $chain$4$kvz_intra_pred_filtered_dc_avx2
$pdata$pred_filtered_dc_32x32 DD imagerel pred_filtered_dc_32x32
	DD	imagerel pred_filtered_dc_32x32+323
	DD	imagerel $unwind$pred_filtered_dc_32x32
$pdata$pred_filtered_dc_16x16 DD imagerel pred_filtered_dc_16x16
	DD	imagerel pred_filtered_dc_16x16+301
	DD	imagerel $unwind$pred_filtered_dc_16x16
$pdata$pred_filtered_dc_8x8 DD imagerel pred_filtered_dc_8x8
	DD	imagerel pred_filtered_dc_8x8+326
	DD	imagerel $unwind$pred_filtered_dc_8x8
$pdata$pred_filtered_dc_4x4 DD imagerel pred_filtered_dc_4x4
	DD	imagerel pred_filtered_dc_4x4+197
	DD	imagerel $unwind$pred_filtered_dc_4x4
$pdata$kvz_intra_pred_planar_avx2 DD imagerel kvz_intra_pred_planar_avx2
	DD	imagerel kvz_intra_pred_planar_avx2+25
	DD	imagerel $unwind$kvz_intra_pred_planar_avx2
$pdata$7$kvz_intra_pred_planar_avx2 DD imagerel kvz_intra_pred_planar_avx2+25
	DD	imagerel kvz_intra_pred_planar_avx2+176
	DD	imagerel $chain$7$kvz_intra_pred_planar_avx2
$pdata$8$kvz_intra_pred_planar_avx2 DD imagerel kvz_intra_pred_planar_avx2+176
	DD	imagerel kvz_intra_pred_planar_avx2+230
	DD	imagerel $chain$8$kvz_intra_pred_planar_avx2
$pdata$9$kvz_intra_pred_planar_avx2 DD imagerel kvz_intra_pred_planar_avx2+230
	DD	imagerel kvz_intra_pred_planar_avx2+431
	DD	imagerel $chain$9$kvz_intra_pred_planar_avx2
$pdata$10$kvz_intra_pred_planar_avx2 DD imagerel kvz_intra_pred_planar_avx2+431
	DD	imagerel kvz_intra_pred_planar_avx2+723
	DD	imagerel $chain$10$kvz_intra_pred_planar_avx2
$pdata$kvz_angular_pred_avx2 DD imagerel kvz_angular_pred_avx2
	DD	imagerel kvz_angular_pred_avx2+485
	DD	imagerel $unwind$kvz_angular_pred_avx2
$pdata$filter_NxN_avx2 DD imagerel filter_NxN_avx2
	DD	imagerel filter_NxN_avx2+2081
	DD	imagerel $unwind$filter_NxN_avx2
$pdata$filter_16x16_avx2 DD imagerel filter_16x16_avx2
	DD	imagerel filter_16x16_avx2+1348
	DD	imagerel $unwind$filter_16x16_avx2
$pdata$filter_8x8_avx2 DD imagerel filter_8x8_avx2
	DD	imagerel filter_8x8_avx2+955
	DD	imagerel $unwind$filter_8x8_avx2
$pdata$filter_4x4_avx2 DD imagerel filter_4x4_avx2
	DD	imagerel filter_4x4_avx2+459
	DD	imagerel $unwind$filter_4x4_avx2
$pdata$kvz_strategy_register_intra_avx2 DD imagerel $LN5
	DD	imagerel $LN5+162
	DD	imagerel $unwind$kvz_strategy_register_intra_avx2
xdata	SEGMENT
$unwind$kvz_intra_pred_filtered_dc_avx2 DD 081401H
	DD	0106414H
	DD	0f5414H
	DD	0e3414H
	DD	07010b214H
$chain$3$kvz_intra_pred_filtered_dc_avx2 DD 082221H
	DD	029822H
	DD	03881cH
	DD	047816H
	DD	056806H
	DD	imagerel kvz_intra_pred_filtered_dc_avx2
	DD	imagerel kvz_intra_pred_filtered_dc_avx2+79
	DD	imagerel $unwind$kvz_intra_pred_filtered_dc_avx2
$chain$4$kvz_intra_pred_filtered_dc_avx2 DD 021H
	DD	imagerel kvz_intra_pred_filtered_dc_avx2
	DD	imagerel kvz_intra_pred_filtered_dc_avx2+79
	DD	imagerel $unwind$kvz_intra_pred_filtered_dc_avx2
$unwind$pred_filtered_dc_32x32 DD 0d9601H
	DD	0b896H
	DD	01a872H
	DD	029839H
	DD	03882eH
	DD	04781aH
	DD	05680aH
	DD	0c204H
$unwind$pred_filtered_dc_16x16 DD 098601H
	DD	09886H
	DD	01885cH
	DD	02784dH
	DD	036825H
	DD	08204H
$unwind$pred_filtered_dc_8x8 DD 0f8f01H
	DD	0c88fH
	DD	01b88aH
	DD	02a849H
	DD	03982dH
	DD	048827H
	DD	057821H
	DD	066816H
	DD	0e207H
$unwind$pred_filtered_dc_4x4 DD 092a01H
	DD	0982aH
	DD	018825H
	DD	02781fH
	DD	03680fH
	DD	08204H
$unwind$kvz_intra_pred_planar_avx2 DD 0b1901H
	DD	01d5419H
	DD	01c3419H
	DD	0140119H
	DD	0d010f012H
	DD	0700cc00eH
	DD	0600bH
$chain$7$kvz_intra_pred_planar_avx2 DD 0107d21H
	DD	03c87dH
	DD	04b877H
	DD	05a866H
	DD	069860H
	DD	078857H
	DD	08784cH
	DD	096840H
	DD	01be408H
	DD	imagerel kvz_intra_pred_planar_avx2
	DD	imagerel kvz_intra_pred_planar_avx2+25
	DD	imagerel $unwind$kvz_intra_pred_planar_avx2
$chain$8$kvz_intra_pred_planar_avx2 DD 0e0021H
	DD	03c800H
	DD	04b800H
	DD	05a800H
	DD	069800H
	DD	078800H
	DD	087800H
	DD	096800H
	DD	imagerel kvz_intra_pred_planar_avx2
	DD	imagerel kvz_intra_pred_planar_avx2+25
	DD	imagerel $unwind$kvz_intra_pred_planar_avx2
$chain$9$kvz_intra_pred_planar_avx2 DD 020621H
	DD	02d806H
	DD	imagerel kvz_intra_pred_planar_avx2+176
	DD	imagerel kvz_intra_pred_planar_avx2+230
	DD	imagerel $chain$8$kvz_intra_pred_planar_avx2
$chain$10$kvz_intra_pred_planar_avx2 DD 021H
	DD	imagerel kvz_intra_pred_planar_avx2+176
	DD	imagerel kvz_intra_pred_planar_avx2+230
	DD	imagerel $chain$8$kvz_intra_pred_planar_avx2
$unwind$kvz_angular_pred_avx2 DD 0c1c01H
	DD	016641cH
	DD	015541cH
	DD	014341cH
	DD	0f018d21cH
	DD	0d014e016H
	DD	07010c012H
$unwind$filter_NxN_avx2 DD 01f6e01H
	DD	012f865H
	DD	013e85dH
	DD	014d855H
	DD	015c84dH
	DD	016b845H
	DD	017a83dH
	DD	0189835H
	DD	0198830H
	DD	01a782bH
	DD	01b6826H
	DD	0403421H
	DD	0380121H
	DD	0e018f01aH
	DD	0c014d016H
	DD	060117012H
	DD	05010H
$unwind$filter_16x16_avx2 DD 0123c01H
	DD	0c83cH
	DD	01b833H
	DD	02a82eH
	DD	039829H
	DD	048821H
	DD	057819H
	DD	066811H
	DD	010340cH
	DD	07008d20cH
$unwind$filter_8x8_avx2 DD 0105a01H
	DD	0b85aH
	DD	01a83fH
	DD	029837H
	DD	03882dH
	DD	047824H
	DD	05681bH
	DD	0e340aH
	DD	07006b20aH
$unwind$filter_4x4_avx2 DD 061a01H
	DD	0681aH
	DD	04340aH
	DD	07006120aH
$unwind$kvz_strategy_register_intra_avx2 DD 060f01H
	DD	09640fH
	DD	08340fH
	DD	0700b520fH
$SG4294947216 DB 'intra_pred_filtered_dc', 00H
	ORG $+2
$SG4294947217 DB 'avx2', 00H
	ORG $+2
$SG4294947218 DB 'intra_pred_planar', 00H
$SG4294947219 DB 'avx2', 00H
	ORG $+2
$SG4294947221 DB 'avx2', 00H
	ORG $+2
$SG4294947220 DB 'angular_pred', 00H
	ORG $+3
$SG4294947222 DB 'l', 00H, 'o', 00H, 'g', 00H, '2', 00H, '_', 00H, 'w', 00H
	DB	'i', 00H, 'd', 00H, 't', 00H, 'h', 00H, ' ', 00H, '>', 00H, '='
	DB	00H, ' ', 00H, '2', 00H, ' ', 00H, '&', 00H, '&', 00H, ' ', 00H
	DB	'l', 00H, 'o', 00H, 'g', 00H, '2', 00H, '_', 00H, 'w', 00H, 'i'
	DB	00H, 'd', 00H, 't', 00H, 'h', 00H, ' ', 00H, '<', 00H, '=', 00H
	DB	' ', 00H, '5', 00H, 00H, 00H
	ORG $+2
$SG4294947224 DB 'w', 00H, 'i', 00H, 'd', 00H, 't', 00H, 'h', 00H, ' ', 00H
	DB	'=', 00H, '=', 00H, ' ', 00H, '4', 00H, 00H, 00H
	ORG $+2
$SG4294947223 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'i', 00H, 'n', 00H, 't', 00H, 'r', 00H, 'a'
	DB	00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H, '2', 00H, '.', 00H
	DB	'c', 00H, 00H, 00H
	ORG $+7
$SG4294947225 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'i', 00H, 'n', 00H, 't', 00H, 'r', 00H, 'a'
	DB	00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H, '2', 00H, '.', 00H
	DB	'c', 00H, 00H, 00H
$SG4294947226 DB 'l', 00H, 'o', 00H, 'g', 00H, '2', 00H, '_', 00H, 'w', 00H
	DB	'i', 00H, 'd', 00H, 't', 00H, 'h', 00H, ' ', 00H, '>', 00H, '='
	DB	00H, ' ', 00H, '2', 00H, ' ', 00H, '&', 00H, '&', 00H, ' ', 00H
	DB	'l', 00H, 'o', 00H, 'g', 00H, '2', 00H, '_', 00H, 'w', 00H, 'i'
	DB	00H, 'd', 00H, 't', 00H, 'h', 00H, ' ', 00H, '<', 00H, '=', 00H
	DB	' ', 00H, '5', 00H, 00H, 00H
	ORG $+7
$SG4294947227 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'i', 00H, 'n', 00H, 't', 00H, 'r', 00H, 'a'
	DB	00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H, '2', 00H, '.', 00H
	DB	'c', 00H, 00H, 00H
	ORG $+3
$SG4294947228 DB 'i', 00H, 'n', 00H, 't', 00H, 'r', 00H, 'a', 00H, '_', 00H
	DB	'm', 00H, 'o', 00H, 'd', 00H, 'e', 00H, ' ', 00H, '>', 00H, '='
	DB	00H, ' ', 00H, '2', 00H, ' ', 00H, '&', 00H, '&', 00H, ' ', 00H
	DB	'i', 00H, 'n', 00H, 't', 00H, 'r', 00H, 'a', 00H, '_', 00H, 'm'
	DB	00H, 'o', 00H, 'd', 00H, 'e', 00H, ' ', 00H, '<', 00H, '=', 00H
	DB	' ', 00H, '3', 00H, '4', 00H, 00H, 00H
	ORG $+4
$SG4294947229 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'i', 00H, 'n', 00H, 't', 00H, 'r', 00H, 'a'
	DB	00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H, '2', 00H, '.', 00H
	DB	'c', 00H, 00H, 00H
	ORG $+1
$SG4294947230 DB 'l', 00H, 'o', 00H, 'g', 00H, '2', 00H, '_', 00H, 'w', 00H
	DB	'i', 00H, 'd', 00H, 't', 00H, 'h', 00H, ' ', 00H, '>', 00H, '='
	DB	00H, ' ', 00H, '2', 00H, ' ', 00H, '&', 00H, '&', 00H, ' ', 00H
	DB	'l', 00H, 'o', 00H, 'g', 00H, '2', 00H, '_', 00H, 'w', 00H, 'i'
	DB	00H, 'd', 00H, 't', 00H, 'h', 00H, ' ', 00H, '<', 00H, '=', 00H
	DB	' ', 00H, '5', 00H, 00H, 00H
	ORG $+2
$SG4294947231 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'i', 00H, 'n', 00H, 't', 00H, 'r', 00H, 'a'
	DB	00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H, '2', 00H, '.', 00H
	DB	'c', 00H, 00H, 00H
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\intra-avx2.c
_TEXT	SEGMENT
opaque$ = 64
bitdepth$dead$ = 72
kvz_strategy_register_intra_avx2 PROC

; 926  : {

$LN5:
	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rsi
	push	rdi
	sub	rsp, 48					; 00000030H

; 927  :   bool success = true;
; 928  : #if COMPILE_INTEL_AVX2 && defined X86_64
; 929  : #if KVZ_BIT_DEPTH == 8
; 930  :   if (bitdepth == 8) {
; 931  :     success &= kvz_strategyselector_register(opaque, "angular_pred", "avx2", 40, &kvz_angular_pred_avx2);

	lea	rax, OFFSET FLAT:kvz_angular_pred_avx2
	mov	r9d, 40					; 00000028H
	lea	r8, OFFSET FLAT:$SG4294947221
	mov	QWORD PTR [rsp+32], rax
	lea	rdx, OFFSET FLAT:$SG4294947220
	mov	rsi, rcx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 932  :     success &= kvz_strategyselector_register(opaque, "intra_pred_planar", "avx2", 40, &kvz_intra_pred_planar_avx2);

	lea	r8, OFFSET FLAT:$SG4294947219
	lea	rax, OFFSET FLAT:kvz_intra_pred_planar_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294947218
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	ebx, 1
	call	kvz_strategyselector_register
	mov	edi, eax

; 933  :     success &= kvz_strategyselector_register(opaque, "intra_pred_filtered_dc", "avx2", 40, &kvz_intra_pred_filtered_dc_avx2);

	lea	r8, OFFSET FLAT:$SG4294947217
	lea	rax, OFFSET FLAT:kvz_intra_pred_filtered_dc_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294947216
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	edi, ebx
	call	kvz_strategyselector_register

; 934  :   }
; 935  : #endif //KVZ_BIT_DEPTH == 8
; 936  : #endif //COMPILE_INTEL_AVX2 && defined X86_64
; 937  :   return success;
; 938  : }

	mov	rbx, QWORD PTR [rsp+64]
	and	eax, edi
	mov	rsi, QWORD PTR [rsp+72]
	add	rsp, 48					; 00000030H
	pop	rdi
	ret	0
kvz_strategy_register_intra_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\intra-avx2.c
_TEXT	SEGMENT
ref_main$ = 8
delta_pos$ = 16
x$dead$ = 24
filter_4x1_avx2 PROC

; 53   : 
; 54   :   int8_t delta_int = delta_pos >> 5;

	movzx	eax, dx

; 55   :   int8_t delta_fract = delta_pos & (32-1);

	and	dl, 31
	sar	ax, 5

; 56   :   __m128i sample0 = _mm_cvtsi32_si128(*(uint32_t*)&(ref_main[x + delta_int]));

	movsx	rax, al

; 57   :   __m128i sample1 = _mm_cvtsi32_si128(*(uint32_t*)&(ref_main[x + delta_int + 1]));
; 58   :   __m128i pairs = _mm_unpacklo_epi8(sample0, sample1);
; 59   :   __m128i weight = _mm_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	movzx	r8d, dl
	vmovd	xmm1, DWORD PTR [rax+rcx]
	vmovd	xmm0, DWORD PTR [rax+rcx+1]
	vpunpcklbw xmm2, xmm1, xmm0
	mov	eax, 32					; 00000020H
	sub	ax, r8w
	shl	r8w, 8
	or	ax, r8w
	cwde
	vmovd	xmm0, eax
	vpbroadcastw xmm0, xmm0

; 60   :   sample0 = _mm_maddubs_epi16(pairs, weight);

	vpmaddubsw xmm1, xmm2, xmm0

; 61   :   sample0 = _mm_add_epi16(sample0, _mm_set1_epi16(16));

	vpaddw	xmm1, xmm1, XMMWORD PTR __xmm@00100010001000100010001000100010

; 62   :   sample0 = _mm_srli_epi16(sample0, 5);

	vpsrlw	xmm2, xmm1, 5

; 63   :   sample0 = _mm_packus_epi16(sample0, sample0);

	vpackuswb xmm0, xmm2, xmm2

; 64   : 
; 65   :   return sample0;
; 66   : }

	ret	0
filter_4x1_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\intra-avx2.c
_TEXT	SEGMENT
dst$ = 32
ref_main$ = 40
sample_disp$ = 48
vertical_mode$ = 56
filter_4x4_avx2 PROC

; 75   : static void filter_4x4_avx2(uint8_t *dst, const uint8_t *ref_main, int sample_disp, bool vertical_mode){

	mov	QWORD PTR [rsp+8], rbx
	push	rdi
	sub	rsp, 16

; 61   :   sample0 = _mm_add_epi16(sample0, _mm_set1_epi16(16));

	vmovdqu	xmm3, XMMWORD PTR __xmm@00100010001000100010001000100010

; 75   : static void filter_4x4_avx2(uint8_t *dst, const uint8_t *ref_main, int sample_disp, bool vertical_mode){

	mov	rdi, rcx
	vmovaps	XMMWORD PTR [rsp], xmm6
	mov	rbx, rdx
	mov	r11d, r8d

; 55   :   int8_t delta_fract = delta_pos & (32-1);

	movzx	eax, r8b
	and	al, 31

; 58   :   __m128i pairs = _mm_unpacklo_epi8(sample0, sample1);
; 59   :   __m128i weight = _mm_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	movzx	r10d, al
	movzx	eax, r8w
	sar	ax, 5
	mov	r8d, 32					; 00000020H
	movsx	rax, al
	movzx	ecx, r10w
	shl	cx, 8
	vmovd	xmm1, DWORD PTR [rax+rdx]
	vmovd	xmm0, DWORD PTR [rax+rdx+1]
	vpunpcklbw xmm2, xmm1, xmm0
	mov	eax, r8d
	sub	ax, r10w
	or	ax, cx
	cwde
	vmovd	xmm0, eax
	vpbroadcastw xmm0, xmm0

; 60   :   sample0 = _mm_maddubs_epi16(pairs, weight);

	vpmaddubsw xmm0, xmm2, xmm0

; 61   :   sample0 = _mm_add_epi16(sample0, _mm_set1_epi16(16));

	vpaddw	xmm1, xmm0, xmm3

; 62   :   sample0 = _mm_srli_epi16(sample0, 5);

	vpsrlw	xmm2, xmm1, 5

; 63   :   sample0 = _mm_packus_epi16(sample0, sample0);

	vpackuswb xmm4, xmm2, xmm2

; 76   : 
; 77   :   __m128i row0 = filter_4x1_avx2(ref_main, 1 * sample_disp, 0);
; 78   :   __m128i row1 = filter_4x1_avx2(ref_main, 2 * sample_disp, 0);

	movzx	eax, r11w
	add	ax, ax

; 59   :   __m128i weight = _mm_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	movzx	edx, ax
	sar	ax, 5
	movsx	rax, al
	and	dx, 31
	movzx	ecx, dx
	shl	cx, 8
	vmovd	xmm1, DWORD PTR [rax+rbx]
	vmovd	xmm0, DWORD PTR [rax+rbx+1]
	vpunpcklbw xmm2, xmm1, xmm0
	mov	eax, r8d
	sub	ax, dx
	or	ax, cx
	cwde
	vmovd	xmm0, eax
	vpbroadcastw xmm0, xmm0

; 60   :   sample0 = _mm_maddubs_epi16(pairs, weight);

	vpmaddubsw xmm0, xmm2, xmm0

; 61   :   sample0 = _mm_add_epi16(sample0, _mm_set1_epi16(16));

	vpaddw	xmm1, xmm0, xmm3

; 79   :   __m128i row2 = filter_4x1_avx2(ref_main, 3 * sample_disp, 0);

	movzx	eax, r11w
	add	ax, ax

; 62   :   sample0 = _mm_srli_epi16(sample0, 5);

	vpsrlw	xmm2, xmm1, 5

; 63   :   sample0 = _mm_packus_epi16(sample0, sample0);

	vpackuswb xmm5, xmm2, xmm2

; 79   :   __m128i row2 = filter_4x1_avx2(ref_main, 3 * sample_disp, 0);

	lea	ecx, DWORD PTR [rax+r11]

; 80   :   __m128i row3 = filter_4x1_avx2(ref_main, 4 * sample_disp, 0);

	shl	r11w, 2

; 59   :   __m128i weight = _mm_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	movzx	edx, cx
	sar	cx, 5
	movsx	rax, cl
	and	dx, 31
	movzx	ecx, dx
	shl	cx, 8
	vmovd	xmm1, DWORD PTR [rax+rbx]
	vmovd	xmm0, DWORD PTR [rax+rbx+1]
	vpunpcklbw xmm2, xmm1, xmm0
	mov	eax, r8d
	sub	ax, dx
	or	ax, cx
	movzx	ecx, r11w
	cwde
	and	cx, 31
	vmovd	xmm0, eax
	vpbroadcastw xmm0, xmm0

; 60   :   sample0 = _mm_maddubs_epi16(pairs, weight);

	vpmaddubsw xmm0, xmm2, xmm0

; 61   :   sample0 = _mm_add_epi16(sample0, _mm_set1_epi16(16));

	vpaddw	xmm1, xmm0, xmm3

; 62   :   sample0 = _mm_srli_epi16(sample0, 5);

	vpsrlw	xmm2, xmm1, 5

; 54   :   int8_t delta_int = delta_pos >> 5;

	sar	r11w, 5

; 56   :   __m128i sample0 = _mm_cvtsi32_si128(*(uint32_t*)&(ref_main[x + delta_int]));

	movsx	rax, r11b

; 63   :   sample0 = _mm_packus_epi16(sample0, sample0);

	vpackuswb xmm6, xmm2, xmm2

; 56   :   __m128i sample0 = _mm_cvtsi32_si128(*(uint32_t*)&(ref_main[x + delta_int]));

	vmovd	xmm1, DWORD PTR [rax+rbx]

; 57   :   __m128i sample1 = _mm_cvtsi32_si128(*(uint32_t*)&(ref_main[x + delta_int + 1]));

	vmovd	xmm0, DWORD PTR [rax+rbx+1]

; 58   :   __m128i pairs = _mm_unpacklo_epi8(sample0, sample1);
; 59   :   __m128i weight = _mm_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	movzx	eax, cx
	shl	ax, 8
	sub	r8w, cx
	vpunpcklbw xmm2, xmm1, xmm0
	or	ax, r8w
	cwde
	vmovd	xmm0, eax
	vpbroadcastw xmm0, xmm0

; 60   :   sample0 = _mm_maddubs_epi16(pairs, weight);

	vpmaddubsw xmm0, xmm2, xmm0

; 61   :   sample0 = _mm_add_epi16(sample0, _mm_set1_epi16(16));

	vpaddw	xmm1, xmm0, xmm3

; 62   :   sample0 = _mm_srli_epi16(sample0, 5);

	vpsrlw	xmm2, xmm1, 5

; 63   :   sample0 = _mm_packus_epi16(sample0, sample0);

	vpackuswb xmm0, xmm2, xmm2

; 81   : 
; 82   :   //Transpose if horizontal mode
; 83   :   if (!vertical_mode) {

	test	r9b, r9b
	jne	SHORT $LN2@filter_4x4

; 84   :     __m128i temp = _mm_unpacklo_epi16(_mm_unpacklo_epi8(row0, row1), _mm_unpacklo_epi8(row2, row3));

	vpunpcklbw xmm1, xmm4, xmm5
	vpunpcklbw xmm0, xmm6, xmm0
	vpunpcklwd xmm2, xmm1, xmm0

; 85   :     row0 = _mm_cvtsi32_si128(_mm_extract_epi32(temp, 0));

	vpextrd	eax, xmm2, 0
	vmovd	xmm4, eax

; 86   :     row1 = _mm_cvtsi32_si128(_mm_extract_epi32(temp, 1));

	vpextrd	eax, xmm2, 1
	vmovd	xmm5, eax

; 87   :     row2 = _mm_cvtsi32_si128(_mm_extract_epi32(temp, 2));

	vpextrd	eax, xmm2, 2
	vmovd	xmm6, eax

; 88   :     row3 = _mm_cvtsi32_si128(_mm_extract_epi32(temp, 3));

	vpextrd	eax, xmm2, 3
	vmovd	xmm0, eax
$LN2@filter_4x4:

; 89   :   }
; 90   : 
; 91   :   *(int32_t*)(dst + 0 * 4) = _mm_cvtsi128_si32(row0);
; 92   :   *(int32_t*)(dst + 1 * 4) = _mm_cvtsi128_si32(row1);
; 93   :   *(int32_t*)(dst + 2 * 4) = _mm_cvtsi128_si32(row2);
; 94   :   *(int32_t*)(dst + 3 * 4) = _mm_cvtsi128_si32(row3);
; 95   : }

	mov	rbx, QWORD PTR [rsp+32]
	vmovd	DWORD PTR [rdi+8], xmm6
	vmovaps	xmm6, XMMWORD PTR [rsp]
	vmovd	DWORD PTR [rdi], xmm4
	vmovd	DWORD PTR [rdi+4], xmm5
	vmovd	DWORD PTR [rdi+12], xmm0
	add	rsp, 16
	pop	rdi
	ret	0
filter_4x4_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\intra-avx2.c
_TEXT	SEGMENT
ref_main$ = 8
delta_pos$ = 16
x$dead$ = 24
filter_8x1_avx2 PROC

; 104  : 
; 105  :   int8_t delta_int = delta_pos >> 5;

	movzx	eax, dx

; 106  :   int8_t delta_fract = delta_pos & (32-1);

	and	dl, 31
	sar	ax, 5

; 107  :   __m128i sample0 = _mm_cvtsi64_si128(*(uint64_t*)&(ref_main[x + delta_int]));

	movsx	rax, al

; 108  :   __m128i sample1 = _mm_cvtsi64_si128(*(uint64_t*)&(ref_main[x + delta_int + 1]));
; 109  :   __m128i pairs_lo = _mm_unpacklo_epi8(sample0, sample1);
; 110  : 
; 111  :   __m128i weight = _mm_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	movzx	r8d, dl
	vmovq	xmm1, QWORD PTR [rax+rcx]
	vmovq	xmm0, QWORD PTR [rax+rcx+1]
	vpunpcklbw xmm2, xmm1, xmm0
	mov	eax, 32					; 00000020H
	sub	ax, r8w
	shl	r8w, 8
	or	ax, r8w
	cwde
	vmovd	xmm0, eax
	vpbroadcastw xmm0, xmm0

; 112  :   __m128i v_temp_lo = _mm_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw xmm1, xmm2, xmm0

; 113  :   v_temp_lo = _mm_add_epi16(v_temp_lo, _mm_set1_epi16(16));

	vpaddw	xmm1, xmm1, XMMWORD PTR __xmm@00100010001000100010001000100010

; 114  :   v_temp_lo = _mm_srli_epi16(v_temp_lo, 5);

	vpsrlw	xmm2, xmm1, 5

; 115  :   sample0 = _mm_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb xmm0, xmm2, xmm2

; 116  : 
; 117  :   return sample0;
; 118  : }

	ret	0
filter_8x1_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\intra-avx2.c
_TEXT	SEGMENT
dst$ = 112
ref_main$ = 120
sample_disp$ = 128
vertical_mode$ = 136
filter_8x8_avx2 PROC

; 127  : static void filter_8x8_avx2(uint8_t *dst, const uint8_t *ref_main, int sample_disp, bool vertical_mode){

	mov	QWORD PTR [rsp+8], rbx
	push	rdi
	sub	rsp, 96					; 00000060H

; 113  :   v_temp_lo = _mm_add_epi16(v_temp_lo, _mm_set1_epi16(16));

	vmovdqu	xmm3, XMMWORD PTR __xmm@00100010001000100010001000100010

; 127  : static void filter_8x8_avx2(uint8_t *dst, const uint8_t *ref_main, int sample_disp, bool vertical_mode){

	mov	rdi, rcx
	vmovaps	XMMWORD PTR [rsp+80], xmm6
	mov	rbx, rdx
	vmovaps	XMMWORD PTR [rsp+64], xmm7
	mov	r11d, r8d
	vmovaps	XMMWORD PTR [rsp+48], xmm8

; 106  :   int8_t delta_fract = delta_pos & (32-1);

	movzx	eax, r8b
	vmovaps	XMMWORD PTR [rsp+32], xmm9
	and	al, 31
	vmovaps	XMMWORD PTR [rsp+16], xmm10

; 110  : 
; 111  :   __m128i weight = _mm_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	movzx	r10d, al
	movzx	eax, r8w
	sar	ax, 5
	mov	r8d, 32					; 00000020H
	movsx	rax, al
	vmovaps	XMMWORD PTR [rsp], xmm11

; 107  :   __m128i sample0 = _mm_cvtsi64_si128(*(uint64_t*)&(ref_main[x + delta_int]));

	vmovq	xmm1, QWORD PTR [rax+rdx]

; 108  :   __m128i sample1 = _mm_cvtsi64_si128(*(uint64_t*)&(ref_main[x + delta_int + 1]));

	vmovq	xmm0, QWORD PTR [rax+rdx+1]

; 109  :   __m128i pairs_lo = _mm_unpacklo_epi8(sample0, sample1);

	vpunpcklbw xmm2, xmm1, xmm0

; 110  : 
; 111  :   __m128i weight = _mm_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	mov	eax, r8d
	sub	ax, r10w
	shl	r10w, 8
	or	ax, r10w
	cwde
	vmovd	xmm0, eax
	vpbroadcastw xmm0, xmm0

; 112  :   __m128i v_temp_lo = _mm_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw xmm0, xmm2, xmm0

; 113  :   v_temp_lo = _mm_add_epi16(v_temp_lo, _mm_set1_epi16(16));

	vpaddw	xmm1, xmm0, xmm3

; 114  :   v_temp_lo = _mm_srli_epi16(v_temp_lo, 5);

	vpsrlw	xmm2, xmm1, 5

; 115  :   sample0 = _mm_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb xmm5, xmm2, xmm2

; 128  :   __m128i row0 = filter_8x1_avx2(ref_main, 1 * sample_disp, 0);
; 129  :   __m128i row1 = filter_8x1_avx2(ref_main, 2 * sample_disp, 0);

	movzx	eax, r11w
	add	ax, ax

; 111  :   __m128i weight = _mm_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	movzx	ecx, ax
	sar	ax, 5
	movsx	rax, al
	and	cx, 31
	vmovq	xmm1, QWORD PTR [rax+rdx]
	vmovq	xmm0, QWORD PTR [rax+rdx+1]
	vpunpcklbw xmm2, xmm1, xmm0
	mov	eax, r8d
	sub	ax, cx
	shl	cx, 8
	or	ax, cx
	cwde
	vmovd	xmm0, eax
	vpbroadcastw xmm0, xmm0

; 112  :   __m128i v_temp_lo = _mm_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw xmm0, xmm2, xmm0

; 113  :   v_temp_lo = _mm_add_epi16(v_temp_lo, _mm_set1_epi16(16));

	vpaddw	xmm1, xmm0, xmm3

; 114  :   v_temp_lo = _mm_srli_epi16(v_temp_lo, 5);

	vpsrlw	xmm2, xmm1, 5

; 130  :   __m128i row2 = filter_8x1_avx2(ref_main, 3 * sample_disp, 0);

	movzx	eax, r11w
	add	ax, ax

; 115  :   sample0 = _mm_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb xmm6, xmm2, xmm2

; 130  :   __m128i row2 = filter_8x1_avx2(ref_main, 3 * sample_disp, 0);

	lea	ecx, DWORD PTR [rax+r11]

; 111  :   __m128i weight = _mm_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	movzx	edx, cx
	sar	cx, 5
	movsx	rax, cl
	and	dx, 31
	vmovq	xmm1, QWORD PTR [rax+rbx]
	vmovq	xmm0, QWORD PTR [rax+rbx+1]
	vpunpcklbw xmm2, xmm1, xmm0
	mov	eax, r8d
	sub	ax, dx
	shl	dx, 8
	or	ax, dx
	cwde
	vmovd	xmm0, eax

; 131  :   __m128i row3 = filter_8x1_avx2(ref_main, 4 * sample_disp, 0);

	movzx	eax, r11w
	shl	ax, 2

; 111  :   __m128i weight = _mm_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	movzx	ecx, ax
	vpbroadcastw xmm0, xmm0

; 112  :   __m128i v_temp_lo = _mm_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw xmm0, xmm2, xmm0

; 113  :   v_temp_lo = _mm_add_epi16(v_temp_lo, _mm_set1_epi16(16));

	vpaddw	xmm1, xmm0, xmm3

; 114  :   v_temp_lo = _mm_srli_epi16(v_temp_lo, 5);

	vpsrlw	xmm2, xmm1, 5

; 111  :   __m128i weight = _mm_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	and	cx, 31
	sar	ax, 5
	movsx	rax, al

; 115  :   sample0 = _mm_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb xmm7, xmm2, xmm2

; 107  :   __m128i sample0 = _mm_cvtsi64_si128(*(uint64_t*)&(ref_main[x + delta_int]));

	vmovq	xmm1, QWORD PTR [rax+rbx]

; 108  :   __m128i sample1 = _mm_cvtsi64_si128(*(uint64_t*)&(ref_main[x + delta_int + 1]));

	vmovq	xmm0, QWORD PTR [rax+rbx+1]

; 109  :   __m128i pairs_lo = _mm_unpacklo_epi8(sample0, sample1);

	vpunpcklbw xmm2, xmm1, xmm0

; 110  : 
; 111  :   __m128i weight = _mm_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	mov	eax, r8d
	sub	ax, cx
	shl	cx, 8
	or	ax, cx
	cwde
	vmovd	xmm0, eax
	vpbroadcastw xmm0, xmm0

; 112  :   __m128i v_temp_lo = _mm_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw xmm0, xmm2, xmm0

; 113  :   v_temp_lo = _mm_add_epi16(v_temp_lo, _mm_set1_epi16(16));

	vpaddw	xmm1, xmm0, xmm3

; 114  :   v_temp_lo = _mm_srli_epi16(v_temp_lo, 5);

	vpsrlw	xmm2, xmm1, 5

; 115  :   sample0 = _mm_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb xmm8, xmm2, xmm2

; 132  :   __m128i row4 = filter_8x1_avx2(ref_main, 5 * sample_disp, 0);

	movzx	eax, r11w
	shl	ax, 2
	lea	ecx, DWORD PTR [rax+r11]

; 111  :   __m128i weight = _mm_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	movzx	edx, cx
	sar	cx, 5
	movsx	rax, cl
	and	dx, 31
	movzx	ecx, dx
	shl	cx, 8
	vmovq	xmm1, QWORD PTR [rax+rbx]
	vmovq	xmm0, QWORD PTR [rax+rbx+1]
	vpunpcklbw xmm2, xmm1, xmm0
	mov	eax, r8d
	sub	ax, dx
	or	ax, cx
	cwde
	vmovd	xmm0, eax
	vpbroadcastw xmm0, xmm0

; 112  :   __m128i v_temp_lo = _mm_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw xmm0, xmm2, xmm0

; 113  :   v_temp_lo = _mm_add_epi16(v_temp_lo, _mm_set1_epi16(16));

	vpaddw	xmm1, xmm0, xmm3

; 114  :   v_temp_lo = _mm_srli_epi16(v_temp_lo, 5);

	vpsrlw	xmm2, xmm1, 5

; 133  :   __m128i row5 = filter_8x1_avx2(ref_main, 6 * sample_disp, 0);

	movzx	eax, r11w
	add	ax, ax

; 115  :   sample0 = _mm_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb xmm9, xmm2, xmm2

; 133  :   __m128i row5 = filter_8x1_avx2(ref_main, 6 * sample_disp, 0);

	lea	ecx, DWORD PTR [rax+r11]
	add	cx, cx

; 111  :   __m128i weight = _mm_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	movzx	edx, cx
	sar	cx, 5
	movsx	rax, cl
	and	dx, 31
	movzx	ecx, dx
	shl	cx, 8
	vmovq	xmm1, QWORD PTR [rax+rbx]
	vmovq	xmm0, QWORD PTR [rax+rbx+1]
	vpunpcklbw xmm2, xmm1, xmm0
	mov	eax, r8d
	sub	ax, dx
	or	ax, cx
	cwde
	vmovd	xmm0, eax
	vpbroadcastw xmm0, xmm0

; 112  :   __m128i v_temp_lo = _mm_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw xmm0, xmm2, xmm0

; 113  :   v_temp_lo = _mm_add_epi16(v_temp_lo, _mm_set1_epi16(16));

	vpaddw	xmm1, xmm0, xmm3

; 134  :   __m128i row6 = filter_8x1_avx2(ref_main, 7 * sample_disp, 0);

	movsx	eax, r11w
	imul	ecx, eax, 7

; 114  :   v_temp_lo = _mm_srli_epi16(v_temp_lo, 5);

	vpsrlw	xmm2, xmm1, 5

; 115  :   sample0 = _mm_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb xmm10, xmm2, xmm2

; 135  :   __m128i row7 = filter_8x1_avx2(ref_main, 8 * sample_disp, 0);

	shl	r11w, 3

; 111  :   __m128i weight = _mm_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	movzx	edx, cx
	sar	cx, 5
	movsx	rax, cl
	and	dx, 31
	movzx	ecx, dx
	shl	cx, 8
	vmovq	xmm1, QWORD PTR [rax+rbx]
	vmovq	xmm0, QWORD PTR [rax+rbx+1]
	vpunpcklbw xmm2, xmm1, xmm0
	mov	eax, r8d
	sub	ax, dx
	or	ax, cx
	movzx	ecx, r11w
	cwde
	and	cx, 31
	vmovd	xmm0, eax
	vpbroadcastw xmm0, xmm0

; 112  :   __m128i v_temp_lo = _mm_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw xmm0, xmm2, xmm0

; 113  :   v_temp_lo = _mm_add_epi16(v_temp_lo, _mm_set1_epi16(16));

	vpaddw	xmm1, xmm0, xmm3

; 114  :   v_temp_lo = _mm_srli_epi16(v_temp_lo, 5);

	vpsrlw	xmm2, xmm1, 5

; 115  :   sample0 = _mm_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb xmm11, xmm2, xmm2

; 105  :   int8_t delta_int = delta_pos >> 5;

	sar	r11w, 5

; 107  :   __m128i sample0 = _mm_cvtsi64_si128(*(uint64_t*)&(ref_main[x + delta_int]));

	movsx	rax, r11b

; 110  : 
; 111  :   __m128i weight = _mm_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	sub	r8w, cx
	vmovq	xmm1, QWORD PTR [rax+rbx]
	vmovq	xmm0, QWORD PTR [rax+rbx+1]
	vpunpcklbw xmm2, xmm1, xmm0
	movzx	eax, cx
	shl	ax, 8
	or	ax, r8w
	cwde
	vmovd	xmm0, eax
	vpbroadcastw xmm0, xmm0

; 112  :   __m128i v_temp_lo = _mm_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw xmm0, xmm2, xmm0

; 113  :   v_temp_lo = _mm_add_epi16(v_temp_lo, _mm_set1_epi16(16));

	vpaddw	xmm1, xmm0, xmm3

; 114  :   v_temp_lo = _mm_srli_epi16(v_temp_lo, 5);

	vpsrlw	xmm2, xmm1, 5

; 115  :   sample0 = _mm_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb xmm0, xmm2, xmm2

; 136  : 
; 137  :   //Transpose if horizontal mode
; 138  :   if (!vertical_mode) {

	test	r9b, r9b
	jne	$LN2@filter_8x8

; 139  :     __m128i q0 = _mm_unpacklo_epi8(row0, row1);
; 140  :     __m128i q1 = _mm_unpacklo_epi8(row2, row3);

	vpunpcklbw xmm2, xmm7, xmm8

; 141  :     __m128i q2 = _mm_unpacklo_epi8(row4, row5);

	vpunpcklbw xmm3, xmm9, xmm10

; 142  :     __m128i q3 = _mm_unpacklo_epi8(row6, row7);

	vpunpcklbw xmm0, xmm11, xmm0
	vpunpcklbw xmm5, xmm5, xmm6

; 143  : 
; 144  :     __m128i h0 = _mm_unpacklo_epi16(q0, q1);

	vpunpcklwd xmm4, xmm5, xmm2

; 145  :     __m128i h1 = _mm_unpacklo_epi16(q2, q3);
; 146  :     __m128i h2 = _mm_unpackhi_epi16(q0, q1);

	vpunpckhwd xmm5, xmm5, xmm2

; 147  :     __m128i h3 = _mm_unpackhi_epi16(q2, q3);

	vpunpckhwd xmm2, xmm3, xmm0
	vpunpcklwd xmm1, xmm3, xmm0

; 148  : 
; 149  :     __m128i temp0 = _mm_unpacklo_epi32(h0, h1);

	vpunpckldq xmm0, xmm4, xmm1

; 150  :     __m128i temp1 = _mm_unpackhi_epi32(h0, h1);

	vpunpckhdq xmm3, xmm4, xmm1

; 151  :     __m128i temp2 = _mm_unpacklo_epi32(h2, h3);
; 152  :     __m128i temp3 = _mm_unpackhi_epi32(h2, h3);
; 153  : 
; 154  :     row0 = _mm_cvtsi64_si128(_mm_extract_epi64(temp0, 0));

	vpextrq	rax, xmm0, 0
	vpunpckldq xmm1, xmm5, xmm2
	vpunpckhdq xmm4, xmm5, xmm2
	vmovq	xmm5, rax

; 155  :     row1 = _mm_cvtsi64_si128(_mm_extract_epi64(temp0, 1));

	vpextrq	rax, xmm0, 1
	vmovq	xmm6, rax

; 156  :     row2 = _mm_cvtsi64_si128(_mm_extract_epi64(temp1, 0));

	vpextrq	rax, xmm3, 0
	vmovq	xmm7, rax

; 157  :     row3 = _mm_cvtsi64_si128(_mm_extract_epi64(temp1, 1));

	vpextrq	rax, xmm3, 1
	vmovq	xmm8, rax

; 158  :     row4 = _mm_cvtsi64_si128(_mm_extract_epi64(temp2, 0));

	vpextrq	rax, xmm1, 0
	vmovq	xmm9, rax

; 159  :     row5 = _mm_cvtsi64_si128(_mm_extract_epi64(temp2, 1));

	vpextrq	rax, xmm1, 1
	vmovq	xmm10, rax

; 160  :     row6 = _mm_cvtsi64_si128(_mm_extract_epi64(temp3, 0));

	vpextrq	rax, xmm4, 0
	vmovq	xmm11, rax

; 161  :     row7 = _mm_cvtsi64_si128(_mm_extract_epi64(temp3, 1));

	vpextrq	rax, xmm4, 1
	vmovq	xmm0, rax
$LN2@filter_8x8:

; 162  :   }
; 163  :       
; 164  :   _mm_storel_epi64((__m128i*)(dst + 0 * 8), row0);
; 165  :   _mm_storel_epi64((__m128i*)(dst + 1 * 8), row1);
; 166  :   _mm_storel_epi64((__m128i*)(dst + 2 * 8), row2);
; 167  :   _mm_storel_epi64((__m128i*)(dst + 3 * 8), row3);
; 168  :   _mm_storel_epi64((__m128i*)(dst + 4 * 8), row4);
; 169  :   _mm_storel_epi64((__m128i*)(dst + 5 * 8), row5);
; 170  :   _mm_storel_epi64((__m128i*)(dst + 6 * 8), row6);
; 171  :   _mm_storel_epi64((__m128i*)(dst + 7 * 8), row7);
; 172  : } 

	mov	rbx, QWORD PTR [rsp+112]
	vmovq	QWORD PTR [rdi], xmm5
	vmovq	QWORD PTR [rdi+8], xmm6
	vmovaps	xmm6, XMMWORD PTR [rsp+80]
	vmovq	QWORD PTR [rdi+16], xmm7
	vmovaps	xmm7, XMMWORD PTR [rsp+64]
	vmovq	QWORD PTR [rdi+24], xmm8
	vmovaps	xmm8, XMMWORD PTR [rsp+48]
	vmovq	QWORD PTR [rdi+32], xmm9
	vmovaps	xmm9, XMMWORD PTR [rsp+32]
	vmovq	QWORD PTR [rdi+40], xmm10
	vmovaps	xmm10, XMMWORD PTR [rsp+16]
	vmovq	QWORD PTR [rdi+48], xmm11
	vmovaps	xmm11, XMMWORD PTR [rsp]
	vmovq	QWORD PTR [rdi+56], xmm0
	add	rsp, 96					; 00000060H
	pop	rdi
	ret	0
filter_8x8_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\intra-avx2.c
_TEXT	SEGMENT
ref_main$ = 8
delta_pos$ = 16
x$ = 24
filter_16x1_avx2 PROC

; 181  : 
; 182  :   int8_t delta_int = delta_pos >> 5;

	movzx	eax, dx

; 183  :   int8_t delta_fract = delta_pos & (32-1);

	and	dl, 31
	sar	ax, 5

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	movsx	eax, al
	add	eax, r8d
	cdqe

; 185  :   sample0 = _mm256_packus_epi16(sample0, sample0);
; 186  :   __m256i sample1 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int + 1])));

	vpmovzxbw ymm0, XMMWORD PTR [rax+rcx+1]
	vpmovzxbw ymm4, XMMWORD PTR [rax+rcx]

; 187  :   sample1 = _mm256_packus_epi16(sample1, sample1);

	vpackuswb ymm1, ymm0, ymm0

; 188  :   __m256i pairs_lo = _mm256_unpacklo_epi8(sample0, sample1);
; 189  : 
; 190  :   __m256i weight = _mm256_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	movzx	ecx, dl
	mov	eax, 32					; 00000020H
	sub	ax, cx
	shl	cx, 8
	vpackuswb ymm0, ymm4, ymm4
	vpunpcklbw ymm1, ymm0, ymm1
	or	ax, cx
	cwde
	vmovd	xmm2, eax
	vpbroadcastw ymm2, xmm2

; 191  :   __m256i v_temp_lo = _mm256_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw ymm2, ymm1, ymm2

; 192  :   v_temp_lo = _mm256_add_epi16(v_temp_lo, _mm256_set1_epi16(16));

	vpaddw	ymm3, ymm2, YMMWORD PTR __ymm@0010001000100010001000100010001000100010001000100010001000100010

; 193  :   v_temp_lo = _mm256_srli_epi16(v_temp_lo, 5);

	vpsrlw	ymm0, ymm3, 5

; 194  :   sample0 = _mm256_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb ymm0, ymm0, ymm0

; 195  : 
; 196  :   return sample0;
; 197  : }

	ret	0
filter_16x1_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\intra-avx2.c
_TEXT	SEGMENT
dst$ = 128
ref_main$ = 136
sample_disp$ = 144
vertical_mode$ = 152
filter_16x16_avx2 PROC

; 206  : static void filter_16x16_avx2(uint8_t *dst, const uint8_t *ref_main, int sample_disp, bool vertical_mode){

	mov	rax, rsp
	mov	QWORD PTR [rax+8], rbx
	push	rdi
	sub	rsp, 112				; 00000070H
	vmovaps	XMMWORD PTR [rax-24], xmm6
	mov	ebx, r8d
	vmovaps	XMMWORD PTR [rax-40], xmm7
	mov	rdi, rdx
	vmovaps	XMMWORD PTR [rax-56], xmm8

; 207  :   for (int y = 0; y < 16; y += 8) {

	xor	r11d, r11d
	vmovaps	XMMWORD PTR [rax-72], xmm9
	vmovaps	XMMWORD PTR [rax-88], xmm10
	vmovaps	XMMWORD PTR [rax-104], xmm11
	lea	rax, QWORD PTR [rcx+32]
	vmovaps	XMMWORD PTR [rsp], xmm12
	mov	r10, rax
	vmovdqu	ymm12, YMMWORD PTR __ymm@0010001000100010001000100010001000100010001000100010001000100010
	npad	9
$LL4@filter_16x:

; 208  :     __m256i row0 = filter_16x1_avx2(ref_main, (y + 1) * sample_disp, 0);

	lea	ecx, DWORD PTR [r11+1]
	movsx	edx, bx
	movsx	r8d, cx
	imul	r8d, edx

; 182  :   int8_t delta_int = delta_pos >> 5;

	movzx	ecx, r8w

; 189  : 
; 190  :   __m256i weight = _mm256_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	and	r8w, 31
	sar	cx, 5
	movzx	edx, r8w
	movsx	rcx, cl
	shl	dx, 8
	vpmovzxbw ymm3, XMMWORD PTR [rcx+rdi]
	vpmovzxbw ymm0, XMMWORD PTR [rcx+rdi+1]
	vpackuswb ymm1, ymm0, ymm0
	vpackuswb ymm0, ymm3, ymm3
	vpunpcklbw ymm1, ymm0, ymm1
	mov	ecx, 32					; 00000020H
	sub	cx, r8w
	or	dx, cx
	movsx	ecx, dx
	vmovd	xmm2, ecx
	vpbroadcastw ymm2, xmm2

; 191  :   __m256i v_temp_lo = _mm256_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw ymm2, ymm1, ymm2

; 192  :   v_temp_lo = _mm256_add_epi16(v_temp_lo, _mm256_set1_epi16(16));

	vpaddw	ymm3, ymm2, ymm12

; 193  :   v_temp_lo = _mm256_srli_epi16(v_temp_lo, 5);

	vpsrlw	ymm0, ymm3, 5

; 194  :   sample0 = _mm256_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb ymm4, ymm0, ymm0

; 209  :     __m256i row1 = filter_16x1_avx2(ref_main, (y + 2) * sample_disp, 0);

	lea	ecx, DWORD PTR [r11+2]
	movsx	edx, bx
	movsx	r8d, cx
	imul	r8d, edx

; 182  :   int8_t delta_int = delta_pos >> 5;

	movzx	ecx, r8w

; 189  : 
; 190  :   __m256i weight = _mm256_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	and	r8w, 31
	sar	cx, 5
	movzx	edx, r8w
	movsx	rcx, cl
	shl	dx, 8
	vpmovzxbw ymm3, XMMWORD PTR [rcx+rdi]
	vpmovzxbw ymm0, XMMWORD PTR [rcx+rdi+1]
	vpackuswb ymm1, ymm0, ymm0
	vpackuswb ymm0, ymm3, ymm3
	vpunpcklbw ymm1, ymm0, ymm1
	mov	ecx, 32					; 00000020H
	sub	cx, r8w
	or	dx, cx
	movsx	ecx, dx
	vmovd	xmm2, ecx
	vpbroadcastw ymm2, xmm2

; 191  :   __m256i v_temp_lo = _mm256_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw ymm2, ymm1, ymm2

; 192  :   v_temp_lo = _mm256_add_epi16(v_temp_lo, _mm256_set1_epi16(16));

	vpaddw	ymm3, ymm2, ymm12

; 193  :   v_temp_lo = _mm256_srli_epi16(v_temp_lo, 5);

	vpsrlw	ymm0, ymm3, 5

; 210  :     __m256i row2 = filter_16x1_avx2(ref_main, (y + 3) * sample_disp, 0);

	lea	ecx, DWORD PTR [r11+3]
	movsx	edx, bx
	movsx	r8d, cx
	imul	r8d, edx

; 194  :   sample0 = _mm256_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb ymm5, ymm0, ymm0

; 182  :   int8_t delta_int = delta_pos >> 5;

	movzx	ecx, r8w

; 189  : 
; 190  :   __m256i weight = _mm256_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	and	r8w, 31
	sar	cx, 5
	movzx	edx, r8w
	movsx	rcx, cl
	shl	dx, 8
	vpmovzxbw ymm3, XMMWORD PTR [rcx+rdi]
	vpmovzxbw ymm0, XMMWORD PTR [rcx+rdi+1]
	vpackuswb ymm1, ymm0, ymm0
	vpackuswb ymm0, ymm3, ymm3
	mov	ecx, 32					; 00000020H
	sub	cx, r8w
	or	dx, cx
	movsx	ecx, dx
	vmovd	xmm2, ecx

; 211  :     __m256i row3 = filter_16x1_avx2(ref_main, (y + 4) * sample_disp, 0);

	lea	ecx, DWORD PTR [r11+4]
	movsx	edx, bx
	movsx	r8d, cx
	imul	r8d, edx

; 188  :   __m256i pairs_lo = _mm256_unpacklo_epi8(sample0, sample1);

	vpunpcklbw ymm1, ymm0, ymm1

; 189  : 
; 190  :   __m256i weight = _mm256_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	vpbroadcastw ymm2, xmm2

; 191  :   __m256i v_temp_lo = _mm256_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw ymm2, ymm1, ymm2

; 192  :   v_temp_lo = _mm256_add_epi16(v_temp_lo, _mm256_set1_epi16(16));

	vpaddw	ymm3, ymm2, ymm12

; 182  :   int8_t delta_int = delta_pos >> 5;

	movzx	ecx, r8w

; 193  :   v_temp_lo = _mm256_srli_epi16(v_temp_lo, 5);

	vpsrlw	ymm0, ymm3, 5

; 182  :   int8_t delta_int = delta_pos >> 5;

	sar	cx, 5

; 194  :   sample0 = _mm256_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb ymm6, ymm0, ymm0

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	movsx	rcx, cl

; 189  : 
; 190  :   __m256i weight = _mm256_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	and	r8w, 31

; 212  :     __m256i row4 = filter_16x1_avx2(ref_main, (y + 5) * sample_disp, 0);

	movsx	edx, bx

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	vpmovzxbw ymm3, XMMWORD PTR [rcx+rdi]

; 185  :   sample0 = _mm256_packus_epi16(sample0, sample0);
; 186  :   __m256i sample1 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int + 1])));

	vpmovzxbw ymm0, XMMWORD PTR [rcx+rdi+1]

; 187  :   sample1 = _mm256_packus_epi16(sample1, sample1);

	vpackuswb ymm1, ymm0, ymm0
	vpackuswb ymm0, ymm3, ymm3

; 188  :   __m256i pairs_lo = _mm256_unpacklo_epi8(sample0, sample1);

	vpunpcklbw ymm1, ymm0, ymm1

; 189  : 
; 190  :   __m256i weight = _mm256_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	mov	ecx, 32					; 00000020H
	sub	cx, r8w
	shl	r8w, 8
	or	cx, r8w
	movsx	ecx, cx
	vmovd	xmm2, ecx
	vpbroadcastw ymm2, xmm2

; 191  :   __m256i v_temp_lo = _mm256_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw ymm2, ymm1, ymm2

; 192  :   v_temp_lo = _mm256_add_epi16(v_temp_lo, _mm256_set1_epi16(16));

	vpaddw	ymm3, ymm2, ymm12

; 193  :   v_temp_lo = _mm256_srli_epi16(v_temp_lo, 5);

	vpsrlw	ymm0, ymm3, 5

; 194  :   sample0 = _mm256_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb ymm7, ymm0, ymm0

; 212  :     __m256i row4 = filter_16x1_avx2(ref_main, (y + 5) * sample_disp, 0);

	lea	ecx, DWORD PTR [r11+5]
	movsx	r8d, cx
	imul	r8d, edx

; 213  :     __m256i row5 = filter_16x1_avx2(ref_main, (y + 6) * sample_disp, 0);

	movsx	edx, bx

; 182  :   int8_t delta_int = delta_pos >> 5;

	movzx	ecx, r8w

; 189  : 
; 190  :   __m256i weight = _mm256_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	and	r8w, 31
	sar	cx, 5
	movsx	rcx, cl
	vpmovzxbw ymm3, XMMWORD PTR [rcx+rdi]
	vpmovzxbw ymm0, XMMWORD PTR [rcx+rdi+1]
	vpackuswb ymm1, ymm0, ymm0
	vpackuswb ymm0, ymm3, ymm3
	vpunpcklbw ymm1, ymm0, ymm1
	mov	ecx, 32					; 00000020H
	sub	cx, r8w
	shl	r8w, 8
	or	cx, r8w
	movsx	ecx, cx
	vmovd	xmm2, ecx
	vpbroadcastw ymm2, xmm2

; 191  :   __m256i v_temp_lo = _mm256_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw ymm2, ymm1, ymm2

; 192  :   v_temp_lo = _mm256_add_epi16(v_temp_lo, _mm256_set1_epi16(16));

	vpaddw	ymm3, ymm2, ymm12

; 193  :   v_temp_lo = _mm256_srli_epi16(v_temp_lo, 5);

	vpsrlw	ymm0, ymm3, 5

; 194  :   sample0 = _mm256_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb ymm8, ymm0, ymm0

; 213  :     __m256i row5 = filter_16x1_avx2(ref_main, (y + 6) * sample_disp, 0);

	lea	ecx, DWORD PTR [r11+6]
	movsx	r8d, cx
	imul	r8d, edx

; 214  :     __m256i row6 = filter_16x1_avx2(ref_main, (y + 7) * sample_disp, 0);

	movsx	edx, bx

; 182  :   int8_t delta_int = delta_pos >> 5;

	movzx	ecx, r8w

; 189  : 
; 190  :   __m256i weight = _mm256_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	and	r8w, 31
	sar	cx, 5
	movsx	rcx, cl
	vpmovzxbw ymm3, XMMWORD PTR [rcx+rdi]
	vpmovzxbw ymm0, XMMWORD PTR [rcx+rdi+1]
	vpackuswb ymm1, ymm0, ymm0
	vpackuswb ymm0, ymm3, ymm3
	mov	ecx, 32					; 00000020H
	sub	cx, r8w
	shl	r8w, 8
	or	cx, r8w
	movsx	ecx, cx
	vmovd	xmm2, ecx

; 214  :     __m256i row6 = filter_16x1_avx2(ref_main, (y + 7) * sample_disp, 0);

	lea	ecx, DWORD PTR [r11+7]
	movsx	r8d, cx
	imul	r8d, edx

; 188  :   __m256i pairs_lo = _mm256_unpacklo_epi8(sample0, sample1);

	vpunpcklbw ymm1, ymm0, ymm1

; 189  : 
; 190  :   __m256i weight = _mm256_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	vpbroadcastw ymm2, xmm2

; 191  :   __m256i v_temp_lo = _mm256_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw ymm2, ymm1, ymm2

; 192  :   v_temp_lo = _mm256_add_epi16(v_temp_lo, _mm256_set1_epi16(16));

	vpaddw	ymm3, ymm2, ymm12

; 182  :   int8_t delta_int = delta_pos >> 5;

	movzx	ecx, r8w

; 189  : 
; 190  :   __m256i weight = _mm256_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	and	r8w, 31
	sar	cx, 5
	movsx	rcx, cl

; 193  :   v_temp_lo = _mm256_srli_epi16(v_temp_lo, 5);

	vpsrlw	ymm0, ymm3, 5

; 194  :   sample0 = _mm256_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb ymm9, ymm0, ymm0

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	vpmovzxbw ymm3, XMMWORD PTR [rcx+rdi]

; 185  :   sample0 = _mm256_packus_epi16(sample0, sample0);
; 186  :   __m256i sample1 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int + 1])));

	vpmovzxbw ymm0, XMMWORD PTR [rcx+rdi+1]

; 189  : 
; 190  :   __m256i weight = _mm256_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	mov	ecx, 32					; 00000020H
	sub	cx, r8w
	shl	r8w, 8
	or	cx, r8w
	movsx	ecx, cx
	vmovd	xmm2, ecx
	vpackuswb ymm1, ymm0, ymm0
	vpackuswb ymm0, ymm3, ymm3
	vpunpcklbw ymm1, ymm0, ymm1
	vpbroadcastw ymm2, xmm2

; 191  :   __m256i v_temp_lo = _mm256_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw ymm2, ymm1, ymm2

; 192  :   v_temp_lo = _mm256_add_epi16(v_temp_lo, _mm256_set1_epi16(16));

	vpaddw	ymm3, ymm2, ymm12

; 193  :   v_temp_lo = _mm256_srli_epi16(v_temp_lo, 5);

	vpsrlw	ymm0, ymm3, 5

; 215  :     __m256i row7 = filter_16x1_avx2(ref_main, (y + 8) * sample_disp, 0);

	lea	ecx, DWORD PTR [r11+8]
	movsx	edx, bx
	movsx	r8d, cx
	imul	r8d, edx

; 194  :   sample0 = _mm256_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb ymm10, ymm0, ymm0

; 182  :   int8_t delta_int = delta_pos >> 5;

	movzx	ecx, r8w

; 189  : 
; 190  :   __m256i weight = _mm256_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	and	r8w, 31
	sar	cx, 5
	movsx	rcx, cl
	vpmovzxbw ymm3, XMMWORD PTR [rcx+rdi]
	vpmovzxbw ymm0, XMMWORD PTR [rcx+rdi+1]
	vpackuswb ymm1, ymm0, ymm0
	vpackuswb ymm0, ymm3, ymm3
	mov	ecx, 32					; 00000020H
	sub	cx, r8w
	shl	r8w, 8
	or	cx, r8w
	movsx	ecx, cx
	vpunpcklbw ymm1, ymm0, ymm1
	vmovd	xmm2, ecx
	vpbroadcastw ymm2, xmm2

; 191  :   __m256i v_temp_lo = _mm256_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw ymm2, ymm1, ymm2

; 192  :   v_temp_lo = _mm256_add_epi16(v_temp_lo, _mm256_set1_epi16(16));

	vpaddw	ymm3, ymm2, ymm12

; 193  :   v_temp_lo = _mm256_srli_epi16(v_temp_lo, 5);

	vpsrlw	ymm0, ymm3, 5

; 194  :   sample0 = _mm256_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb ymm11, ymm0, ymm0

; 216  : 
; 217  :     if (!vertical_mode) {

	test	r9b, r9b
	jne	$LN5@filter_16x

; 218  :       __m256i q0 = _mm256_unpacklo_epi8(row0, row1);

	vpunpcklbw ymm5, ymm4, ymm5

; 219  :       __m256i q1 = _mm256_unpacklo_epi8(row2, row3);

	vpunpcklbw ymm2, ymm6, ymm7

; 220  :       __m256i q2 = _mm256_unpacklo_epi8(row4, row5);
; 221  :       __m256i q3 = _mm256_unpacklo_epi8(row6, row7);
; 222  : 
; 223  :       __m256i h0 = _mm256_unpacklo_epi16(q0, q1);

	vpunpcklwd ymm4, ymm5, ymm2
	vpunpcklbw ymm3, ymm8, ymm9

; 224  :       __m256i h1 = _mm256_unpacklo_epi16(q2, q3);
; 225  :       __m256i h2 = _mm256_unpackhi_epi16(q0, q1);

	vpunpckhwd ymm5, ymm5, ymm2
	vpunpcklbw ymm0, ymm10, ymm11

; 226  :       __m256i h3 = _mm256_unpackhi_epi16(q2, q3);

	vpunpckhwd ymm2, ymm3, ymm0
	vpunpcklwd ymm1, ymm3, ymm0

; 227  : 
; 228  :       __m256i temp0 = _mm256_unpacklo_epi32(h0, h1);

	vpunpckldq ymm0, ymm4, ymm1

; 229  :       __m256i temp1 = _mm256_unpackhi_epi32(h0, h1);

	vpunpckhdq ymm3, ymm4, ymm1

; 230  :       __m256i temp2 = _mm256_unpacklo_epi32(h2, h3);

	vpunpckldq ymm1, ymm5, ymm2

; 231  :       __m256i temp3 = _mm256_unpackhi_epi32(h2, h3);
; 232  : 
; 233  :       row0 = _mm256_unpacklo_epi64(temp0, temp0);
; 234  :       row1 = _mm256_unpackhi_epi64(temp0, temp0);
; 235  :       row2 = _mm256_unpacklo_epi64(temp1, temp1);
; 236  :       row3 = _mm256_unpackhi_epi64(temp1, temp1);
; 237  :       row4 = _mm256_unpacklo_epi64(temp2, temp2);

	vpunpcklqdq ymm7, ymm1, ymm1

; 238  :       row5 = _mm256_unpackhi_epi64(temp2, temp2);

	vpunpckhqdq ymm8, ymm1, ymm1
	vpunpckhdq ymm4, ymm5, ymm2
	vpunpckhqdq ymm5, ymm0, ymm0
	vpunpcklqdq ymm2, ymm0, ymm0
	vpunpcklqdq ymm6, ymm3, ymm3

; 239  :       row6 = _mm256_unpacklo_epi64(temp3, temp3);

	vpunpcklqdq ymm9, ymm4, ymm4

; 240  :       row7 = _mm256_unpackhi_epi64(temp3, temp3);
; 241  : 
; 242  :       //x and y must be flipped due to transpose
; 243  :       int rx = y;
; 244  :       int ry = 0;
; 245  : 
; 246  :       *(int64_t*)(dst + (ry + 0) * 16 + rx) = _mm_cvtsi128_si64(_mm256_castsi256_si128(row0));
; 247  :       *(int64_t*)(dst + (ry + 1) * 16 + rx) = _mm_cvtsi128_si64(_mm256_castsi256_si128(row1));
; 248  :       *(int64_t*)(dst + (ry + 2) * 16 + rx) = _mm_cvtsi128_si64(_mm256_castsi256_si128(row2));
; 249  :       *(int64_t*)(dst + (ry + 3) * 16 + rx) = _mm_cvtsi128_si64(_mm256_castsi256_si128(row3));
; 250  :       *(int64_t*)(dst + (ry + 4) * 16 + rx) = _mm_cvtsi128_si64(_mm256_castsi256_si128(row4));
; 251  :       *(int64_t*)(dst + (ry + 5) * 16 + rx) = _mm_cvtsi128_si64(_mm256_castsi256_si128(row5));
; 252  :       *(int64_t*)(dst + (ry + 6) * 16 + rx) = _mm_cvtsi128_si64(_mm256_castsi256_si128(row6));
; 253  :       *(int64_t*)(dst + (ry + 7) * 16 + rx) = _mm_cvtsi128_si64(_mm256_castsi256_si128(row7));
; 254  : 
; 255  :       *(int64_t*)(dst + (ry + 8) * 16 + rx) = _mm_cvtsi128_si64(_mm256_extracti128_si256(row0, 1));

	vextracti128 xmm0, ymm2, 1
	vmovq	QWORD PTR [rax+96], xmm0

; 256  :       *(int64_t*)(dst + (ry + 9) * 16 + rx) = _mm_cvtsi128_si64(_mm256_extracti128_si256(row1, 1));

	vextracti128 xmm1, ymm5, 1
	vmovq	QWORD PTR [rax+112], xmm1
	vpunpckhqdq ymm3, ymm3, ymm3

; 257  :       *(int64_t*)(dst + (ry + 10) * 16 + rx) = _mm_cvtsi128_si64(_mm256_extracti128_si256(row2, 1));

	vextracti128 xmm0, ymm6, 1

; 258  :       *(int64_t*)(dst + (ry + 11) * 16 + rx) = _mm_cvtsi128_si64(_mm256_extracti128_si256(row3, 1));

	vextracti128 xmm1, ymm3, 1
	vmovq	QWORD PTR [rax+128], xmm0
	vmovq	QWORD PTR [rax+144], xmm1

; 259  :       *(int64_t*)(dst + (ry + 12) * 16 + rx) = _mm_cvtsi128_si64(_mm256_extracti128_si256(row4, 1));

	vextracti128 xmm0, ymm7, 1

; 260  :       *(int64_t*)(dst + (ry + 13) * 16 + rx) = _mm_cvtsi128_si64(_mm256_extracti128_si256(row5, 1));

	vextracti128 xmm1, ymm8, 1
	vpunpckhqdq ymm4, ymm4, ymm4
	vmovq	QWORD PTR [rax+160], xmm0
	vmovq	QWORD PTR [rax+176], xmm1

; 261  :       *(int64_t*)(dst + (ry + 14) * 16 + rx) = _mm_cvtsi128_si64(_mm256_extracti128_si256(row6, 1));

	vextracti128 xmm0, ymm9, 1

; 262  :       *(int64_t*)(dst + (ry + 15) * 16 + rx) = _mm_cvtsi128_si64(_mm256_extracti128_si256(row7, 1));

	vextracti128 xmm1, ymm4, 1
	vmovq	QWORD PTR [rax+192], xmm0
	vmovq	QWORD PTR [rax+208], xmm1
	vmovq	QWORD PTR [rax-32], xmm2
	vmovq	QWORD PTR [rax-16], xmm5
	vmovq	QWORD PTR [rax], xmm6
	vmovq	QWORD PTR [rax+16], xmm3
	vmovq	QWORD PTR [rax+32], xmm7
	vmovq	QWORD PTR [rax+48], xmm8
	vmovq	QWORD PTR [rax+64], xmm9
	vmovq	QWORD PTR [rax+80], xmm4

; 263  :     } else {

	jmp	SHORT $LN2@filter_16x
$LN5@filter_16x:

; 264  : 
; 265  :       //Set ry for the lower half of the block
; 266  :       int rx = 0;
; 267  :       int ry = y;
; 268  : 
; 269  :       row0 = _mm256_permute4x64_epi64(row0, _MM_SHUFFLE(3,1,2,0));

	vpermq	ymm0, ymm4, 216				; 000000d8H

; 270  :       row1 = _mm256_permute4x64_epi64(row1, _MM_SHUFFLE(2,0,3,1));
; 271  :       row2 = _mm256_permute4x64_epi64(row2, _MM_SHUFFLE(3,1,2,0));
; 272  :       row3 = _mm256_permute4x64_epi64(row3, _MM_SHUFFLE(2,0,3,1));
; 273  :       row4 = _mm256_permute4x64_epi64(row4, _MM_SHUFFLE(3,1,2,0));
; 274  :       row5 = _mm256_permute4x64_epi64(row5, _MM_SHUFFLE(2,0,3,1));
; 275  :       row6 = _mm256_permute4x64_epi64(row6, _MM_SHUFFLE(3,1,2,0));
; 276  :       row7 = _mm256_permute4x64_epi64(row7, _MM_SHUFFLE(2,0,3,1));
; 277  : 
; 278  :       _mm_storeu_si128((__m128i*)(dst + (ry + 0) * 16 + rx), _mm256_castsi256_si128(row0));

	vmovdqu	XMMWORD PTR [r10-32], xmm0
	vpermq	ymm1, ymm5, 141				; 0000008dH

; 279  :       _mm_storeu_si128((__m128i*)(dst + (ry + 1) * 16 + rx), _mm256_castsi256_si128(row1));

	vmovdqu	XMMWORD PTR [r10-16], xmm1
	vpermq	ymm0, ymm6, 216				; 000000d8H

; 280  :       _mm_storeu_si128((__m128i*)(dst + (ry + 2) * 16 + rx), _mm256_castsi256_si128(row2));

	vmovdqu	XMMWORD PTR [r10], xmm0
	vpermq	ymm1, ymm7, 141				; 0000008dH

; 281  :       _mm_storeu_si128((__m128i*)(dst + (ry + 3) * 16 + rx), _mm256_castsi256_si128(row3));

	vmovdqu	XMMWORD PTR [r10+16], xmm1
	vpermq	ymm0, ymm8, 216				; 000000d8H

; 282  :       _mm_storeu_si128((__m128i*)(dst + (ry + 4) * 16 + rx), _mm256_castsi256_si128(row4));

	vmovdqu	XMMWORD PTR [r10+32], xmm0
	vpermq	ymm1, ymm9, 141				; 0000008dH

; 283  :       _mm_storeu_si128((__m128i*)(dst + (ry + 5) * 16 + rx), _mm256_castsi256_si128(row5));

	vmovdqu	XMMWORD PTR [r10+48], xmm1
	vpermq	ymm0, ymm10, 216			; 000000d8H
	vpermq	ymm1, ymm11, 141			; 0000008dH

; 284  :       _mm_storeu_si128((__m128i*)(dst + (ry + 6) * 16 + rx), _mm256_castsi256_si128(row6));

	vmovdqu	XMMWORD PTR [r10+64], xmm0

; 285  :       _mm_storeu_si128((__m128i*)(dst + (ry + 7) * 16 + rx), _mm256_castsi256_si128(row7));

	vmovdqu	XMMWORD PTR [r10+80], xmm1
$LN2@filter_16x:

; 207  :   for (int y = 0; y < 16; y += 8) {

	add	r11d, 8
	add	rax, 8
	sub	r10, -128				; ffffffffffffff80H
	cmp	r11d, 16
	jl	$LL4@filter_16x

; 286  :     }
; 287  :   }
; 288  : }

	vzeroupper
	vmovaps	xmm6, XMMWORD PTR [rsp+96]
	lea	r11, QWORD PTR [rsp+112]
	mov	rbx, QWORD PTR [r11+16]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm9, XMMWORD PTR [r11-64]
	vmovaps	xmm10, XMMWORD PTR [r11-80]
	vmovaps	xmm11, XMMWORD PTR [r11-96]
	vmovaps	xmm12, XMMWORD PTR [r11-112]
	vmovaps	xmm7, XMMWORD PTR [rsp+80]
	mov	rsp, r11
	pop	rdi
	ret	0
filter_16x16_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\intra-avx2.c
_TEXT	SEGMENT
tv8606 = 0
tv8602 = 2
tv8600 = 4
tv8596 = 6
tv8594 = 8
tv8590 = 10
tv8588 = 12
tv8584 = 14
y$1$ = 16
tv7837 = 20
tv7845 = 24
tv7844 = 28
tv7841 = 32
tv7840 = 36
tv7839 = 40
tv7838 = 44
tv7791 = 48
tv7790 = 56
tv7789 = 64
tv7788 = 72
tv7787 = 80
tv7786 = 88
tv7820 = 96
tv7816 = 128
tv7812 = 160
tv7808 = 192
tv7804 = 224
dst$ = 512
ref_main$ = 520
sample_disp$ = 528
vertical_mode$ = 536
width$ = 544
filter_NxN_avx2 PROC

; 298  : static void filter_NxN_avx2(uint8_t *dst, const uint8_t *ref_main, int sample_disp, bool vertical_mode, int width){

	mov	rax, rsp
	mov	QWORD PTR [rax+8], rbx
	mov	BYTE PTR [rax+32], r9b
	mov	DWORD PTR [rax+24], r8d
	push	rbp
	push	rsi
	push	rdi
	push	r12
	push	r13
	push	r14
	push	r15
	sub	rsp, 448				; 000001c0H
	vmovaps	XMMWORD PTR [rax-72], xmm6
	vmovaps	XMMWORD PTR [rax-88], xmm7
	vmovaps	XMMWORD PTR [rax-104], xmm8
	vmovaps	XMMWORD PTR [rax-120], xmm9
	vmovaps	XMMWORD PTR [rax-136], xmm10
	vmovaps	XMMWORD PTR [rax-152], xmm11
	vmovaps	XMMWORD PTR [rax-168], xmm12
	vmovaps	XMMWORD PTR [rax-184], xmm13
	vmovaps	XMMWORD PTR [rax-200], xmm14
	vmovaps	XMMWORD PTR [rax-216], xmm15
	lea	rbp, QWORD PTR [rsp+32]
	and	rbp, -32				; ffffffffffffffe0H
	movsxd	rbx, DWORD PTR width$[rsp]

; 299  :   for (int y = 0; y < width; y += 8) {

	xor	r14d, r14d
	mov	DWORD PTR y$1$[rbp], r14d
	mov	edi, r8d
	mov	rsi, rdx
	mov	r10, rcx
	test	ebx, ebx
	jle	$LN3@filter_NxN
	vmovdqu	ymm9, YMMWORD PTR __ymm@0010001000100010001000100010001000100010001000100010001000100010
	lea	eax, DWORD PTR [rbx+rbx*4]
	mov	r12d, ebx
	add	eax, eax
	shl	r12d, 4
	mov	DWORD PTR tv7841[rbp], eax
	xor	r13d, r13d
	imul	eax, ebx, 11
	mov	DWORD PTR tv7840[rbp], eax
	lea	eax, DWORD PTR [rbx+rbx*2]
	lea	eax, DWORD PTR [rax*4]
	mov	DWORD PTR tv7839[rbp], eax
	imul	eax, ebx, 13
	mov	DWORD PTR tv7838[rbp], eax
	mov	eax, ebx
	neg	eax
	add	eax, eax
	mov	DWORD PTR tv7837[rbp], eax
	lea	eax, DWORD PTR [rbx+rbx]
	movsxd	r15, eax
	mov	rax, rbx
	sub	rax, r15
	mov	r9, r15
	mov	QWORD PTR tv7791[rbp], rax
	lea	eax, DWORD PTR [rbx+rbx*2]
	cdqe
	sub	rax, r15
	mov	QWORD PTR tv7790[rbp], rax
	lea	eax, DWORD PTR [rbx*4]
	cdqe
	sub	rax, r15
	mov	QWORD PTR tv7789[rbp], rax
	lea	eax, DWORD PTR [rbx+rbx*4]
	cdqe
	sub	rax, r15
	mov	QWORD PTR tv7788[rbp], rax
	lea	eax, DWORD PTR [rbx+rbx*2]
	add	eax, eax
	mov	DWORD PTR tv7845[rbp], eax
	cdqe
	sub	rax, r15
	mov	QWORD PTR tv7787[rbp], rax
	imul	eax, ebx, 7
	mov	DWORD PTR tv7844[rbp], eax
	cdqe
	sub	rax, r15
	mov	QWORD PTR tv7786[rbp], rax
	npad	11
$LL4@filter_NxN:

; 300  :     for (int x = 0; x < width; x += 16) {

	lea	ecx, DWORD PTR [r14+1]
	movsx	edx, di
	movsx	r8d, cx
	xor	r11d, r11d
	imul	r8d, edx
	xor	eax, eax
	movzx	ecx, r8w
	and	r8w, 31
	sar	cx, 5
	movzx	edx, r8w
	mov	WORD PTR tv8606[rbp], cx
	mov	ecx, 32					; 00000020H
	sub	cx, r8w
	shl	dx, 8
	or	dx, cx
	movsx	ecx, dx
	vmovd	xmm13, ecx
	lea	ecx, DWORD PTR [r14+2]
	movsx	edx, di
	movsx	r8d, cx
	imul	r8d, edx
	vpbroadcastw ymm13, xmm13
	movzx	ecx, r8w
	and	r8w, 31
	sar	cx, 5
	movzx	edx, r8w
	mov	WORD PTR tv8602[rbp], cx

; 190  :   __m256i weight = _mm256_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	mov	ecx, 32					; 00000020H
	sub	cx, r8w
	shl	dx, 8
	or	dx, cx
	movsx	ecx, dx
	vmovd	xmm14, ecx
	lea	ecx, DWORD PTR [r14+3]
	movsx	edx, di
	movsx	r8d, cx
	imul	r8d, edx
	vpbroadcastw ymm14, xmm14
	movzx	ecx, r8w
	and	r8w, 31
	sar	cx, 5
	movzx	edx, r8w
	mov	WORD PTR tv8600[rbp], cx
	mov	ecx, 32					; 00000020H
	sub	cx, r8w
	shl	dx, 8
	or	dx, cx
	movsx	ecx, dx
	vmovd	xmm15, ecx
	lea	ecx, DWORD PTR [r14+4]
	movsx	edx, di
	movsx	r8d, cx
	imul	r8d, edx
	vpbroadcastw ymm15, xmm15
	movzx	ecx, r8w
	and	r8w, 31
	sar	cx, 5
	movzx	edx, r8w
	mov	WORD PTR tv8596[rbp], cx
	mov	ecx, 32					; 00000020H
	sub	cx, r8w
	shl	dx, 8
	or	dx, cx
	movsx	ecx, dx
	vmovd	xmm7, ecx
	lea	ecx, DWORD PTR [r14+5]
	movsx	edx, di
	movsx	r8d, cx
	imul	r8d, edx
	vpbroadcastw ymm7, xmm7
	vmovdqu	YMMWORD PTR tv7820[rbp], ymm7
	movzx	ecx, r8w
	and	r8w, 31
	sar	cx, 5
	movzx	edx, r8w
	mov	WORD PTR tv8594[rbp], cx
	mov	ecx, 32					; 00000020H
	sub	cx, r8w
	shl	dx, 8
	or	dx, cx
	movsx	ecx, dx
	vmovd	xmm8, ecx
	lea	ecx, DWORD PTR [r14+6]
	movsx	edx, di
	movsx	r8d, cx
	imul	r8d, edx
	vpbroadcastw ymm8, xmm8
	vmovdqu	YMMWORD PTR tv7816[rbp], ymm8
	movzx	ecx, r8w
	and	r8w, 31
	sar	cx, 5
	movzx	edx, r8w
	mov	WORD PTR tv8590[rbp], cx
	mov	ecx, 32					; 00000020H
	sub	cx, r8w
	shl	dx, 8
	or	dx, cx
	movsx	ecx, dx
	vmovd	xmm10, ecx
	lea	ecx, DWORD PTR [r14+7]
	movsx	edx, di
	movsx	r8d, cx
	imul	r8d, edx
	vpbroadcastw ymm10, xmm10
	vmovdqu	YMMWORD PTR tv7812[rbp], ymm10
	movzx	ecx, r8w
	and	r8w, 31
	sar	cx, 5
	movzx	edx, r8w
	mov	WORD PTR tv8588[rbp], cx
	mov	ecx, 32					; 00000020H
	sub	cx, r8w
	shl	dx, 8
	or	dx, cx
	movsx	ecx, dx
	vmovd	xmm11, ecx
	lea	ecx, DWORD PTR [r14+8]
	movsx	edx, di
	mov	r14, QWORD PTR tv7791[rbp]

; 300  :     for (int x = 0; x < width; x += 16) {

	lea	edi, DWORD PTR [rbx+rbx]
	movsx	r8d, cx
	imul	r8d, edx
	vpbroadcastw ymm11, xmm11
	vmovdqu	YMMWORD PTR tv7808[rbp], ymm11
	movzx	ecx, r8w
	and	r8w, 31
	sar	cx, 5
	movzx	edx, r8w
	mov	WORD PTR tv8584[rbp], cx

; 190  :   __m256i weight = _mm256_set1_epi16( (delta_fract << 8) | (32 - delta_fract) );

	mov	ecx, 32					; 00000020H
	sub	cx, r8w
	shl	dx, 8
	or	dx, cx
	movsx	ecx, dx
	vmovd	xmm12, ecx
	vpbroadcastw ymm12, xmm12
	vmovdqu	YMMWORD PTR tv7804[rbp], ymm12
$LL7@filter_NxN:

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	movsx	ecx, BYTE PTR tv8606[rbp]
	add	ecx, r11d
	movsxd	rcx, ecx

; 186  :   __m256i sample1 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int + 1])));

	vpmovzxbw ymm0, XMMWORD PTR [rcx+rsi+1]
	vpmovzxbw ymm2, XMMWORD PTR [rcx+rsi]

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	movsx	ecx, BYTE PTR tv8602[rbp]

; 187  :   sample1 = _mm256_packus_epi16(sample1, sample1);

	vpackuswb ymm1, ymm0, ymm0

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	add	ecx, r11d
	movsxd	rcx, ecx

; 185  :   sample0 = _mm256_packus_epi16(sample0, sample0);

	vpackuswb ymm0, ymm2, ymm2

; 188  :   __m256i pairs_lo = _mm256_unpacklo_epi8(sample0, sample1);

	vpunpcklbw ymm1, ymm0, ymm1

; 191  :   __m256i v_temp_lo = _mm256_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw ymm2, ymm1, ymm13

; 192  :   v_temp_lo = _mm256_add_epi16(v_temp_lo, _mm256_set1_epi16(16));

	vpaddw	ymm3, ymm2, ymm9

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	vpmovzxbw ymm2, XMMWORD PTR [rcx+rsi]

; 193  :   v_temp_lo = _mm256_srli_epi16(v_temp_lo, 5);

	vpsrlw	ymm0, ymm3, 5

; 194  :   sample0 = _mm256_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb ymm4, ymm0, ymm0

; 186  :   __m256i sample1 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int + 1])));

	vpmovzxbw ymm0, XMMWORD PTR [rcx+rsi+1]

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	movsx	ecx, BYTE PTR tv8600[rbp]

; 187  :   sample1 = _mm256_packus_epi16(sample1, sample1);

	vpackuswb ymm1, ymm0, ymm0
	vpackuswb ymm0, ymm2, ymm2

; 188  :   __m256i pairs_lo = _mm256_unpacklo_epi8(sample0, sample1);

	vpunpcklbw ymm1, ymm0, ymm1

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	add	ecx, r11d
	movsxd	rcx, ecx

; 191  :   __m256i v_temp_lo = _mm256_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw ymm2, ymm1, ymm14

; 192  :   v_temp_lo = _mm256_add_epi16(v_temp_lo, _mm256_set1_epi16(16));

	vpaddw	ymm3, ymm2, ymm9

; 193  :   v_temp_lo = _mm256_srli_epi16(v_temp_lo, 5);

	vpsrlw	ymm0, ymm3, 5

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	vpmovzxbw ymm2, XMMWORD PTR [rcx+rsi]

; 194  :   sample0 = _mm256_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb ymm5, ymm0, ymm0

; 186  :   __m256i sample1 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int + 1])));

	vpmovzxbw ymm0, XMMWORD PTR [rcx+rsi+1]

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	movsx	ecx, BYTE PTR tv8596[rbp]

; 187  :   sample1 = _mm256_packus_epi16(sample1, sample1);

	vpackuswb ymm1, ymm0, ymm0
	vpackuswb ymm0, ymm2, ymm2

; 188  :   __m256i pairs_lo = _mm256_unpacklo_epi8(sample0, sample1);

	vpunpcklbw ymm1, ymm0, ymm1

; 191  :   __m256i v_temp_lo = _mm256_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw ymm2, ymm1, ymm15

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	add	ecx, r11d
	movsxd	rcx, ecx

; 192  :   v_temp_lo = _mm256_add_epi16(v_temp_lo, _mm256_set1_epi16(16));

	vpaddw	ymm3, ymm2, ymm9

; 193  :   v_temp_lo = _mm256_srli_epi16(v_temp_lo, 5);

	vpsrlw	ymm0, ymm3, 5

; 194  :   sample0 = _mm256_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb ymm6, ymm0, ymm0

; 186  :   __m256i sample1 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int + 1])));

	vpmovzxbw ymm0, XMMWORD PTR [rcx+rsi+1]
	vpmovzxbw ymm2, XMMWORD PTR [rcx+rsi]

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	movsx	ecx, BYTE PTR tv8594[rbp]

; 187  :   sample1 = _mm256_packus_epi16(sample1, sample1);

	vpackuswb ymm1, ymm0, ymm0
	vpackuswb ymm0, ymm2, ymm2

; 188  :   __m256i pairs_lo = _mm256_unpacklo_epi8(sample0, sample1);

	vpunpcklbw ymm1, ymm0, ymm1

; 191  :   __m256i v_temp_lo = _mm256_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw ymm2, ymm1, ymm7

; 192  :   v_temp_lo = _mm256_add_epi16(v_temp_lo, _mm256_set1_epi16(16));

	vpaddw	ymm3, ymm2, ymm9

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	add	ecx, r11d
	movsxd	rcx, ecx

; 193  :   v_temp_lo = _mm256_srli_epi16(v_temp_lo, 5);

	vpsrlw	ymm0, ymm3, 5

; 194  :   sample0 = _mm256_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb ymm7, ymm0, ymm0

; 186  :   __m256i sample1 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int + 1])));

	vpmovzxbw ymm0, XMMWORD PTR [rcx+rsi+1]
	vpmovzxbw ymm2, XMMWORD PTR [rcx+rsi]

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	movsx	ecx, BYTE PTR tv8590[rbp]

; 187  :   sample1 = _mm256_packus_epi16(sample1, sample1);

	vpackuswb ymm1, ymm0, ymm0
	vpackuswb ymm0, ymm2, ymm2

; 188  :   __m256i pairs_lo = _mm256_unpacklo_epi8(sample0, sample1);

	vpunpcklbw ymm1, ymm0, ymm1

; 191  :   __m256i v_temp_lo = _mm256_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw ymm2, ymm1, ymm8

; 192  :   v_temp_lo = _mm256_add_epi16(v_temp_lo, _mm256_set1_epi16(16));

	vpaddw	ymm3, ymm2, ymm9

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	add	ecx, r11d
	movsxd	rcx, ecx

; 193  :   v_temp_lo = _mm256_srli_epi16(v_temp_lo, 5);

	vpsrlw	ymm0, ymm3, 5

; 194  :   sample0 = _mm256_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb ymm8, ymm0, ymm0

; 186  :   __m256i sample1 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int + 1])));

	vpmovzxbw ymm0, XMMWORD PTR [rcx+rsi+1]
	vpmovzxbw ymm2, XMMWORD PTR [rcx+rsi]

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	movsx	ecx, BYTE PTR tv8588[rbp]

; 187  :   sample1 = _mm256_packus_epi16(sample1, sample1);

	vpackuswb ymm1, ymm0, ymm0
	vpackuswb ymm0, ymm2, ymm2

; 188  :   __m256i pairs_lo = _mm256_unpacklo_epi8(sample0, sample1);

	vpunpcklbw ymm1, ymm0, ymm1

; 191  :   __m256i v_temp_lo = _mm256_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw ymm2, ymm1, ymm10

; 192  :   v_temp_lo = _mm256_add_epi16(v_temp_lo, _mm256_set1_epi16(16));

	vpaddw	ymm3, ymm2, ymm9

; 193  :   v_temp_lo = _mm256_srli_epi16(v_temp_lo, 5);

	vpsrlw	ymm0, ymm3, 5

; 194  :   sample0 = _mm256_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb ymm10, ymm0, ymm0

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	add	ecx, r11d
	movsxd	rcx, ecx

; 186  :   __m256i sample1 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int + 1])));

	vpmovzxbw ymm0, XMMWORD PTR [rcx+rsi+1]
	vpmovzxbw ymm2, XMMWORD PTR [rcx+rsi]

; 187  :   sample1 = _mm256_packus_epi16(sample1, sample1);

	vpackuswb ymm1, ymm0, ymm0
	vpackuswb ymm0, ymm2, ymm2

; 188  :   __m256i pairs_lo = _mm256_unpacklo_epi8(sample0, sample1);

	vpunpcklbw ymm1, ymm0, ymm1

; 191  :   __m256i v_temp_lo = _mm256_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw ymm2, ymm1, ymm11

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	movsx	ecx, BYTE PTR tv8584[rbp]

; 192  :   v_temp_lo = _mm256_add_epi16(v_temp_lo, _mm256_set1_epi16(16));

	vpaddw	ymm3, ymm2, ymm9

; 193  :   v_temp_lo = _mm256_srli_epi16(v_temp_lo, 5);

	vpsrlw	ymm0, ymm3, 5

; 194  :   sample0 = _mm256_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb ymm11, ymm0, ymm0

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	add	ecx, r11d

; 301  :       __m256i row0 = filter_16x1_avx2(ref_main, (y + 1) * sample_disp, x);
; 302  :       __m256i row1 = filter_16x1_avx2(ref_main, (y + 2) * sample_disp, x);
; 303  :       __m256i row2 = filter_16x1_avx2(ref_main, (y + 3) * sample_disp, x);
; 304  :       __m256i row3 = filter_16x1_avx2(ref_main, (y + 4) * sample_disp, x);
; 305  :       __m256i row4 = filter_16x1_avx2(ref_main, (y + 5) * sample_disp, x);
; 306  :       __m256i row5 = filter_16x1_avx2(ref_main, (y + 6) * sample_disp, x);
; 307  :       __m256i row6 = filter_16x1_avx2(ref_main, (y + 7) * sample_disp, x);
; 308  :       __m256i row7 = filter_16x1_avx2(ref_main, (y + 8) * sample_disp, x);
; 309  : 
; 310  :       //Transpose if horizontal mode
; 311  :       if (!vertical_mode) {

	cmp	BYTE PTR vertical_mode$[rsp], 0

; 184  :   __m256i sample0 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int])));

	movsxd	rcx, ecx

; 186  :   __m256i sample1 = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&(ref_main[x + delta_int + 1])));

	vpmovzxbw ymm0, XMMWORD PTR [rcx+rsi+1]
	vpmovzxbw ymm2, XMMWORD PTR [rcx+rsi]

; 187  :   sample1 = _mm256_packus_epi16(sample1, sample1);

	vpackuswb ymm1, ymm0, ymm0
	vpackuswb ymm0, ymm2, ymm2

; 188  :   __m256i pairs_lo = _mm256_unpacklo_epi8(sample0, sample1);

	vpunpcklbw ymm1, ymm0, ymm1

; 191  :   __m256i v_temp_lo = _mm256_maddubs_epi16(pairs_lo, weight);

	vpmaddubsw ymm2, ymm1, ymm12

; 192  :   v_temp_lo = _mm256_add_epi16(v_temp_lo, _mm256_set1_epi16(16));

	vpaddw	ymm3, ymm2, ymm9

; 193  :   v_temp_lo = _mm256_srli_epi16(v_temp_lo, 5);

	vpsrlw	ymm0, ymm3, 5

; 194  :   sample0 = _mm256_packus_epi16(v_temp_lo, v_temp_lo);

	vpackuswb ymm12, ymm0, ymm0

; 301  :       __m256i row0 = filter_16x1_avx2(ref_main, (y + 1) * sample_disp, x);
; 302  :       __m256i row1 = filter_16x1_avx2(ref_main, (y + 2) * sample_disp, x);
; 303  :       __m256i row2 = filter_16x1_avx2(ref_main, (y + 3) * sample_disp, x);
; 304  :       __m256i row3 = filter_16x1_avx2(ref_main, (y + 4) * sample_disp, x);
; 305  :       __m256i row4 = filter_16x1_avx2(ref_main, (y + 5) * sample_disp, x);
; 306  :       __m256i row5 = filter_16x1_avx2(ref_main, (y + 6) * sample_disp, x);
; 307  :       __m256i row6 = filter_16x1_avx2(ref_main, (y + 7) * sample_disp, x);
; 308  :       __m256i row7 = filter_16x1_avx2(ref_main, (y + 8) * sample_disp, x);
; 309  : 
; 310  :       //Transpose if horizontal mode
; 311  :       if (!vertical_mode) {

	jne	$LN8@filter_NxN

; 312  :         __m256i q0 = _mm256_unpacklo_epi8(row0, row1);
; 313  :         __m256i q1 = _mm256_unpacklo_epi8(row2, row3);
; 314  :         __m256i q2 = _mm256_unpacklo_epi8(row4, row5);
; 315  :         __m256i q3 = _mm256_unpacklo_epi8(row6, row7);
; 316  : 
; 317  :         __m256i h0 = _mm256_unpacklo_epi16(q0, q1);
; 318  :         __m256i h1 = _mm256_unpacklo_epi16(q2, q3);
; 319  :         __m256i h2 = _mm256_unpackhi_epi16(q0, q1);
; 320  :         __m256i h3 = _mm256_unpackhi_epi16(q2, q3);
; 321  : 
; 322  :         __m256i temp0 = _mm256_unpacklo_epi32(h0, h1);
; 323  :         __m256i temp1 = _mm256_unpackhi_epi32(h0, h1);
; 324  :         __m256i temp2 = _mm256_unpacklo_epi32(h2, h3);
; 325  :         __m256i temp3 = _mm256_unpackhi_epi32(h2, h3);
; 326  : 
; 327  :         row0 = _mm256_unpacklo_epi64(temp0, temp0);
; 328  :         row1 = _mm256_unpackhi_epi64(temp0, temp0);
; 329  :         row2 = _mm256_unpacklo_epi64(temp1, temp1);
; 330  :         row3 = _mm256_unpackhi_epi64(temp1, temp1);
; 331  :         row4 = _mm256_unpacklo_epi64(temp2, temp2);
; 332  :         row5 = _mm256_unpackhi_epi64(temp2, temp2);
; 333  :         row6 = _mm256_unpacklo_epi64(temp3, temp3);
; 334  :         row7 = _mm256_unpackhi_epi64(temp3, temp3);
; 335  : 
; 336  :         //x and y must be flipped due to transpose
; 337  :         int rx = y;
; 338  :         int ry = x;
; 339  : 
; 340  :         *(int64_t*)(dst + (ry + 0) * width + rx) = _mm_cvtsi128_si64(_mm256_castsi256_si128(row0));

	mov	ecx, DWORD PTR tv7837[rbp]
	add	ecx, edi
	movsxd	rdx, ecx

; 341  :         *(int64_t*)(dst + (ry + 1) * width + rx) = _mm_cvtsi128_si64(_mm256_castsi256_si128(row1));

	mov	ecx, edi
	add	rdx, r13
	sub	ecx, ebx
	vpunpcklbw ymm5, ymm4, ymm5
	vpunpcklbw ymm2, ymm6, ymm7
	vpunpcklwd ymm4, ymm5, ymm2
	vpunpckhwd ymm5, ymm5, ymm2
	vpunpcklbw ymm3, ymm8, ymm10
	vpunpcklbw ymm0, ymm11, ymm12
	vpunpckhwd ymm2, ymm3, ymm0
	vpunpcklwd ymm1, ymm3, ymm0
	vpunpckldq ymm0, ymm4, ymm1
	vpunpckhdq ymm3, ymm4, ymm1
	vpunpckldq ymm1, ymm5, ymm2
	vpunpckhdq ymm4, ymm5, ymm2
	vpunpcklqdq ymm2, ymm0, ymm0
	vmovq	QWORD PTR [rdx+r10], xmm2
	movsxd	rdx, ecx
	add	rdx, r13

; 342  :         *(int64_t*)(dst + (ry + 2) * width + rx) = _mm_cvtsi128_si64(_mm256_castsi256_si128(row2));

	movsxd	rcx, edi
	add	rcx, r13
	vpunpckhqdq ymm5, ymm0, ymm0
	vpunpcklqdq ymm6, ymm3, ymm3
	vmovq	QWORD PTR [rdx+r10], xmm5
	vmovq	QWORD PTR [rcx+r10], xmm6

; 343  :         *(int64_t*)(dst + (ry + 3) * width + rx) = _mm_cvtsi128_si64(_mm256_castsi256_si128(row3));

	lea	ecx, DWORD PTR [rdi+rbx]
	movsxd	rdx, ecx

; 344  :         *(int64_t*)(dst + (ry + 4) * width + rx) = _mm_cvtsi128_si64(_mm256_castsi256_si128(row4));

	lea	ecx, DWORD PTR [rbx+rbx]
	add	rdx, r13
	add	ecx, edi
	vpunpckhqdq ymm3, ymm3, ymm3
	vpunpcklqdq ymm7, ymm1, ymm1
	vpunpcklqdq ymm8, ymm4, ymm4
	vmovq	QWORD PTR [rdx+r10], xmm3
	movsxd	rdx, ecx

; 345  :         *(int64_t*)(dst + (ry + 5) * width + rx) = _mm_cvtsi128_si64(_mm256_castsi256_si128(row5));

	lea	ecx, DWORD PTR [rbx+rbx*2]
	add	rdx, r13
	add	ecx, edi

; 346  :         *(int64_t*)(dst + (ry + 6) * width + rx) = _mm_cvtsi128_si64(_mm256_castsi256_si128(row6));
; 347  :         *(int64_t*)(dst + (ry + 7) * width + rx) = _mm_cvtsi128_si64(_mm256_castsi256_si128(row7));
; 348  : 
; 349  :         *(int64_t*)(dst + (ry + 8) * width + rx) = _mm_cvtsi128_si64(_mm256_extracti128_si256(row0, 1));

	vextracti128 xmm0, ymm2, 1
	vpunpckhqdq ymm1, ymm1, ymm1
	vpunpckhqdq ymm4, ymm4, ymm4
	vmovq	QWORD PTR [rdx+r10], xmm7
	movsxd	rdx, ecx
	lea	ecx, DWORD PTR [rbx*4]
	add	rdx, r13
	add	ecx, edi
	vmovq	QWORD PTR [rdx+r10], xmm1
	movsxd	rdx, ecx
	lea	ecx, DWORD PTR [rbx+rbx*4]
	add	rdx, r13
	add	ecx, edi
	vmovq	QWORD PTR [rdx+r10], xmm8
	movsxd	rdx, ecx
	mov	ecx, DWORD PTR tv7845[rbp]
	add	rdx, r13
	add	ecx, edi
	vmovq	QWORD PTR [rdx+r10], xmm4
	movsxd	rdx, ecx

; 350  :         *(int64_t*)(dst + (ry + 9) * width + rx) = _mm_cvtsi128_si64(_mm256_extracti128_si256(row1, 1));

	mov	ecx, DWORD PTR tv7844[rbp]
	add	rdx, r13
	add	ecx, edi
	vmovq	QWORD PTR [rdx+r10], xmm0
	movsxd	rdx, ecx

; 351  :         *(int64_t*)(dst + (ry + 10) * width + rx) = _mm_cvtsi128_si64(_mm256_extracti128_si256(row2, 1));

	lea	ecx, DWORD PTR [rbx*8]
	add	rdx, r13
	add	ecx, edi
	vextracti128 xmm0, ymm5, 1
	vmovq	QWORD PTR [rdx+r10], xmm0
	movsxd	rdx, ecx

; 352  :         *(int64_t*)(dst + (ry + 11) * width + rx) = _mm_cvtsi128_si64(_mm256_extracti128_si256(row3, 1));

	lea	ecx, DWORD PTR [rbx+rbx*8]
	add	rdx, r13
	add	ecx, edi
	vextracti128 xmm0, ymm6, 1
	vmovq	QWORD PTR [rdx+r10], xmm0
	movsxd	rdx, ecx
	add	rdx, r13
	vextracti128 xmm0, ymm3, 1
	vmovq	QWORD PTR [rdx+r10], xmm0

; 353  :         *(int64_t*)(dst + (ry + 12) * width + rx) = _mm_cvtsi128_si64(_mm256_extracti128_si256(row4, 1));

	mov	ecx, DWORD PTR tv7841[rbp]
	add	ecx, edi
	movsxd	rdx, ecx

; 354  :         *(int64_t*)(dst + (ry + 13) * width + rx) = _mm_cvtsi128_si64(_mm256_extracti128_si256(row5, 1));

	mov	ecx, DWORD PTR tv7840[rbp]
	add	rdx, r13
	add	ecx, edi
	vextracti128 xmm0, ymm7, 1
	vmovq	QWORD PTR [rdx+r10], xmm0
	movsxd	rdx, ecx

; 355  :         *(int64_t*)(dst + (ry + 14) * width + rx) = _mm_cvtsi128_si64(_mm256_extracti128_si256(row6, 1));

	mov	ecx, DWORD PTR tv7839[rbp]
	add	rdx, r13
	add	ecx, edi
	vextracti128 xmm0, ymm1, 1
	vmovq	QWORD PTR [rdx+r10], xmm0
	movsxd	rdx, ecx

; 356  :         *(int64_t*)(dst + (ry + 15) * width + rx) = _mm_cvtsi128_si64(_mm256_extracti128_si256(row7, 1));

	mov	ecx, DWORD PTR tv7838[rbp]
	add	rdx, r13
	vextracti128 xmm0, ymm8, 1
	add	ecx, edi
	vmovq	QWORD PTR [rdx+r10], xmm0
	movsxd	rdx, ecx
	add	rdx, r13
	vextracti128 xmm0, ymm4, 1
	vmovq	QWORD PTR [rdx+r10], xmm0

; 357  :       } else {

	jmp	$LN5@filter_NxN
$LN8@filter_NxN:

; 358  : 
; 359  :         //Move all filtered pixels to the lower lane to reduce memory accesses
; 360  :         row0 = _mm256_permute4x64_epi64(row0, _MM_SHUFFLE(3,1,2,0));

	vpermq	ymm0, ymm4, 216				; 000000d8H

; 361  :         row1 = _mm256_permute4x64_epi64(row1, _MM_SHUFFLE(2,0,3,1));
; 362  :         row2 = _mm256_permute4x64_epi64(row2, _MM_SHUFFLE(3,1,2,0));
; 363  :         row3 = _mm256_permute4x64_epi64(row3, _MM_SHUFFLE(2,0,3,1));

	vpermq	ymm1, ymm7, 141				; 0000008dH
	mov	rcx, rax
	sub	rcx, r15

; 364  :         row4 = _mm256_permute4x64_epi64(row4, _MM_SHUFFLE(3,1,2,0));
; 365  :         row5 = _mm256_permute4x64_epi64(row5, _MM_SHUFFLE(2,0,3,1));
; 366  :         row6 = _mm256_permute4x64_epi64(row6, _MM_SHUFFLE(3,1,2,0));
; 367  :         row7 = _mm256_permute4x64_epi64(row7, _MM_SHUFFLE(2,0,3,1));
; 368  : 
; 369  :         _mm_storeu_si128((__m128i*)(dst + (y + 0) * width + x), _mm256_castsi256_si128(row0));

	add	rcx, r9
	vmovdqu	XMMWORD PTR [rcx+r10], xmm0
	vpermq	ymm0, ymm5, 141				; 0000008dH

; 370  :         _mm_storeu_si128((__m128i*)(dst + (y + 1) * width + x), _mm256_castsi256_si128(row1));

	lea	rcx, QWORD PTR [r14+r9]
	add	rcx, rax
	vmovdqu	XMMWORD PTR [rcx+r10], xmm0
	vpermq	ymm0, ymm6, 216				; 000000d8H

; 371  :         _mm_storeu_si128((__m128i*)(dst + (y + 2) * width + x), _mm256_castsi256_si128(row2));

	lea	rcx, QWORD PTR [r9+rax]
	vmovdqu	XMMWORD PTR [rcx+r10], xmm0

; 372  :         _mm_storeu_si128((__m128i*)(dst + (y + 3) * width + x), _mm256_castsi256_si128(row3));

	mov	rcx, QWORD PTR tv7790[rbp]
	add	rcx, r9
	add	rcx, rax
	vpermq	ymm0, ymm8, 216				; 000000d8H
	vmovdqu	XMMWORD PTR [rcx+r10], xmm1

; 373  :         _mm_storeu_si128((__m128i*)(dst + (y + 4) * width + x), _mm256_castsi256_si128(row4));

	mov	rcx, QWORD PTR tv7789[rbp]
	add	rcx, r9
	add	rcx, rax
	vpermq	ymm1, ymm10, 141			; 0000008dH
	vmovdqu	XMMWORD PTR [rcx+r10], xmm0

; 374  :         _mm_storeu_si128((__m128i*)(dst + (y + 5) * width + x), _mm256_castsi256_si128(row5));

	mov	rcx, QWORD PTR tv7788[rbp]
	add	rcx, r9
	add	rcx, rax
	vpermq	ymm0, ymm11, 216			; 000000d8H
	vmovdqu	XMMWORD PTR [rcx+r10], xmm1

; 375  :         _mm_storeu_si128((__m128i*)(dst + (y + 6) * width + x), _mm256_castsi256_si128(row6));

	mov	rcx, QWORD PTR tv7787[rbp]
	add	rcx, r9
	add	rcx, rax
	vpermq	ymm1, ymm12, 141			; 0000008dH
	vmovdqu	XMMWORD PTR [rcx+r10], xmm0

; 376  :         _mm_storeu_si128((__m128i*)(dst + (y + 7) * width + x), _mm256_castsi256_si128(row7));

	mov	rcx, QWORD PTR tv7786[rbp]
	add	rcx, r9
	add	rcx, rax
	vmovdqu	XMMWORD PTR [rcx+r10], xmm1
$LN5@filter_NxN:

; 300  :     for (int x = 0; x < width; x += 16) {

	vmovdqu	ymm7, YMMWORD PTR tv7820[rbp]
	vmovdqu	ymm8, YMMWORD PTR tv7816[rbp]
	vmovdqu	ymm10, YMMWORD PTR tv7812[rbp]
	vmovdqu	ymm11, YMMWORD PTR tv7808[rbp]
	vmovdqu	ymm12, YMMWORD PTR tv7804[rbp]
	add	r11d, 16
	add	edi, r12d
	add	rax, 16
	cmp	r11d, ebx
	jl	$LL7@filter_NxN

; 299  :   for (int y = 0; y < width; y += 8) {

	mov	r14d, DWORD PTR y$1$[rbp]
	lea	eax, DWORD PTR [rbx*8]
	mov	edi, DWORD PTR sample_disp$[rsp]
	add	r14d, 8
	cdqe
	add	r13, 8
	add	r9, rax
	mov	DWORD PTR y$1$[rbp], r14d
	cmp	r14d, ebx
	jl	$LL4@filter_NxN
$LN3@filter_NxN:
	vzeroupper

; 377  :       }
; 378  :     }
; 379  :   }
; 380  : }

	lea	r11, QWORD PTR [rsp+448]
	mov	rbx, QWORD PTR [r11+64]
	vmovaps	xmm6, XMMWORD PTR [r11-16]
	vmovaps	xmm7, XMMWORD PTR [r11-32]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm9, XMMWORD PTR [r11-64]
	vmovaps	xmm10, XMMWORD PTR [r11-80]
	vmovaps	xmm11, XMMWORD PTR [r11-96]
	vmovaps	xmm12, XMMWORD PTR [r11-112]
	vmovaps	xmm13, XMMWORD PTR [r11-128]
	vmovaps	xmm14, XMMWORD PTR [r11-144]
	vmovaps	xmm15, XMMWORD PTR [r11-160]
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r13
	pop	r12
	pop	rdi
	pop	rsi
	pop	rbp
	ret	0
filter_NxN_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\intra-avx2.c
_TEXT	SEGMENT
tmp_ref$ = 48
log2_width$ = 160
intra_mode$ = 168
in_ref_above$ = 176
in_ref_left$ = 184
dst$ = 192
kvz_angular_pred_avx2 PROC

; 396  : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rbp
	mov	QWORD PTR [rsp+24], rsi
	push	rdi
	push	r12
	push	r13
	push	r14
	push	r15
	sub	rsp, 112				; 00000070H
	movzx	esi, cl
	mov	rbx, r9
	mov	rbp, r8
	movzx	edi, dl

; 397  :   assert(log2_width >= 2 && log2_width <= 5);

	lea	eax, DWORD PTR [rsi-2]
	cmp	al, 3
	jbe	SHORT $LN18@kvz_angula
	mov	r8d, 397				; 0000018dH
	lea	rdx, OFFSET FLAT:$SG4294947231
	lea	rcx, OFFSET FLAT:$SG4294947230
	call	QWORD PTR __imp__wassert
$LN18@kvz_angula:

; 398  :   assert(intra_mode >= 2 && intra_mode <= 34);

	lea	eax, DWORD PTR [rdi-2]
	cmp	al, 32					; 00000020H
	jbe	SHORT $LN20@kvz_angula
	mov	r8d, 398				; 0000018eH
	lea	rdx, OFFSET FLAT:$SG4294947229
	lea	rcx, OFFSET FLAT:$SG4294947228
	call	QWORD PTR __imp__wassert
$LN20@kvz_angula:

; 399  : 
; 400  :   static const int8_t modedisp2sampledisp[9] = { 0, 2, 5, 9, 13, 17, 21, 26, 32 };
; 401  :   static const int16_t modedisp2invsampledisp[9] = { 0, 4096, 1638, 910, 630, 482, 390, 315, 256 }; // (256 * 32) / sampledisp
; 402  : 
; 403  :                                                     // Temporary buffer for modes 11-25.
; 404  :                                                     // It only needs to be big enough to hold indices from -width to width-1.
; 405  :   uint8_t tmp_ref[2 * 32];
; 406  :   const int_fast8_t width = 1 << log2_width;

	movzx	ecx, sil

; 407  : 
; 408  :   // Whether to swap references to always project on the left reference row.
; 409  :   const bool vertical_mode = intra_mode >= 18;
; 410  :   // Modes distance to horizontal or vertical mode.
; 411  :   const int_fast8_t mode_disp = vertical_mode ? intra_mode - 26 : 10 - intra_mode;
; 412  :   // Sample displacement per column in fractions of 32.
; 413  :   const int_fast8_t sample_disp = (mode_disp < 0 ? -1 : 1) * modedisp2sampledisp[abs(mode_disp)];

	lea	eax, DWORD PTR [rdi-26]
	movzx	r8d, al
	lea	r13, OFFSET FLAT:__ImageBase
	mov	r9d, 1
	shl	r9b, cl
	mov	ecx, 10
	cmp	dil, 18
	setge	r14b
	sub	cl, dil
	cmp	dil, 18
	movzx	eax, cl
	cmovl	r8d, eax
	movsx	eax, r8b
	cdq
	sar	r8b, 7
	xor	eax, edx
	and	r8b, -2
	sub	eax, edx
	inc	r8b
	movsxd	r12, eax
	movsx	r15d, r8b
	movsx	eax, BYTE PTR ?modedisp2sampledisp@?1??kvz_angular_pred_avx2@@9@9[r12+r13]
	imul	r15d, eax

; 414  : 
; 415  :   // Pointer for the reference we are interpolating from.
; 416  :   const uint8_t *ref_main;
; 417  :   // Pointer for the other reference.
; 418  :   const uint8_t *ref_side;
; 419  : 
; 420  :   // Set ref_main and ref_side such that, when indexed with 0, they point to
; 421  :   // index 0 in block coordinates.
; 422  :   if (sample_disp < 0) {

	test	r15b, r15b
	jns	$LN10@kvz_angula

; 423  :     // Negative sample_disp means, we need to use both references.
; 424  : 
; 425  :     ref_side = (vertical_mode ? in_ref_left : in_ref_above) + 1;

	cmp	dil, 18

; 426  :     ref_main = (vertical_mode ? in_ref_above : in_ref_left) + 1;
; 427  : 
; 428  :     // Move the reference pixels to start from the middle to the later half of
; 429  :     // the tmp_ref, so there is room for negative indices.
; 430  :     for (int_fast8_t x = -1; x < width; ++x) {

	movsx	r11d, r9b
	mov	rsi, rbp
	mov	dl, -1
	cmovge	rsi, rbx
	cmovge	rbx, rbp
	cmp	r9b, dl
	jle	SHORT $LN29@kvz_angula
	npad	5
$LL4@kvz_angula:

; 431  :       tmp_ref[x + width] = ref_main[x];

	movsx	eax, dl
	lea	rbx, QWORD PTR [rbx+1]
	add	eax, r11d
	inc	dl
	movsxd	rcx, eax
	movzx	eax, BYTE PTR [rbx-1]
	mov	BYTE PTR tmp_ref$[rsp+rcx], al
	cmp	dl, r9b
	jl	SHORT $LL4@kvz_angula
$LN29@kvz_angula:

; 432  :     }
; 433  :     // Get a pointer to block index 0 in tmp_ref.
; 434  :     ref_main = tmp_ref + width;
; 435  : 
; 436  :     // Extend the side reference to the negative indices of main reference.
; 437  :     int_fast32_t col_sample_disp = 128; // rounding for the ">> 8"
; 438  :     int_fast16_t inv_abs_sample_disp = modedisp2invsampledisp[abs(mode_disp)];

	movsx	ebp, WORD PTR ?modedisp2invsampledisp@?1??kvz_angular_pred_avx2@@9@9[r13+r12*2]
	lea	rbx, QWORD PTR tmp_ref$[rsp]

; 439  :     int_fast8_t most_negative_index = (width * sample_disp) >> 5;

	movsx	edi, r15b

; 440  :     for (int_fast8_t x = -2; x >= most_negative_index; --x) {

	mov	r8b, -2
	imul	edi, r11d
	mov	r10d, 128				; 00000080H
	movsx	rax, r9b
	add	rbx, rax
	sar	edi, 5
	cmp	dil, r8b
	jg	SHORT $LN11@kvz_angula
	npad	6
$LL7@kvz_angula:

; 441  :       col_sample_disp += inv_abs_sample_disp;

	add	r10d, ebp

; 442  :       int_fast8_t side_index = col_sample_disp >> 8;

	mov	eax, r10d
	sar	eax, 8

; 443  :       tmp_ref[x + width] = ref_side[side_index - 1];

	movsx	rdx, al
	movsx	eax, r8b
	dec	r8b
	add	eax, r11d
	movsxd	rcx, eax
	movzx	eax, BYTE PTR [rdx+rsi]
	mov	BYTE PTR tmp_ref$[rsp+rcx], al
	cmp	r8b, dil
	jge	SHORT $LL7@kvz_angula

; 444  :     }
; 445  :   }

	jmp	SHORT $LN11@kvz_angula
$LN10@kvz_angula:

; 446  :   else {
; 447  :     // sample_disp >= 0 means we don't need to refer to negative indices,
; 448  :     // which means we can just use the references as is.
; 449  :     ref_main = (vertical_mode ? in_ref_above : in_ref_left) + 1;

	cmp	dil, 18
	cmovge	rbx, rbp
	inc	rbx
$LN11@kvz_angula:

; 450  :     ref_side = (vertical_mode ? in_ref_left : in_ref_above) + 1;
; 451  :   }
; 452  : 
; 453  : 
; 454  :   // The mode is not horizontal or vertical, we have to do interpolation.
; 455  :   switch (width) {

	mov	rcx, QWORD PTR dst$[rsp]
	mov	rdx, rbx
	movsx	r8d, r15b
	cmp	r9b, 4
	je	SHORT $LN12@kvz_angula
	cmp	r9b, 8
	je	SHORT $LN13@kvz_angula
	cmp	r9b, 16
	je	SHORT $LN14@kvz_angula

; 465  :     default:
; 466  :       filter_NxN_avx2(dst, ref_main, sample_disp, vertical_mode, width);

	movsx	eax, r9b
	movzx	r9d, r14b
	mov	DWORD PTR [rsp+32], eax
	call	filter_NxN_avx2
	jmp	SHORT $LN8@kvz_angula
$LN14@kvz_angula:

; 462  :     case 16:
; 463  :       filter_16x16_avx2(dst, ref_main, sample_disp, vertical_mode);

	movzx	r9d, r14b
	call	filter_16x16_avx2

; 464  :       break;

	jmp	SHORT $LN8@kvz_angula
$LN13@kvz_angula:

; 458  :       break;
; 459  :     case 8:
; 460  :       filter_8x8_avx2(dst, ref_main, sample_disp, vertical_mode);

	movzx	r9d, r14b
	call	filter_8x8_avx2

; 461  :       break;

	jmp	SHORT $LN8@kvz_angula
$LN12@kvz_angula:

; 456  :     case 4:
; 457  :       filter_4x4_avx2(dst, ref_main, sample_disp, vertical_mode);

	movzx	r9d, r14b
	call	filter_4x4_avx2
$LN8@kvz_angula:

; 467  :       break;
; 468  :   }
; 469  : }

	lea	r11, QWORD PTR [rsp+112]
	mov	rbx, QWORD PTR [r11+48]
	mov	rbp, QWORD PTR [r11+56]
	mov	rsi, QWORD PTR [r11+64]
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r13
	pop	r12
	pop	rdi
	ret	0
kvz_angular_pred_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\intra-avx2.c
_TEXT	SEGMENT
tv872 = 208
log2_width$ = 208
ref_top$ = 216
ref_left$ = 224
dst$ = 232
kvz_intra_pred_planar_avx2 PROC

; 483  : {

	mov	QWORD PTR [rsp+24], rbx
	mov	QWORD PTR [rsp+32], rbp
	push	rsi
	push	rdi
	push	r12
	push	r13
	push	r15
	sub	rsp, 160				; 000000a0H
	mov	QWORD PTR [rsp+216], r14
	mov	rdi, r9
	movsx	r14d, cl
	mov	r15, r8
	mov	rbx, rdx

; 484  :   assert(log2_width >= 2 && log2_width <= 5);

	lea	eax, DWORD PTR [r14-2]
	cmp	al, 3
	jbe	SHORT $LN12@kvz_intra_
	mov	r8d, 484				; 000001e4H
	lea	rdx, OFFSET FLAT:$SG4294947227
	lea	rcx, OFFSET FLAT:$SG4294947226
	call	QWORD PTR __imp__wassert
$LN12@kvz_intra_:

; 485  : 
; 486  :   const int_fast8_t width = 1 << log2_width;

	vmovaps	XMMWORD PTR [rsp+144], xmm6
	mov	ecx, r14d
	vmovaps	XMMWORD PTR [rsp+128], xmm7
	mov	ebp, 1
	vmovaps	XMMWORD PTR [rsp+112], xmm8
	mov	edx, r14d
	vmovaps	XMMWORD PTR [rsp+96], xmm9
	vmovaps	XMMWORD PTR [rsp+80], xmm10
	shl	bpl, cl

; 487  :   const uint8_t top_right = ref_top[width + 1];

	movsx	rsi, bpl

; 488  :   const uint8_t bottom_left = ref_left[width + 1];
; 489  : 
; 490  :   if (log2_width > 2) {

	cmp	r14b, 2
	vmovaps	XMMWORD PTR [rsp+64], xmm11
	vmovaps	XMMWORD PTR [rsp+48], xmm12
	mov	r14, QWORD PTR [rsp+216]
	movsx	r12d, BYTE PTR [rsi+rbx+1]
	movsx	r13d, BYTE PTR [rsi+r15+1]
	jle	$LN8@kvz_intra_

; 491  :     
; 492  :     __m128i v_width = _mm_set1_epi16(width);

	movsx	eax, bpl

; 493  :     __m128i v_top_right = _mm_set1_epi16(top_right);
; 494  :     __m128i v_bottom_left = _mm_set1_epi16(bottom_left);
; 495  : 
; 496  :     for (int y = 0; y < width; ++y) {

	xor	r8d, r8d
	vmovd	xmm6, eax
	movzx	eax, r12b
	vmovd	xmm11, eax
	movzx	eax, r13b
	vmovd	xmm12, eax
	vpbroadcastw xmm12, xmm12
	vpbroadcastw xmm6, xmm6
	vpbroadcastw xmm11, xmm11
	test	bpl, bpl
	jle	$LN9@kvz_intra_

; 491  :     
; 492  :     __m128i v_width = _mm_set1_epi16(width);

	sub	rbx, rdi
	vmovaps	XMMWORD PTR [rsp+32], xmm13
	vmovdqu	xmm13, XMMWORD PTR __xmm@0000fffffffefffdfffcfffbfffafff9
	inc	rbx
	lea	r9, QWORD PTR [r15+1]
	npad	5
$LL4@kvz_intra_:

; 497  : 
; 498  :       __m128i x_plus_1 = _mm_setr_epi16(-7, -6, -5, -4, -3, -2, -1, 0);
; 499  :       __m128i v_ref_left = _mm_set1_epi16(ref_left[y + 1]);

	movzx	eax, BYTE PTR [r9]
	lea	r10d, DWORD PTR [rdx+1]
	vmovdqu	xmm8, XMMWORD PTR __xmm@00080008000800080008000800080008
	vmovd	xmm7, eax

; 500  :       __m128i y_plus_1 = _mm_set1_epi16(y + 1);

	lea	eax, DWORD PTR [r8+1]
	cwde
	lea	edx, DWORD PTR [rsi-1]
	vmovd	xmm0, eax
	vpbroadcastw xmm0, xmm0
	shr	edx, 3
	mov	rax, rdi
	vpsubw	xmm9, xmm6, xmm0
	vpmullw	xmm10, xmm0, xmm12
	vmovdqu	xmm5, xmm13
	vpbroadcastw xmm7, xmm7
	inc	edx
$LL7@kvz_intra_:

; 501  : 
; 502  :       for (int x = 0; x < width; x += 8) {
; 503  :         x_plus_1 = _mm_add_epi16(x_plus_1, _mm_set1_epi16(8));
; 504  :         __m128i v_ref_top = _mm_loadl_epi64((__m128i*)&(ref_top[x + 1]));

	vmovq	xmm0, QWORD PTR [rbx+rax]
	lea	rax, QWORD PTR [rax+8]

; 505  :         v_ref_top = _mm_cvtepu8_epi16(v_ref_top);

	vpmovzxbw xmm1, xmm0

; 506  : 
; 507  :         __m128i hor = _mm_add_epi16(_mm_mullo_epi16(_mm_sub_epi16(v_width, x_plus_1), v_ref_left), _mm_mullo_epi16(x_plus_1, v_top_right));
; 508  :         __m128i ver = _mm_add_epi16(_mm_mullo_epi16(_mm_sub_epi16(v_width, y_plus_1), v_ref_top), _mm_mullo_epi16(y_plus_1, v_bottom_left));

	vpmullw	xmm2, xmm1, xmm9
	vpaddw	xmm4, xmm2, xmm10
	vpaddw	xmm5, xmm8, xmm5
	vpsubw	xmm0, xmm6, xmm5
	vpmullw	xmm3, xmm0, xmm7
	vpmullw	xmm1, xmm5, xmm11
	vpaddw	xmm2, xmm3, xmm1

; 509  : 
; 510  :         //dst[y * width + x] = ho
; 511  : 
; 512  :         __m128i chunk = _mm_srli_epi16(_mm_add_epi16(_mm_add_epi16(ver, hor), v_width), (log2_width + 1));

	vpaddw	xmm0, xmm4, xmm2
	vpaddw	xmm4, xmm0, xmm6
	vmovd	xmm1, r10d
	vpsrlw	xmm2, xmm4, xmm1

; 513  :         chunk = _mm_packus_epi16(chunk, chunk);

	vpackuswb xmm0, xmm2, xmm2

; 514  :         _mm_storel_epi64((__m128i*)&(dst[y * width + x]), chunk);

	vmovq	QWORD PTR [rax-8], xmm0
	sub	rdx, 1
	jne	SHORT $LL7@kvz_intra_

; 493  :     __m128i v_top_right = _mm_set1_epi16(top_right);
; 494  :     __m128i v_bottom_left = _mm_set1_epi16(bottom_left);
; 495  : 
; 496  :     for (int y = 0; y < width; ++y) {

	inc	r8d
	inc	r9
	add	rdi, rsi
	sub	rbx, rsi
	mov	edx, ecx
	cmp	r8d, esi
	jl	$LL4@kvz_intra_

; 515  :       }
; 516  :     }
; 517  :   } else {

	vmovaps	xmm13, XMMWORD PTR [rsp+32]
	jmp	$LN9@kvz_intra_
$LN8@kvz_intra_:

; 518  :     // Only if log2_width == 2 <=> width == 4
; 519  :     assert(width == 4);

	cmp	bpl, 4
	je	SHORT $LN13@kvz_intra_
	mov	r8d, 519				; 00000207H
	lea	rdx, OFFSET FLAT:$SG4294947225
	lea	rcx, OFFSET FLAT:$SG4294947224
	call	QWORD PTR __imp__wassert
$LN13@kvz_intra_:

; 520  :     const __m128i rl_shufmask = _mm_setr_epi32(0x04040404, 0x05050505,

	vmovdqu	xmm1, XMMWORD PTR __xmm@07070707060606060505050504040404

; 521  :                                                0x06060606, 0x07070707);
; 522  : 
; 523  :     const __m128i xp1   = _mm_set1_epi32  (0x04030201);

	vmovdqu	xmm11, XMMWORD PTR __xmm@04030201040302010403020104030201

; 524  :     const __m128i yp1   = _mm_shuffle_epi8(xp1,   rl_shufmask);
; 525  : 
; 526  :     const __m128i rdist = _mm_set1_epi32  (0x00010203);

	vmovdqu	xmm12, XMMWORD PTR __xmm@00010203000102030001020300010203

; 527  :     const __m128i bdist = _mm_shuffle_epi8(rdist, rl_shufmask);
; 528  : 
; 529  :     const __m128i wid16 = _mm_set1_epi16  (width);
; 530  :     const __m128i tr    = _mm_set1_epi8   (top_right);
; 531  :     const __m128i bl    = _mm_set1_epi8   (bottom_left);
; 532  : 
; 533  :     uint32_t rt14    = *(const uint32_t *)(ref_top  + 1);
; 534  :     uint32_t rl14    = *(const uint32_t *)(ref_left + 1);
; 535  :     uint64_t rt14_64 = (uint64_t)rt14;
; 536  :     uint64_t rl14_64 = (uint64_t)rl14;

	mov	ecx, DWORD PTR [r15+1]
	vpshufb	xmm9, xmm11, xmm1
	vpshufb	xmm10, xmm12, xmm1
	movsx	eax, bpl
	vmovd	xmm8, eax
	mov	eax, DWORD PTR [rbx+1]
	vpbroadcastw xmm8, xmm8

; 537  :     uint64_t rtl14   = rt14_64 | (rl14_64 << 32);

	shl	rcx, 32					; 00000020H
	or	rcx, rax

; 538  : 
; 539  :     __m128i rtl_v    = _mm_cvtsi64_si128   (rtl14);

	vmovq	xmm0, rcx

; 540  :     __m128i rt       = _mm_broadcastd_epi32(rtl_v);
; 541  :     __m128i rl       = _mm_shuffle_epi8    (rtl_v,    rl_shufmask);

	vpshufb	xmm6, xmm0, xmm1
	vpbroadcastd xmm7, xmm0

; 542  : 
; 543  :     __m128i rtrl_l   = _mm_unpacklo_epi8   (rt,       rl);

	vpunpcklbw xmm2, xmm7, xmm6
	vmovd	xmm1, r12d
	vpbroadcastb xmm1, xmm1
	vmovd	xmm0, r13d
	vpbroadcastb xmm0, xmm0

; 544  :     __m128i rtrl_h   = _mm_unpackhi_epi8   (rt,       rl);
; 545  : 
; 546  :     __m128i bdrd_l   = _mm_unpacklo_epi8   (bdist,    rdist);
; 547  :     __m128i bdrd_h   = _mm_unpackhi_epi8   (bdist,    rdist);
; 548  : 
; 549  :     __m128i hvs_lo   = _mm_maddubs_epi16   (rtrl_l,   bdrd_l);
; 550  :     __m128i hvs_hi   = _mm_maddubs_epi16   (rtrl_h,   bdrd_h);
; 551  : 
; 552  :     __m128i xp1yp1_l = _mm_unpacklo_epi8   (xp1,      yp1);
; 553  :     __m128i xp1yp1_h = _mm_unpackhi_epi8   (xp1,      yp1);
; 554  :     __m128i trbl_lh  = _mm_unpacklo_epi8   (tr,       bl);

	vpunpcklbw xmm5, xmm1, xmm0
	vpunpcklbw xmm0, xmm11, xmm9

; 555  : 
; 556  :     __m128i addend_l = _mm_maddubs_epi16   (trbl_lh,  xp1yp1_l);

	vpmaddubsw xmm1, xmm5, xmm0

; 557  :     __m128i addend_h = _mm_maddubs_epi16   (trbl_lh,  xp1yp1_h);
; 558  : 
; 559  :             addend_l = _mm_add_epi16       (addend_l, wid16);

	vpaddw	xmm3, xmm1, xmm8
	vpunpcklbw xmm0, xmm10, xmm12
	vpmaddubsw xmm1, xmm2, xmm0

; 560  :             addend_h = _mm_add_epi16       (addend_h, wid16);
; 561  : 
; 562  :     __m128i sum_l    = _mm_add_epi16       (hvs_lo,   addend_l);

	vpaddw	xmm2, xmm3, xmm1

; 563  :     __m128i sum_h    = _mm_add_epi16       (hvs_hi,   addend_h);
; 564  : 
; 565  :     // Shift right by log2_width + 1
; 566  :     __m128i sum_l_t  = _mm_srli_epi16      (sum_l,    3);

	vpsrlw	xmm4, xmm2, 3
	vpunpckhbw xmm0, xmm11, xmm9
	vpmaddubsw xmm1, xmm5, xmm0
	vpaddw	xmm3, xmm1, xmm8
	vpunpckhbw xmm2, xmm7, xmm6
	vpunpckhbw xmm0, xmm10, xmm12
	vpmaddubsw xmm1, xmm2, xmm0
	vpaddw	xmm2, xmm3, xmm1

; 567  :     __m128i sum_h_t  = _mm_srli_epi16      (sum_h,    3);

	vpsrlw	xmm3, xmm2, 3

; 568  :     __m128i result   = _mm_packus_epi16    (sum_l_t,  sum_h_t);

	vpackuswb xmm0, xmm4, xmm3

; 569  :     _mm_storeu_si128((__m128i *)dst, result);

	vmovdqu	XMMWORD PTR [rdi], xmm0
$LN9@kvz_intra_:

; 570  :   }
; 571  : }

	vmovaps	xmm12, XMMWORD PTR [rsp+48]
	lea	r11, QWORD PTR [rsp+160]
	mov	rbx, QWORD PTR [r11+64]
	mov	rbp, QWORD PTR [r11+72]
	vmovaps	xmm11, XMMWORD PTR [rsp+64]
	vmovaps	xmm10, XMMWORD PTR [rsp+80]
	vmovaps	xmm9, XMMWORD PTR [rsp+96]
	vmovaps	xmm8, XMMWORD PTR [rsp+112]
	vmovaps	xmm7, XMMWORD PTR [rsp+128]
	vmovaps	xmm6, XMMWORD PTR [rsp+144]
	mov	rsp, r11
	pop	r15
	pop	r13
	pop	r12
	pop	rdi
	pop	rsi
	ret	0
kvz_intra_pred_planar_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\intra-avx2.c
_TEXT	SEGMENT
ref_top$ = 80
ref_left$ = 88
out_block$ = 96
pred_filtered_dc_4x4 PROC

; 580  : {

	sub	rsp, 72					; 00000048H

; 581  :   const uint32_t rt_u32 = *(const uint32_t *)(ref_top  + 1);
; 582  :   const uint32_t rl_u32 = *(const uint32_t *)(ref_left + 1);
; 583  : 
; 584  :   const __m128i zero    = _mm_setzero_si128();
; 585  :   const __m128i twos    = _mm_set1_epi8(2);
; 586  : 
; 587  :   // Hack. Move 4 u8's to bit positions 0, 64, 128 and 192 in two regs, to
; 588  :   // expand them to 16 bits sort of "for free". Set highest bits on all the
; 589  :   // other bytes in vectors to zero those bits in the result vector.
; 590  :   const __m128i rl_shuf_lo = _mm_setr_epi32(0x80808000, 0x80808080,
; 591  :                                             0x80808001, 0x80808080);
; 592  :   const __m128i rl_shuf_hi = _mm_add_epi8  (rl_shuf_lo, twos);
; 593  : 
; 594  :   // Every second multiplier is 1, because we want maddubs to calculate
; 595  :   // a + bc = 1 * a + bc (actually 2 + bc). We need to fill a vector with
; 596  :   // ((u8)2)'s for other stuff anyway, so that can also be used here.
; 597  :   const __m128i mult_lo = _mm_setr_epi32(0x01030102, 0x01030103,
; 598  :                                          0x01040103, 0x01040104);
; 599  :   const __m128i mult_hi = _mm_setr_epi32(0x01040103, 0x01040104,
; 600  :                                          0x01040103, 0x01040104);
; 601  :   __m128i four         = _mm_cvtsi32_si128  (4);
; 602  :   __m128i rt           = _mm_cvtsi32_si128  (rt_u32);

	vmovd	xmm4, DWORD PTR [rcx+1]
	vmovaps	XMMWORD PTR [rsp+48], xmm6
	mov	eax, 4

; 603  :   __m128i rl           = _mm_cvtsi32_si128  (rl_u32);

	vmovd	xmm6, DWORD PTR [rdx+1]
	vmovaps	XMMWORD PTR [rsp+32], xmm7
	vmovaps	XMMWORD PTR [rsp+16], xmm8
	vmovaps	XMMWORD PTR [rsp], xmm9
	vmovdqu	xmm9, XMMWORD PTR __xmm@02020202020202020202020202020202

; 604  :   __m128i rtrl         = _mm_unpacklo_epi32 (rt, rl);
; 605  : 
; 606  :   __m128i sad0         = _mm_sad_epu8       (rtrl, zero);
; 607  :   __m128i sad1         = _mm_shuffle_epi32  (sad0, _MM_SHUFFLE(1, 0, 3, 2));
; 608  :   __m128i sad2         = _mm_add_epi64      (sad0, sad1);
; 609  :   __m128i sad3         = _mm_add_epi64      (sad2, four);
; 610  : 
; 611  :   __m128i dc_64        = _mm_srli_epi64     (sad3, 3);
; 612  :   __m128i dc_8         = _mm_broadcastb_epi8(dc_64);
; 613  : 
; 614  :   __m128i rl_lo        = _mm_shuffle_epi8   (rl, rl_shuf_lo);
; 615  :   __m128i rl_hi        = _mm_shuffle_epi8   (rl, rl_shuf_hi);
; 616  : 
; 617  :   __m128i rt_lo        = _mm_unpacklo_epi8  (rt, zero);
; 618  :   __m128i rt_hi        = zero;
; 619  : 
; 620  :   __m128i dc_addend    = _mm_unpacklo_epi8(dc_8, twos);
; 621  : 
; 622  :   __m128i dc_multd_lo  = _mm_maddubs_epi16(dc_addend,    mult_lo);
; 623  :   __m128i dc_multd_hi  = _mm_maddubs_epi16(dc_addend,    mult_hi);
; 624  : 
; 625  :   __m128i rl_rt_lo     = _mm_add_epi16    (rl_lo,        rt_lo);
; 626  :   __m128i rl_rt_hi     = _mm_add_epi16    (rl_hi,        rt_hi);
; 627  : 
; 628  :   __m128i res_lo       = _mm_add_epi16    (dc_multd_lo,  rl_rt_lo);
; 629  :   __m128i res_hi       = _mm_add_epi16    (dc_multd_hi,  rl_rt_hi);
; 630  : 
; 631  :           res_lo       = _mm_srli_epi16   (res_lo,       2);
; 632  :           res_hi       = _mm_srli_epi16   (res_hi,       2);
; 633  : 
; 634  :   __m128i final        = _mm_packus_epi16 (res_lo,       res_hi);
; 635  :   _mm_storeu_si128((__m128i *)out_block, final);
; 636  : }

	vmovaps	xmm8, XMMWORD PTR [rsp+16]
	vpunpckldq xmm0, xmm4, xmm6
	vpxor	xmm7, xmm7, xmm7
	vpsadbw	xmm2, xmm0, xmm7
	vpshufd	xmm1, xmm2, 78				; 0000004eH
	vpaddq	xmm3, xmm1, xmm2
	vmovd	xmm0, eax
	vpaddq	xmm1, xmm3, xmm0
	vpsrlq	xmm2, xmm1, 3
	vpshufb	xmm1, xmm6, XMMWORD PTR __xmm@80808080808080018080808080808000
	vpbroadcastb xmm3, xmm2
	vpunpcklbw xmm5, xmm3, xmm9
	vpmaddubsw xmm2, xmm5, XMMWORD PTR __xmm@01040104010401030103010301030102
	vpunpcklbw xmm0, xmm4, xmm7
	vmovaps	xmm7, XMMWORD PTR [rsp+32]
	vpaddw	xmm3, xmm1, xmm0
	vpaddb	xmm1, xmm9, XMMWORD PTR __xmm@80808080808080018080808080808000
	vmovaps	xmm9, XMMWORD PTR [rsp]
	vpaddw	xmm0, xmm3, xmm2
	vpmaddubsw xmm2, xmm5, XMMWORD PTR __xmm@01040104010401030104010401040103
	vpshufb	xmm3, xmm6, xmm1
	vmovaps	xmm6, XMMWORD PTR [rsp+48]
	vpaddw	xmm1, xmm3, xmm2
	vpsrlw	xmm3, xmm1, 2
	vpsrlw	xmm4, xmm0, 2
	vpackuswb xmm0, xmm4, xmm3
	vmovdqu	XMMWORD PTR [r8], xmm0
	add	rsp, 72					; 00000048H
	ret	0
pred_filtered_dc_4x4 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\intra-avx2.c
_TEXT	SEGMENT
ref_top$ = 128
ref_left$ = 136
out_block$ = 144
pred_filtered_dc_8x8 PROC

; 641  : {

	mov	r11, rsp
	sub	rsp, 120				; 00000078H

; 642  :   const uint64_t rt_u64 = *(const uint64_t *)(ref_top  + 1);
; 643  :   const uint64_t rl_u64 = *(const uint64_t *)(ref_left + 1);
; 644  : 
; 645  :   const __m128i zero128 = _mm_setzero_si128();
; 646  :   const __m256i twos    = _mm256_set1_epi8(2);
; 647  : 
; 648  :   // DC multiplier is 2 at (0, 0), 3 at (*, 0) and (0, *), and 4 at (*, *).
; 649  :   // There is a constant addend of 2 on each pixel, use values from the twos
; 650  :   // register and multipliers of 1 for that, to use maddubs for an (a*b)+c
; 651  :   // operation.
; 652  :   const __m256i mult_up_lo = _mm256_setr_epi32(0x01030102, 0x01030103,
; 653  :                                                0x01030103, 0x01030103,
; 654  :                                                0x01040103, 0x01040104,
; 655  :                                                0x01040104, 0x01040104);
; 656  : 
; 657  :   // The 6 lowest rows have same multipliers, also the DC values and addends
; 658  :   // are the same so this works for all of those
; 659  :   const __m256i mult_rest  = _mm256_permute4x64_epi64(mult_up_lo, _MM_SHUFFLE(3, 2, 3, 2));
; 660  : 
; 661  :   // Every 8-pixel row starts with the next pixel of ref_left. Along with
; 662  :   // doing the shuffling, also expand u8->u16, ie. move bytes 0 and 1 from
; 663  :   // ref_left to bit positions 0 and 128 in rl_up_lo, 2 and 3 to rl_up_hi,
; 664  :   // etc. The places to be zeroed out are 0x80 instead of the usual 0xff,
; 665  :   // because this allows us to form new masks on the fly by adding 0x02-bytes
; 666  :   // to this mask and still retain the highest bits as 1 where things should
; 667  :   // be zeroed out.
; 668  :   const __m256i rl_shuf_up_lo = _mm256_setr_epi32(0x80808000, 0x80808080,
; 669  :                                                   0x80808080, 0x80808080,
; 670  :                                                   0x80808001, 0x80808080,
; 671  :                                                   0x80808080, 0x80808080);
; 672  :   // And don't waste memory or architectural regs, hope these instructions
; 673  :   // will be placed in between the shuffles by the compiler to only use one
; 674  :   // register for the shufmasks, and executed way ahead of time because their
; 675  :   // regs can be renamed.
; 676  :   const __m256i rl_shuf_up_hi = _mm256_add_epi8 (rl_shuf_up_lo, twos);
; 677  :   const __m256i rl_shuf_dn_lo = _mm256_add_epi8 (rl_shuf_up_hi, twos);
; 678  :   const __m256i rl_shuf_dn_hi = _mm256_add_epi8 (rl_shuf_dn_lo, twos);
; 679  : 
; 680  :   __m128i eight         = _mm_cvtsi32_si128     (8);
; 681  :   __m128i rt            = _mm_cvtsi64_si128     (rt_u64);
; 682  :   __m128i rl            = _mm_cvtsi64_si128     (rl_u64);

	vmovq	xmm4, QWORD PTR [rdx+1]
	vpxor	xmm0, xmm0, xmm0
	vmovaps	XMMWORD PTR [rsp+96], xmm6
	mov	eax, 8
	vmovaps	XMMWORD PTR [rsp+80], xmm7
	vmovaps	XMMWORD PTR [r11-56], xmm8
	vmovaps	XMMWORD PTR [r11-72], xmm9
	vmovq	xmm9, QWORD PTR [rcx+1]

; 683  :   __m128i rtrl          = _mm_unpacklo_epi64    (rt, rl);

	vpunpcklqdq xmm1, xmm9, xmm4

; 684  : 
; 685  :   __m128i sad0          = _mm_sad_epu8          (rtrl, zero128);

	vpsadbw	xmm2, xmm1, xmm0

; 686  :   __m128i sad1          = _mm_shuffle_epi32     (sad0, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm1, xmm2, 78				; 0000004eH

; 687  :   __m128i sad2          = _mm_add_epi64         (sad0, sad1);

	vpaddq	xmm3, xmm1, xmm2
	vmovaps	XMMWORD PTR [r11-88], xmm10
	vmovdqu	ymm10, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202
	vpaddb	ymm5, ymm10, YMMWORD PTR __ymm@8080808080808080808080808080800180808080808080808080808080808000
	vpaddb	ymm6, ymm5, ymm10
	vmovd	xmm0, eax

; 688  :   __m128i sad3          = _mm_add_epi64         (sad2, eight);

	vpaddq	xmm1, xmm3, xmm0
	vpermq	ymm0, YMMWORD PTR __ymm@0104010401040104010401040104010301030103010301030103010301030102, 238 ; 000000eeH

; 689  : 
; 690  :   __m128i dc_64         = _mm_srli_epi64        (sad3, 4);

	vpsrlq	xmm2, xmm1, 4

; 691  :   __m256i dc_8          = _mm256_broadcastb_epi8(dc_64);

	vpbroadcastb ymm3, xmm2

; 692  : 
; 693  :   __m256i dc_addend     = _mm256_unpacklo_epi8  (dc_8, twos);

	vpunpcklbw ymm3, ymm3, ymm10

; 694  : 
; 695  :   __m256i dc_up_lo      = _mm256_maddubs_epi16  (dc_addend, mult_up_lo);
; 696  :   __m256i dc_rest       = _mm256_maddubs_epi16  (dc_addend, mult_rest);

	vpmaddubsw ymm8, ymm3, ymm0
	vmovaps	XMMWORD PTR [r11-104], xmm11
	vmovaps	XMMWORD PTR [rsp], xmm12

; 697  : 
; 698  :   // rt_dn is all zeros, as is rt_up_hi. This'll get us the rl and rt parts
; 699  :   // in A|B, C|D order instead of A|C, B|D that could be packed into abcd
; 700  :   // order, so these need to be permuted before adding to the weighed DC
; 701  :   // values.
; 702  :   __m256i rt_up_lo      = _mm256_cvtepu8_epi16   (rt);
; 703  : 
; 704  :   __m256i rlrlrlrl      = _mm256_broadcastq_epi64(rl);

	vpbroadcastq ymm2, xmm4

; 705  :   __m256i rl_up_lo      = _mm256_shuffle_epi8    (rlrlrlrl, rl_shuf_up_lo);
; 706  : 
; 707  :   // Everything ref_top is zero except on the very first row
; 708  :   __m256i rt_rl_up_hi   = _mm256_shuffle_epi8    (rlrlrlrl, rl_shuf_up_hi);
; 709  :   __m256i rt_rl_dn_lo   = _mm256_shuffle_epi8    (rlrlrlrl, rl_shuf_dn_lo);

	vpshufb	ymm7, ymm2, ymm6
	vpshufb	ymm5, ymm2, ymm5
	vpaddb	ymm0, ymm6, ymm10

; 710  :   __m256i rt_rl_dn_hi   = _mm256_shuffle_epi8    (rlrlrlrl, rl_shuf_dn_hi);

	vpshufb	ymm6, ymm2, ymm0
	vpshufb	ymm0, ymm2, YMMWORD PTR __ymm@8080808080808080808080808080800180808080808080808080808080808000
	vpmaddubsw ymm2, ymm3, YMMWORD PTR __ymm@0104010401040104010401040104010301030103010301030103010301030102
	vpmovzxbw ymm1, xmm9

; 711  : 
; 712  :   __m256i rt_rl_up_lo   = _mm256_add_epi16       (rt_up_lo, rl_up_lo);

	vpaddw	ymm4, ymm0, ymm1

; 713  : 
; 714  :   __m256i rt_rl_up_lo_2 = _mm256_permute2x128_si256(rt_rl_up_lo, rt_rl_up_hi, 0x20);

	vperm2i128 ymm0, ymm4, ymm5, 32			; 00000020H

; 715  :   __m256i rt_rl_up_hi_2 = _mm256_permute2x128_si256(rt_rl_up_lo, rt_rl_up_hi, 0x31);
; 716  :   __m256i rt_rl_dn_lo_2 = _mm256_permute2x128_si256(rt_rl_dn_lo, rt_rl_dn_hi, 0x20);
; 717  :   __m256i rt_rl_dn_hi_2 = _mm256_permute2x128_si256(rt_rl_dn_lo, rt_rl_dn_hi, 0x31);
; 718  : 
; 719  :   __m256i up_lo = _mm256_add_epi16(rt_rl_up_lo_2, dc_up_lo);

	vpaddw	ymm1, ymm0, ymm2

; 720  :   __m256i up_hi = _mm256_add_epi16(rt_rl_up_hi_2, dc_rest);
; 721  :   __m256i dn_lo = _mm256_add_epi16(rt_rl_dn_lo_2, dc_rest);
; 722  :   __m256i dn_hi = _mm256_add_epi16(rt_rl_dn_hi_2, dc_rest);
; 723  : 
; 724  :           up_lo = _mm256_srli_epi16(up_lo, 2);

	vpsrlw	ymm3, ymm1, 2
	vperm2i128 ymm2, ymm4, ymm5, 49			; 00000031H
	vpaddw	ymm0, ymm2, ymm8

; 725  :           up_hi = _mm256_srli_epi16(up_hi, 2);

	vpsrlw	ymm1, ymm0, 2

; 726  :           dn_lo = _mm256_srli_epi16(dn_lo, 2);
; 727  :           dn_hi = _mm256_srli_epi16(dn_hi, 2);
; 728  : 
; 729  :   __m256i res_up = _mm256_packus_epi16(up_lo, up_hi);

	vpackuswb ymm3, ymm3, ymm1
	vperm2i128 ymm0, ymm7, ymm6, 49			; 00000031H
	vpaddw	ymm1, ymm0, ymm8

; 730  :   __m256i res_dn = _mm256_packus_epi16(dn_lo, dn_hi);
; 731  : 
; 732  :   _mm256_storeu_si256(((__m256i *)out_block) + 0, res_up);

	vmovdqu	YMMWORD PTR [r8], ymm3
	vpsrlw	ymm3, ymm1, 2
	vperm2i128 ymm2, ymm7, ymm6, 32			; 00000020H
	vpaddw	ymm0, ymm2, ymm8
	vpsrlw	ymm1, ymm0, 2
	vpackuswb ymm3, ymm1, ymm3

; 733  :   _mm256_storeu_si256(((__m256i *)out_block) + 1, res_dn);

	vmovdqu	YMMWORD PTR [r8+32], ymm3
	vzeroupper

; 734  : }

	vmovaps	xmm6, XMMWORD PTR [rsp+96]
	vmovaps	xmm7, XMMWORD PTR [rsp+80]
	vmovaps	xmm8, XMMWORD PTR [r11-56]
	vmovaps	xmm9, XMMWORD PTR [r11-72]
	vmovaps	xmm10, XMMWORD PTR [r11-88]
	vmovaps	xmm11, XMMWORD PTR [r11-104]
	vmovaps	xmm12, XMMWORD PTR [rsp]
	add	rsp, 120				; 00000078H
	ret	0
pred_filtered_dc_8x8 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\intra-avx2.c
_TEXT	SEGMENT
u$ = 8
cvt_u32_si256 PROC

; 738  :   const __m256i zero = _mm256_setzero_si256();
; 739  :   return _mm256_insert_epi32(zero, u, 0);

	vmovd	xmm1, ecx
	vpbroadcastd ymm1, xmm1
	vpxor	xmm0, xmm0, xmm0
	vpblendd ymm0, ymm0, ymm1, 1

; 740  : }

	ret	0
cvt_u32_si256 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\intra-avx2.c
_TEXT	SEGMENT
ref_top$ = 80
ref_left$ = 88
out_block$ = 96
pred_filtered_dc_16x16 PROC

; 745  : {

	sub	rsp, 72					; 00000048H
	vpxor	xmm0, xmm0, xmm0

; 763  : 
; 764  :   __m128i sixteen = _mm_cvtsi32_si128(16);
; 765  :   __m128i sad0_t  = _mm_sad_epu8 (rt_128, zero_128);
; 766  :   __m128i sad0_l  = _mm_sad_epu8 (rl_128, zero_128);

	vpsadbw	xmm1, xmm0, XMMWORD PTR [rdx+1]
	vpsadbw	xmm0, xmm0, XMMWORD PTR [rcx+1]

; 767  :   __m128i sad0    = _mm_add_epi64(sad0_t, sad0_l);

	vpaddq	xmm2, xmm1, xmm0

; 768  : 
; 769  :   __m128i sad1    = _mm_shuffle_epi32      (sad0, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm1, xmm2, 78				; 0000004eH

; 770  :   __m128i sad2    = _mm_add_epi64          (sad0, sad1);

	vpaddq	xmm3, xmm1, xmm2
	vmovaps	XMMWORD PTR [rsp+48], xmm6
	mov	eax, 16
	vmovd	xmm0, eax

; 739  :   return _mm256_insert_epi32(zero, u, 0);

	movzx	eax, BYTE PTR [rdx+1]

; 771  :   __m128i sad3    = _mm_add_epi64          (sad2, sixteen);

	vpaddq	xmm1, xmm3, xmm0

; 775  : 
; 776  :   __m256i rt      = _mm256_cvtepu8_epi16   (rt_128);
; 777  :   __m256i rl      = _mm256_cvtepu8_epi16   (rl_128);

	vpmovzxbw ymm3, XMMWORD PTR [rdx+1]
	vpsrlq	xmm2, xmm1, 5
	vpmovzxbw ymm1, XMMWORD PTR [rcx+1]
	vmovaps	XMMWORD PTR [rsp+32], xmm7

; 739  :   return _mm256_insert_epi32(zero, u, 0);

	vmovd	xmm0, eax
	vpbroadcastd ymm0, xmm0
	vmovaps	XMMWORD PTR [rsp+16], xmm8

; 746  :   const __m128i rt_128 = _mm_loadu_si128((const __m128i *)(ref_top  + 1));

	mov	eax, 2

; 772  : 
; 773  :   __m128i dc_64   = _mm_srli_epi64         (sad3, 5);
; 774  :   __m256i dc_8    = _mm256_broadcastb_epi8 (dc_64);

	vpbroadcastb ymm8, xmm2

; 783  : 
; 784  :   __m256i dc_addend = _mm256_unpacklo_epi8(dc_8, twos);

	vpunpcklbw ymm4, ymm8, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202

; 785  :   __m256i r0        = _mm256_maddubs_epi16(dc_addend, mult_r0);

	vpmaddubsw ymm2, ymm4, YMMWORD PTR __ymm@0103010301030103010301030103010301030103010301030103010301030102
	vpxor	xmm6, xmm6, xmm6

; 739  :   return _mm256_insert_epi32(zero, u, 0);

	vpblendd ymm0, ymm6, ymm0, 1
	vmovaps	XMMWORD PTR [rsp], xmm9

; 747  :   const __m128i rl_128 = _mm_loadu_si128((const __m128i *)(ref_left + 1));
; 748  : 
; 749  :   const __m128i zero_128 = _mm_setzero_si128();
; 750  :   const __m256i zero     = _mm256_setzero_si256();
; 751  :   const __m256i twos     = _mm256_set1_epi8(2);
; 752  : 
; 753  :   const __m256i mult_r0  = _mm256_setr_epi32(0x01030102, 0x01030103,
; 754  :                                              0x01030103, 0x01030103,
; 755  :                                              0x01030103, 0x01030103,
; 756  :                                              0x01030103, 0x01030103);
; 757  : 
; 758  :   const __m256i mult_left = _mm256_set1_epi16(0x0103);
; 759  : 
; 760  :   // Leftmost bytes' blend mask, to move bytes (pixels) from the leftmost
; 761  :   // column vector to the result row
; 762  :   const __m256i lm8_bmask = _mm256_setr_epi32(0xff, 0, 0, 0, 0xff, 0, 0, 0);

	vmovdqu	ymm9, YMMWORD PTR __ymm@000000000000000000000000000000ff000000000000000000000000000000ff

; 778  : 
; 779  :   uint8_t rl0       = *(uint8_t *)(ref_left + 1);
; 780  :   __m256i rl_r0     = cvt_u32_si256((uint32_t)rl0);
; 781  : 
; 782  :   __m256i rlrt_r0   = _mm256_add_epi16(rl_r0, rt);

	vpaddw	ymm1, ymm0, ymm1

; 786  :   __m256i left_dcs  = _mm256_maddubs_epi16(dc_addend, mult_left);
; 787  : 
; 788  :           r0        = _mm256_add_epi16    (r0,       rlrt_r0);

	vpaddw	ymm2, ymm1, ymm2
	vpmaddubsw ymm1, ymm4, YMMWORD PTR __ymm@0103010301030103010301030103010301030103010301030103010301030103

; 789  :           r0        = _mm256_srli_epi16   (r0, 2);

	vpsrlw	ymm5, ymm2, 2

; 790  :   __m256i r0r0      = _mm256_packus_epi16 (r0, r0);
; 791  :           r0r0      = _mm256_permute4x64_epi64(r0r0, _MM_SHUFFLE(3, 1, 2, 0));
; 792  : 
; 793  :   __m256i leftmosts = _mm256_add_epi16    (left_dcs,  rl);

	vpaddw	ymm2, ymm1, ymm3

; 794  :           leftmosts = _mm256_srli_epi16   (leftmosts, 2);

	vpsrlw	ymm3, ymm2, 2

; 795  : 
; 796  :   // Contain the leftmost column's bytes in both lanes of lm_8
; 797  :   __m256i lm_8      = _mm256_packus_epi16 (leftmosts, zero);

	vpackuswb ymm0, ymm3, ymm6

; 798  :           lm_8      = _mm256_permute4x64_epi64(lm_8,  _MM_SHUFFLE(2, 0, 2, 0));

	vpermq	ymm4, ymm0, 136				; 00000088H

; 799  : 
; 800  :   __m256i lm8_r1    = _mm256_srli_epi32       (lm_8, 8);

	vpsrld	ymm1, ymm4, 8

; 801  :   __m256i r1r1      = _mm256_blendv_epi8      (dc_8, lm8_r1, lm8_bmask);

	vpblendvb ymm3, ymm8, ymm1, ymm9
	vpackuswb ymm0, ymm5, ymm5
	vpermq	ymm2, ymm0, 216				; 000000d8H

; 802  :   __m256i r0r1      = _mm256_blend_epi32      (r0r0, r1r1, 0xf0);

	vpblendd ymm1, ymm2, ymm3, 240			; 000000f0H

; 803  : 
; 804  :   _mm256_storeu_si256((__m256i *)out_block, r0r1);
; 805  : 
; 806  :   // Starts from 2 because row 0 (and row 1) is handled separately
; 807  :   __m256i lm8_l     = _mm256_bsrli_epi128     (lm_8, 2);
; 808  :   __m256i lm8_h     = _mm256_bsrli_epi128     (lm_8, 3);

	vpsrldq	ymm3, ymm4, 3
	vpsrldq	ymm0, ymm4, 2
	vmovdqu	YMMWORD PTR [r8], ymm1

; 809  :           lm_8      = _mm256_blend_epi32      (lm8_l, lm8_h, 0xf0);

	vpblendd ymm1, ymm0, ymm3, 240			; 000000f0H
	npad	9
$LL4@pred_filte:

; 810  : 
; 811  :   for (uint32_t y = 2; y < 16; y += 2) {
; 812  :     __m256i curr_row = _mm256_blendv_epi8 (dc_8, lm_8, lm8_bmask);
; 813  :     _mm256_storeu_si256((__m256i *)(out_block + (y << 4)), curr_row);

	mov	ecx, eax
	add	eax, 2
	shl	ecx, 4
	vpblendvb ymm0, ymm8, ymm1, ymm9

; 814  :     lm_8 = _mm256_bsrli_epi128(lm_8, 2);

	vpsrldq	ymm1, ymm1, 2
	vmovdqu	YMMWORD PTR [rcx+r8], ymm0
	cmp	eax, 16
	jb	SHORT $LL4@pred_filte

; 815  :   }
; 816  : }

	vzeroupper
	vmovaps	xmm6, XMMWORD PTR [rsp+48]
	vmovaps	xmm7, XMMWORD PTR [rsp+32]
	vmovaps	xmm8, XMMWORD PTR [rsp+16]
	vmovaps	xmm9, XMMWORD PTR [rsp]
	add	rsp, 72					; 00000048H
	ret	0
pred_filtered_dc_16x16 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\intra-avx2.c
_TEXT	SEGMENT
ref_top$ = 112
ref_left$ = 120
out_block$ = 128
pred_filtered_dc_32x32 PROC

; 821  : {

	sub	rsp, 104				; 00000068H
	vmovaps	XMMWORD PTR [rsp+80], xmm6

; 822  :   const __m256i rt = _mm256_loadu_si256((const __m256i *)(ref_top  + 1));

	mov	eax, 1
	vmovdqu	ymm6, YMMWORD PTR [rcx+1]
	vmovaps	XMMWORD PTR [rsp+64], xmm7
	vpxor	xmm7, xmm7, xmm7

; 739  :   return _mm256_insert_epi32(zero, u, 0);

	vpblendd ymm3, ymm7, YMMWORD PTR __ymm@0000002000000020000000200000002000000020000000200000002000000020, 1
	vmovaps	XMMWORD PTR [rsp+48], xmm8

; 823  :   const __m256i rl = _mm256_loadu_si256((const __m256i *)(ref_left + 1));

	vmovdqu	ymm8, YMMWORD PTR [rdx+1]
	vmovaps	XMMWORD PTR [rsp+32], xmm9

; 739  :   return _mm256_insert_epi32(zero, u, 0);

	vpblendd ymm9, ymm7, YMMWORD PTR __ymm@000000ff000000ff000000ff000000ff000000ff000000ff000000ff000000ff, 1

; 824  : 
; 825  :   const __m256i zero = _mm256_setzero_si256();
; 826  :   const __m256i twos = _mm256_set1_epi8(2);
; 827  : 
; 828  :   const __m256i mult_r0lo = _mm256_setr_epi32(0x01030102, 0x01030103,
; 829  :                                               0x01030103, 0x01030103,
; 830  :                                               0x01030103, 0x01030103,
; 831  :                                               0x01030103, 0x01030103);
; 832  : 
; 833  :   const __m256i mult_left = _mm256_set1_epi16(0x0103);
; 834  :   const __m256i lm8_bmask = cvt_u32_si256    (0xff);
; 835  : 
; 836  :   const __m256i bshif_msk = _mm256_setr_epi32(0x04030201, 0x08070605,
; 837  :                                               0x0c0b0a09, 0x800f0e0d,
; 838  :                                               0x03020100, 0x07060504,
; 839  :                                               0x0b0a0908, 0x0f0e0d0c);
; 840  :   __m256i debias = cvt_u32_si256(32);
; 841  :   __m256i sad0_t = _mm256_sad_epu8         (rt,     zero);
; 842  :   __m256i sad0_l = _mm256_sad_epu8         (rl,     zero);

	vpsadbw	ymm0, ymm7, ymm8
	vpsadbw	ymm1, ymm7, ymm6

; 843  :   __m256i sad0   = _mm256_add_epi64        (sad0_t, sad0_l);

	vpaddq	ymm2, ymm0, ymm1

; 844  : 
; 845  :   __m256i sad1   = _mm256_permute4x64_epi64(sad0,   _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm1, ymm2, 78				; 0000004eH

; 846  :   __m256i sad2   = _mm256_add_epi64        (sad0,   sad1);

	vpaddq	ymm4, ymm1, ymm2

; 847  :   __m256i sad3   = _mm256_shuffle_epi32    (sad2,   _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	ymm1, ymm4, 78				; 0000004eH

; 848  :   __m256i sad4   = _mm256_add_epi64        (sad2,   sad3);

	vpaddq	ymm2, ymm1, ymm4

; 849  :   __m256i sad5   = _mm256_add_epi64        (sad4,   debias);

	vpaddq	ymm0, ymm2, ymm3

; 850  :   __m256i dc_64  = _mm256_srli_epi64       (sad5,   6);

	vpsrlq	ymm3, ymm0, 6
	vmovaps	XMMWORD PTR [rsp+16], xmm10

; 851  : 
; 852  :   __m128i dc_64_ = _mm256_castsi256_si128  (dc_64);
; 853  :   __m256i dc_8   = _mm256_broadcastb_epi8  (dc_64_);

	vpbroadcastb ymm10, xmm3

; 854  : 
; 855  :   __m256i rtlo   = _mm256_unpacklo_epi8    (rt, zero);
; 856  :   __m256i rllo   = _mm256_unpacklo_epi8    (rl, zero);
; 857  :   __m256i rthi   = _mm256_unpackhi_epi8    (rt, zero);
; 858  :   __m256i rlhi   = _mm256_unpackhi_epi8    (rl, zero);
; 859  : 
; 860  :   __m256i dc_addend = _mm256_unpacklo_epi8 (dc_8, twos);

	vpunpcklbw ymm4, ymm10, YMMWORD PTR __ymm@0202020202020202020202020202020202020202020202020202020202020202

; 861  :   __m256i r0lo   = _mm256_maddubs_epi16    (dc_addend, mult_r0lo);
; 862  :   __m256i r0hi   = _mm256_maddubs_epi16    (dc_addend, mult_left);

	vpmaddubsw ymm5, ymm4, YMMWORD PTR __ymm@0103010301030103010301030103010301030103010301030103010301030103
	vpmaddubsw ymm1, ymm4, YMMWORD PTR __ymm@0103010301030103010301030103010301030103010301030103010301030102
	vmovaps	XMMWORD PTR [rsp], xmm11
	vmovdqu	ymm11, YMMWORD PTR __ymm@0f0e0d0c0b0a09080706050403020100800f0e0d0c0b0a090807060504030201
	vpunpckhbw ymm0, ymm6, ymm7
	vpunpcklbw ymm2, ymm6, ymm7

; 863  :   __m256i c0dc   = r0hi;
; 864  : 
; 865  :           r0lo   = _mm256_add_epi16        (r0lo, rtlo);

	vpaddw	ymm2, ymm1, ymm2

; 866  :           r0hi   = _mm256_add_epi16        (r0hi, rthi);

	vpaddw	ymm1, ymm0, ymm5
	vpunpckhbw ymm0, ymm8, ymm7

; 867  : 
; 868  :   __m256i rlr0   = _mm256_blendv_epi8      (zero, rl, lm8_bmask);

	vpblendvb ymm3, ymm7, ymm8, ymm9

; 869  :           r0lo   = _mm256_add_epi16        (r0lo, rlr0);

	vpaddw	ymm3, ymm2, ymm3

; 870  : 
; 871  :           r0lo   = _mm256_srli_epi16       (r0lo, 2);
; 872  :           r0hi   = _mm256_srli_epi16       (r0hi, 2);

	vpsrlw	ymm2, ymm1, 2
	vpsrlw	ymm4, ymm3, 2

; 873  :   __m256i r0     = _mm256_packus_epi16     (r0lo, r0hi);

	vpackuswb ymm3, ymm4, ymm2

; 874  : 
; 875  :   _mm256_storeu_si256((__m256i *)out_block, r0);
; 876  : 
; 877  :   __m256i c0lo   = _mm256_add_epi16        (c0dc, rllo);
; 878  :   __m256i c0hi   = _mm256_add_epi16        (c0dc, rlhi);

	vpaddw	ymm1, ymm0, ymm5
	vmovdqu	YMMWORD PTR [r8], ymm3

; 879  :           c0lo   = _mm256_srli_epi16       (c0lo, 2);
; 880  :           c0hi   = _mm256_srli_epi16       (c0hi, 2);

	vpsrlw	ymm3, ymm1, 2
	vpunpcklbw ymm2, ymm8, ymm7
	vpaddw	ymm0, ymm2, ymm5
	vpsrlw	ymm1, ymm0, 2

; 881  : 
; 882  :   __m256i c0     = _mm256_packus_epi16     (c0lo, c0hi);

	vpackuswb ymm4, ymm1, ymm3
	add	r8, 32					; 00000020H
	npad	3
$LL4@pred_filte:

; 883  : 
; 884  :   // r0 already handled!
; 885  :   for (uint32_t y = 1; y < 32; y++) {
; 886  :     if (y == 16) {

	cmp	eax, 16
	jne	SHORT $LN5@pred_filte

; 887  :       c0 = _mm256_permute4x64_epi64(c0, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm4, ymm4, 78				; 0000004eH

; 888  :     } else {

	jmp	SHORT $LN6@pred_filte
$LN5@pred_filte:

; 889  :       c0 = _mm256_shuffle_epi8     (c0, bshif_msk);

	vpshufb	ymm4, ymm4, ymm11
$LN6@pred_filte:

; 890  :     }
; 891  :     __m256i curr_row = _mm256_blendv_epi8 (dc_8, c0, lm8_bmask);

	vpblendvb ymm0, ymm10, ymm4, ymm9

; 892  :     _mm256_storeu_si256(((__m256i *)out_block) + y, curr_row);

	vmovdqu	YMMWORD PTR [r8], ymm0
	add	r8, 32					; 00000020H
	inc	eax
	cmp	eax, 32					; 00000020H
	jb	SHORT $LL4@pred_filte

; 893  :   }
; 894  : }

	vzeroupper
	vmovaps	xmm6, XMMWORD PTR [rsp+80]
	vmovaps	xmm7, XMMWORD PTR [rsp+64]
	vmovaps	xmm8, XMMWORD PTR [rsp+48]
	vmovaps	xmm9, XMMWORD PTR [rsp+32]
	vmovaps	xmm10, XMMWORD PTR [rsp+16]
	vmovaps	xmm11, XMMWORD PTR [rsp]
	add	rsp, 104				; 00000068H
	ret	0
pred_filtered_dc_32x32 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\intra-avx2.c
_TEXT	SEGMENT
log2_width$ = 112
ref_top$ = 120
ref_left$ = 128
out_block$ = 136
kvz_intra_pred_filtered_dc_avx2 PROC

; 908  : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rbp
	mov	QWORD PTR [rsp+24], rsi
	push	rdi
	sub	rsp, 96					; 00000060H
	movzx	ebx, cl
	mov	rdi, r9
	mov	rsi, r8
	mov	rbp, rdx

; 909  :   assert(log2_width >= 2 && log2_width <= 5);

	lea	eax, DWORD PTR [rbx-2]
	cmp	al, 3
	jbe	SHORT $LN11@kvz_intra_
	mov	r8d, 909				; 0000038dH
	lea	rdx, OFFSET FLAT:$SG4294947223
	lea	rcx, OFFSET FLAT:$SG4294947222
	call	QWORD PTR __imp__wassert
$LN11@kvz_intra_:

; 910  : 
; 911  :   if (log2_width == 2) {

	cmp	bl, 2
	jne	$LN2@kvz_intra_

; 602  :   __m128i rt           = _mm_cvtsi32_si128  (rt_u32);

	vmovd	xmm4, DWORD PTR [rbp+1]
	vmovaps	XMMWORD PTR [rsp+80], xmm6
	mov	eax, 4

; 603  :   __m128i rl           = _mm_cvtsi32_si128  (rl_u32);

	vmovd	xmm6, DWORD PTR [rsi+1]
	vmovaps	XMMWORD PTR [rsp+64], xmm7
	vmovaps	XMMWORD PTR [rsp+48], xmm8
	vmovaps	XMMWORD PTR [rsp+32], xmm9
	vmovdqu	xmm9, XMMWORD PTR __xmm@02020202020202020202020202020202

; 623  :   __m128i dc_multd_hi  = _mm_maddubs_epi16(dc_addend,    mult_hi);
; 624  : 
; 625  :   __m128i rl_rt_lo     = _mm_add_epi16    (rl_lo,        rt_lo);
; 626  :   __m128i rl_rt_hi     = _mm_add_epi16    (rl_hi,        rt_hi);
; 627  : 
; 628  :   __m128i res_lo       = _mm_add_epi16    (dc_multd_lo,  rl_rt_lo);
; 629  :   __m128i res_hi       = _mm_add_epi16    (dc_multd_hi,  rl_rt_hi);
; 630  : 
; 631  :           res_lo       = _mm_srli_epi16   (res_lo,       2);
; 632  :           res_hi       = _mm_srli_epi16   (res_hi,       2);
; 633  : 
; 634  :   __m128i final        = _mm_packus_epi16 (res_lo,       res_hi);
; 635  :   _mm_storeu_si128((__m128i *)out_block, final);

	vmovaps	xmm8, XMMWORD PTR [rsp+48]
	vpunpckldq xmm0, xmm4, xmm6
	vpxor	xmm7, xmm7, xmm7

; 604  :   __m128i rtrl         = _mm_unpacklo_epi32 (rt, rl);
; 605  : 
; 606  :   __m128i sad0         = _mm_sad_epu8       (rtrl, zero);

	vpsadbw	xmm2, xmm0, xmm7

; 607  :   __m128i sad1         = _mm_shuffle_epi32  (sad0, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm1, xmm2, 78				; 0000004eH

; 608  :   __m128i sad2         = _mm_add_epi64      (sad0, sad1);

	vpaddq	xmm3, xmm1, xmm2
	vmovd	xmm0, eax

; 609  :   __m128i sad3         = _mm_add_epi64      (sad2, four);

	vpaddq	xmm1, xmm3, xmm0

; 610  : 
; 611  :   __m128i dc_64        = _mm_srli_epi64     (sad3, 3);

	vpsrlq	xmm2, xmm1, 3

; 612  :   __m128i dc_8         = _mm_broadcastb_epi8(dc_64);
; 613  : 
; 614  :   __m128i rl_lo        = _mm_shuffle_epi8   (rl, rl_shuf_lo);

	vpshufb	xmm1, xmm6, XMMWORD PTR __xmm@80808080808080018080808080808000
	vpbroadcastb xmm3, xmm2

; 615  :   __m128i rl_hi        = _mm_shuffle_epi8   (rl, rl_shuf_hi);
; 616  : 
; 617  :   __m128i rt_lo        = _mm_unpacklo_epi8  (rt, zero);
; 618  :   __m128i rt_hi        = zero;
; 619  : 
; 620  :   __m128i dc_addend    = _mm_unpacklo_epi8(dc_8, twos);

	vpunpcklbw xmm5, xmm3, xmm9

; 621  : 
; 622  :   __m128i dc_multd_lo  = _mm_maddubs_epi16(dc_addend,    mult_lo);

	vpmaddubsw xmm2, xmm5, XMMWORD PTR __xmm@01040104010401030103010301030102
	vpunpcklbw xmm0, xmm4, xmm7

; 623  :   __m128i dc_multd_hi  = _mm_maddubs_epi16(dc_addend,    mult_hi);
; 624  : 
; 625  :   __m128i rl_rt_lo     = _mm_add_epi16    (rl_lo,        rt_lo);
; 626  :   __m128i rl_rt_hi     = _mm_add_epi16    (rl_hi,        rt_hi);
; 627  : 
; 628  :   __m128i res_lo       = _mm_add_epi16    (dc_multd_lo,  rl_rt_lo);
; 629  :   __m128i res_hi       = _mm_add_epi16    (dc_multd_hi,  rl_rt_hi);
; 630  : 
; 631  :           res_lo       = _mm_srli_epi16   (res_lo,       2);
; 632  :           res_hi       = _mm_srli_epi16   (res_hi,       2);
; 633  : 
; 634  :   __m128i final        = _mm_packus_epi16 (res_lo,       res_hi);
; 635  :   _mm_storeu_si128((__m128i *)out_block, final);

	vmovaps	xmm7, XMMWORD PTR [rsp+64]
	vpaddw	xmm3, xmm1, xmm0
	vpaddb	xmm1, xmm9, XMMWORD PTR __xmm@80808080808080018080808080808000
	vmovaps	xmm9, XMMWORD PTR [rsp+32]
	vpaddw	xmm0, xmm3, xmm2
	vpmaddubsw xmm2, xmm5, XMMWORD PTR __xmm@01040104010401030104010401040103
	vpshufb	xmm3, xmm6, xmm1
	vmovaps	xmm6, XMMWORD PTR [rsp+80]
	vpaddw	xmm1, xmm3, xmm2
	vpsrlw	xmm3, xmm1, 2
	vpsrlw	xmm4, xmm0, 2
	vpackuswb xmm0, xmm4, xmm3
	vmovdqu	XMMWORD PTR [rdi], xmm0

; 636  : }

	jmp	SHORT $LN8@kvz_intra_
$LN2@kvz_intra_:

; 912  :     pred_filtered_dc_4x4(ref_top, ref_left, out_block);
; 913  :   } else if (log2_width == 3) {

	cmp	bl, 3
	jne	SHORT $LN4@kvz_intra_

; 914  :     pred_filtered_dc_8x8(ref_top, ref_left, out_block);

	mov	r8, rdi
	mov	rdx, rsi
	mov	rcx, rbp
	call	pred_filtered_dc_8x8
	jmp	SHORT $LN8@kvz_intra_
$LN4@kvz_intra_:

; 915  :   } else if (log2_width == 4) {

	cmp	bl, 4
	jne	SHORT $LN6@kvz_intra_

; 916  :     pred_filtered_dc_16x16(ref_top, ref_left, out_block);

	mov	r8, rdi
	mov	rdx, rsi
	mov	rcx, rbp
	call	pred_filtered_dc_16x16
	jmp	SHORT $LN8@kvz_intra_
$LN6@kvz_intra_:

; 917  :   } else if (log2_width == 5) {

	cmp	bl, 5
	jne	SHORT $LN8@kvz_intra_

; 918  :     pred_filtered_dc_32x32(ref_top, ref_left, out_block);

	mov	r8, rdi
	mov	rdx, rsi
	mov	rcx, rbp
	call	pred_filtered_dc_32x32
$LN8@kvz_intra_:

; 919  :   }
; 920  : }

	mov	rbx, QWORD PTR [rsp+112]
	mov	rbp, QWORD PTR [rsp+120]
	mov	rsi, QWORD PTR [rsp+128]
	add	rsp, 96					; 00000060H
	pop	rdi
	ret	0
kvz_intra_pred_filtered_dc_avx2 ENDP
_TEXT	ENDS
END
