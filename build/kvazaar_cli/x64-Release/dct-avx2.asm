; Listing generated by Microsoft (R) Optimizing Compiler Version 19.26.28806.0 

include listing.inc

INCLUDELIB OLDNAMES

cbf_masks DW	01fH
	DW	0fH
	DW	07H
	DW	03H
	DW	01H
g_sig_last_scan_16x16 DD 00H
	DD	04H
	DD	01H
	DD	08H
	DD	05H
	DD	02H
	DD	0cH
	DD	09H
	DD	06H
	DD	03H
	DD	0dH
	DD	0aH
	DD	07H
	DD	0eH
	DD	0bH
	DD	0fH
g_group_idx DB	00H
	DB	01H
	DB	02H
	DB	03H
	DB	04H
	DB	04H
	DB	05H
	DB	05H
	DB	06H
	DB	06H
	DB	06H
	DB	06H
	DB	07H
	DB	07H
	DB	07H
	DB	07H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
strategies_to_select DQ FLAT:$SG4294952304
	DQ	FLAT:kvz_array_checksum
	DQ	FLAT:$SG4294952303
	DQ	FLAT:kvz_array_md5
	DQ	FLAT:$SG4294952302
	DQ	FLAT:kvz_reg_sad
	DQ	FLAT:$SG4294952301
	DQ	FLAT:kvz_sad_4x4
	DQ	FLAT:$SG4294952300
	DQ	FLAT:kvz_sad_8x8
	DQ	FLAT:$SG4294952299
	DQ	FLAT:kvz_sad_16x16
	DQ	FLAT:$SG4294952298
	DQ	FLAT:kvz_sad_32x32
	DQ	FLAT:$SG4294952297
	DQ	FLAT:kvz_sad_64x64
	DQ	FLAT:$SG4294952296
	DQ	FLAT:kvz_satd_4x4
	DQ	FLAT:$SG4294952295
	DQ	FLAT:kvz_satd_8x8
	DQ	FLAT:$SG4294952294
	DQ	FLAT:kvz_satd_16x16
	DQ	FLAT:$SG4294952293
	DQ	FLAT:kvz_satd_32x32
	DQ	FLAT:$SG4294952292
	DQ	FLAT:kvz_satd_64x64
	DQ	FLAT:$SG4294952291
	DQ	FLAT:kvz_satd_any_size
	DQ	FLAT:$SG4294952290
	DQ	FLAT:kvz_sad_4x4_dual
	DQ	FLAT:$SG4294952289
	DQ	FLAT:kvz_sad_8x8_dual
	DQ	FLAT:$SG4294952288
	DQ	FLAT:kvz_sad_16x16_dual
	DQ	FLAT:$SG4294952287
	DQ	FLAT:kvz_sad_32x32_dual
	DQ	FLAT:$SG4294952286
	DQ	FLAT:kvz_sad_64x64_dual
	DQ	FLAT:$SG4294952285
	DQ	FLAT:kvz_satd_4x4_dual
	DQ	FLAT:$SG4294952284
	DQ	FLAT:kvz_satd_8x8_dual
	DQ	FLAT:$SG4294952283
	DQ	FLAT:kvz_satd_16x16_dual
	DQ	FLAT:$SG4294952282
	DQ	FLAT:kvz_satd_32x32_dual
	DQ	FLAT:$SG4294952281
	DQ	FLAT:kvz_satd_64x64_dual
	DQ	FLAT:$SG4294952280
	DQ	FLAT:kvz_satd_any_size_quad
	DQ	FLAT:$SG4294952279
	DQ	FLAT:kvz_pixels_calc_ssd
	DQ	FLAT:$SG4294952278
	DQ	FLAT:kvz_bipred_average
	DQ	FLAT:$SG4294952277
	DQ	FLAT:kvz_get_optimized_sad
	DQ	FLAT:$SG4294952276
	DQ	FLAT:kvz_ver_sad
	DQ	FLAT:$SG4294952275
	DQ	FLAT:kvz_hor_sad
	DQ	FLAT:$SG4294952274
	DQ	FLAT:kvz_pixel_var
	DQ	FLAT:$SG4294952273
	DQ	FLAT:kvz_fast_forward_dst_4x4
	DQ	FLAT:$SG4294952272
	DQ	FLAT:kvz_dct_4x4
	DQ	FLAT:$SG4294952271
	DQ	FLAT:kvz_dct_8x8
	DQ	FLAT:$SG4294952270
	DQ	FLAT:kvz_dct_16x16
	DQ	FLAT:$SG4294952269
	DQ	FLAT:kvz_dct_32x32
	DQ	FLAT:$SG4294952268
	DQ	FLAT:kvz_fast_inverse_dst_4x4
	DQ	FLAT:$SG4294952267
	DQ	FLAT:kvz_idct_4x4
	DQ	FLAT:$SG4294952266
	DQ	FLAT:kvz_idct_8x8
	DQ	FLAT:$SG4294952265
	DQ	FLAT:kvz_idct_16x16
	DQ	FLAT:$SG4294952264
	DQ	FLAT:kvz_idct_32x32
	DQ	FLAT:$SG4294952263
	DQ	FLAT:kvz_filter_hpel_blocks_hor_ver_luma
	DQ	FLAT:$SG4294952262
	DQ	FLAT:kvz_filter_hpel_blocks_diag_luma
	DQ	FLAT:$SG4294952261
	DQ	FLAT:kvz_filter_qpel_blocks_hor_ver_luma
	DQ	FLAT:$SG4294952260
	DQ	FLAT:kvz_filter_qpel_blocks_diag_luma
	DQ	FLAT:$SG4294952259
	DQ	FLAT:kvz_sample_quarterpel_luma
	DQ	FLAT:$SG4294952258
	DQ	FLAT:kvz_sample_octpel_chroma
	DQ	FLAT:$SG4294952257
	DQ	FLAT:kvz_sample_quarterpel_luma_hi
	DQ	FLAT:$SG4294952256
	DQ	FLAT:kvz_sample_octpel_chroma_hi
	DQ	FLAT:$SG4294952255
	DQ	FLAT:kvz_get_extended_block
	DQ	FLAT:$SG4294952254
	DQ	FLAT:kvz_quant
	DQ	FLAT:$SG4294952253
	DQ	FLAT:kvz_quantize_residual
	DQ	FLAT:$SG4294952252
	DQ	FLAT:kvz_dequant
	DQ	FLAT:$SG4294952251
	DQ	FLAT:kvz_coeff_abs_sum
	DQ	FLAT:$SG4294952250
	DQ	FLAT:kvz_fast_coeff_cost
	DQ	FLAT:$SG4294952249
	DQ	FLAT:kvz_angular_pred
	DQ	FLAT:$SG4294952248
	DQ	FLAT:kvz_intra_pred_planar
	DQ	FLAT:$SG4294952247
	DQ	FLAT:kvz_intra_pred_filtered_dc
	DQ	FLAT:$SG4294952246
	DQ	FLAT:kvz_sao_edge_ddistortion
	DQ	FLAT:$SG4294952245
	DQ	FLAT:kvz_calc_sao_edge_dir
	DQ	FLAT:$SG4294952244
	DQ	FLAT:kvz_sao_reconstruct_color
	DQ	FLAT:$SG4294952243
	DQ	FLAT:kvz_sao_band_ddistortion
	DQ	FLAT:$SG4294952242
	DQ	FLAT:kvz_encode_coeff_nxn
	DQ	0000000000000000H
	DQ	0000000000000000H
g_sig_last_scan_32x32 DD 00H
	DD	08H
	DD	01H
	DD	010H
	DD	09H
	DD	02H
	DD	018H
	DD	011H
	DD	0aH
	DD	03H
	DD	020H
	DD	019H
	DD	012H
	DD	0bH
	DD	04H
	DD	028H
	DD	021H
	DD	01aH
	DD	013H
	DD	0cH
	DD	05H
	DD	030H
	DD	029H
	DD	022H
	DD	01bH
	DD	014H
	DD	0dH
	DD	06H
	DD	038H
	DD	031H
	DD	02aH
	DD	023H
	DD	01cH
	DD	015H
	DD	0eH
	DD	07H
	DD	039H
	DD	032H
	DD	02bH
	DD	024H
	DD	01dH
	DD	016H
	DD	0fH
	DD	03aH
	DD	033H
	DD	02cH
	DD	025H
	DD	01eH
	DD	017H
	DD	03bH
	DD	034H
	DD	02dH
	DD	026H
	DD	01fH
	DD	03cH
	DD	035H
	DD	02eH
	DD	027H
	DD	03dH
	DD	036H
	DD	02fH
	DD	03eH
	DD	037H
	DD	03fH
g_min_in_group DB 00H
	DB	01H
	DB	02H
	DB	03H
	DB	04H
	DB	06H
	DB	08H
	DB	0cH
	DB	010H
	DB	018H
	ORG $+6
default_fast_coeff_cost_wts DD 03e282e88r	; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e26c094r			; 0.162844
	DD	04081ca43r			; 4.05594
	DD	04064203ar			; 3.56447
	DD	040db915ar			; 6.86149
	DD	03e03d189r			; 0.128729
	DD	04089fbafr			; 4.31197
	DD	0407c5771r			; 3.94284
	DD	040ddeed2r			; 6.9354
	DD	03de33ce6r			; 0.110956
	DD	0408ddcb1r			; 4.43319
	DD	0407c8738r			; 3.94575
	DD	040dc1618r			; 6.8777
	DD	03dc29cfer			; 0.095026
	DD	0408f7938r			; 4.48355
	DD	0408636aar			; 4.19417
	DD	040d90260r			; 6.78154
	DD	03d99b1b8r			; 0.075046
	DD	04094474cr			; 4.6337
	DD	04082b1b6r			; 4.08419
	DD	040d65aeer			; 6.6986
	DD	03d56bca5r			; 0.052426
	DD	0409ef37er			; 4.96722
	DD	04080dee8r			; 4.02721
	DD	040d19306r			; 6.5492
	DD	03d24bcaer			; 0.040219
	DD	040a489car			; 5.14182
	DD	0407ee3bdr			; 3.98265
	DD	040cec513r			; 6.46156
	DD	03d0fba88r			; 0.03509
	DD	040a628e7r			; 5.19249
	DD	040752e49r			; 3.83095
	DD	040cd642ar			; 6.41848
	DD	03cf47d80r			; 0.029845
	DD	040a6c5d0r			; 5.21165
	DD	040743073r			; 3.81546
	DD	040cb0dd8r			; 6.34544
	DD	03cc0b136r			; 0.023522
	DD	040aa4f92r			; 5.32221
	DD	040744224r			; 3.81654
	DD	040cb8aaar			; 6.36068
	DD	03cae87d3r			; 0.021305
	DD	040a73ac3r			; 5.22592
	DD	04075eeccr			; 3.8427
	DD	040ca6cd9r			; 6.32579
	DD	03c821294r			; 0.015878
	DD	040a5dbe0r			; 5.18309
	DD	0407d2f27r			; 3.956
	DD	040ca8cbdr			; 6.32968
	DD	03c2ae297r			; 0.01043
	DD	040a32ce4r			; 5.09923
	DD	04085a85fr			; 4.1768
	DD	040c9c5d6r			; 6.3054
	DD	03c0a2a91r			; 0.008433
	DD	040a0f7der			; 5.03026
	DD	040879a50r			; 4.23759
	DD	040c8a4eer			; 6.27013
	DD	03bd4fdf4r			; 0.0065
	DD	0409f0412r			; 4.96925
	DD	0408adc57r			; 4.3394
	DD	040c6f870r			; 6.21783
	DD	03ba18373r			; 0.004929
	DD	0409d8d50r			; 4.9235
	DD	0408e283fr			; 4.44241
	DD	040c5df6cr			; 6.18352
	DD	03b73775cr			; 0.003715
	DD	0409d4c75r			; 4.91558
	DD	0408dbb1br			; 4.42909
	DD	040c4029fr			; 6.12532
	DD	03b4a70d2r			; 0.003089
	DD	0409c48f7r			; 4.88391
	DD	040920260r			; 4.56279
	DD	040c5019dr			; 6.15645
	DD	03b219c9dr			; 0.002466
	DD	0409c31abr			; 4.88106
	DD	040942800r			; 4.62988
	DD	040c49088r			; 6.14264
	DD	03b0e25c8r			; 0.002169
	DD	0409c3d62r			; 4.88249
	DD	04094ae99r			; 4.64631
	DD	040c415d1r			; 6.12766
	DD	03b26dacbr			; 0.002546
	DD	040996304r			; 4.79334
	DD	0409acc16r			; 4.83741
	DD	040c6606br			; 6.19927
	DD	03aac3a86r			; 0.001314
	DD	04099e220r			; 4.80885
	DD	0409a81bdr			; 4.82834
	DD	040c7ca3cr			; 6.24344
	DD	03a9741d1r			; 0.001154
	DD	0409b9a72r			; 4.8626
	DD	0409b19aar			; 4.84688
	DD	040c693a5r			; 6.20552
	DD	03a80f990r			; 0.000984
	DD	0409bb993r			; 4.8664
	DD	0409b7fa2r			; 4.85933
	DD	040c7b565r			; 6.24089
	DD	03a551f82r			; 0.000813
	DD	0409b698ar			; 4.85663
	DD	0409d95bar			; 4.92453
	DD	040c963a4r			; 6.29341
	DD	03a91c087r			; 0.001112
	DD	04099419er			; 4.78926
	DD	040a050f0r			; 5.00988
	DD	040cddf8fr			; 6.43354
	DD	03a10b418r			; 0.000552
	DD	04098580ar			; 4.76075
	DD	040a2e4f1r			; 5.09045
	DD	040d32e1fr			; 6.59938
	DD	039ccff22r			; 0.000391
	DD	0409ec42cr			; 4.96145
	DD	040a38d95r			; 5.11103
	DD	040d8342fr			; 6.75637
	DD	039ae1049r			; 0.000332
	DD	0409f63f8r			; 4.98095
	DD	040a46b89r			; 5.13813
	DD	040dbc1e8r			; 6.86742
	DD	03952c387r			; 0.000201
	DD	040a5d297r			; 5.18196
	DD	04097af64r			; 4.74016
	DD	040cec07dr			; 6.461
	DD	0397ba882r			; 0.00024
	DD	040a5eeb7r			; 5.18539
	DD	0409bfeb0r			; 4.87484
	DD	040da3603r			; 6.81909
	DD	03908509cr			; 0.00013
	DD	040a8a6b5r			; 5.27035
	DD	040977eacr			; 4.73421
	DD	040da708fr			; 6.82624
	DD	038da1a93r			; 0.000104
	DD	040abe6e8r			; 5.37194
	DD	040930af4r			; 4.59509
	DD	040d5189ar			; 6.65925
	DD	038ae1049r			; 8.3e-05
	DD	040ab9581r			; 5.362
	DD	04093c250r			; 4.61747
	DD	040dacf03r			; 6.83777
	DD	03890b418r			; 6.9e-05
	DD	040a926e3r			; 5.286
	DD	0409828e7r			; 4.75499
	DD	040e516e1r			; 7.15904
	DD	0384d8559r			; 4.9e-05
	DD	040afa18cr			; 5.48847
	DD	0408cace9r			; 4.39611
	DD	040d74682r			; 6.72736
	DD	038734507r			; 5.8e-05
	DD	0409eafa3r			; 4.95894
	DD	040929321r			; 4.58046
	DD	040cf49a5r			; 6.47774
	DD	037eae18br			; 2.8e-05
	DD	040b0ae1br			; 5.52125
	DD	0408e1885r			; 4.44049
	DD	040e68f80r			; 7.20502
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	0379f6230r			; 1.9e-05
	DD	040b9f5d8r			; 5.81126
	DD	0408cc582r			; 4.39911
	DD	040eac30dr			; 7.33631
g_sao_edge_offsets DD 0ffffffffH
	DD	00H
	DD	01H
	DD	00H
	DD	00H
	DD	0ffffffffH
	DD	00H
	DD	01H
	DD	0ffffffffH
	DD	0ffffffffH
	DD	01H
	DD	01H
	DD	01H
	DD	0ffffffffH
	DD	0ffffffffH
	DD	01H
g_sig_last_scan_cg DQ FLAT:g_sig_last_scan_8x8
	DQ	FLAT:g_sig_last_scan_8x8+16
	DQ	FLAT:g_sig_last_scan_8x8+32
	DQ	FLAT:g_sig_last_scan_8x8
	DQ	FLAT:g_sig_last_scan_8x8+16
	DQ	FLAT:g_sig_last_scan_8x8+32
	DQ	FLAT:g_sig_last_scan_16x16
	DQ	0000000000000000H
	DQ	0000000000000000H
	DQ	FLAT:g_sig_last_scan_32x32
	DQ	0000000000000000H
	DQ	0000000000000000H
g_sig_last_scan_8x8 DD 00H
	DD	02H
	DD	01H
	DD	03H
	DD	00H
	DD	01H
	DD	02H
	DD	03H
	DD	00H
	DD	02H
	DD	01H
	DD	03H
	ORG $+3
$SG4294952254 DB 'quant', 00H
	ORG $+7
$SG4294952304 DB 'array_checksum', 00H
	ORG $+1
$SG4294952303 DB 'array_md5', 00H
	ORG $+6
$SG4294952302 DB 'reg_sad', 00H
$SG4294952301 DB 'sad_4x4', 00H
$SG4294952300 DB 'sad_8x8', 00H
$SG4294952299 DB 'sad_16x16', 00H
	ORG $+6
$SG4294952298 DB 'sad_32x32', 00H
	ORG $+6
$SG4294952297 DB 'sad_64x64', 00H
	ORG $+6
$SG4294952296 DB 'satd_4x4', 00H
	ORG $+7
$SG4294952295 DB 'satd_8x8', 00H
	ORG $+7
$SG4294952294 DB 'satd_16x16', 00H
	ORG $+5
$SG4294952293 DB 'satd_32x32', 00H
	ORG $+5
$SG4294952292 DB 'satd_64x64', 00H
	ORG $+5
$SG4294952291 DB 'satd_any_size', 00H
	ORG $+2
$SG4294952290 DB 'sad_4x4_dual', 00H
	ORG $+3
$SG4294952289 DB 'sad_8x8_dual', 00H
	ORG $+3
$SG4294952288 DB 'sad_16x16_dual', 00H
	ORG $+1
$SG4294952287 DB 'sad_32x32_dual', 00H
	ORG $+1
$SG4294952286 DB 'sad_64x64_dual', 00H
	ORG $+1
$SG4294952285 DB 'satd_4x4_dual', 00H
	ORG $+2
$SG4294952284 DB 'satd_8x8_dual', 00H
	ORG $+2
$SG4294952283 DB 'satd_16x16_dual', 00H
$SG4294952282 DB 'satd_32x32_dual', 00H
$SG4294952281 DB 'satd_64x64_dual', 00H
$SG4294952280 DB 'satd_any_size_quad', 00H
	ORG $+5
$SG4294952279 DB 'pixels_calc_ssd', 00H
$SG4294952278 DB 'bipred_average', 00H
	ORG $+1
$SG4294952277 DB 'get_optimized_sad', 00H
	ORG $+6
$SG4294952276 DB 'ver_sad', 00H
$SG4294952275 DB 'hor_sad', 00H
$SG4294952274 DB 'pixel_var', 00H
	ORG $+6
$SG4294952273 DB 'fast_forward_dst_4x4', 00H
	ORG $+3
$SG4294952272 DB 'dct_4x4', 00H
$SG4294952271 DB 'dct_8x8', 00H
$SG4294952270 DB 'dct_16x16', 00H
	ORG $+6
$SG4294952269 DB 'dct_32x32', 00H
	ORG $+6
$SG4294952268 DB 'fast_inverse_dst_4x4', 00H
	ORG $+3
$SG4294952267 DB 'idct_4x4', 00H
	ORG $+7
$SG4294952266 DB 'idct_8x8', 00H
	ORG $+7
$SG4294952265 DB 'idct_16x16', 00H
	ORG $+5
$SG4294952264 DB 'idct_32x32', 00H
	ORG $+5
$SG4294952263 DB 'filter_hpel_blocks_hor_ver_luma', 00H
$SG4294952262 DB 'filter_hpel_blocks_diag_luma', 00H
	ORG $+3
$SG4294952261 DB 'filter_qpel_blocks_hor_ver_luma', 00H
$SG4294952260 DB 'filter_qpel_blocks_diag_luma', 00H
	ORG $+3
$SG4294952259 DB 'sample_quarterpel_luma', 00H
	ORG $+1
$SG4294952258 DB 'sample_octpel_chroma', 00H
	ORG $+3
$SG4294952257 DB 'sample_quarterpel_luma_hi', 00H
	ORG $+6
$SG4294952256 DB 'sample_octpel_chroma_hi', 00H
$SG4294952255 DB 'get_extended_block', 00H
	ORG $+5
$SG4294952253 DB 'quantize_residual', 00H
	ORG $+6
$SG4294952252 DB 'dequant', 00H
$SG4294952251 DB 'coeff_abs_sum', 00H
	ORG $+2
$SG4294952250 DB 'fast_coeff_cost', 00H
$SG4294952249 DB 'angular_pred', 00H
	ORG $+3
$SG4294952248 DB 'intra_pred_planar', 00H
	ORG $+6
$SG4294952247 DB 'intra_pred_filtered_dc', 00H
	ORG $+1
$SG4294952246 DB 'sao_edge_ddistortion', 00H
	ORG $+3
$SG4294952245 DB 'calc_sao_edge_dir', 00H
	ORG $+6
$SG4294952244 DB 'sao_reconstruct_color', 00H
	ORG $+2
$SG4294952243 DB 'sao_band_ddistortion', 00H
	ORG $+3
$SG4294952242 DB 'encode_coeff_nxn', 00H
PUBLIC	kvz_strategy_register_dct_avx2
pdata	SEGMENT
$pdata$matrix_idct_32x32_avx2 DD imagerel matrix_idct_32x32_avx2
	DD	imagerel matrix_idct_32x32_avx2+104
	DD	imagerel $unwind$matrix_idct_32x32_avx2
$pdata$matrix_dct_32x32_avx2 DD imagerel matrix_dct_32x32_avx2
	DD	imagerel matrix_dct_32x32_avx2+97
	DD	imagerel $unwind$matrix_dct_32x32_avx2
$pdata$mul_clip_matrix_32x32_avx2 DD imagerel mul_clip_matrix_32x32_avx2
	DD	imagerel mul_clip_matrix_32x32_avx2+1130
	DD	imagerel $unwind$mul_clip_matrix_32x32_avx2
$pdata$matrix_dct_16x16_avx2 DD imagerel matrix_dct_16x16_avx2
	DD	imagerel matrix_dct_16x16_avx2+65
	DD	imagerel $unwind$matrix_dct_16x16_avx2
$pdata$matrix_idct_16x16_avx2 DD imagerel matrix_idct_16x16_avx2
	DD	imagerel matrix_idct_16x16_avx2+96
	DD	imagerel $unwind$matrix_idct_16x16_avx2
$pdata$partial_butterfly_inverse_16_avx2 DD imagerel partial_butterfly_inverse_16_avx2
	DD	imagerel partial_butterfly_inverse_16_avx2+756
	DD	imagerel $unwind$partial_butterfly_inverse_16_avx2
$pdata$transpose_16x16_stride DD imagerel transpose_16x16_stride
	DD	imagerel transpose_16x16_stride+752
	DD	imagerel $unwind$transpose_16x16_stride
$pdata$matmul_16x16_a_bt DD imagerel matmul_16x16_a_bt
	DD	imagerel matmul_16x16_a_bt+513
	DD	imagerel $unwind$matmul_16x16_a_bt
$pdata$matrix_idct_8x8_avx2 DD imagerel matrix_idct_8x8_avx2
	DD	imagerel matrix_idct_8x8_avx2+104
	DD	imagerel $unwind$matrix_idct_8x8_avx2
$pdata$matrix_dct_8x8_avx2 DD imagerel matrix_dct_8x8_avx2
	DD	imagerel matrix_dct_8x8_avx2+61
	DD	imagerel $unwind$matrix_dct_8x8_avx2
$pdata$matmul_8x8_a_bt DD imagerel matmul_8x8_a_bt
	DD	imagerel matmul_8x8_a_bt+695
	DD	imagerel $unwind$matmul_8x8_a_bt
$pdata$matmul_8x8_a_bt_t DD imagerel matmul_8x8_a_bt_t
	DD	imagerel matmul_8x8_a_bt_t+695
	DD	imagerel $unwind$matmul_8x8_a_bt_t
$pdata$mul_clip_matrix_8x8_avx2 DD imagerel mul_clip_matrix_8x8_avx2
	DD	imagerel mul_clip_matrix_8x8_avx2+849
	DD	imagerel $unwind$mul_clip_matrix_8x8_avx2
$pdata$matrix_idct_4x4_avx2 DD imagerel matrix_idct_4x4_avx2
	DD	imagerel matrix_idct_4x4_avx2+289
	DD	imagerel $unwind$matrix_idct_4x4_avx2
$pdata$matrix_dct_4x4_avx2 DD imagerel matrix_dct_4x4_avx2
	DD	imagerel matrix_dct_4x4_avx2+294
	DD	imagerel $unwind$matrix_dct_4x4_avx2
$pdata$matrix_idst_4x4_avx2 DD imagerel matrix_idst_4x4_avx2
	DD	imagerel matrix_idst_4x4_avx2+289
	DD	imagerel $unwind$matrix_idst_4x4_avx2
$pdata$matrix_dst_4x4_avx2 DD imagerel matrix_dst_4x4_avx2
	DD	imagerel matrix_dst_4x4_avx2+294
	DD	imagerel $unwind$matrix_dst_4x4_avx2
$pdata$mul_clip_matrix_4x4_avx2 DD imagerel mul_clip_matrix_4x4_avx2
	DD	imagerel mul_clip_matrix_4x4_avx2+167
	DD	imagerel $unwind$mul_clip_matrix_4x4_avx2
$pdata$kvz_strategy_register_dct_avx2 DD imagerel $LN5
	DD	imagerel $LN5+475
	DD	imagerel $unwind$kvz_strategy_register_dct_avx2
xdata	SEGMENT
$unwind$matrix_idct_32x32_avx2 DD 071b01H
	DD	010f7412H
	DD	010e3412H
	DD	010c0112H
	DD	0500bH
$unwind$matrix_dct_32x32_avx2 DD 051601H
	DD	010e340dH
	DD	010c010dH
	DD	05006H
$unwind$mul_clip_matrix_32x32_avx2 DD 0114601H
	DD	0106883dH
	DD	01077834H
	DD	0108682bH
	DD	0217e422H
	DD	02167422H
	DD	02156422H
	DD	02143422H
	DD	02120122H
	DD	05015H
$unwind$matrix_dct_16x16_avx2 DD 031201H
	DD	0480109H
	DD	05002H
$unwind$matrix_idct_16x16_avx2 DD 071b01H
	DD	04f7412H
	DD	04e3412H
	DD	04c0112H
	DD	0500bH
$unwind$partial_butterfly_inverse_16_avx2 DD 01b5701H
	DD	02cf84eH
	DD	02de846H
	DD	02ed83eH
	DD	02fc836H
	DD	030b831H
	DD	031a82cH
	DD	0329827H
	DD	0338822H
	DD	034781dH
	DD	0356818H
	DD	06f7413H
	DD	06e3413H
	DD	06c0113H
	DD	0500cH
$unwind$transpose_16x16_stride DD 0153f01H
	DD	022c836H
	DD	023b831H
	DD	024a82cH
	DD	0259827H
	DD	0268822H
	DD	027781dH
	DD	0286818H
	DD	0557413H
	DD	0543413H
	DD	0520113H
	DD	0500cH
$unwind$matmul_16x16_a_bt DD 0124001H
	DD	0d840H
	DD	01c832H
	DD	02b82dH
	DD	03a828H
	DD	049823H
	DD	05881eH
	DD	067819H
	DD	07680fH
	DD	011010aH
$unwind$matrix_idct_8x8_avx2 DD 071b01H
	DD	01f7412H
	DD	01e3412H
	DD	01c0112H
	DD	0500bH
$unwind$matrix_dct_8x8_avx2 DD 031201H
	DD	0180109H
	DD	05002H
$unwind$matmul_8x8_a_bt DD 0169201H
	DD	0f892H
	DD	01e88dH
	DD	02d887H
	DD	03c855H
	DD	04b84aH
	DD	05a83fH
	DD	06982aH
	DD	078824H
	DD	08781eH
	DD	096810H
	DD	015010aH
$unwind$matmul_8x8_a_bt_t DD 0169201H
	DD	0f892H
	DD	01e88dH
	DD	02d887H
	DD	03c855H
	DD	04b84aH
	DD	05a83fH
	DD	06982aH
	DD	078824H
	DD	08781eH
	DD	096810H
	DD	015010aH
$unwind$mul_clip_matrix_8x8_avx2 DD 0175901H
	DD	0ef850H
	DD	0fe847H
	DD	010d83eH
	DD	011c835H
	DD	012b82fH
	DD	013a829H
	DD	0149823H
	DD	015881dH
	DD	0167817H
	DD	0176811H
	DD	030010bH
	DD	05004H
$unwind$matrix_idct_4x4_avx2 DD 078f01H
	DD	0888fH
	DD	01782dH
	DD	02681aH
	DD	06204H
$unwind$matrix_dct_4x4_avx2 DD 073e01H
	DD	0883eH
	DD	01782cH
	DD	026822H
	DD	06204H
$unwind$matrix_idst_4x4_avx2 DD 078f01H
	DD	0888fH
	DD	01782dH
	DD	02681aH
	DD	06204H
$unwind$matrix_dst_4x4_avx2 DD 073e01H
	DD	0883eH
	DD	01782cH
	DD	026822H
	DD	06204H
$unwind$mul_clip_matrix_4x4_avx2 DD 073701H
	DD	08837H
	DD	017828H
	DD	02681aH
	DD	06204H
$unwind$kvz_strategy_register_dct_avx2 DD 060f01H
	DD	09640fH
	DD	08340fH
	DD	0700b520fH
$SG4294952223 DB 'avx2', 00H
$SG4294952225 DB 'avx2', 00H
$SG4294952227 DB 'avx2', 00H
$SG4294952229 DB 'avx2', 00H
$SG4294952231 DB 'avx2', 00H
$SG4294952233 DB 'avx2', 00H
$SG4294952235 DB 'avx2', 00H
$SG4294952222 DB 'idct_32x32', 00H
	ORG $+5
$SG4294952224 DB 'idct_16x16', 00H
	ORG $+5
$SG4294952226 DB 'idct_8x8', 00H
	ORG $+7
$SG4294952228 DB 'idct_4x4', 00H
	ORG $+7
$SG4294952230 DB 'fast_inverse_dst_4x4', 00H
	ORG $+3
$SG4294952232 DB 'dct_32x32', 00H
	ORG $+6
$SG4294952234 DB 'dct_16x16', 00H
	ORG $+6
$SG4294952236 DB 'dct_8x8', 00H
$SG4294952237 DB 'avx2', 00H
	ORG $+3
$SG4294952238 DB 'dct_4x4', 00H
$SG4294952239 DB 'avx2', 00H
	ORG $+3
$SG4294952240 DB 'fast_forward_dst_4x4', 00H
	ORG $+3
$SG4294952241 DB 'avx2', 00H
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
opaque$ = 64
bitdepth$dead$ = 72
kvz_strategy_register_dct_avx2 PROC

; 945  : {

$LN5:
	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rsi
	push	rdi
	sub	rsp, 48					; 00000030H

; 946  :   bool success = true;
; 947  : #if COMPILE_INTEL_AVX2
; 948  : #if KVZ_BIT_DEPTH == 8
; 949  :   if (bitdepth == 8){
; 950  :     success &= kvz_strategyselector_register(opaque, "fast_forward_dst_4x4", "avx2", 40, &matrix_dst_4x4_avx2);

	lea	rax, OFFSET FLAT:matrix_dst_4x4_avx2
	mov	r9d, 40					; 00000028H
	lea	r8, OFFSET FLAT:$SG4294952241
	mov	QWORD PTR [rsp+32], rax
	lea	rdx, OFFSET FLAT:$SG4294952240
	mov	rsi, rcx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 951  : 
; 952  :     success &= kvz_strategyselector_register(opaque, "dct_4x4", "avx2", 40, &matrix_dct_4x4_avx2);

	lea	r8, OFFSET FLAT:$SG4294952239
	lea	rax, OFFSET FLAT:matrix_dct_4x4_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294952238
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	ebx, 1
	call	kvz_strategyselector_register
	mov	edi, eax

; 953  :     success &= kvz_strategyselector_register(opaque, "dct_8x8", "avx2", 40, &matrix_dct_8x8_avx2);

	lea	r8, OFFSET FLAT:$SG4294952237
	lea	rax, OFFSET FLAT:matrix_dct_8x8_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294952236
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	edi, ebx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 954  :     success &= kvz_strategyselector_register(opaque, "dct_16x16", "avx2", 40, &matrix_dct_16x16_avx2);

	lea	r8, OFFSET FLAT:$SG4294952235
	lea	rax, OFFSET FLAT:matrix_dct_16x16_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294952234
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	ebx, edi
	call	kvz_strategyselector_register
	mov	edi, eax

; 955  :     success &= kvz_strategyselector_register(opaque, "dct_32x32", "avx2", 40, &matrix_dct_32x32_avx2);

	lea	r8, OFFSET FLAT:$SG4294952233
	lea	rax, OFFSET FLAT:matrix_dct_32x32_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294952232
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	edi, ebx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 956  : 
; 957  :     success &= kvz_strategyselector_register(opaque, "fast_inverse_dst_4x4", "avx2", 40, &matrix_idst_4x4_avx2);

	lea	r8, OFFSET FLAT:$SG4294952231
	lea	rax, OFFSET FLAT:matrix_idst_4x4_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294952230
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	ebx, edi
	call	kvz_strategyselector_register
	mov	edi, eax

; 958  : 
; 959  :     success &= kvz_strategyselector_register(opaque, "idct_4x4", "avx2", 40, &matrix_idct_4x4_avx2);

	lea	r8, OFFSET FLAT:$SG4294952229
	lea	rax, OFFSET FLAT:matrix_idct_4x4_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294952228
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	edi, ebx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 960  :     success &= kvz_strategyselector_register(opaque, "idct_8x8", "avx2", 40, &matrix_idct_8x8_avx2);

	lea	r8, OFFSET FLAT:$SG4294952227
	lea	rax, OFFSET FLAT:matrix_idct_8x8_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294952226
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	ebx, edi
	call	kvz_strategyselector_register
	mov	edi, eax

; 961  :     success &= kvz_strategyselector_register(opaque, "idct_16x16", "avx2", 40, &matrix_idct_16x16_avx2);

	lea	r8, OFFSET FLAT:$SG4294952225
	lea	rax, OFFSET FLAT:matrix_idct_16x16_avx2
	and	edi, ebx
	mov	r9d, 40					; 00000028H
	mov	QWORD PTR [rsp+32], rax
	lea	rdx, OFFSET FLAT:$SG4294952224
	mov	rcx, rsi
	call	kvz_strategyselector_register
	mov	ebx, eax

; 962  :     success &= kvz_strategyselector_register(opaque, "idct_32x32", "avx2", 40, &matrix_idct_32x32_avx2);

	lea	rax, OFFSET FLAT:matrix_idct_32x32_avx2
	mov	r9d, 40					; 00000028H
	mov	QWORD PTR [rsp+32], rax
	lea	r8, OFFSET FLAT:$SG4294952223
	mov	rcx, rsi
	lea	rdx, OFFSET FLAT:$SG4294952222
	call	kvz_strategyselector_register

; 963  :   }
; 964  : #endif // KVZ_BIT_DEPTH == 8
; 965  : #endif //COMPILE_INTEL_AVX2  
; 966  :   return success;
; 967  : }

	mov	rsi, QWORD PTR [rsp+72]
	xor	ecx, ecx
	and	ebx, edi
	mov	rbx, QWORD PTR [rsp+64]
	setne	cl
	and	eax, ecx
	add	rsp, 48					; 00000030H
	pop	rdi
	ret	0
kvz_strategy_register_dct_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
v$ = 8
swap_lanes PROC

; 66   :   return _mm256_permute4x64_epi64(v, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm0, YMMWORD PTR [rcx], 78		; 0000004eH

; 67   : }

	ret	0
swap_lanes ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
v$ = 8
debias$ = 16
shift$ = 24
truncate_avx2 PROC

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vmovdqu	ymm0, YMMWORD PTR [rcx]
	vpaddd	ymm2, ymm0, YMMWORD PTR [rdx]

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, r8d
	vpsrad	ymm0, ymm2, xmm1

; 73   : }

	ret	0
truncate_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
left$ = 64
right$ = 72
shift$ = 80
mul_clip_matrix_4x4_avx2 PROC

; 79   : {

	sub	rsp, 56					; 00000038H

; 82   : 
; 83   :   __m256i right_los = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(2, 0, 2, 0));

	vpermq	ymm2, YMMWORD PTR [rdx], 136		; 00000088H

; 84   :   __m256i right_his = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(3, 1, 3, 1));

	vpermq	ymm1, YMMWORD PTR [rdx], 221		; 000000ddH

; 85   : 
; 86   :   __m256i right_cols_up = _mm256_unpacklo_epi16(right_los, right_his);

	vpunpcklwd ymm4, ymm2, ymm1
	vmovaps	XMMWORD PTR [rsp+32], xmm6
	lea	eax, DWORD PTR [r8-1]

; 91   :   __m256i left_slice3 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(2, 2, 2, 2));

	vmovdqu	ymm6, YMMWORD PTR [rcx]
	vmovaps	XMMWORD PTR [rsp+16], xmm7
	mov	r9d, 1
	vpunpckhwd ymm7, ymm2, ymm1
	vmovaps	XMMWORD PTR [rsp], xmm8

; 92   :   __m256i left_slice4 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(3, 3, 3, 3));

	vpshufd	ymm1, ymm6, 255				; 000000ffH

; 96   :   __m256i prod3 = _mm256_madd_epi16(left_slice3, right_cols_up);
; 97   :   __m256i prod4 = _mm256_madd_epi16(left_slice4, right_cols_dn);

	vpmaddwd ymm2, ymm1, ymm7
	vpshufd	ymm0, ymm6, 170				; 000000aaH
	vpmaddwd ymm3, ymm0, ymm4

; 98   : 
; 99   :   __m256i rows_up = _mm256_add_epi32(prod1, prod2);
; 100  :   __m256i rows_dn = _mm256_add_epi32(prod3, prod4);

	vpaddd	ymm0, ymm2, ymm3

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, r8d

; 80   :   const int32_t add    = 1 << (shift - 1);

	shlx	eax, r9d, eax

; 81   :   const __m256i debias = _mm256_set1_epi32(add);

	vmovd	xmm8, eax
	vpbroadcastd ymm8, xmm8

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm0, ymm8

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vpsrad	ymm5, ymm3, xmm1

; 87   :   __m256i right_cols_dn = _mm256_unpackhi_epi16(right_los, right_his);
; 88   : 
; 89   :   __m256i left_slice1 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(0, 0, 0, 0));
; 90   :   __m256i left_slice2 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(1, 1, 1, 1));

	vpshufd	ymm1, ymm6, 85				; 00000055H

; 93   : 
; 94   :   __m256i prod1 = _mm256_madd_epi16(left_slice1, right_cols_up);
; 95   :   __m256i prod2 = _mm256_madd_epi16(left_slice2, right_cols_dn);

	vpmaddwd ymm2, ymm1, ymm7

; 106  :   return result;
; 107  : }

	vmovaps	xmm7, XMMWORD PTR [rsp+16]
	vpshufd	ymm0, ymm6, 0
	vmovaps	xmm6, XMMWORD PTR [rsp+32]
	vpmaddwd ymm4, ymm0, ymm4
	vpaddd	ymm0, ymm2, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm0, ymm8

; 106  :   return result;
; 107  : }

	vmovaps	xmm8, XMMWORD PTR [rsp]

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, r8d
	vpsrad	ymm2, ymm3, xmm1

; 101  : 
; 102  :   __m256i rows_up_tr = truncate_avx2(rows_up, debias, shift);
; 103  :   __m256i rows_dn_tr = truncate_avx2(rows_dn, debias, shift);
; 104  : 
; 105  :   __m256i result = _mm256_packs_epi32(rows_up_tr, rows_dn_tr);

	vpackssdw ymm0, ymm2, ymm5

; 106  :   return result;
; 107  : }

	add	rsp, 56					; 00000038H
	ret	0
mul_clip_matrix_4x4_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
bitdepth$ = 64
input$ = 72
output$ = 80
matrix_dst_4x4_avx2 PROC

; 110  : {

	sub	rsp, 56					; 00000038H

; 83   :   __m256i right_los = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(2, 0, 2, 0));

	vpermq	ymm2, YMMWORD PTR kvz_g_dst_4_t, 136	; 00000088H

; 84   :   __m256i right_his = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(3, 1, 3, 1));

	vpermq	ymm1, YMMWORD PTR kvz_g_dst_4_t, 221	; 000000ddH

; 85   : 
; 86   :   __m256i right_cols_up = _mm256_unpacklo_epi16(right_los, right_his);

	vpunpcklwd ymm4, ymm2, ymm1
	vmovaps	XMMWORD PTR [rsp+32], xmm6

; 87   :   __m256i right_cols_dn = _mm256_unpackhi_epi16(right_los, right_his);

	vpunpckhwd ymm6, ymm2, ymm1
	vmovaps	XMMWORD PTR [rsp+16], xmm7

; 111  :   int32_t shift_1st = kvz_g_convert_to_bit[4] + 1 + (bitdepth - 8);

	movsx	r9d, cl

; 80   :   const int32_t add    = 1 << (shift - 1);

	mov	ecx, 1

; 111  :   int32_t shift_1st = kvz_g_convert_to_bit[4] + 1 + (bitdepth - 8);

	sub	r9d, 7
	vmovaps	XMMWORD PTR [rsp], xmm8

; 112  :   int32_t shift_2nd = kvz_g_convert_to_bit[4] + 8;
; 113  :   const int16_t *tdst = &kvz_g_dst_4_t[0][0];
; 114  :   const int16_t *dst  = &kvz_g_dst_4  [0][0];
; 115  : 
; 116  :   __m256i tdst_v = _mm256_load_si256((const __m256i *) tdst);
; 117  :   __m256i  dst_v = _mm256_load_si256((const __m256i *)  dst);
; 118  :   __m256i   in_v = _mm256_load_si256((const __m256i *)input);
; 119  : 
; 120  :   __m256i tmp    = mul_clip_matrix_4x4_avx2(in_v,  tdst_v, shift_1st);

	vmovdqu	ymm8, YMMWORD PTR [rdx]

; 92   :   __m256i left_slice4 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(3, 3, 3, 3));

	vpshufd	ymm1, ymm8, 255				; 000000ffH

; 97   :   __m256i prod4 = _mm256_madd_epi16(left_slice4, right_cols_dn);

	vpmaddwd ymm2, ymm1, ymm6
	vpshufd	ymm0, ymm8, 170				; 000000aaH
	vpmaddwd ymm3, ymm0, ymm4

; 100  :   __m256i rows_dn = _mm256_add_epi32(prod3, prod4);

	vpaddd	ymm0, ymm2, ymm3

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, r9d

; 80   :   const int32_t add    = 1 << (shift - 1);

	lea	eax, DWORD PTR [r9-1]
	shlx	eax, ecx, eax

; 81   :   const __m256i debias = _mm256_set1_epi32(add);

	vmovd	xmm7, eax
	vpbroadcastd ymm7, xmm7

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm0, ymm7

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vpsrad	ymm5, ymm3, xmm1

; 90   :   __m256i left_slice2 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(1, 1, 1, 1));

	vpshufd	ymm1, ymm8, 85				; 00000055H

; 95   :   __m256i prod2 = _mm256_madd_epi16(left_slice2, right_cols_dn);

	vpmaddwd ymm2, ymm1, ymm6

; 91   :   __m256i left_slice3 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(2, 2, 2, 2));

	vmovdqu	ymm6, YMMWORD PTR kvz_g_dst_4

; 89   :   __m256i left_slice1 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(0, 0, 0, 0));

	vpshufd	ymm0, ymm8, 0

; 93   : 
; 94   :   __m256i prod1 = _mm256_madd_epi16(left_slice1, right_cols_up);

	vpmaddwd ymm4, ymm0, ymm4

; 98   : 
; 99   :   __m256i rows_up = _mm256_add_epi32(prod1, prod2);

	vpaddd	ymm0, ymm2, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm0, ymm7

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, r9d
	vpsrad	ymm2, ymm3, xmm1

; 105  :   __m256i result = _mm256_packs_epi32(rows_up_tr, rows_dn_tr);

	vpackssdw ymm0, ymm2, ymm5

; 83   :   __m256i right_los = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(2, 0, 2, 0));

	vpermq	ymm1, ymm0, 136				; 00000088H

; 84   :   __m256i right_his = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(3, 1, 3, 1));

	vpermq	ymm0, ymm0, 221				; 000000ddH

; 85   : 
; 86   :   __m256i right_cols_up = _mm256_unpacklo_epi16(right_los, right_his);

	vpunpcklwd ymm4, ymm1, ymm0

; 87   :   __m256i right_cols_dn = _mm256_unpackhi_epi16(right_los, right_his);

	vpunpckhwd ymm7, ymm1, ymm0

; 91   :   __m256i left_slice3 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(2, 2, 2, 2));

	vpshufd	ymm0, ymm6, 170				; 000000aaH

; 96   :   __m256i prod3 = _mm256_madd_epi16(left_slice3, right_cols_up);

	vpmaddwd ymm3, ymm0, ymm4
	vpshufd	ymm1, ymm6, 255				; 000000ffH

; 97   :   __m256i prod4 = _mm256_madd_epi16(left_slice4, right_cols_dn);

	vpmaddwd ymm2, ymm1, ymm7

; 100  :   __m256i rows_dn = _mm256_add_epi32(prod3, prod4);

	vpaddd	ymm0, ymm2, ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm0, YMMWORD PTR __ymm@0000008000000080000000800000008000000080000000800000008000000080

; 90   :   __m256i left_slice2 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(1, 1, 1, 1));

	vpshufd	ymm0, ymm6, 85				; 00000055H

; 95   :   __m256i prod2 = _mm256_madd_epi16(left_slice2, right_cols_dn);

	vpmaddwd ymm2, ymm0, ymm7
	vpshufd	ymm1, ymm6, 0

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vpsrad	ymm5, ymm3, 8

; 94   :   __m256i prod1 = _mm256_madd_epi16(left_slice1, right_cols_up);

	vpmaddwd ymm4, ymm1, ymm4

; 98   : 
; 99   :   __m256i rows_up = _mm256_add_epi32(prod1, prod2);

	vpaddd	ymm1, ymm2, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm1, YMMWORD PTR __ymm@0000008000000080000000800000008000000080000000800000008000000080

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vpsrad	ymm0, ymm3, 8

; 105  :   __m256i result = _mm256_packs_epi32(rows_up_tr, rows_dn_tr);

	vpackssdw ymm2, ymm0, ymm5

; 121  :   __m256i result = mul_clip_matrix_4x4_avx2(dst_v, tmp,    shift_2nd);
; 122  : 
; 123  :   _mm256_store_si256((__m256i *)output, result);

	vmovdqu	YMMWORD PTR [r8], ymm2
	vzeroupper

; 124  : }

	vmovaps	xmm6, XMMWORD PTR [rsp+32]
	vmovaps	xmm7, XMMWORD PTR [rsp+16]
	vmovaps	xmm8, XMMWORD PTR [rsp]
	add	rsp, 56					; 00000038H
	ret	0
matrix_dst_4x4_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
bitdepth$ = 64
input$ = 72
output$ = 80
matrix_idst_4x4_avx2 PROC

; 127  : {

	sub	rsp, 56					; 00000038H

; 83   :   __m256i right_los = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(2, 0, 2, 0));

	vpermq	ymm1, YMMWORD PTR [rdx], 136		; 00000088H

; 84   :   __m256i right_his = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(3, 1, 3, 1));

	vpermq	ymm0, YMMWORD PTR [rdx], 221		; 000000ddH

; 85   : 
; 86   :   __m256i right_cols_up = _mm256_unpacklo_epi16(right_los, right_his);

	vpunpcklwd ymm4, ymm1, ymm0
	vmovaps	XMMWORD PTR [rsp+32], xmm6

; 80   :   const int32_t add    = 1 << (shift - 1);

	mov	edx, 1

; 91   :   __m256i left_slice3 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(2, 2, 2, 2));

	vmovdqu	ymm6, YMMWORD PTR kvz_g_dst_4_t
	vmovaps	XMMWORD PTR [rsp+16], xmm7

; 87   :   __m256i right_cols_dn = _mm256_unpackhi_epi16(right_los, right_his);

	vpunpckhwd ymm7, ymm1, ymm0

; 91   :   __m256i left_slice3 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(2, 2, 2, 2));

	vpshufd	ymm0, ymm6, 170				; 000000aaH

; 96   :   __m256i prod3 = _mm256_madd_epi16(left_slice3, right_cols_up);

	vpmaddwd ymm3, ymm0, ymm4
	vpshufd	ymm1, ymm6, 255				; 000000ffH

; 97   :   __m256i prod4 = _mm256_madd_epi16(left_slice4, right_cols_dn);

	vpmaddwd ymm2, ymm1, ymm7

; 100  :   __m256i rows_dn = _mm256_add_epi32(prod3, prod4);

	vpaddd	ymm0, ymm2, ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm0, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vpsrad	ymm5, ymm3, 7

; 90   :   __m256i left_slice2 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(1, 1, 1, 1));

	vpshufd	ymm0, ymm6, 85				; 00000055H

; 93   : 
; 94   :   __m256i prod1 = _mm256_madd_epi16(left_slice1, right_cols_up);
; 95   :   __m256i prod2 = _mm256_madd_epi16(left_slice2, right_cols_dn);

	vpmaddwd ymm2, ymm0, ymm7
	vpshufd	ymm1, ymm6, 0
	vpmaddwd ymm4, ymm1, ymm4

; 98   : 
; 99   :   __m256i rows_up = _mm256_add_epi32(prod1, prod2);

	vpaddd	ymm1, ymm2, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm1, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 83   :   __m256i right_los = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(2, 0, 2, 0));

	vpermq	ymm2, YMMWORD PTR kvz_g_dst_4, 136	; 00000088H

; 84   :   __m256i right_his = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(3, 1, 3, 1));

	vpermq	ymm1, YMMWORD PTR kvz_g_dst_4, 221	; 000000ddH

; 85   : 
; 86   :   __m256i right_cols_up = _mm256_unpacklo_epi16(right_los, right_his);

	vpunpcklwd ymm4, ymm2, ymm1
	vmovaps	XMMWORD PTR [rsp], xmm8

; 87   :   __m256i right_cols_dn = _mm256_unpackhi_epi16(right_los, right_his);

	vpunpckhwd ymm6, ymm2, ymm1

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vpsrad	ymm0, ymm3, 7

; 105  :   __m256i result = _mm256_packs_epi32(rows_up_tr, rows_dn_tr);

	vpackssdw ymm8, ymm0, ymm5

; 92   :   __m256i left_slice4 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(3, 3, 3, 3));

	vpshufd	ymm1, ymm8, 255				; 000000ffH

; 97   :   __m256i prod4 = _mm256_madd_epi16(left_slice4, right_cols_dn);

	vpmaddwd ymm2, ymm1, ymm6

; 128  :   int32_t shift_1st = 7;
; 129  :   int32_t shift_2nd = 12 - (bitdepth - 8);

	movsx	eax, cl
	mov	ecx, 20
	sub	ecx, eax

; 91   :   __m256i left_slice3 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(2, 2, 2, 2));

	vpshufd	ymm0, ymm8, 170				; 000000aaH

; 96   :   __m256i prod3 = _mm256_madd_epi16(left_slice3, right_cols_up);

	vpmaddwd ymm3, ymm0, ymm4

; 100  :   __m256i rows_dn = _mm256_add_epi32(prod3, prod4);

	vpaddd	ymm0, ymm2, ymm3

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, ecx

; 80   :   const int32_t add    = 1 << (shift - 1);

	lea	eax, DWORD PTR [rcx-1]
	shlx	eax, edx, eax

; 81   :   const __m256i debias = _mm256_set1_epi32(add);

	vmovd	xmm7, eax
	vpbroadcastd ymm7, xmm7

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm0, ymm7

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vpsrad	ymm5, ymm3, xmm1

; 90   :   __m256i left_slice2 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(1, 1, 1, 1));

	vpshufd	ymm1, ymm8, 85				; 00000055H

; 93   : 
; 94   :   __m256i prod1 = _mm256_madd_epi16(left_slice1, right_cols_up);
; 95   :   __m256i prod2 = _mm256_madd_epi16(left_slice2, right_cols_dn);

	vpmaddwd ymm2, ymm1, ymm6
	vpshufd	ymm0, ymm8, 0
	vpmaddwd ymm4, ymm0, ymm4

; 98   : 
; 99   :   __m256i rows_up = _mm256_add_epi32(prod1, prod2);

	vpaddd	ymm0, ymm2, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm0, ymm7

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, ecx
	vpsrad	ymm2, ymm3, xmm1

; 105  :   __m256i result = _mm256_packs_epi32(rows_up_tr, rows_dn_tr);

	vpackssdw ymm0, ymm2, ymm5

; 130  : 
; 131  :   const int16_t *tdst = &kvz_g_dst_4_t[0][0];
; 132  :   const int16_t *dst  = &kvz_g_dst_4  [0][0];
; 133  : 
; 134  :   __m256i tdst_v = _mm256_load_si256((const __m256i *)tdst);
; 135  :   __m256i  dst_v = _mm256_load_si256((const __m256i *) dst);
; 136  :   __m256i   in_v = _mm256_load_si256((const __m256i *)input);
; 137  : 
; 138  :   __m256i tmp    = mul_clip_matrix_4x4_avx2(tdst_v, in_v,  shift_1st);
; 139  :   __m256i result = mul_clip_matrix_4x4_avx2(tmp,    dst_v, shift_2nd);
; 140  : 
; 141  :   _mm256_store_si256((__m256i *)output, result);

	vmovdqu	YMMWORD PTR [r8], ymm0
	vzeroupper

; 142  : }

	vmovaps	xmm6, XMMWORD PTR [rsp+32]
	vmovaps	xmm7, XMMWORD PTR [rsp+16]
	vmovaps	xmm8, XMMWORD PTR [rsp]
	add	rsp, 56					; 00000038H
	ret	0
matrix_idst_4x4_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
bitdepth$ = 64
input$ = 72
output$ = 80
matrix_dct_4x4_avx2 PROC

; 145  : {

	sub	rsp, 56					; 00000038H

; 83   :   __m256i right_los = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(2, 0, 2, 0));

	vpermq	ymm2, YMMWORD PTR kvz_g_dct_4_t, 136	; 00000088H

; 84   :   __m256i right_his = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(3, 1, 3, 1));

	vpermq	ymm1, YMMWORD PTR kvz_g_dct_4_t, 221	; 000000ddH

; 85   : 
; 86   :   __m256i right_cols_up = _mm256_unpacklo_epi16(right_los, right_his);

	vpunpcklwd ymm4, ymm2, ymm1
	vmovaps	XMMWORD PTR [rsp+32], xmm6

; 87   :   __m256i right_cols_dn = _mm256_unpackhi_epi16(right_los, right_his);

	vpunpckhwd ymm6, ymm2, ymm1
	vmovaps	XMMWORD PTR [rsp+16], xmm7

; 146  :   int32_t shift_1st = kvz_g_convert_to_bit[4] + 1 + (bitdepth - 8);

	movsx	r9d, cl

; 80   :   const int32_t add    = 1 << (shift - 1);

	mov	ecx, 1

; 146  :   int32_t shift_1st = kvz_g_convert_to_bit[4] + 1 + (bitdepth - 8);

	sub	r9d, 7
	vmovaps	XMMWORD PTR [rsp], xmm8

; 147  :   int32_t shift_2nd = kvz_g_convert_to_bit[4] + 8;
; 148  :   const int16_t *tdct = &kvz_g_dct_4_t[0][0];
; 149  :   const int16_t *dct  = &kvz_g_dct_4  [0][0];
; 150  : 
; 151  :   __m256i tdct_v = _mm256_load_si256((const __m256i *) tdct);
; 152  :   __m256i  dct_v = _mm256_load_si256((const __m256i *)  dct);
; 153  :   __m256i   in_v = _mm256_load_si256((const __m256i *)input);
; 154  : 
; 155  :   __m256i tmp    = mul_clip_matrix_4x4_avx2(in_v,  tdct_v, shift_1st);

	vmovdqu	ymm8, YMMWORD PTR [rdx]

; 92   :   __m256i left_slice4 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(3, 3, 3, 3));

	vpshufd	ymm1, ymm8, 255				; 000000ffH

; 97   :   __m256i prod4 = _mm256_madd_epi16(left_slice4, right_cols_dn);

	vpmaddwd ymm2, ymm1, ymm6
	vpshufd	ymm0, ymm8, 170				; 000000aaH
	vpmaddwd ymm3, ymm0, ymm4

; 100  :   __m256i rows_dn = _mm256_add_epi32(prod3, prod4);

	vpaddd	ymm0, ymm2, ymm3

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, r9d

; 80   :   const int32_t add    = 1 << (shift - 1);

	lea	eax, DWORD PTR [r9-1]
	shlx	eax, ecx, eax

; 81   :   const __m256i debias = _mm256_set1_epi32(add);

	vmovd	xmm7, eax
	vpbroadcastd ymm7, xmm7

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm0, ymm7

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vpsrad	ymm5, ymm3, xmm1

; 90   :   __m256i left_slice2 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(1, 1, 1, 1));

	vpshufd	ymm1, ymm8, 85				; 00000055H

; 95   :   __m256i prod2 = _mm256_madd_epi16(left_slice2, right_cols_dn);

	vpmaddwd ymm2, ymm1, ymm6

; 91   :   __m256i left_slice3 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(2, 2, 2, 2));

	vmovdqu	ymm6, YMMWORD PTR kvz_g_dct_4

; 89   :   __m256i left_slice1 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(0, 0, 0, 0));

	vpshufd	ymm0, ymm8, 0

; 93   : 
; 94   :   __m256i prod1 = _mm256_madd_epi16(left_slice1, right_cols_up);

	vpmaddwd ymm4, ymm0, ymm4

; 98   : 
; 99   :   __m256i rows_up = _mm256_add_epi32(prod1, prod2);

	vpaddd	ymm0, ymm2, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm0, ymm7

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, r9d
	vpsrad	ymm2, ymm3, xmm1

; 105  :   __m256i result = _mm256_packs_epi32(rows_up_tr, rows_dn_tr);

	vpackssdw ymm0, ymm2, ymm5

; 83   :   __m256i right_los = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(2, 0, 2, 0));

	vpermq	ymm1, ymm0, 136				; 00000088H

; 84   :   __m256i right_his = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(3, 1, 3, 1));

	vpermq	ymm0, ymm0, 221				; 000000ddH

; 85   : 
; 86   :   __m256i right_cols_up = _mm256_unpacklo_epi16(right_los, right_his);

	vpunpcklwd ymm4, ymm1, ymm0

; 87   :   __m256i right_cols_dn = _mm256_unpackhi_epi16(right_los, right_his);

	vpunpckhwd ymm7, ymm1, ymm0

; 91   :   __m256i left_slice3 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(2, 2, 2, 2));

	vpshufd	ymm0, ymm6, 170				; 000000aaH

; 96   :   __m256i prod3 = _mm256_madd_epi16(left_slice3, right_cols_up);

	vpmaddwd ymm3, ymm0, ymm4
	vpshufd	ymm1, ymm6, 255				; 000000ffH

; 97   :   __m256i prod4 = _mm256_madd_epi16(left_slice4, right_cols_dn);

	vpmaddwd ymm2, ymm1, ymm7

; 100  :   __m256i rows_dn = _mm256_add_epi32(prod3, prod4);

	vpaddd	ymm0, ymm2, ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm0, YMMWORD PTR __ymm@0000008000000080000000800000008000000080000000800000008000000080

; 90   :   __m256i left_slice2 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(1, 1, 1, 1));

	vpshufd	ymm0, ymm6, 85				; 00000055H

; 95   :   __m256i prod2 = _mm256_madd_epi16(left_slice2, right_cols_dn);

	vpmaddwd ymm2, ymm0, ymm7
	vpshufd	ymm1, ymm6, 0

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vpsrad	ymm5, ymm3, 8

; 94   :   __m256i prod1 = _mm256_madd_epi16(left_slice1, right_cols_up);

	vpmaddwd ymm4, ymm1, ymm4

; 98   : 
; 99   :   __m256i rows_up = _mm256_add_epi32(prod1, prod2);

	vpaddd	ymm1, ymm2, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm1, YMMWORD PTR __ymm@0000008000000080000000800000008000000080000000800000008000000080

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vpsrad	ymm0, ymm3, 8

; 105  :   __m256i result = _mm256_packs_epi32(rows_up_tr, rows_dn_tr);

	vpackssdw ymm2, ymm0, ymm5

; 156  :   __m256i result = mul_clip_matrix_4x4_avx2(dct_v, tmp,    shift_2nd);
; 157  : 
; 158  :   _mm256_store_si256((__m256i *)output, result);

	vmovdqu	YMMWORD PTR [r8], ymm2
	vzeroupper

; 159  : }

	vmovaps	xmm6, XMMWORD PTR [rsp+32]
	vmovaps	xmm7, XMMWORD PTR [rsp+16]
	vmovaps	xmm8, XMMWORD PTR [rsp]
	add	rsp, 56					; 00000038H
	ret	0
matrix_dct_4x4_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
bitdepth$ = 64
input$ = 72
output$ = 80
matrix_idct_4x4_avx2 PROC

; 162  : {

	sub	rsp, 56					; 00000038H

; 83   :   __m256i right_los = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(2, 0, 2, 0));

	vpermq	ymm1, YMMWORD PTR [rdx], 136		; 00000088H

; 84   :   __m256i right_his = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(3, 1, 3, 1));

	vpermq	ymm0, YMMWORD PTR [rdx], 221		; 000000ddH

; 85   : 
; 86   :   __m256i right_cols_up = _mm256_unpacklo_epi16(right_los, right_his);

	vpunpcklwd ymm4, ymm1, ymm0
	vmovaps	XMMWORD PTR [rsp+32], xmm6

; 80   :   const int32_t add    = 1 << (shift - 1);

	mov	edx, 1

; 91   :   __m256i left_slice3 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(2, 2, 2, 2));

	vmovdqu	ymm6, YMMWORD PTR kvz_g_dct_4_t
	vmovaps	XMMWORD PTR [rsp+16], xmm7

; 87   :   __m256i right_cols_dn = _mm256_unpackhi_epi16(right_los, right_his);

	vpunpckhwd ymm7, ymm1, ymm0

; 91   :   __m256i left_slice3 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(2, 2, 2, 2));

	vpshufd	ymm0, ymm6, 170				; 000000aaH

; 96   :   __m256i prod3 = _mm256_madd_epi16(left_slice3, right_cols_up);

	vpmaddwd ymm3, ymm0, ymm4
	vpshufd	ymm1, ymm6, 255				; 000000ffH

; 97   :   __m256i prod4 = _mm256_madd_epi16(left_slice4, right_cols_dn);

	vpmaddwd ymm2, ymm1, ymm7

; 100  :   __m256i rows_dn = _mm256_add_epi32(prod3, prod4);

	vpaddd	ymm0, ymm2, ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm0, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vpsrad	ymm5, ymm3, 7

; 90   :   __m256i left_slice2 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(1, 1, 1, 1));

	vpshufd	ymm0, ymm6, 85				; 00000055H

; 93   : 
; 94   :   __m256i prod1 = _mm256_madd_epi16(left_slice1, right_cols_up);
; 95   :   __m256i prod2 = _mm256_madd_epi16(left_slice2, right_cols_dn);

	vpmaddwd ymm2, ymm0, ymm7
	vpshufd	ymm1, ymm6, 0
	vpmaddwd ymm4, ymm1, ymm4

; 98   : 
; 99   :   __m256i rows_up = _mm256_add_epi32(prod1, prod2);

	vpaddd	ymm1, ymm2, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm1, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 83   :   __m256i right_los = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(2, 0, 2, 0));

	vpermq	ymm2, YMMWORD PTR kvz_g_dct_4, 136	; 00000088H

; 84   :   __m256i right_his = _mm256_permute4x64_epi64(right, _MM_SHUFFLE(3, 1, 3, 1));

	vpermq	ymm1, YMMWORD PTR kvz_g_dct_4, 221	; 000000ddH

; 85   : 
; 86   :   __m256i right_cols_up = _mm256_unpacklo_epi16(right_los, right_his);

	vpunpcklwd ymm4, ymm2, ymm1
	vmovaps	XMMWORD PTR [rsp], xmm8

; 87   :   __m256i right_cols_dn = _mm256_unpackhi_epi16(right_los, right_his);

	vpunpckhwd ymm6, ymm2, ymm1

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vpsrad	ymm0, ymm3, 7

; 105  :   __m256i result = _mm256_packs_epi32(rows_up_tr, rows_dn_tr);

	vpackssdw ymm8, ymm0, ymm5

; 92   :   __m256i left_slice4 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(3, 3, 3, 3));

	vpshufd	ymm1, ymm8, 255				; 000000ffH

; 97   :   __m256i prod4 = _mm256_madd_epi16(left_slice4, right_cols_dn);

	vpmaddwd ymm2, ymm1, ymm6

; 163  :   int32_t shift_1st = 7;
; 164  :   int32_t shift_2nd = 12 - (bitdepth - 8);

	movsx	eax, cl
	mov	ecx, 20
	sub	ecx, eax

; 91   :   __m256i left_slice3 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(2, 2, 2, 2));

	vpshufd	ymm0, ymm8, 170				; 000000aaH

; 96   :   __m256i prod3 = _mm256_madd_epi16(left_slice3, right_cols_up);

	vpmaddwd ymm3, ymm0, ymm4

; 100  :   __m256i rows_dn = _mm256_add_epi32(prod3, prod4);

	vpaddd	ymm0, ymm2, ymm3

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, ecx

; 80   :   const int32_t add    = 1 << (shift - 1);

	lea	eax, DWORD PTR [rcx-1]
	shlx	eax, edx, eax

; 81   :   const __m256i debias = _mm256_set1_epi32(add);

	vmovd	xmm7, eax
	vpbroadcastd ymm7, xmm7

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm0, ymm7

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vpsrad	ymm5, ymm3, xmm1

; 90   :   __m256i left_slice2 = _mm256_shuffle_epi32(left, _MM_SHUFFLE(1, 1, 1, 1));

	vpshufd	ymm1, ymm8, 85				; 00000055H

; 93   : 
; 94   :   __m256i prod1 = _mm256_madd_epi16(left_slice1, right_cols_up);
; 95   :   __m256i prod2 = _mm256_madd_epi16(left_slice2, right_cols_dn);

	vpmaddwd ymm2, ymm1, ymm6
	vpshufd	ymm0, ymm8, 0
	vpmaddwd ymm4, ymm0, ymm4

; 98   : 
; 99   :   __m256i rows_up = _mm256_add_epi32(prod1, prod2);

	vpaddd	ymm0, ymm2, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm0, ymm7

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, ecx
	vpsrad	ymm2, ymm3, xmm1

; 105  :   __m256i result = _mm256_packs_epi32(rows_up_tr, rows_dn_tr);

	vpackssdw ymm0, ymm2, ymm5

; 165  : 
; 166  :   const int16_t *tdct = &kvz_g_dct_4_t[0][0];
; 167  :   const int16_t *dct  = &kvz_g_dct_4  [0][0];
; 168  : 
; 169  :   __m256i tdct_v = _mm256_load_si256((const __m256i *)tdct);
; 170  :   __m256i  dct_v = _mm256_load_si256((const __m256i *) dct);
; 171  :   __m256i   in_v = _mm256_load_si256((const __m256i *)input);
; 172  : 
; 173  :   __m256i tmp    = mul_clip_matrix_4x4_avx2(tdct_v, in_v,  shift_1st);
; 174  :   __m256i result = mul_clip_matrix_4x4_avx2(tmp,    dct_v, shift_2nd);
; 175  : 
; 176  :   _mm256_store_si256((__m256i *)output, result);

	vmovdqu	YMMWORD PTR [r8], ymm0
	vzeroupper

; 177  : }

	vmovaps	xmm6, XMMWORD PTR [rsp+32]
	vmovaps	xmm7, XMMWORD PTR [rsp+16]
	vmovaps	xmm8, XMMWORD PTR [rsp]
	add	rsp, 56					; 00000038H
	ret	0
matrix_idct_4x4_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
debias$1$ = 0
tv1272 = 32
tv1273 = 64
tv1271 = 96
tv1263 = 128
tv1262 = 160
left$ = 400
right$ = 408
dst$ = 416
shift$ = 424
mul_clip_matrix_8x8_avx2 PROC

; 180  : {

	mov	r11, rsp
	push	rbp
	sub	rsp, 384				; 00000180H
	vmovaps	XMMWORD PTR [r11-24], xmm6
	vmovaps	XMMWORD PTR [r11-40], xmm7
	vmovaps	XMMWORD PTR [r11-56], xmm8
	vmovaps	XMMWORD PTR [r11-72], xmm9
	vmovaps	XMMWORD PTR [r11-88], xmm10
	vmovaps	XMMWORD PTR [r11-104], xmm11
	vmovaps	XMMWORD PTR [r11-120], xmm12
	vmovaps	XMMWORD PTR [r11-136], xmm13
	vmovaps	XMMWORD PTR [r11-152], xmm14
	vmovaps	XMMWORD PTR [r11-168], xmm15
	lea	rbp, QWORD PTR [rsp+32]
	and	rbp, -32				; ffffffffffffffe0H

; 181  :   const __m256i transp_mask = _mm256_broadcastsi128_si256(_mm_setr_epi8(0, 1, 8, 9, 2, 3, 10, 11, 4, 5, 12, 13, 6, 7, 14, 15));

	vbroadcasti128 ymm5, XMMWORD PTR __xmm@0f0e07060d0c05040b0a030209080100

; 182  : 
; 183  :   const int32_t add    = 1 << (shift - 1);
; 184  :   const __m256i debias = _mm256_set1_epi32(add);
; 185  : 
; 186  :   __m256i left_dr[4] = {

	vmovdqu	ymm9, YMMWORD PTR [rcx]
	vmovdqu	ymm10, YMMWORD PTR [rcx+32]

; 187  :     _mm256_load_si256((const __m256i *)left + 0),
; 188  :     _mm256_load_si256((const __m256i *)left + 1),
; 189  :     _mm256_load_si256((const __m256i *)left + 2),
; 190  :     _mm256_load_si256((const __m256i *)left + 3),
; 191  :   };
; 192  :   __m256i right_dr[4] = {
; 193  :     _mm256_load_si256((const __m256i *)right + 0),
; 194  :     _mm256_load_si256((const __m256i *)right + 1),
; 195  :     _mm256_load_si256((const __m256i *)right + 2),
; 196  :     _mm256_load_si256((const __m256i *)right + 3),
; 197  :   };
; 198  : 
; 199  :   __m256i rdrs_rearr[8];
; 200  : 
; 201  :   // Rearrange right matrix
; 202  :   for (int32_t dry = 0; dry < 4; dry++) {
; 203  :     __m256i rdr = right_dr[dry];
; 204  :     __m256i rdr_los = _mm256_permute4x64_epi64(rdr, _MM_SHUFFLE(2, 0, 2, 0));
; 205  :     __m256i rdr_his = _mm256_permute4x64_epi64(rdr, _MM_SHUFFLE(3, 1, 3, 1));

	vpermq	ymm1, YMMWORD PTR [rdx], 221		; 000000ddH
	vmovdqu	ymm11, YMMWORD PTR [rcx+64]

; 220  :     __m256i ldr_slice56 = _mm256_shuffle_epi32(ldr, _MM_SHUFFLE(2, 2, 2, 2));

	vpshufd	ymm6, ymm9, 170				; 000000aaH

; 221  :     __m256i ldr_slice78 = _mm256_shuffle_epi32(ldr, _MM_SHUFFLE(3, 3, 3, 3));

	vpshufd	ymm4, ymm9, 255				; 000000ffH
	vpshufd	ymm8, ymm9, 0
	vpshufd	ymm7, ymm9, 85				; 00000055H
	lea	eax, DWORD PTR [r9-1]
	mov	r10d, 1
	shlx	eax, r10d, eax
	vmovd	xmm0, eax
	vpbroadcastd ymm0, xmm0
	vmovdqu	YMMWORD PTR debias$1$[rbp], ymm0
	vmovdqu	ymm0, YMMWORD PTR [rcx+96]
	vmovdqu	YMMWORD PTR tv1262[rbp], ymm0
	vpermq	ymm0, YMMWORD PTR [rdx], 136		; 00000088H
	vpshufb	ymm0, ymm0, ymm5
	vmovdqu	YMMWORD PTR tv1271[rbp], ymm0

; 222  : 
; 223  :     __m256i prod1 = _mm256_madd_epi16(ldr_slice12, rdrs_rearr[0]);

	vpmaddwd ymm2, ymm8, YMMWORD PTR tv1271[rbp]
	vpshufb	ymm0, ymm1, ymm5
	vmovdqu	YMMWORD PTR tv1272[rbp], ymm0
	vpermq	ymm0, YMMWORD PTR [rdx+32], 136		; 00000088H
	vpshufb	ymm1, ymm0, ymm5
	vmovdqu	YMMWORD PTR tv1263[rbp], ymm1
	vpermq	ymm1, YMMWORD PTR [rdx+32], 221		; 000000ddH

; 225  :     __m256i prod3 = _mm256_madd_epi16(ldr_slice34, rdrs_rearr[2]);

	vmovdqu	ymm9, YMMWORD PTR tv1263[rbp]
	vpshufb	ymm0, ymm1, ymm5
	vpermq	ymm1, YMMWORD PTR [rdx+64], 221		; 000000ddH
	vpshufb	ymm15, ymm1, ymm5
	vpermq	ymm1, YMMWORD PTR [rdx+96], 221		; 000000ddH
	vmovdqu	YMMWORD PTR tv1273[rbp], ymm0
	vpermq	ymm0, YMMWORD PTR [rdx+64], 136		; 00000088H
	vpshufb	ymm14, ymm1, ymm5
	vpshufb	ymm13, ymm0, ymm5
	vpermq	ymm0, YMMWORD PTR [rdx+96], 136		; 00000088H
	vpshufb	ymm12, ymm0, ymm5

; 227  :     __m256i prod5 = _mm256_madd_epi16(ldr_slice56, rdrs_rearr[4]);
; 228  :     __m256i prod6 = _mm256_madd_epi16(ldr_slice56, rdrs_rearr[5]);
; 229  :     __m256i prod7 = _mm256_madd_epi16(ldr_slice78, rdrs_rearr[6]);

	vpmaddwd ymm1, ymm12, ymm4
	vpmaddwd ymm0, ymm13, ymm6

; 234  :     __m256i lo_2 = _mm256_add_epi32(prod5, prod7);

	vpaddd	ymm3, ymm0, ymm1
	vpmaddwd ymm1, ymm9, ymm7
	vpaddd	ymm0, ymm1, ymm2

; 236  : 
; 237  :     __m256i lo   = _mm256_add_epi32(lo_1,  lo_2);

	vpaddd	ymm2, ymm0, ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm2, YMMWORD PTR debias$1$[rbp]

; 230  :     __m256i prod8 = _mm256_madd_epi16(ldr_slice78, rdrs_rearr[7]);

	vpmaddwd ymm2, ymm14, ymm4
	vpmaddwd ymm0, ymm15, ymm6

; 235  :     __m256i hi_2 = _mm256_add_epi32(prod6, prod8);

	vpaddd	ymm4, ymm0, ymm2
	vpshufd	ymm6, ymm10, 170			; 000000aaH

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, r9d
	vpsrad	ymm5, ymm3, xmm1

; 224  :     __m256i prod2 = _mm256_madd_epi16(ldr_slice12, rdrs_rearr[1]);

	vpmaddwd ymm1, ymm8, YMMWORD PTR tv1272[rbp]

; 226  :     __m256i prod4 = _mm256_madd_epi16(ldr_slice34, rdrs_rearr[3]);

	vpmaddwd ymm3, ymm7, YMMWORD PTR tv1273[rbp]

; 231  : 
; 232  :     __m256i lo_1 = _mm256_add_epi32(prod1, prod3);
; 233  :     __m256i hi_1 = _mm256_add_epi32(prod2, prod4);

	vpaddd	ymm0, ymm1, ymm3

; 238  :     __m256i hi   = _mm256_add_epi32(hi_1,  hi_2);

	vpaddd	ymm2, ymm0, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm4, ymm2, YMMWORD PTR debias$1$[rbp]

; 206  : 
; 207  :     __m256i rdr_lo_rearr = _mm256_shuffle_epi8(rdr_los, transp_mask);
; 208  :     __m256i rdr_hi_rearr = _mm256_shuffle_epi8(rdr_his, transp_mask);
; 209  : 
; 210  :     rdrs_rearr[dry * 2 + 0] = rdr_lo_rearr;
; 211  :     rdrs_rearr[dry * 2 + 1] = rdr_hi_rearr;
; 212  :   }
; 213  : 
; 214  :   // Double-Row Y for destination matrix
; 215  :   for (int32_t dry = 0; dry < 4; dry++) {
; 216  :     __m256i ldr = left_dr[dry];
; 217  : 
; 218  :     __m256i ldr_slice12 = _mm256_shuffle_epi32(ldr, _MM_SHUFFLE(0, 0, 0, 0));

	vpshufd	ymm8, ymm10, 0

; 219  :     __m256i ldr_slice34 = _mm256_shuffle_epi32(ldr, _MM_SHUFFLE(1, 1, 1, 1));

	vpshufd	ymm7, ymm10, 85				; 00000055H

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, r9d
	vpsrad	ymm0, ymm4, xmm1

; 239  : 
; 240  :     __m256i lo_tr = truncate_avx2(lo, debias, shift);
; 241  :     __m256i hi_tr = truncate_avx2(hi, debias, shift);
; 242  : 
; 243  :     __m256i final_dr = _mm256_packs_epi32(lo_tr, hi_tr);

	vpackssdw ymm2, ymm5, ymm0
	vpshufd	ymm4, ymm10, 255			; 000000ffH
	vmovdqu	ymm10, YMMWORD PTR tv1271[rbp]

; 244  : 
; 245  :     _mm256_store_si256((__m256i *)dst + dry, final_dr);

	vmovdqu	YMMWORD PTR [r8], ymm2
	vpmaddwd ymm1, ymm12, ymm4
	vpmaddwd ymm0, ymm13, ymm6
	vpaddd	ymm3, ymm0, ymm1
	vpmaddwd ymm1, ymm9, ymm7
	vpmaddwd ymm2, ymm10, ymm8
	vpaddd	ymm0, ymm1, ymm2
	vpaddd	ymm2, ymm0, ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm2, YMMWORD PTR debias$1$[rbp]

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, r9d
	vpsrad	ymm5, ymm3, xmm1

; 226  :     __m256i prod4 = _mm256_madd_epi16(ldr_slice34, rdrs_rearr[3]);

	vpmaddwd ymm3, ymm7, YMMWORD PTR tv1273[rbp]
	vpmaddwd ymm1, ymm8, YMMWORD PTR tv1272[rbp]

; 230  :     __m256i prod8 = _mm256_madd_epi16(ldr_slice78, rdrs_rearr[7]);

	vpmaddwd ymm2, ymm14, ymm4
	vpmaddwd ymm0, ymm15, ymm6

; 235  :     __m256i hi_2 = _mm256_add_epi32(prod6, prod8);

	vpaddd	ymm4, ymm0, ymm2
	vpaddd	ymm0, ymm1, ymm3

; 238  :     __m256i hi   = _mm256_add_epi32(hi_1,  hi_2);

	vpaddd	ymm2, ymm0, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm4, ymm2, YMMWORD PTR debias$1$[rbp]

; 220  :     __m256i ldr_slice56 = _mm256_shuffle_epi32(ldr, _MM_SHUFFLE(2, 2, 2, 2));

	vpshufd	ymm6, ymm11, 170			; 000000aaH
	vpshufd	ymm8, ymm11, 0
	vpshufd	ymm7, ymm11, 85				; 00000055H

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, r9d
	vpsrad	ymm0, ymm4, xmm1

; 239  : 
; 240  :     __m256i lo_tr = truncate_avx2(lo, debias, shift);
; 241  :     __m256i hi_tr = truncate_avx2(hi, debias, shift);
; 242  : 
; 243  :     __m256i final_dr = _mm256_packs_epi32(lo_tr, hi_tr);

	vpackssdw ymm2, ymm5, ymm0

; 244  : 
; 245  :     _mm256_store_si256((__m256i *)dst + dry, final_dr);

	vmovdqu	YMMWORD PTR [r8+32], ymm2
	vpshufd	ymm4, ymm11, 255			; 000000ffH
	vpmaddwd ymm1, ymm12, ymm4
	vpmaddwd ymm0, ymm13, ymm6
	vpaddd	ymm3, ymm0, ymm1
	vpmaddwd ymm1, ymm9, ymm7
	vpmaddwd ymm2, ymm10, ymm8
	vpaddd	ymm0, ymm1, ymm2
	vpaddd	ymm2, ymm0, ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm2, YMMWORD PTR debias$1$[rbp]

; 230  :     __m256i prod8 = _mm256_madd_epi16(ldr_slice78, rdrs_rearr[7]);

	vpmaddwd ymm2, ymm14, ymm4
	vpmaddwd ymm0, ymm15, ymm6

; 235  :     __m256i hi_2 = _mm256_add_epi32(prod6, prod8);

	vpaddd	ymm4, ymm0, ymm2

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, r9d
	vpsrad	ymm5, ymm3, xmm1

; 224  :     __m256i prod2 = _mm256_madd_epi16(ldr_slice12, rdrs_rearr[1]);

	vpmaddwd ymm1, ymm8, YMMWORD PTR tv1272[rbp]

; 226  :     __m256i prod4 = _mm256_madd_epi16(ldr_slice34, rdrs_rearr[3]);

	vpmaddwd ymm3, ymm7, YMMWORD PTR tv1273[rbp]

; 231  : 
; 232  :     __m256i lo_1 = _mm256_add_epi32(prod1, prod3);
; 233  :     __m256i hi_1 = _mm256_add_epi32(prod2, prod4);

	vpaddd	ymm0, ymm1, ymm3

; 238  :     __m256i hi   = _mm256_add_epi32(hi_1,  hi_2);

	vpaddd	ymm2, ymm0, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm4, ymm2, YMMWORD PTR debias$1$[rbp]

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, r9d
	vpsrad	ymm0, ymm4, xmm1

; 239  : 
; 240  :     __m256i lo_tr = truncate_avx2(lo, debias, shift);
; 241  :     __m256i hi_tr = truncate_avx2(hi, debias, shift);
; 242  : 
; 243  :     __m256i final_dr = _mm256_packs_epi32(lo_tr, hi_tr);

	vpackssdw ymm2, ymm5, ymm0
	vmovdqu	ymm0, YMMWORD PTR tv1262[rbp]
	vpshufd	ymm4, ymm0, 255				; 000000ffH
	vpshufd	ymm8, ymm0, 0
	vpshufd	ymm7, ymm0, 85				; 00000055H

; 244  : 
; 245  :     _mm256_store_si256((__m256i *)dst + dry, final_dr);

	vmovdqu	YMMWORD PTR [r8+64], ymm2
	vpshufd	ymm6, ymm0, 170				; 000000aaH
	vpmaddwd ymm1, ymm12, ymm4
	vpmaddwd ymm0, ymm13, ymm6
	vpaddd	ymm3, ymm0, ymm1
	vpmaddwd ymm1, ymm9, ymm7
	vpmaddwd ymm2, ymm10, ymm8
	vpaddd	ymm0, ymm1, ymm2
	vpaddd	ymm2, ymm0, ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm2, YMMWORD PTR debias$1$[rbp]

; 230  :     __m256i prod8 = _mm256_madd_epi16(ldr_slice78, rdrs_rearr[7]);

	vpmaddwd ymm2, ymm14, ymm4
	vpmaddwd ymm0, ymm15, ymm6

; 235  :     __m256i hi_2 = _mm256_add_epi32(prod6, prod8);

	vpaddd	ymm4, ymm0, ymm2

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, r9d
	vpsrad	ymm5, ymm3, xmm1

; 224  :     __m256i prod2 = _mm256_madd_epi16(ldr_slice12, rdrs_rearr[1]);

	vpmaddwd ymm1, ymm8, YMMWORD PTR tv1272[rbp]

; 226  :     __m256i prod4 = _mm256_madd_epi16(ldr_slice34, rdrs_rearr[3]);

	vpmaddwd ymm3, ymm7, YMMWORD PTR tv1273[rbp]

; 231  : 
; 232  :     __m256i lo_1 = _mm256_add_epi32(prod1, prod3);
; 233  :     __m256i hi_1 = _mm256_add_epi32(prod2, prod4);

	vpaddd	ymm0, ymm1, ymm3

; 238  :     __m256i hi   = _mm256_add_epi32(hi_1,  hi_2);

	vpaddd	ymm2, ymm0, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm4, ymm2, YMMWORD PTR debias$1$[rbp]

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, r9d
	vpsrad	ymm0, ymm4, xmm1

; 239  : 
; 240  :     __m256i lo_tr = truncate_avx2(lo, debias, shift);
; 241  :     __m256i hi_tr = truncate_avx2(hi, debias, shift);
; 242  : 
; 243  :     __m256i final_dr = _mm256_packs_epi32(lo_tr, hi_tr);

	vpackssdw ymm2, ymm5, ymm0

; 244  : 
; 245  :     _mm256_store_si256((__m256i *)dst + dry, final_dr);

	vmovdqu	YMMWORD PTR [r8+96], ymm2
	vzeroupper

; 246  :   }
; 247  : }

	lea	r11, QWORD PTR [r11-8]
	vmovaps	xmm6, XMMWORD PTR [r11-16]
	vmovaps	xmm7, XMMWORD PTR [r11-32]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm9, XMMWORD PTR [r11-64]
	vmovaps	xmm10, XMMWORD PTR [r11-80]
	vmovaps	xmm11, XMMWORD PTR [r11-96]
	vmovaps	xmm12, XMMWORD PTR [r11-112]
	vmovaps	xmm13, XMMWORD PTR [r11-128]
	vmovaps	xmm14, XMMWORD PTR [r11-144]
	vmovaps	xmm15, XMMWORD PTR [r11-160]
	mov	rsp, r11
	pop	rbp
	ret	0
mul_clip_matrix_8x8_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
a$ = 176
b_t$dead$ = 184
output$ = 192
shift$ = 200
matmul_8x8_a_bt_t PROC

; 253  : {

	mov	r11, rsp
	sub	rsp, 168				; 000000a8H
	vmovaps	XMMWORD PTR [r11-24], xmm6

; 256  : 
; 257  :   // Keep upper row intact and swap neighboring 16-bit words in lower row
; 258  :   const __m256i shuf_lorow_mask =
; 259  :       _mm256_setr_epi8(0,  1,  2,  3,  4,  5,  6,  7,
; 260  :                        8,  9,  10, 11, 12, 13, 14, 15,
; 261  :                        18, 19, 16, 17, 22, 23, 20, 21,
; 262  :                        26, 27, 24, 25, 30, 31, 28, 29);
; 263  : 
; 264  :   const __m256i *b_t_256 = (const __m256i *)b_t;
; 265  : 
; 266  :   // Dual Rows, because two 8x16b words fit in one YMM
; 267  :   __m256i a_dr_0      = _mm256_load_si256((__m256i *)a + 0);
; 268  :   __m256i a_dr_1      = _mm256_load_si256((__m256i *)a + 1);
; 269  :   __m256i a_dr_2      = _mm256_load_si256((__m256i *)a + 2);
; 270  :   __m256i a_dr_3      = _mm256_load_si256((__m256i *)a + 3);
; 271  : 
; 272  :   __m256i a_dr_0_swp  = swap_lanes(a_dr_0);
; 273  :   __m256i a_dr_1_swp  = swap_lanes(a_dr_1);
; 274  :   __m256i a_dr_2_swp  = swap_lanes(a_dr_2);
; 275  :   __m256i a_dr_3_swp  = swap_lanes(a_dr_3);
; 276  : 
; 277  :   for (int dry = 0; dry < 4; dry++) {
; 278  : 
; 279  :     // Read dual columns of B matrix by reading rows of its transpose
; 280  :     __m256i b_dc        = _mm256_load_si256(b_t_256 + dry);

	vmovdqu	ymm6, YMMWORD PTR kvz_g_dct_8
	vmovaps	XMMWORD PTR [r11-40], xmm7
	vmovaps	XMMWORD PTR [r11-56], xmm8
	vmovaps	XMMWORD PTR [r11-72], xmm9
	vmovdqu	ymm9, YMMWORD PTR [rcx+96]
	movsx	edx, r9b
	mov	r9d, 1
	vmovaps	XMMWORD PTR [r11-88], xmm10
	vmovdqu	ymm10, YMMWORD PTR [rcx+64]
	vmovaps	XMMWORD PTR [r11-104], xmm11

; 288  :     __m256i prod3       = _mm256_madd_epi16(b_dc,     a_dr_3);

	vpmaddwd ymm0, ymm6, ymm9
	vmovaps	XMMWORD PTR [r11-120], xmm12
	lea	eax, DWORD PTR [rdx-1]

; 66   :   return _mm256_permute4x64_epi64(v, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm7, ymm9, 78				; 0000004eH

; 289  :     __m256i prod3_swp   = _mm256_madd_epi16(b_dc,     a_dr_3_swp);

	vpmaddwd ymm1, ymm6, ymm7

; 294  :     __m256i hsum3       = _mm256_hadd_epi32(prod3,    prod3_swp);

	vphaddd	ymm3, ymm0, ymm1
	vpmaddwd ymm1, ymm6, ymm10

; 66   :   return _mm256_permute4x64_epi64(v, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm8, ymm10, 78				; 0000004eH

; 286  :     __m256i prod2       = _mm256_madd_epi16(b_dc,     a_dr_2);
; 287  :     __m256i prod2_swp   = _mm256_madd_epi16(b_dc,     a_dr_2_swp);

	vpmaddwd ymm2, ymm6, ymm8

; 293  :     __m256i hsum2       = _mm256_hadd_epi32(prod2,    prod2_swp);

	vphaddd	ymm0, ymm1, ymm2

; 297  :     __m256i hsum2c_1    = _mm256_hadd_epi32(hsum2,    hsum3);

	vphaddd	ymm2, ymm0, ymm3
	vmovaps	XMMWORD PTR [rsp+32], xmm13
	vmovaps	XMMWORD PTR [rsp+16], xmm14
	vmovaps	XMMWORD PTR [rsp], xmm15
	vmovdqu	ymm13, YMMWORD PTR [rcx+32]
	vmovdqu	ymm14, YMMWORD PTR [rcx]
	vpmaddwd ymm0, ymm6, ymm13

; 66   :   return _mm256_permute4x64_epi64(v, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm11, ymm13, 78			; 0000004eH

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, edx

; 66   :   return _mm256_permute4x64_epi64(v, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm12, ymm14, 78			; 0000004eH

; 254  :   const int32_t add    = 1 << (shift - 1);

	shlx	eax, r9d, eax

; 255  :   const __m256i debias = _mm256_set1_epi32(add);

	vmovd	xmm15, eax
	vpbroadcastd ymm15, xmm15

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm2, ymm15

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vpsrad	ymm5, ymm3, xmm1

; 281  : 
; 282  :     __m256i prod0       = _mm256_madd_epi16(b_dc,     a_dr_0);

	vpmaddwd ymm1, ymm6, ymm14

; 285  :     __m256i prod1_swp   = _mm256_madd_epi16(b_dc,     a_dr_1_swp);

	vpmaddwd ymm2, ymm6, ymm11

; 292  :     __m256i hsum1       = _mm256_hadd_epi32(prod1,    prod1_swp);

	vphaddd	ymm4, ymm0, ymm2
	vpmaddwd ymm3, ymm6, ymm12
	vmovdqu	ymm6, YMMWORD PTR kvz_g_dct_8+32
	vphaddd	ymm0, ymm1, ymm3

; 295  : 
; 296  :     __m256i hsum2c_0    = _mm256_hadd_epi32(hsum0,    hsum1);

	vphaddd	ymm2, ymm0, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm4, ymm2, ymm15

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, edx
	vpsrad	ymm0, ymm4, xmm1

; 298  : 
; 299  :     __m256i hsum2c_0_tr = truncate_avx2(hsum2c_0, debias, shift);
; 300  :     __m256i hsum2c_1_tr = truncate_avx2(hsum2c_1, debias, shift);
; 301  : 
; 302  :     __m256i tmp_dc      = _mm256_packs_epi32(hsum2c_0_tr, hsum2c_1_tr);

	vpackssdw ymm2, ymm0, ymm5

; 303  : 
; 304  :     output[dry]         = _mm256_shuffle_epi8(tmp_dc, shuf_lorow_mask);

	vpshufb	ymm3, ymm2, YMMWORD PTR __ymm@1d1c1f1e19181b1a15141716111013120f0e0d0c0b0a09080706050403020100
	vmovdqu	YMMWORD PTR [r8], ymm3
	vpmaddwd ymm1, ymm6, ymm7
	vpmaddwd ymm0, ymm6, ymm9
	vphaddd	ymm3, ymm0, ymm1
	vpmaddwd ymm1, ymm6, ymm10
	vpmaddwd ymm2, ymm6, ymm8
	vphaddd	ymm0, ymm1, ymm2
	vphaddd	ymm2, ymm0, ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm2, ymm15

; 285  :     __m256i prod1_swp   = _mm256_madd_epi16(b_dc,     a_dr_1_swp);

	vpmaddwd ymm2, ymm6, ymm11
	vpmaddwd ymm0, ymm6, ymm13

; 292  :     __m256i hsum1       = _mm256_hadd_epi32(prod1,    prod1_swp);

	vphaddd	ymm4, ymm0, ymm2

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, edx
	vpsrad	ymm5, ymm3, xmm1

; 281  : 
; 282  :     __m256i prod0       = _mm256_madd_epi16(b_dc,     a_dr_0);

	vpmaddwd ymm1, ymm6, ymm14

; 283  :     __m256i prod0_swp   = _mm256_madd_epi16(b_dc,     a_dr_0_swp);

	vpmaddwd ymm3, ymm6, ymm12
	vmovdqu	ymm6, YMMWORD PTR kvz_g_dct_8+64

; 290  : 
; 291  :     __m256i hsum0       = _mm256_hadd_epi32(prod0,    prod0_swp);

	vphaddd	ymm0, ymm1, ymm3

; 295  : 
; 296  :     __m256i hsum2c_0    = _mm256_hadd_epi32(hsum0,    hsum1);

	vphaddd	ymm2, ymm0, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm4, ymm2, ymm15

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, edx
	vpsrad	ymm0, ymm4, xmm1

; 298  : 
; 299  :     __m256i hsum2c_0_tr = truncate_avx2(hsum2c_0, debias, shift);
; 300  :     __m256i hsum2c_1_tr = truncate_avx2(hsum2c_1, debias, shift);
; 301  : 
; 302  :     __m256i tmp_dc      = _mm256_packs_epi32(hsum2c_0_tr, hsum2c_1_tr);

	vpackssdw ymm2, ymm0, ymm5

; 303  : 
; 304  :     output[dry]         = _mm256_shuffle_epi8(tmp_dc, shuf_lorow_mask);

	vpshufb	ymm3, ymm2, YMMWORD PTR __ymm@1d1c1f1e19181b1a15141716111013120f0e0d0c0b0a09080706050403020100
	vpmaddwd ymm1, ymm6, ymm7
	vpmaddwd ymm0, ymm6, ymm9
	vmovdqu	YMMWORD PTR [r8+32], ymm3
	vphaddd	ymm3, ymm0, ymm1
	vpmaddwd ymm2, ymm6, ymm8
	vpmaddwd ymm1, ymm6, ymm10
	vphaddd	ymm0, ymm1, ymm2
	vphaddd	ymm2, ymm0, ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm2, ymm15

; 284  :     __m256i prod1       = _mm256_madd_epi16(b_dc,     a_dr_1);

	vpmaddwd ymm2, ymm6, ymm11
	vpmaddwd ymm0, ymm6, ymm13

; 292  :     __m256i hsum1       = _mm256_hadd_epi32(prod1,    prod1_swp);

	vphaddd	ymm4, ymm0, ymm2

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, edx
	vpsrad	ymm5, ymm3, xmm1

; 281  : 
; 282  :     __m256i prod0       = _mm256_madd_epi16(b_dc,     a_dr_0);

	vpmaddwd ymm1, ymm6, ymm14

; 283  :     __m256i prod0_swp   = _mm256_madd_epi16(b_dc,     a_dr_0_swp);

	vpmaddwd ymm3, ymm6, ymm12
	vmovdqu	ymm6, YMMWORD PTR kvz_g_dct_8+96

; 290  : 
; 291  :     __m256i hsum0       = _mm256_hadd_epi32(prod0,    prod0_swp);

	vphaddd	ymm0, ymm1, ymm3

; 295  : 
; 296  :     __m256i hsum2c_0    = _mm256_hadd_epi32(hsum0,    hsum1);

	vphaddd	ymm2, ymm0, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm4, ymm2, ymm15

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, edx
	vpsrad	ymm0, ymm4, xmm1

; 298  : 
; 299  :     __m256i hsum2c_0_tr = truncate_avx2(hsum2c_0, debias, shift);
; 300  :     __m256i hsum2c_1_tr = truncate_avx2(hsum2c_1, debias, shift);
; 301  : 
; 302  :     __m256i tmp_dc      = _mm256_packs_epi32(hsum2c_0_tr, hsum2c_1_tr);

	vpackssdw ymm2, ymm0, ymm5

; 303  : 
; 304  :     output[dry]         = _mm256_shuffle_epi8(tmp_dc, shuf_lorow_mask);

	vpshufb	ymm3, ymm2, YMMWORD PTR __ymm@1d1c1f1e19181b1a15141716111013120f0e0d0c0b0a09080706050403020100
	vmovdqu	YMMWORD PTR [r8+64], ymm3
	vpmaddwd ymm1, ymm6, ymm7
	vpmaddwd ymm0, ymm6, ymm9
	vphaddd	ymm3, ymm0, ymm1
	vpmaddwd ymm1, ymm6, ymm10
	vpmaddwd ymm2, ymm6, ymm8
	vphaddd	ymm0, ymm1, ymm2
	vphaddd	ymm2, ymm0, ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm2, ymm15

; 285  :     __m256i prod1_swp   = _mm256_madd_epi16(b_dc,     a_dr_1_swp);

	vpmaddwd ymm2, ymm6, ymm11
	vpmaddwd ymm0, ymm6, ymm13

; 292  :     __m256i hsum1       = _mm256_hadd_epi32(prod1,    prod1_swp);

	vphaddd	ymm4, ymm0, ymm2

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, edx
	vpsrad	ymm5, ymm3, xmm1

; 283  :     __m256i prod0_swp   = _mm256_madd_epi16(b_dc,     a_dr_0_swp);

	vpmaddwd ymm3, ymm6, ymm12
	vpmaddwd ymm1, ymm6, ymm14

; 290  : 
; 291  :     __m256i hsum0       = _mm256_hadd_epi32(prod0,    prod0_swp);

	vphaddd	ymm0, ymm1, ymm3

; 295  : 
; 296  :     __m256i hsum2c_0    = _mm256_hadd_epi32(hsum0,    hsum1);

	vphaddd	ymm2, ymm0, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm4, ymm2, ymm15

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, edx
	vpsrad	ymm0, ymm4, xmm1

; 298  : 
; 299  :     __m256i hsum2c_0_tr = truncate_avx2(hsum2c_0, debias, shift);
; 300  :     __m256i hsum2c_1_tr = truncate_avx2(hsum2c_1, debias, shift);
; 301  : 
; 302  :     __m256i tmp_dc      = _mm256_packs_epi32(hsum2c_0_tr, hsum2c_1_tr);

	vpackssdw ymm2, ymm0, ymm5

; 303  : 
; 304  :     output[dry]         = _mm256_shuffle_epi8(tmp_dc, shuf_lorow_mask);

	vpshufb	ymm3, ymm2, YMMWORD PTR __ymm@1d1c1f1e19181b1a15141716111013120f0e0d0c0b0a09080706050403020100
	vmovdqu	YMMWORD PTR [r8+96], ymm3
	vzeroupper

; 305  :   }
; 306  : }

	vmovaps	xmm6, XMMWORD PTR [r11-24]
	lea	r11, QWORD PTR [r11]
	vmovaps	xmm7, XMMWORD PTR [r11-40]
	vmovaps	xmm8, XMMWORD PTR [r11-56]
	vmovaps	xmm9, XMMWORD PTR [r11-72]
	vmovaps	xmm10, XMMWORD PTR [r11-88]
	vmovaps	xmm11, XMMWORD PTR [r11-104]
	vmovaps	xmm12, XMMWORD PTR [r11-120]
	vmovaps	xmm13, XMMWORD PTR [rsp+32]
	vmovaps	xmm14, XMMWORD PTR [rsp+16]
	vmovaps	xmm15, XMMWORD PTR [rsp]
	mov	rsp, r11
	ret	0
matmul_8x8_a_bt_t ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
a$dead$ = 176
b_t$ = 184
output$ = 192
shift$ = 200
matmul_8x8_a_bt PROC

; 312  : {

	mov	r11, rsp
	sub	rsp, 168				; 000000a8H
	vmovaps	XMMWORD PTR [r11-24], xmm6

; 315  : 
; 316  :   const __m256i shuf_lorow_mask =
; 317  :       _mm256_setr_epi8(0,  1,  2,  3,  4,  5,  6,  7,
; 318  :                        8,  9,  10, 11, 12, 13, 14, 15,
; 319  :                        18, 19, 16, 17, 22, 23, 20, 21,
; 320  :                        26, 27, 24, 25, 30, 31, 28, 29);
; 321  : 
; 322  :   const __m256i *a_256 = (const __m256i *)a;
; 323  : 
; 324  :   __m256i b_dc_0      = b_t[0];
; 325  :   __m256i b_dc_1      = b_t[1];
; 326  :   __m256i b_dc_2      = b_t[2];
; 327  :   __m256i b_dc_3      = b_t[3];
; 328  : 
; 329  :   __m256i b_dc_0_swp  = swap_lanes(b_dc_0);
; 330  :   __m256i b_dc_1_swp  = swap_lanes(b_dc_1);
; 331  :   __m256i b_dc_2_swp  = swap_lanes(b_dc_2);
; 332  :   __m256i b_dc_3_swp  = swap_lanes(b_dc_3);
; 333  : 
; 334  :   for (int dry = 0; dry < 4; dry++) {
; 335  :     __m256i a_dr        = _mm256_load_si256(a_256 + dry);

	vmovdqu	ymm6, YMMWORD PTR kvz_g_dct_8
	vmovaps	XMMWORD PTR [r11-40], xmm7
	vmovaps	XMMWORD PTR [r11-56], xmm8
	vmovaps	XMMWORD PTR [r11-72], xmm9
	vmovdqu	ymm9, YMMWORD PTR [rdx+96]
	movsx	ecx, r9b
	mov	r9d, 1
	vmovaps	XMMWORD PTR [r11-88], xmm10
	vmovdqu	ymm10, YMMWORD PTR [rdx+64]
	vmovaps	XMMWORD PTR [r11-104], xmm11

; 343  :     __m256i prod3       = _mm256_madd_epi16(a_dr,     b_dc_3);

	vpmaddwd ymm0, ymm6, ymm9
	vmovaps	XMMWORD PTR [r11-120], xmm12
	lea	eax, DWORD PTR [rcx-1]

; 66   :   return _mm256_permute4x64_epi64(v, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm7, ymm9, 78				; 0000004eH

; 344  :     __m256i prod3_swp   = _mm256_madd_epi16(a_dr,     b_dc_3_swp);

	vpmaddwd ymm1, ymm6, ymm7

; 349  :     __m256i hsum3       = _mm256_hadd_epi32(prod3,    prod3_swp);

	vphaddd	ymm3, ymm0, ymm1
	vpmaddwd ymm1, ymm6, ymm10

; 66   :   return _mm256_permute4x64_epi64(v, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm8, ymm10, 78				; 0000004eH

; 341  :     __m256i prod2       = _mm256_madd_epi16(a_dr,     b_dc_2);
; 342  :     __m256i prod2_swp   = _mm256_madd_epi16(a_dr,     b_dc_2_swp);

	vpmaddwd ymm2, ymm6, ymm8

; 348  :     __m256i hsum2       = _mm256_hadd_epi32(prod2,    prod2_swp);

	vphaddd	ymm0, ymm1, ymm2

; 352  :     __m256i hsum2c_1    = _mm256_hadd_epi32(hsum2,    hsum3);

	vphaddd	ymm2, ymm0, ymm3
	vmovaps	XMMWORD PTR [rsp+32], xmm13
	vmovaps	XMMWORD PTR [rsp+16], xmm14
	vmovaps	XMMWORD PTR [rsp], xmm15
	vmovdqu	ymm13, YMMWORD PTR [rdx+32]
	vmovdqu	ymm14, YMMWORD PTR [rdx]
	vpmaddwd ymm0, ymm6, ymm13

; 66   :   return _mm256_permute4x64_epi64(v, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm11, ymm13, 78			; 0000004eH

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, ecx

; 66   :   return _mm256_permute4x64_epi64(v, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm12, ymm14, 78			; 0000004eH

; 313  :   const int32_t add    = 1 << (shift - 1);

	shlx	eax, r9d, eax

; 314  :   const __m256i debias = _mm256_set1_epi32(add);

	vmovd	xmm15, eax
	vpbroadcastd ymm15, xmm15

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm2, ymm15

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vpsrad	ymm5, ymm3, xmm1

; 336  : 
; 337  :     __m256i prod0       = _mm256_madd_epi16(a_dr,     b_dc_0);

	vpmaddwd ymm1, ymm6, ymm14

; 340  :     __m256i prod1_swp   = _mm256_madd_epi16(a_dr,     b_dc_1_swp);

	vpmaddwd ymm2, ymm6, ymm11

; 347  :     __m256i hsum1       = _mm256_hadd_epi32(prod1,    prod1_swp);

	vphaddd	ymm4, ymm0, ymm2
	vpmaddwd ymm3, ymm6, ymm12
	vphaddd	ymm0, ymm1, ymm3

; 350  : 
; 351  :     __m256i hsum2c_0    = _mm256_hadd_epi32(hsum0,    hsum1);

	vphaddd	ymm2, ymm0, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm4, ymm2, ymm15

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, ecx
	vpsrad	ymm0, ymm4, xmm1

; 353  : 
; 354  :     __m256i hsum2c_0_tr = truncate_avx2(hsum2c_0, debias, shift);
; 355  :     __m256i hsum2c_1_tr = truncate_avx2(hsum2c_1, debias, shift);
; 356  : 
; 357  :     __m256i tmp_dr      = _mm256_packs_epi32(hsum2c_0_tr, hsum2c_1_tr);

	vpackssdw ymm2, ymm0, ymm5

; 358  : 
; 359  :     __m256i final_dr    = _mm256_shuffle_epi8(tmp_dr, shuf_lorow_mask);

	vpshufb	ymm3, ymm2, YMMWORD PTR __ymm@1d1c1f1e19181b1a15141716111013120f0e0d0c0b0a09080706050403020100

; 360  : 
; 361  :     _mm256_store_si256((__m256i *)output + dry, final_dr);

	vmovdqu	YMMWORD PTR [r8], ymm3
	vmovdqu	ymm6, YMMWORD PTR kvz_g_dct_8+32
	vpmaddwd ymm1, ymm6, ymm7
	vpmaddwd ymm0, ymm6, ymm9
	vphaddd	ymm3, ymm0, ymm1
	vpmaddwd ymm1, ymm6, ymm10
	vpmaddwd ymm2, ymm6, ymm8
	vphaddd	ymm0, ymm1, ymm2
	vphaddd	ymm2, ymm0, ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm2, ymm15

; 340  :     __m256i prod1_swp   = _mm256_madd_epi16(a_dr,     b_dc_1_swp);

	vpmaddwd ymm2, ymm6, ymm11
	vpmaddwd ymm0, ymm6, ymm13

; 347  :     __m256i hsum1       = _mm256_hadd_epi32(prod1,    prod1_swp);

	vphaddd	ymm4, ymm0, ymm2

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, ecx
	vpsrad	ymm5, ymm3, xmm1

; 336  : 
; 337  :     __m256i prod0       = _mm256_madd_epi16(a_dr,     b_dc_0);

	vpmaddwd ymm1, ymm6, ymm14

; 338  :     __m256i prod0_swp   = _mm256_madd_epi16(a_dr,     b_dc_0_swp);

	vpmaddwd ymm3, ymm6, ymm12

; 345  : 
; 346  :     __m256i hsum0       = _mm256_hadd_epi32(prod0,    prod0_swp);

	vphaddd	ymm0, ymm1, ymm3

; 350  : 
; 351  :     __m256i hsum2c_0    = _mm256_hadd_epi32(hsum0,    hsum1);

	vphaddd	ymm2, ymm0, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm4, ymm2, ymm15

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, ecx
	vpsrad	ymm0, ymm4, xmm1

; 353  : 
; 354  :     __m256i hsum2c_0_tr = truncate_avx2(hsum2c_0, debias, shift);
; 355  :     __m256i hsum2c_1_tr = truncate_avx2(hsum2c_1, debias, shift);
; 356  : 
; 357  :     __m256i tmp_dr      = _mm256_packs_epi32(hsum2c_0_tr, hsum2c_1_tr);

	vpackssdw ymm2, ymm0, ymm5

; 358  : 
; 359  :     __m256i final_dr    = _mm256_shuffle_epi8(tmp_dr, shuf_lorow_mask);

	vpshufb	ymm3, ymm2, YMMWORD PTR __ymm@1d1c1f1e19181b1a15141716111013120f0e0d0c0b0a09080706050403020100

; 360  : 
; 361  :     _mm256_store_si256((__m256i *)output + dry, final_dr);

	vmovdqu	YMMWORD PTR [r8+32], ymm3
	vmovdqu	ymm6, YMMWORD PTR kvz_g_dct_8+64
	vpmaddwd ymm1, ymm6, ymm7
	vpmaddwd ymm0, ymm6, ymm9
	vphaddd	ymm3, ymm0, ymm1
	vpmaddwd ymm2, ymm6, ymm8
	vpmaddwd ymm1, ymm6, ymm10
	vphaddd	ymm0, ymm1, ymm2
	vphaddd	ymm2, ymm0, ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm2, ymm15

; 339  :     __m256i prod1       = _mm256_madd_epi16(a_dr,     b_dc_1);

	vpmaddwd ymm2, ymm6, ymm11
	vpmaddwd ymm0, ymm6, ymm13

; 347  :     __m256i hsum1       = _mm256_hadd_epi32(prod1,    prod1_swp);

	vphaddd	ymm4, ymm0, ymm2

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, ecx
	vpsrad	ymm5, ymm3, xmm1

; 336  : 
; 337  :     __m256i prod0       = _mm256_madd_epi16(a_dr,     b_dc_0);

	vpmaddwd ymm1, ymm6, ymm14

; 338  :     __m256i prod0_swp   = _mm256_madd_epi16(a_dr,     b_dc_0_swp);

	vpmaddwd ymm3, ymm6, ymm12

; 345  : 
; 346  :     __m256i hsum0       = _mm256_hadd_epi32(prod0,    prod0_swp);

	vphaddd	ymm0, ymm1, ymm3

; 350  : 
; 351  :     __m256i hsum2c_0    = _mm256_hadd_epi32(hsum0,    hsum1);

	vphaddd	ymm2, ymm0, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm4, ymm2, ymm15

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, ecx
	vpsrad	ymm0, ymm4, xmm1

; 353  : 
; 354  :     __m256i hsum2c_0_tr = truncate_avx2(hsum2c_0, debias, shift);
; 355  :     __m256i hsum2c_1_tr = truncate_avx2(hsum2c_1, debias, shift);
; 356  : 
; 357  :     __m256i tmp_dr      = _mm256_packs_epi32(hsum2c_0_tr, hsum2c_1_tr);

	vpackssdw ymm2, ymm0, ymm5

; 358  : 
; 359  :     __m256i final_dr    = _mm256_shuffle_epi8(tmp_dr, shuf_lorow_mask);

	vpshufb	ymm3, ymm2, YMMWORD PTR __ymm@1d1c1f1e19181b1a15141716111013120f0e0d0c0b0a09080706050403020100

; 360  : 
; 361  :     _mm256_store_si256((__m256i *)output + dry, final_dr);

	vmovdqu	YMMWORD PTR [r8+64], ymm3
	vmovdqu	ymm6, YMMWORD PTR kvz_g_dct_8+96
	vpmaddwd ymm1, ymm6, ymm7
	vpmaddwd ymm0, ymm6, ymm9
	vphaddd	ymm3, ymm0, ymm1
	vpmaddwd ymm1, ymm6, ymm10
	vpmaddwd ymm2, ymm6, ymm8
	vphaddd	ymm0, ymm1, ymm2
	vphaddd	ymm2, ymm0, ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm2, ymm15

; 340  :     __m256i prod1_swp   = _mm256_madd_epi16(a_dr,     b_dc_1_swp);

	vpmaddwd ymm2, ymm6, ymm11
	vpmaddwd ymm0, ymm6, ymm13

; 347  :     __m256i hsum1       = _mm256_hadd_epi32(prod1,    prod1_swp);

	vphaddd	ymm4, ymm0, ymm2

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, ecx
	vpsrad	ymm5, ymm3, xmm1

; 338  :     __m256i prod0_swp   = _mm256_madd_epi16(a_dr,     b_dc_0_swp);

	vpmaddwd ymm3, ymm6, ymm12
	vpmaddwd ymm1, ymm6, ymm14

; 345  : 
; 346  :     __m256i hsum0       = _mm256_hadd_epi32(prod0,    prod0_swp);

	vphaddd	ymm0, ymm1, ymm3

; 350  : 
; 351  :     __m256i hsum2c_0    = _mm256_hadd_epi32(hsum0,    hsum1);

	vphaddd	ymm2, ymm0, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm4, ymm2, ymm15

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, ecx
	vpsrad	ymm0, ymm4, xmm1

; 353  : 
; 354  :     __m256i hsum2c_0_tr = truncate_avx2(hsum2c_0, debias, shift);
; 355  :     __m256i hsum2c_1_tr = truncate_avx2(hsum2c_1, debias, shift);
; 356  : 
; 357  :     __m256i tmp_dr      = _mm256_packs_epi32(hsum2c_0_tr, hsum2c_1_tr);

	vpackssdw ymm2, ymm0, ymm5

; 358  : 
; 359  :     __m256i final_dr    = _mm256_shuffle_epi8(tmp_dr, shuf_lorow_mask);

	vpshufb	ymm3, ymm2, YMMWORD PTR __ymm@1d1c1f1e19181b1a15141716111013120f0e0d0c0b0a09080706050403020100

; 360  : 
; 361  :     _mm256_store_si256((__m256i *)output + dry, final_dr);

	vmovdqu	YMMWORD PTR [r8+96], ymm3
	vzeroupper

; 362  :   }
; 363  : }

	vmovaps	xmm6, XMMWORD PTR [r11-24]
	lea	r11, QWORD PTR [r11]
	vmovaps	xmm7, XMMWORD PTR [r11-40]
	vmovaps	xmm8, XMMWORD PTR [r11-56]
	vmovaps	xmm9, XMMWORD PTR [r11-72]
	vmovaps	xmm10, XMMWORD PTR [r11-88]
	vmovaps	xmm11, XMMWORD PTR [r11-104]
	vmovaps	xmm12, XMMWORD PTR [r11-120]
	vmovaps	xmm13, XMMWORD PTR [rsp+32]
	vmovaps	xmm14, XMMWORD PTR [rsp+16]
	vmovaps	xmm15, XMMWORD PTR [rsp]
	mov	rsp, r11
	ret	0
matmul_8x8_a_bt ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
tmpres$ = 0
bitdepth$ = 208
input$ = 216
output$ = 224
matrix_dct_8x8_avx2 PROC

; 366  : {

	push	rbp
	sub	rsp, 192				; 000000c0H
	lea	rbp, QWORD PTR [rsp+64]
	and	rbp, -32				; ffffffffffffffe0H

; 367  :   int32_t shift_1st = kvz_g_convert_to_bit[8] + 1 + (bitdepth - 8);

	lea	r9d, DWORD PTR [rcx-6]
	mov	r10, r8

; 368  :   int32_t shift_2nd = kvz_g_convert_to_bit[8] + 8;
; 369  : 
; 370  :   const int16_t *dct  = &kvz_g_dct_8[0][0];
; 371  : 
; 372  :   /*
; 373  :    * Multiply input by the tranpose of DCT matrix into tmpres, and DCT matrix
; 374  :    * by tmpres - this is then our output matrix
; 375  :    *
; 376  :    * It's easier to implement an AVX2 matrix multiplication if you can multiply
; 377  :    * the left term with the transpose of the right term. Here things are stored
; 378  :    * row-wise, not column-wise, so we can effectively read DCT_T column-wise
; 379  :    * into YMM registers by reading DCT row-wise. Also because of this, the
; 380  :    * first multiplication is hacked to produce the transpose of the result
; 381  :    * instead, since it will be used in similar fashion as the right operand
; 382  :    * in the second multiplication.
; 383  :    */
; 384  : 
; 385  :   __m256i tmpres[4];
; 386  : 
; 387  :   matmul_8x8_a_bt_t(input,  dct, tmpres, shift_1st);

	lea	r8, QWORD PTR tmpres$[rbp]
	mov	rcx, rdx
	call	matmul_8x8_a_bt_t

; 388  :   matmul_8x8_a_bt  (dct, tmpres, output, shift_2nd);

	mov	r9b, 9
	lea	rdx, QWORD PTR tmpres$[rbp]
	mov	r8, r10
	call	matmul_8x8_a_bt

; 389  : }

	add	rsp, 192				; 000000c0H
	pop	rbp
	ret	0
matrix_dct_8x8_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
tmp$ = 0
bitdepth$ = 240
input$ = 248
output$ = 256
matrix_idct_8x8_avx2 PROC

; 392  : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rdi
	push	rbp
	sub	rsp, 224				; 000000e0H
	lea	rbp, QWORD PTR [rsp+96]
	and	rbp, -64				; ffffffffffffffc0H
	mov	rdi, r8
	movsx	ebx, cl

; 393  :   int32_t shift_1st = 7;
; 394  :   int32_t shift_2nd = 12 - (bitdepth - 8);
; 395  :   ALIGNED(64) int16_t tmp[8 * 8];
; 396  : 
; 397  :   const int16_t *tdct = &kvz_g_dct_8_t[0][0];
; 398  :   const int16_t *dct  = &kvz_g_dct_8  [0][0];
; 399  : 
; 400  :   mul_clip_matrix_8x8_avx2(tdct, input, tmp,    shift_1st);

	lea	r8, QWORD PTR tmp$[rbp]
	mov	r9d, 7
	lea	rcx, OFFSET FLAT:kvz_g_dct_8_t
	call	mul_clip_matrix_8x8_avx2
	mov	r9d, 20

; 401  :   mul_clip_matrix_8x8_avx2(tmp,  dct,   output, shift_2nd);

	lea	rdx, OFFSET FLAT:kvz_g_dct_8
	sub	r9d, ebx
	lea	rcx, QWORD PTR tmp$[rbp]
	mov	r8, rdi
	call	mul_clip_matrix_8x8_avx2

; 402  : 
; 403  :   /*
; 404  :    * Because:
; 405  :    * out = tdct * input * dct = tdct * (input * dct) = tdct * (input * transpose(tdct))
; 406  :    * This could almost be done this way:
; 407  :    *
; 408  :    * matmul_8x8_a_bt_t(input, tdct, debias1, shift_1st, tmp);
; 409  :    * matmul_8x8_a_bt  (tdct,  tmp,  debias2, shift_2nd, output);
; 410  :    *
; 411  :    * But not really, since it will fall victim to some very occasional
; 412  :    * rounding errors. Sadly.
; 413  :    */
; 414  : }

	lea	r11, QWORD PTR [rsp+224]
	mov	rbx, QWORD PTR [r11+16]
	mov	rdi, QWORD PTR [r11+24]
	mov	rsp, r11
	pop	rbp
	ret	0
matrix_idct_8x8_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
a$dead$ = 144
b_t$ = 152
output$ = 160
shift$ = 168
matmul_16x16_a_bt PROC

; 420  : {

	mov	rax, rsp
	sub	rsp, 136				; 00000088H
	vmovaps	XMMWORD PTR [rax-24], xmm6

; 421  :   const int32_t add    = 1 << (shift - 1);

	mov	ecx, 1
	vmovaps	XMMWORD PTR [rax-40], xmm7
	vmovaps	XMMWORD PTR [rax-56], xmm8
	vmovaps	XMMWORD PTR [rax-72], xmm9
	vmovaps	XMMWORD PTR [rax-88], xmm10
	vmovaps	XMMWORD PTR [rax-104], xmm11
	vmovaps	XMMWORD PTR [rax-120], xmm12
	lea	eax, DWORD PTR [r9-1]
	shlx	eax, ecx, eax
	vmovaps	XMMWORD PTR [rsp], xmm13
	mov	ecx, 16

; 422  :   const __m256i debias = _mm256_set1_epi32(add);

	vmovd	xmm13, eax
	lea	rax, OFFSET FLAT:kvz_g_dct_16
	sub	rax, r8
	vpbroadcastd ymm13, xmm13
	npad	8
$LL4@matmul_16x:

; 423  : 
; 424  :   for (int32_t y = 0; y < 16; y++) {
; 425  :     __m256i a_r = a[y];

	vmovdqu	ymm12, YMMWORD PTR [rax+r8]
	lea	r8, QWORD PTR [r8+32]

; 426  :     __m256i results_32[2];
; 427  : 
; 428  :     for (int32_t fco = 0; fco < 2; fco++) {
; 429  :       // Read first cols 0, 1, 2, 3, 8, 9, 10, 11, and then next 4
; 430  :       __m256i bt_c0  = b_t[fco * 4 + 0];
; 431  :       __m256i bt_c1  = b_t[fco * 4 + 1];
; 432  :       __m256i bt_c2  = b_t[fco * 4 + 2];
; 433  :       __m256i bt_c3  = b_t[fco * 4 + 3];
; 434  :       __m256i bt_c8  = b_t[fco * 4 + 8];
; 435  :       __m256i bt_c9  = b_t[fco * 4 + 9];
; 436  :       __m256i bt_c10 = b_t[fco * 4 + 10];
; 437  :       __m256i bt_c11 = b_t[fco * 4 + 11];
; 438  : 
; 439  :       __m256i p0  = _mm256_madd_epi16(a_r, bt_c0);

	vpmaddwd ymm6, ymm12, YMMWORD PTR [rdx]

; 440  :       __m256i p1  = _mm256_madd_epi16(a_r, bt_c1);
; 441  :       __m256i p2  = _mm256_madd_epi16(a_r, bt_c2);
; 442  :       __m256i p3  = _mm256_madd_epi16(a_r, bt_c3);
; 443  :       __m256i p8  = _mm256_madd_epi16(a_r, bt_c8);

	vpmaddwd ymm3, ymm12, YMMWORD PTR [rdx+256]
	vpmaddwd ymm9, ymm12, YMMWORD PTR [rdx+32]

; 444  :       __m256i p9  = _mm256_madd_epi16(a_r, bt_c9);

	vpmaddwd ymm4, ymm12, YMMWORD PTR [rdx+288]
	vpmaddwd ymm10, ymm12, YMMWORD PTR [rdx+64]

; 445  :       __m256i p10 = _mm256_madd_epi16(a_r, bt_c10);

	vpmaddwd ymm5, ymm12, YMMWORD PTR [rdx+320]

; 446  :       __m256i p11 = _mm256_madd_epi16(a_r, bt_c11);

	vpmaddwd ymm7, ymm12, YMMWORD PTR [rdx+352]
	vpmaddwd ymm11, ymm12, YMMWORD PTR [rdx+96]

; 447  : 
; 448  :       // Combine low lanes from P0 and P8, high lanes from them, and the same
; 449  :       // with P1:P9 and so on
; 450  :       __m256i p0l = _mm256_permute2x128_si256(p0, p8,  0x20);

	vperm2i128 ymm2, ymm6, ymm3, 32			; 00000020H

; 451  :       __m256i p0h = _mm256_permute2x128_si256(p0, p8,  0x31);

	vperm2i128 ymm0, ymm6, ymm3, 49			; 00000031H

; 452  :       __m256i p1l = _mm256_permute2x128_si256(p1, p9,  0x20);
; 453  :       __m256i p1h = _mm256_permute2x128_si256(p1, p9,  0x31);
; 454  :       __m256i p2l = _mm256_permute2x128_si256(p2, p10, 0x20);
; 455  :       __m256i p2h = _mm256_permute2x128_si256(p2, p10, 0x31);
; 456  :       __m256i p3l = _mm256_permute2x128_si256(p3, p11, 0x20);
; 457  :       __m256i p3h = _mm256_permute2x128_si256(p3, p11, 0x31);
; 458  : 
; 459  :       __m256i s0  = _mm256_add_epi32(p0l, p0h);

	vpaddd	ymm8, ymm0, ymm2
	vperm2i128 ymm0, ymm9, ymm4, 49			; 00000031H
	vperm2i128 ymm1, ymm9, ymm4, 32			; 00000020H
	vpmaddwd ymm9, ymm12, YMMWORD PTR [rdx+224]

; 460  :       __m256i s1  = _mm256_add_epi32(p1l, p1h);

	vpaddd	ymm6, ymm0, ymm1
	vperm2i128 ymm0, ymm10, ymm5, 49		; 00000031H
	vperm2i128 ymm2, ymm10, ymm5, 32		; 00000020H
	vpmaddwd ymm5, ymm12, YMMWORD PTR [rdx+160]

; 461  :       __m256i s2  = _mm256_add_epi32(p2l, p2h);

	vpaddd	ymm4, ymm0, ymm2
	vperm2i128 ymm0, ymm11, ymm7, 49		; 00000031H
	vperm2i128 ymm1, ymm11, ymm7, 32		; 00000020H

; 462  :       __m256i s3  = _mm256_add_epi32(p3l, p3h);

	vpaddd	ymm3, ymm0, ymm1

; 463  : 
; 464  :       __m256i s4  = _mm256_unpacklo_epi64(s0, s1);
; 465  :       __m256i s5  = _mm256_unpackhi_epi64(s0, s1);
; 466  :       __m256i s6  = _mm256_unpacklo_epi64(s2, s3);

	vpunpcklqdq ymm2, ymm4, ymm3

; 467  :       __m256i s7  = _mm256_unpackhi_epi64(s2, s3);

	vpunpckhqdq ymm0, ymm4, ymm3

; 468  : 
; 469  :       __m256i s8  = _mm256_add_epi32(s4, s5);
; 470  :       __m256i s9  = _mm256_add_epi32(s6, s7);

	vpaddd	ymm4, ymm0, ymm2
	vpunpckhqdq ymm1, ymm8, ymm6
	vpunpcklqdq ymm3, ymm8, ymm6
	vpmaddwd ymm8, ymm12, YMMWORD PTR [rdx+192]
	vpmaddwd ymm6, ymm12, YMMWORD PTR [rdx+480]
	vpaddd	ymm0, ymm1, ymm3
	vpmaddwd ymm3, ymm12, YMMWORD PTR [rdx+416]

; 471  : 
; 472  :       __m256i res = _mm256_hadd_epi32(s8, s9);

	vphaddd	ymm2, ymm0, ymm4
	vpmaddwd ymm0, ymm12, YMMWORD PTR [rdx+384]

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm4, ymm2, ymm13

; 426  :     __m256i results_32[2];
; 427  : 
; 428  :     for (int32_t fco = 0; fco < 2; fco++) {
; 429  :       // Read first cols 0, 1, 2, 3, 8, 9, 10, 11, and then next 4
; 430  :       __m256i bt_c0  = b_t[fco * 4 + 0];
; 431  :       __m256i bt_c1  = b_t[fco * 4 + 1];
; 432  :       __m256i bt_c2  = b_t[fco * 4 + 2];
; 433  :       __m256i bt_c3  = b_t[fco * 4 + 3];
; 434  :       __m256i bt_c8  = b_t[fco * 4 + 8];
; 435  :       __m256i bt_c9  = b_t[fco * 4 + 9];
; 436  :       __m256i bt_c10 = b_t[fco * 4 + 10];
; 437  :       __m256i bt_c11 = b_t[fco * 4 + 11];
; 438  : 
; 439  :       __m256i p0  = _mm256_madd_epi16(a_r, bt_c0);

	vpmaddwd ymm2, ymm12, YMMWORD PTR [rdx+128]

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, r9d
	vpsrad	ymm10, ymm4, xmm1

; 445  :       __m256i p10 = _mm256_madd_epi16(a_r, bt_c10);

	vpmaddwd ymm4, ymm12, YMMWORD PTR [rdx+448]

; 447  : 
; 448  :       // Combine low lanes from P0 and P8, high lanes from them, and the same
; 449  :       // with P1:P9 and so on
; 450  :       __m256i p0l = _mm256_permute2x128_si256(p0, p8,  0x20);

	vperm2i128 ymm1, ymm2, ymm0, 32			; 00000020H

; 451  :       __m256i p0h = _mm256_permute2x128_si256(p0, p8,  0x31);

	vperm2i128 ymm0, ymm2, ymm0, 49			; 00000031H

; 452  :       __m256i p1l = _mm256_permute2x128_si256(p1, p9,  0x20);
; 453  :       __m256i p1h = _mm256_permute2x128_si256(p1, p9,  0x31);
; 454  :       __m256i p2l = _mm256_permute2x128_si256(p2, p10, 0x20);
; 455  :       __m256i p2h = _mm256_permute2x128_si256(p2, p10, 0x31);
; 456  :       __m256i p3l = _mm256_permute2x128_si256(p3, p11, 0x20);
; 457  :       __m256i p3h = _mm256_permute2x128_si256(p3, p11, 0x31);
; 458  : 
; 459  :       __m256i s0  = _mm256_add_epi32(p0l, p0h);

	vpaddd	ymm7, ymm0, ymm1
	vperm2i128 ymm0, ymm5, ymm3, 49			; 00000031H
	vperm2i128 ymm2, ymm5, ymm3, 32			; 00000020H

; 460  :       __m256i s1  = _mm256_add_epi32(p1l, p1h);

	vpaddd	ymm5, ymm0, ymm2
	vperm2i128 ymm0, ymm8, ymm4, 49			; 00000031H
	vperm2i128 ymm1, ymm8, ymm4, 32			; 00000020H

; 461  :       __m256i s2  = _mm256_add_epi32(p2l, p2h);

	vpaddd	ymm4, ymm0, ymm1
	vperm2i128 ymm0, ymm9, ymm6, 49			; 00000031H
	vperm2i128 ymm2, ymm9, ymm6, 32			; 00000020H

; 462  :       __m256i s3  = _mm256_add_epi32(p3l, p3h);

	vpaddd	ymm3, ymm0, ymm2

; 463  : 
; 464  :       __m256i s4  = _mm256_unpacklo_epi64(s0, s1);
; 465  :       __m256i s5  = _mm256_unpackhi_epi64(s0, s1);
; 466  :       __m256i s6  = _mm256_unpacklo_epi64(s2, s3);

	vpunpcklqdq ymm1, ymm4, ymm3

; 467  :       __m256i s7  = _mm256_unpackhi_epi64(s2, s3);

	vpunpckhqdq ymm0, ymm4, ymm3

; 468  : 
; 469  :       __m256i s8  = _mm256_add_epi32(s4, s5);
; 470  :       __m256i s9  = _mm256_add_epi32(s6, s7);

	vpaddd	ymm4, ymm0, ymm1
	vpunpckhqdq ymm1, ymm7, ymm5
	vpunpcklqdq ymm2, ymm7, ymm5
	vpaddd	ymm0, ymm1, ymm2

; 471  : 
; 472  :       __m256i res = _mm256_hadd_epi32(s8, s9);

	vphaddd	ymm2, ymm0, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm2, ymm13

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, r9d
	vpsrad	ymm0, ymm3, xmm1

; 473  :       results_32[fco] = truncate_avx2(res, debias, shift);
; 474  :     }
; 475  :     output[y] = _mm256_packs_epi32(results_32[0], results_32[1]);

	vpackssdw ymm2, ymm10, ymm0
	vmovdqu	YMMWORD PTR [r8-32], ymm2
	sub	rcx, 1
	jne	$LL4@matmul_16x

; 476  :   }
; 477  : }

	vzeroupper
	vmovaps	xmm6, XMMWORD PTR [rsp+112]
	vmovaps	xmm7, XMMWORD PTR [rsp+96]
	vmovaps	xmm8, XMMWORD PTR [rsp+80]
	vmovaps	xmm9, XMMWORD PTR [rsp+64]
	vmovaps	xmm10, XMMWORD PTR [rsp+48]
	vmovaps	xmm11, XMMWORD PTR [rsp+32]
	vmovaps	xmm12, XMMWORD PTR [rsp+16]
	vmovaps	xmm13, XMMWORD PTR [rsp]
	add	rsp, 136				; 00000088H
	ret	0
matmul_16x16_a_bt ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
tmp_128$ = 0
src$ = 672
dst$ = 680
s_stride_log2$dead$ = 688
d_stride_log2$dead$ = 696
transpose_16x16_stride PROC

; 485  : {

	mov	rax, rsp
	mov	QWORD PTR [rax+8], rbx
	mov	QWORD PTR [rax+16], rdi
	push	rbp
	sub	rsp, 656				; 00000290H
	vmovaps	XMMWORD PTR [rax-24], xmm6
	vmovaps	XMMWORD PTR [rax-40], xmm7
	vmovaps	XMMWORD PTR [rax-56], xmm8
	vmovaps	XMMWORD PTR [rax-72], xmm9
	vmovaps	XMMWORD PTR [rax-88], xmm10
	vmovaps	XMMWORD PTR [rax-104], xmm11
	vmovaps	XMMWORD PTR [rax-120], xmm12
	lea	rbp, QWORD PTR [rsp+32]
	and	rbp, -32				; ffffffffffffffe0H

; 486  :   __m256i tmp_128[16];
; 487  :   for (uint32_t i = 0; i < 16; i += 8) {

	lea	r8, QWORD PTR tmp_128$[rbp+32]
	sub	r8, rcx
	lea	r10, QWORD PTR tmp_128$[rbp+64]
	sub	r10, rcx
	lea	r11, QWORD PTR tmp_128$[rbp+96]
	sub	r11, rcx
	lea	rbx, QWORD PTR tmp_128$[rbp+128]
	sub	rbx, rcx
	lea	rdi, QWORD PTR tmp_128$[rbp+160]
	sub	rdi, rcx
	lea	r9, QWORD PTR tmp_128$[rbp]
	sub	r9, rcx
	lea	rax, QWORD PTR [rcx+64]
	mov	ecx, 2
	npad	8
$LL4@transpose_:

; 488  : 
; 489  :     // After every n-bit unpack, 2n-bit units in the vectors will be in
; 490  :     // correct order. Pair words first, then dwords, then qwords. After that,
; 491  :     // whole lanes will be correct.
; 492  :     __m256i tmp_32[8];
; 493  :     __m256i tmp_64[8];
; 494  : 
; 495  :     __m256i m[8] = {

	vmovdqu	ymm3, YMMWORD PTR [rax]

; 496  :       _mm256_load_si256((const __m256i *)src + ((i + 0) << s_stride_log2)),
; 497  :       _mm256_load_si256((const __m256i *)src + ((i + 1) << s_stride_log2)),
; 498  :       _mm256_load_si256((const __m256i *)src + ((i + 2) << s_stride_log2)),
; 499  :       _mm256_load_si256((const __m256i *)src + ((i + 3) << s_stride_log2)),
; 500  :       _mm256_load_si256((const __m256i *)src + ((i + 4) << s_stride_log2)),
; 501  :       _mm256_load_si256((const __m256i *)src + ((i + 5) << s_stride_log2)),
; 502  :       _mm256_load_si256((const __m256i *)src + ((i + 6) << s_stride_log2)),
; 503  :       _mm256_load_si256((const __m256i *)src + ((i + 7) << s_stride_log2)),
; 504  :     };
; 505  : 
; 506  :     tmp_32[0]      = _mm256_unpacklo_epi16(     m[0],      m[1]);
; 507  :     tmp_32[1]      = _mm256_unpacklo_epi16(     m[2],      m[3]);
; 508  :     tmp_32[2]      = _mm256_unpackhi_epi16(     m[0],      m[1]);
; 509  :     tmp_32[3]      = _mm256_unpackhi_epi16(     m[2],      m[3]);

	vpunpckhwd ymm0, ymm3, YMMWORD PTR [rax+32]
	vmovdqu	ymm7, YMMWORD PTR [rax+128]
	vmovdqu	ymm5, YMMWORD PTR [rax+160]
	vpunpcklwd ymm8, ymm3, YMMWORD PTR [rax+32]
	vmovdqu	ymm2, YMMWORD PTR [rax-64]
	vpunpcklwd ymm10, ymm2, YMMWORD PTR [rax-32]
	vpunpckhwd ymm2, ymm2, YMMWORD PTR [rax-32]
	vmovdqu	ymm6, YMMWORD PTR [rax+64]

; 510  : 
; 511  :     tmp_32[4]      = _mm256_unpacklo_epi16(     m[4],      m[5]);

	vpunpcklwd ymm11, ymm6, YMMWORD PTR [rax+96]

; 512  :     tmp_32[5]      = _mm256_unpacklo_epi16(     m[6],      m[7]);
; 513  :     tmp_32[6]      = _mm256_unpackhi_epi16(     m[4],      m[5]);

	vpunpckhwd ymm6, ymm6, YMMWORD PTR [rax+96]
	lea	rax, QWORD PTR [rax+256]

; 514  :     tmp_32[7]      = _mm256_unpackhi_epi16(     m[6],      m[7]);
; 515  : 
; 516  : 
; 517  :     tmp_64[0]      = _mm256_unpacklo_epi32(tmp_32[0], tmp_32[1]);
; 518  :     tmp_64[1]      = _mm256_unpacklo_epi32(tmp_32[2], tmp_32[3]);

	vpunpckldq ymm12, ymm2, ymm0
	vpunpcklwd ymm9, ymm7, ymm5
	vpunpckhwd ymm3, ymm7, ymm5
	vpunpckldq ymm7, ymm10, ymm8

; 519  :     tmp_64[2]      = _mm256_unpackhi_epi32(tmp_32[0], tmp_32[1]);

	vpunpckhdq ymm8, ymm10, ymm8

; 520  :     tmp_64[3]      = _mm256_unpackhi_epi32(tmp_32[2], tmp_32[3]);

	vpunpckhdq ymm10, ymm2, ymm0

; 521  : 
; 522  :     tmp_64[4]      = _mm256_unpacklo_epi32(tmp_32[4], tmp_32[5]);

	vpunpckldq ymm1, ymm11, ymm9

; 523  :     tmp_64[5]      = _mm256_unpacklo_epi32(tmp_32[6], tmp_32[7]);
; 524  :     tmp_64[6]      = _mm256_unpackhi_epi32(tmp_32[4], tmp_32[5]);
; 525  :     tmp_64[7]      = _mm256_unpackhi_epi32(tmp_32[6], tmp_32[7]);
; 526  : 
; 527  : 
; 528  :     tmp_128[i + 0] = _mm256_unpacklo_epi64(tmp_64[0], tmp_64[4]);

	vpunpcklqdq ymm0, ymm7, ymm1

; 529  :     tmp_128[i + 1] = _mm256_unpackhi_epi64(tmp_64[0], tmp_64[4]);

	vpunpckhqdq ymm1, ymm7, ymm1
	vmovdqu	YMMWORD PTR [rax+r8-320], ymm1
	vmovdqu	YMMWORD PTR [rax+r9-320], ymm0
	vpunpckhdq ymm2, ymm11, ymm9

; 530  :     tmp_128[i + 2] = _mm256_unpacklo_epi64(tmp_64[2], tmp_64[6]);
; 531  :     tmp_128[i + 3] = _mm256_unpackhi_epi64(tmp_64[2], tmp_64[6]);

	vpunpckhqdq ymm1, ymm8, ymm2
	vmovdqu	YMMWORD PTR [rax+r8-256], ymm1
	vpunpcklqdq ymm0, ymm8, ymm2
	vmovdqu	YMMWORD PTR [rax+r9-256], ymm0
	vpunpckldq ymm4, ymm6, ymm3

; 532  : 
; 533  :     tmp_128[i + 4] = _mm256_unpacklo_epi64(tmp_64[1], tmp_64[5]);
; 534  :     tmp_128[i + 5] = _mm256_unpackhi_epi64(tmp_64[1], tmp_64[5]);

	vpunpckhqdq ymm1, ymm12, ymm4
	vpunpcklqdq ymm0, ymm12, ymm4
	vpunpckhdq ymm5, ymm6, ymm3
	vmovdqu	YMMWORD PTR [rax+r11-256], ymm1
	vmovdqu	YMMWORD PTR [rax+r10-256], ymm0

; 535  :     tmp_128[i + 6] = _mm256_unpacklo_epi64(tmp_64[3], tmp_64[7]);
; 536  :     tmp_128[i + 7] = _mm256_unpackhi_epi64(tmp_64[3], tmp_64[7]);

	vpunpckhqdq ymm1, ymm10, ymm5
	vpunpcklqdq ymm0, ymm10, ymm5
	vmovdqu	YMMWORD PTR [rax+rdi-256], ymm1
	vmovdqu	YMMWORD PTR [rax+rbx-256], ymm0
	sub	rcx, 1
	jne	$LL4@transpose_

; 537  :   }
; 538  : 
; 539  :   for (uint32_t i = 0; i < 8; i++) {
; 540  :     uint32_t loid     = i + 0;
; 541  :     uint32_t hiid     = i + 8;
; 542  : 
; 543  :     uint32_t dst_loid = loid << d_stride_log2;
; 544  :     uint32_t dst_hiid = hiid << d_stride_log2;
; 545  : 
; 546  :     __m256i lo       = tmp_128[loid];
; 547  :     __m256i hi       = tmp_128[hiid];
; 548  :     __m256i final_lo = _mm256_permute2x128_si256(lo, hi, 0x20);

	vmovdqu	ymm3, YMMWORD PTR tmp_128$[rbp+32]
	vmovdqu	ymm4, YMMWORD PTR tmp_128$[rbp+64]
	vmovdqu	ymm2, YMMWORD PTR tmp_128$[rbp]
	vperm2i128 ymm0, ymm2, YMMWORD PTR tmp_128$[rbp+256], 32 ; 00000020H

; 549  :     __m256i final_hi = _mm256_permute2x128_si256(lo, hi, 0x31);

	vperm2i128 ymm1, ymm2, YMMWORD PTR tmp_128$[rbp+256], 49 ; 00000031H

; 550  : 
; 551  :     _mm256_store_si256((__m256i *)dst + dst_loid, final_lo);

	vmovdqu	YMMWORD PTR [rdx], ymm0
	vperm2i128 ymm0, ymm3, YMMWORD PTR tmp_128$[rbp+288], 32 ; 00000020H

; 552  :     _mm256_store_si256((__m256i *)dst + dst_hiid, final_hi);

	vmovdqu	YMMWORD PTR [rdx+256], ymm1
	vperm2i128 ymm1, ymm3, YMMWORD PTR tmp_128$[rbp+288], 49 ; 00000031H
	vmovdqu	ymm3, YMMWORD PTR tmp_128$[rbp+96]
	vmovdqu	YMMWORD PTR [rdx+32], ymm0
	vperm2i128 ymm0, ymm4, YMMWORD PTR tmp_128$[rbp+320], 32 ; 00000020H
	vmovdqu	YMMWORD PTR [rdx+288], ymm1
	vperm2i128 ymm1, ymm4, YMMWORD PTR tmp_128$[rbp+320], 49 ; 00000031H
	vmovdqu	ymm4, YMMWORD PTR tmp_128$[rbp+128]
	vmovdqu	YMMWORD PTR [rdx+64], ymm0
	vperm2i128 ymm0, ymm3, YMMWORD PTR tmp_128$[rbp+352], 32 ; 00000020H
	vmovdqu	YMMWORD PTR [rdx+320], ymm1
	vperm2i128 ymm1, ymm3, YMMWORD PTR tmp_128$[rbp+352], 49 ; 00000031H
	vmovdqu	ymm3, YMMWORD PTR tmp_128$[rbp+160]
	vmovdqu	YMMWORD PTR [rdx+96], ymm0
	vperm2i128 ymm0, ymm4, YMMWORD PTR tmp_128$[rbp+384], 32 ; 00000020H
	vmovdqu	YMMWORD PTR [rdx+352], ymm1
	vperm2i128 ymm1, ymm4, YMMWORD PTR tmp_128$[rbp+384], 49 ; 00000031H
	vmovdqu	ymm4, YMMWORD PTR tmp_128$[rbp+192]
	vmovdqu	YMMWORD PTR [rdx+128], ymm0
	vperm2i128 ymm0, ymm3, YMMWORD PTR tmp_128$[rbp+416], 32 ; 00000020H
	vmovdqu	YMMWORD PTR [rdx+384], ymm1
	vperm2i128 ymm1, ymm3, YMMWORD PTR tmp_128$[rbp+416], 49 ; 00000031H
	vmovdqu	ymm3, YMMWORD PTR tmp_128$[rbp+224]
	vmovdqu	YMMWORD PTR [rdx+160], ymm0
	vperm2i128 ymm0, ymm4, YMMWORD PTR tmp_128$[rbp+448], 32 ; 00000020H
	vmovdqu	YMMWORD PTR [rdx+416], ymm1
	vperm2i128 ymm1, ymm4, YMMWORD PTR tmp_128$[rbp+448], 49 ; 00000031H
	vmovdqu	YMMWORD PTR [rdx+192], ymm0
	vperm2i128 ymm0, ymm3, YMMWORD PTR tmp_128$[rbp+480], 32 ; 00000020H
	vmovdqu	YMMWORD PTR [rdx+448], ymm1
	vperm2i128 ymm1, ymm3, YMMWORD PTR tmp_128$[rbp+480], 49 ; 00000031H
	vmovdqu	YMMWORD PTR [rdx+224], ymm0
	vmovdqu	YMMWORD PTR [rdx+480], ymm1
	vzeroupper

; 553  :   }
; 554  : }

	lea	r11, QWORD PTR [rsp+656]
	mov	rbx, QWORD PTR [r11+16]
	mov	rdi, QWORD PTR [r11+24]
	vmovaps	xmm6, XMMWORD PTR [r11-16]
	vmovaps	xmm7, XMMWORD PTR [r11-32]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm9, XMMWORD PTR [r11-64]
	vmovaps	xmm10, XMMWORD PTR [r11-80]
	vmovaps	xmm11, XMMWORD PTR [r11-96]
	vmovaps	xmm12, XMMWORD PTR [r11-112]
	mov	rsp, r11
	pop	rbp
	ret	0
transpose_16x16_stride ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
src$ = 8
dst$ = 16
transpose_16x16 PROC

; 558  :   transpose_16x16_stride(src, dst, 0, 0);

	jmp	transpose_16x16_stride
transpose_16x16 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
v$ = 8
shift$ = 16
truncate_inv PROC

; 563  :   int32_t add = 1 << (shift - 1);

	lea	eax, DWORD PTR [rdx-1]
	mov	r8d, 1
	shlx	eax, r8d, eax

; 564  : 
; 565  :   __m256i debias  = _mm256_set1_epi32(add);

	vmovd	xmm0, eax
	vpbroadcastd ymm0, xmm0

; 566  :   __m256i v2      = _mm256_add_epi32 (v,  debias);

	vpaddd	ymm1, ymm0, YMMWORD PTR [rcx]

; 567  :   __m256i trunced = _mm256_srai_epi32(v2, shift);

	vmovd	xmm0, edx
	vpsrad	ymm0, ymm1, xmm0

; 568  :   return  trunced;
; 569  : }

	ret	0
truncate_inv ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
v$ = 8
extract_odds PROC

; 573  :   // 0 1 2 3 4 5 6 7 | 8 9 a b c d e f => 1 3 5 7 1 3 5 7 | 9 b d f 9 b d f
; 574  :   const __m256i oddmask = _mm256_setr_epi8( 2,  3,  6,  7, 10, 11, 14, 15,
; 575  :                                             2,  3,  6,  7, 10, 11, 14, 15,
; 576  :                                             2,  3,  6,  7, 10, 11, 14, 15,
; 577  :                                             2,  3,  6,  7, 10, 11, 14, 15);
; 578  : 
; 579  :   __m256i tmp = _mm256_shuffle_epi8 (v,   oddmask);

	vmovdqu	ymm0, YMMWORD PTR [rcx]
	vpshufb	ymm1, ymm0, YMMWORD PTR __ymm@0f0e0b0a070603020f0e0b0a070603020f0e0b0a070603020f0e0b0a07060302

; 580  :   return _mm256_permute4x64_epi64   (tmp, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm0, ymm1, 216				; 000000d8H

; 581  : }

	ret	0
extract_odds ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
v0$ = 8
v1$ = 16
extract_combine_odds PROC

; 585  :   // 0 1 2 3 4 5 6 7 | 8 9 a b c d e f => 1 3 5 7 1 3 5 7 | 9 b d f 9 b d f
; 586  :   const __m256i oddmask = _mm256_setr_epi8( 2,  3,  6,  7, 10, 11, 14, 15,
; 587  :                                             2,  3,  6,  7, 10, 11, 14, 15,
; 588  :                                             2,  3,  6,  7, 10, 11, 14, 15,
; 589  :                                             2,  3,  6,  7, 10, 11, 14, 15);
; 590  : 
; 591  :   __m256i tmp0 = _mm256_shuffle_epi8(v0,   oddmask);
; 592  :   __m256i tmp1 = _mm256_shuffle_epi8(v1,   oddmask);

	vmovdqu	ymm0, YMMWORD PTR [rdx]
	vpshufb	ymm3, ymm0, YMMWORD PTR __ymm@0f0e0b0a070603020f0e0b0a070603020f0e0b0a070603020f0e0b0a07060302
	vmovdqu	ymm1, YMMWORD PTR [rcx]
	vpshufb	ymm2, ymm1, YMMWORD PTR __ymm@0f0e0b0a070603020f0e0b0a070603020f0e0b0a070603020f0e0b0a07060302

; 593  : 
; 594  :   __m256i tmp2 = _mm256_blend_epi32 (tmp0, tmp1, 0xcc); // 1100 1100

	vpblendd ymm0, ymm2, ymm3, 204			; 000000ccH

; 595  : 
; 596  :   return _mm256_permute4x64_epi64   (tmp2, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm0, ymm0, 216				; 000000d8H

; 597  : }

	ret	0
extract_combine_odds ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
tdct$ = 8
extract_26ae PROC

; 604  :   // 02 03 22 23 06 07 26 27 | 0a 0b 2a 2b 02 0f 2e 2f
; 605  :   // =>
; 606  :   // 02 06 22 26 02 06 22 26 | 2a 2e 0a 0e 2a 2e 0a 0e
; 607  :   const __m256i evens_mask = _mm256_setr_epi8( 0,  1,  8,  9,  4,  5, 12, 13,
; 608  :                                                0,  1,  8,  9,  4,  5, 12, 13,
; 609  :                                                4,  5, 12, 13,  0,  1,  8,  9,
; 610  :                                                4,  5, 12, 13,  0,  1,  8,  9);
; 611  : 
; 612  :   __m256i shufd_0 = _mm256_shuffle_epi32(tdct[0], _MM_SHUFFLE(2, 3, 0, 1));
; 613  :   __m256i shufd_2 = _mm256_shuffle_epi32(tdct[2], _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm0, YMMWORD PTR [rcx+64], 177		; 000000b1H

; 614  : 
; 615  :   __m256i cmbd_01 = _mm256_blend_epi32(shufd_0, tdct[1], 0xaa); // 1010 1010
; 616  :   __m256i cmbd_23 = _mm256_blend_epi32(shufd_2, tdct[3], 0xaa); // 1010 1010

	vpblendd ymm1, ymm0, YMMWORD PTR [rcx+96], 170	; 000000aaH

; 617  : 
; 618  :   __m256i evens_01 = _mm256_shuffle_epi8(cmbd_01, evens_mask);
; 619  :   __m256i evens_23 = _mm256_shuffle_epi8(cmbd_23, evens_mask);

	vpshufb	ymm3, ymm1, YMMWORD PTR __ymm@090801000d0c0504090801000d0c05040d0c0504090801000d0c050409080100
	vpshufd	ymm2, YMMWORD PTR [rcx], 177		; 000000b1H
	vpblendd ymm0, ymm2, YMMWORD PTR [rcx+32], 170	; 000000aaH
	vpshufb	ymm1, ymm0, YMMWORD PTR __ymm@090801000d0c0504090801000d0c05040d0c0504090801000d0c050409080100

; 620  : 
; 621  :   __m256i evens_0123 = _mm256_unpacklo_epi64(evens_01, evens_23);

	vpunpcklqdq ymm3, ymm1, ymm3

; 622  : 
; 623  :   return _mm256_permute4x64_epi64(evens_0123, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm0, ymm3, 216				; 000000d8H

; 624  : }

	ret	0
extract_26ae ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
col$ = 8
extract_26ae_vec PROC

; 629  :   const __m256i mask_26ae = _mm256_set1_epi32(0x0d0c0504);
; 630  : 
; 631  :   // 2 6 2 6 2 6 2 6 | a e a e a e a e
; 632  :   __m256i reord = _mm256_shuffle_epi8     (col,   mask_26ae);

	vmovdqu	ymm0, YMMWORD PTR [rcx]
	vpshufb	ymm1, ymm0, YMMWORD PTR __ymm@0d0c05040d0c05040d0c05040d0c05040d0c05040d0c05040d0c05040d0c0504

; 633  :   __m256i final = _mm256_permute4x64_epi64(reord, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm0, ymm1, 216				; 000000d8H

; 634  :   return  final;
; 635  : }

	ret	0
extract_26ae_vec ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
tdct$ = 8
extract_d048c PROC

; 640  :   const __m256i final_shuf = _mm256_setr_epi8( 0,  1,  8,  9,  2,  3, 10, 11,
; 641  :                                                6,  7, 14, 15,  4,  5, 12, 13,
; 642  :                                                4,  5, 12, 13,  6,  7, 14, 15,
; 643  :                                                2,  3, 10, 11,  0,  1,  8,  9);
; 644  :   __m256i c0 = tdct[0];
; 645  :   __m256i c1 = tdct[1];
; 646  : 
; 647  :   __m256i c1_2  = _mm256_slli_epi32       (c1,    16);

	vmovdqu	ymm0, YMMWORD PTR [rcx+32]

; 648  :   __m256i cmbd  = _mm256_blend_epi16      (c0,    c1_2, 0x22); // 0010 0010

	vmovdqu	ymm1, YMMWORD PTR [rcx]
	vpslld	ymm2, ymm0, 16
	vpblendw ymm2, ymm1, ymm2, 34			; 00000022H

; 649  :   __m256i cmbd2 = _mm256_shuffle_epi32    (cmbd,  _MM_SHUFFLE(2, 0, 2, 0));

	vpshufd	ymm0, ymm2, 136				; 00000088H

; 650  :   __m256i cmbd3 = _mm256_permute4x64_epi64(cmbd2, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm3, ymm0, 216				; 000000d8H

; 651  :   __m256i final = _mm256_shuffle_epi8     (cmbd3, final_shuf);

	vpshufb	ymm0, ymm3, YMMWORD PTR __ymm@090801000b0a03020f0e07060d0c05040d0c05040f0e07060b0a030209080100

; 652  : 
; 653  :   return final;
; 654  : }

	ret	0
extract_d048c ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
col$ = 8
extract_d048c_vec PROC

; 659  :   const __m256i shufmask = _mm256_setr_epi8( 0,  1,  0,  1,  8,  9,  8,  9,
; 660  :                                              8,  9,  8,  9,  0,  1,  0,  1,
; 661  :                                              0,  1,  0,  1,  8,  9,  8,  9,
; 662  :                                              8,  9,  8,  9,  0,  1,  0,  1);
; 663  : 
; 664  :   __m256i col_db4s = _mm256_shuffle_epi8     (col, shufmask);

	vmovdqu	ymm0, YMMWORD PTR [rcx]
	vpshufb	ymm3, ymm0, YMMWORD PTR __ymm@0100010009080908090809080100010001000100090809080908090801000100

; 665  :   __m256i col_los  = _mm256_permute4x64_epi64(col_db4s, _MM_SHUFFLE(1, 1, 0, 0));
; 666  :   __m256i col_his  = _mm256_permute4x64_epi64(col_db4s, _MM_SHUFFLE(3, 3, 2, 2));

	vpermq	ymm2, ymm3, 250				; 000000faH
	vpermq	ymm0, ymm3, 80				; 00000050H

; 667  : 
; 668  :   __m256i final    = _mm256_unpacklo_epi16   (col_los,  col_his);

	vpunpcklwd ymm0, ymm0, ymm2

; 669  :   return final;
; 670  : }

	ret	0
extract_d048c_vec ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
dct_col_odds$2$ = 0
dct_col_odds$1$ = 32
tv1475 = 64
tv1476 = 96
tsrc$ = 128
src$ = 880
dst$ = 888
shift$ = 896
partial_butterfly_inverse_16_avx2 PROC

; 673  : {

	mov	rax, rsp
	mov	QWORD PTR [rax+8], rbx
	mov	QWORD PTR [rax+16], rdi
	push	rbp
	sub	rsp, 864				; 00000360H
	vmovaps	XMMWORD PTR [rax-24], xmm6
	vmovaps	XMMWORD PTR [rax-40], xmm7
	vmovaps	XMMWORD PTR [rax-56], xmm8
	vmovaps	XMMWORD PTR [rax-72], xmm9
	vmovaps	XMMWORD PTR [rax-88], xmm10
	vmovaps	XMMWORD PTR [rax-104], xmm11
	vmovaps	XMMWORD PTR [rax-120], xmm12
	vmovaps	XMMWORD PTR [rax-136], xmm13
	vmovaps	XMMWORD PTR [rax-152], xmm14
	vmovaps	XMMWORD PTR [rax-168], xmm15
	lea	rbp, QWORD PTR [rsp+64]
	and	rbp, -32				; ffffffffffffffe0H
	mov	rbx, rdx
	mov	edi, r8d

; 558  :   transpose_16x16_stride(src, dst, 0, 0);

	lea	rdx, QWORD PTR tsrc$[rbp]
	call	transpose_16x16_stride
	vmovdqu	ymm9, YMMWORD PTR __ymm@0f0e0b0a070603020f0e0b0a070603020f0e0b0a070603020f0e0b0a07060302

; 674  :   __m256i tsrc[16];
; 675  : 
; 676  :   const uint32_t width = 16;
; 677  : 
; 678  :   const int16_t *tdct = &kvz_g_dct_16_t[0][0];
; 679  : 
; 680  :   const __m256i  eo_signmask = _mm256_setr_epi32( 1,  1,  1,  1, -1, -1, -1, -1);
; 681  :   const __m256i eeo_signmask = _mm256_setr_epi32( 1,  1, -1, -1, -1, -1,  1,  1);
; 682  :   const __m256i   o_signmask = _mm256_set1_epi32(-1);
; 683  : 
; 684  :   const __m256i final_shufmask = _mm256_setr_epi8( 0,  1,  2,  3,  4,  5,  6,  7,
; 685  :                                                    8,  9, 10, 11, 12, 13, 14, 15,
; 686  :                                                    6,  7,  4,  5,  2,  3,  0,  1,
; 687  :                                                   14, 15, 12, 13, 10, 11,  8,  9);
; 688  :   transpose_16x16(src, (int16_t *)tsrc);
; 689  : 
; 690  :   const __m256i dct_cols[8] = {

	vmovdqu	ymm8, YMMWORD PTR kvz_g_dct_16_t
	vmovdqu	ymm7, YMMWORD PTR kvz_g_dct_16_t+32
	vmovdqu	ymm5, YMMWORD PTR kvz_g_dct_16_t+64
	vmovdqu	ymm0, YMMWORD PTR kvz_g_dct_16_t+128
	vmovdqu	ymm3, YMMWORD PTR kvz_g_dct_16_t+192
	vmovdqu	ymm6, YMMWORD PTR kvz_g_dct_16_t+96
	vmovdqu	ymm2, YMMWORD PTR kvz_g_dct_16_t+160
	vmovdqu	ymm4, YMMWORD PTR kvz_g_dct_16_t+224

; 563  :   int32_t add = 1 << (shift - 1);

	vmovdqu	ymm10, YMMWORD PTR __ymm@ffffffffffffffffffffffffffffffff00000001000000010000000100000001
	vmovdqu	ymm11, YMMWORD PTR __ymm@0000000100000001ffffffffffffffffffffffffffffffff0000000100000001
	vmovdqu	ymm12, YMMWORD PTR __ymm@ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	ymm13, YMMWORD PTR __ymm@09080b0a0d0c0f0e01000302050407060f0e0d0c0b0a09080706050403020100

; 592  :   __m256i tmp1 = _mm256_shuffle_epi8(v1,   oddmask);

	vpshufb	ymm1, ymm0, ymm9
	vpshufb	ymm2, ymm2, ymm9
	vpshufb	ymm0, ymm8, ymm9

; 593  : 
; 594  :   __m256i tmp2 = _mm256_blend_epi32 (tmp0, tmp1, 0xcc); // 1100 1100

	vpblendd ymm1, ymm0, ymm1, 204			; 000000ccH

; 595  : 
; 596  :   return _mm256_permute4x64_epi64   (tmp2, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm14, ymm1, 216			; 000000d8H
	vpshufb	ymm0, ymm7, ymm9
	vpblendd ymm1, ymm0, ymm2, 204			; 000000ccH
	vpshufb	ymm2, ymm3, ymm9
	vpermq	ymm15, ymm1, 216			; 000000d8H
	vpshufb	ymm0, ymm5, ymm9
	vpblendd ymm1, ymm0, ymm2, 204			; 000000ccH
	vpermq	ymm0, ymm1, 216				; 000000d8H
	vmovdqu	YMMWORD PTR dct_col_odds$2$[rbp], ymm0
	vpshufb	ymm0, ymm6, ymm9
	vpshufb	ymm2, ymm4, ymm9
	vpblendd ymm1, ymm0, ymm2, 204			; 000000ccH
	vpermq	ymm0, ymm1, 216				; 000000d8H
	vmovdqu	YMMWORD PTR dct_col_odds$1$[rbp], ymm0
	vpshufd	ymm2, ymm8, 177				; 000000b1H
	vpshufd	ymm0, ymm5, 177				; 000000b1H
	vpblendd ymm1, ymm0, ymm6, 170			; 000000aaH
	vpshufb	ymm3, ymm1, YMMWORD PTR __ymm@090801000d0c0504090801000d0c05040d0c0504090801000d0c050409080100
	vpblendd ymm0, ymm2, ymm7, 170			; 000000aaH
	vpshufb	ymm1, ymm0, YMMWORD PTR __ymm@090801000d0c0504090801000d0c05040d0c0504090801000d0c050409080100
	vpunpcklqdq ymm3, ymm1, ymm3
	vpslld	ymm0, ymm7, 16
	vpblendw ymm1, ymm8, ymm0, 34			; 00000022H

; 563  :   int32_t add = 1 << (shift - 1);

	lea	ecx, DWORD PTR [rdi-1]
	mov	eax, 1
	rol	eax, cl
	lea	rcx, QWORD PTR tsrc$[rbp]
	vpermq	ymm5, ymm3, 216				; 000000d8H
	vmovd	xmm8, eax
	vpshufd	ymm2, ymm1, 136				; 00000088H
	vpermq	ymm3, ymm2, 216				; 000000d8H
	vpshufb	ymm0, ymm3, YMMWORD PTR __ymm@090801000b0a03020f0e07060d0c05040d0c05040f0e07060b0a030209080100
	vmovdqu	YMMWORD PTR tv1475[rbp], ymm0
	vmovdqu	YMMWORD PTR tv1476[rbp], ymm5
	vpbroadcastd ymm8, xmm8
	sub	rcx, rbx
	mov	eax, 16
$LL7@partial_bu:

; 691  :     _mm256_load_si256((const __m256i *)tdct + 0),
; 692  :     _mm256_load_si256((const __m256i *)tdct + 1),
; 693  :     _mm256_load_si256((const __m256i *)tdct + 2),
; 694  :     _mm256_load_si256((const __m256i *)tdct + 3),
; 695  :     _mm256_load_si256((const __m256i *)tdct + 4),
; 696  :     _mm256_load_si256((const __m256i *)tdct + 5),
; 697  :     _mm256_load_si256((const __m256i *)tdct + 6),
; 698  :     _mm256_load_si256((const __m256i *)tdct + 7),
; 699  :   };
; 700  : 
; 701  :   // These contain: D1,0 D3,0 D5,0 D7,0 D9,0 Db,0 Dd,0 Df,0 | D1,4 D3,4 D5,4 D7,4 D9,4 Db,4 Dd,4 Df,4
; 702  :   //                D1,1 D3,1 D5,1 D7,1 D9,1 Db,1 Dd,1 Df,1 | D1,5 D3,5 D5,5 D7,5 D9,5 Db,5 Dd,5 Df,5
; 703  :   //                D1,2 D3,2 D5,2 D7,2 D9,2 Db,2 Dd,2 Df,2 | D1,6 D3,6 D5,6 D7,6 D9,6 Db,6 Dd,6 Df,6
; 704  :   //                D1,3 D3,3 D5,3 D7,3 D9,3 Db,3 Dd,3 Df,3 | D1,7 D3,7 D5,7 D7,7 D9,7 Db,7 Dd,7 Df,7
; 705  :   __m256i dct_col_odds[4];
; 706  :   for (uint32_t j = 0; j < 4; j++) {
; 707  :     dct_col_odds[j] = extract_combine_odds(dct_cols[j + 0], dct_cols[j + 4]);
; 708  :   }
; 709  :   for (uint32_t j = 0; j < width; j++) {
; 710  :     __m256i col = tsrc[j];

	vmovdqu	ymm6, YMMWORD PTR [rcx+rbx]
	lea	rbx, QWORD PTR [rbx+32]

; 579  :   __m256i tmp = _mm256_shuffle_epi8 (v,   oddmask);

	vpshufb	ymm0, ymm6, ymm9

; 580  :   return _mm256_permute4x64_epi64   (tmp, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm4, ymm0, 216				; 000000d8H

; 711  :     __m256i odds = extract_odds(col);
; 712  : 
; 713  :     __m256i o04   = _mm256_madd_epi16           (odds,     dct_col_odds[0]);
; 714  :     __m256i o15   = _mm256_madd_epi16           (odds,     dct_col_odds[1]);
; 715  :     __m256i o26   = _mm256_madd_epi16           (odds,     dct_col_odds[2]);

	vpmaddwd ymm2, ymm4, YMMWORD PTR dct_col_odds$2$[rbp]
	vpmaddwd ymm1, ymm14, ymm4
	vpmaddwd ymm0, ymm15, ymm4

; 716  :     __m256i o37   = _mm256_madd_epi16           (odds,     dct_col_odds[3]);
; 717  : 
; 718  :     __m256i o0145 = _mm256_hadd_epi32           (o04,      o15);

	vphaddd	ymm3, ymm1, ymm0
	vpmaddwd ymm1, ymm4, YMMWORD PTR dct_col_odds$1$[rbp]

; 719  :     __m256i o2367 = _mm256_hadd_epi32           (o26,      o37);

	vphaddd	ymm0, ymm2, ymm1

; 664  :   __m256i col_db4s = _mm256_shuffle_epi8     (col, shufmask);

	vpshufb	ymm2, ymm6, YMMWORD PTR __ymm@0100010009080908090809080100010001000100090809080908090801000100

; 632  :   __m256i reord = _mm256_shuffle_epi8     (col,   mask_26ae);

	vpshufb	ymm1, ymm6, YMMWORD PTR __ymm@0d0c05040d0c05040d0c05040d0c05040d0c05040d0c05040d0c05040d0c0504

; 720  : 
; 721  :     __m256i o     = _mm256_hadd_epi32           (o0145,    o2367);

	vphaddd	ymm7, ymm3, ymm0

; 633  :   __m256i final = _mm256_permute4x64_epi64(reord, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm0, ymm1, 216				; 000000d8H

; 722  : 
; 723  :     // D0,2 D0,6 D1,2 D1,6 D1,a D1,e D0,a D0,e | D2,2 D2,6 D3,2 D3,6 D3,a D3,e D2,a D2,e
; 724  :     __m256i d_db2 = extract_26ae(dct_cols);
; 725  : 
; 726  :     // 2 6 2 6 a e a e | 2 6 2 6 a e a e
; 727  :     __m256i t_db2 = extract_26ae_vec            (col);
; 728  : 
; 729  :     __m256i eo_parts  = _mm256_madd_epi16       (d_db2,    t_db2);

	vpmaddwd ymm5, ymm0, ymm5

; 665  :   __m256i col_los  = _mm256_permute4x64_epi64(col_db4s, _MM_SHUFFLE(1, 1, 0, 0));

	vpermq	ymm0, ymm2, 80				; 00000050H

; 666  :   __m256i col_his  = _mm256_permute4x64_epi64(col_db4s, _MM_SHUFFLE(3, 3, 2, 2));

	vpermq	ymm1, ymm2, 250				; 000000faH

; 667  : 
; 668  :   __m256i final    = _mm256_unpacklo_epi16   (col_los,  col_his);

	vpunpcklwd ymm1, ymm0, ymm1

; 730  :     __m256i eo_parts2 = _mm256_shuffle_epi32    (eo_parts, _MM_SHUFFLE(0, 1, 2, 3));
; 731  : 
; 732  :     // EO0 EO1 EO1 EO0 | EO2 EO3 EO3 EO2
; 733  :     __m256i eo        = _mm256_add_epi32        (eo_parts, eo_parts2);
; 734  :     __m256i eo2       = _mm256_permute4x64_epi64(eo,       _MM_SHUFFLE(1, 3, 2, 0));
; 735  :     __m256i eo3       = _mm256_sign_epi32       (eo2,      eo_signmask);
; 736  : 
; 737  :     __m256i d_db4     = extract_d048c           (dct_cols);
; 738  :     __m256i t_db4     = extract_d048c_vec       (col);
; 739  :     __m256i eee_eeo   = _mm256_madd_epi16       (d_db4,   t_db4);

	vpmaddwd ymm3, ymm1, YMMWORD PTR tv1475[rbp]

; 740  : 
; 741  :     __m256i eee_eee   = _mm256_permute4x64_epi64(eee_eeo,  _MM_SHUFFLE(3, 0, 3, 0));
; 742  :     __m256i eeo_eeo1  = _mm256_permute4x64_epi64(eee_eeo,  _MM_SHUFFLE(1, 2, 1, 2));

	vpermq	ymm0, ymm3, 102				; 00000066H

; 743  : 
; 744  :     __m256i eeo_eeo2  = _mm256_sign_epi32       (eeo_eeo1, eeo_signmask);

	vpsignd	ymm1, ymm0, ymm11
	vpermq	ymm2, ymm3, 204				; 000000ccH

; 745  : 
; 746  :     // EE0 EE1 EE2 EE3 | EE3 EE2 EE1 EE0
; 747  :     __m256i ee        = _mm256_add_epi32        (eee_eee,  eeo_eeo2);

	vpaddd	ymm4, ymm1, ymm2
	vpshufd	ymm2, ymm5, 27
	vpaddd	ymm0, ymm2, ymm5
	vpermq	ymm1, ymm0, 120				; 00000078H
	vpsignd	ymm3, ymm1, ymm10

; 748  :     __m256i e         = _mm256_add_epi32        (ee,       eo3);

	vpaddd	ymm6, ymm3, ymm4

; 749  : 
; 750  :     __m256i o_neg     = _mm256_sign_epi32       (o,        o_signmask);

	vpsignd	ymm5, ymm7, ymm12

; 752  :     __m256i o_hi      = _mm256_blend_epi32      (o,        o_neg, 0x0f); // 0000 1111

	vpblendd ymm0, ymm7, ymm5, 15

; 755  :     __m256i res_hi    = _mm256_add_epi32        (e,        o_hi);

	vpaddd	ymm1, ymm0, ymm6

; 756  :     __m256i res_hi2   = _mm256_permute4x64_epi64(res_hi,   _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm2, ymm1, 78				; 0000004eH

; 566  :   __m256i v2      = _mm256_add_epi32 (v,  debias);

	vpaddd	ymm3, ymm2, ymm8

; 751  :     __m256i o_lo      = _mm256_blend_epi32      (o,        o_neg, 0xf0); // 1111 0000

	vpblendd ymm1, ymm7, ymm5, 240			; 000000f0H
	vmovdqu	ymm5, YMMWORD PTR tv1476[rbp]

; 753  : 
; 754  :     __m256i res_lo    = _mm256_add_epi32        (e,        o_lo);

	vpaddd	ymm2, ymm1, ymm6

; 567  :   __m256i trunced = _mm256_srai_epi32(v2, shift);

	vmovd	xmm0, edi
	vpsrad	ymm4, ymm3, xmm0

; 566  :   __m256i v2      = _mm256_add_epi32 (v,  debias);

	vpaddd	ymm3, ymm2, ymm8

; 567  :   __m256i trunced = _mm256_srai_epi32(v2, shift);

	vmovd	xmm0, edi
	vpsrad	ymm1, ymm3, xmm0

; 757  : 
; 758  :     __m256i res_lo_t  = truncate_inv(res_lo,  shift);
; 759  :     __m256i res_hi_t  = truncate_inv(res_hi2, shift);
; 760  : 
; 761  :     __m256i res_16_1  = _mm256_packs_epi32      (res_lo_t, res_hi_t);

	vpackssdw ymm2, ymm1, ymm4

; 762  :     __m256i final     = _mm256_shuffle_epi8     (res_16_1, final_shufmask);

	vpshufb	ymm3, ymm2, ymm13

; 763  : 
; 764  :     _mm256_store_si256((__m256i *)dst + j, final);

	vmovdqu	YMMWORD PTR [rbx-32], ymm3
	sub	rax, 1
	jne	$LL7@partial_bu
	vzeroupper

; 765  :   }
; 766  : }

	lea	r11, QWORD PTR [rsp+864]
	mov	rbx, QWORD PTR [r11+16]
	mov	rdi, QWORD PTR [r11+24]
	vmovaps	xmm6, XMMWORD PTR [r11-16]
	vmovaps	xmm7, XMMWORD PTR [r11-32]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm9, XMMWORD PTR [r11-64]
	vmovaps	xmm10, XMMWORD PTR [r11-80]
	vmovaps	xmm11, XMMWORD PTR [r11-96]
	vmovaps	xmm12, XMMWORD PTR [r11-112]
	vmovaps	xmm13, XMMWORD PTR [r11-128]
	vmovaps	xmm14, XMMWORD PTR [r11-144]
	vmovaps	xmm15, XMMWORD PTR [r11-160]
	mov	rsp, r11
	pop	rbp
	ret	0
partial_butterfly_inverse_16_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
tmp$ = 0
bitdepth$ = 624
input$ = 632
output$ = 640
matrix_idct_16x16_avx2 PROC

; 769  : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rdi
	push	rbp
	sub	rsp, 608				; 00000260H
	lea	rbp, QWORD PTR [rsp+96]
	and	rbp, -64				; ffffffffffffffc0H
	mov	rax, rdx
	movsx	ebx, cl
	mov	rdi, r8

; 770  :   int32_t shift_1st = 7;
; 771  :   int32_t shift_2nd = 12 - (bitdepth - 8);
; 772  :   ALIGNED(64) int16_t tmp[16 * 16];
; 773  : 
; 774  :   partial_butterfly_inverse_16_avx2(input, tmp,    shift_1st);

	lea	rdx, QWORD PTR tmp$[rbp]
	mov	rcx, rax
	mov	r8d, 7
	call	partial_butterfly_inverse_16_avx2
	mov	r8d, 20

; 775  :   partial_butterfly_inverse_16_avx2(tmp,   output, shift_2nd);

	lea	rcx, QWORD PTR tmp$[rbp]
	sub	r8d, ebx
	mov	rdx, rdi
	call	partial_butterfly_inverse_16_avx2

; 776  : }

	lea	r11, QWORD PTR [rsp+608]
	mov	rbx, QWORD PTR [r11+16]
	mov	rdi, QWORD PTR [r11+24]
	mov	rsp, r11
	pop	rbp
	ret	0
matrix_idct_16x16_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
tmp$ = 0
bitdepth$ = 592
input$ = 600
output$ = 608
matrix_dct_16x16_avx2 PROC

; 779  : {

	push	rbp
	sub	rsp, 576				; 00000240H
	lea	rbp, QWORD PTR [rsp+64]
	and	rbp, -32				; ffffffffffffffe0H

; 780  :   int32_t shift_1st = kvz_g_convert_to_bit[16] + 1 + (bitdepth - 8);

	movsx	r9d, cl
	mov	r10, r8
	sub	r9d, 5

; 781  :   int32_t shift_2nd = kvz_g_convert_to_bit[16] + 8;
; 782  : 
; 783  :   const int16_t *dct  = &kvz_g_dct_16[0][0];
; 784  : 
; 785  :   /*
; 786  :    * Multiply input by the tranpose of DCT matrix into tmpres, and DCT matrix
; 787  :    * by tmpres - this is then our output matrix
; 788  :    *
; 789  :    * It's easier to implement an AVX2 matrix multiplication if you can multiply
; 790  :    * the left term with the transpose of the right term. Here things are stored
; 791  :    * row-wise, not column-wise, so we can effectively read DCT_T column-wise
; 792  :    * into YMM registers by reading DCT row-wise. Also because of this, the
; 793  :    * first multiplication is hacked to produce the transpose of the result
; 794  :    * instead, since it will be used in similar fashion as the right operand
; 795  :    * in the second multiplication.
; 796  :    */
; 797  : 
; 798  :   const __m256i *d_v = (const __m256i *)dct;
; 799  :   const __m256i *i_v = (const __m256i *)input;
; 800  :         __m256i *o_v = (      __m256i *)output;
; 801  :   __m256i tmp[16];
; 802  : 
; 803  :   // Hack! (A * B^T)^T = B * A^T, so we can dispatch the transpose-produciong
; 804  :   // multiply completely
; 805  :   matmul_16x16_a_bt(d_v, i_v, tmp, shift_1st);

	lea	r8, QWORD PTR tmp$[rbp]
	call	matmul_16x16_a_bt

; 806  :   matmul_16x16_a_bt(d_v, tmp, o_v, shift_2nd);

	mov	r8, r10
	lea	rdx, QWORD PTR tmp$[rbp]
	mov	r9d, 10
	call	matmul_16x16_a_bt

; 807  : }

	add	rsp, 576				; 00000240H
	pop	rbp
	ret	0
matrix_dct_16x16_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
debias$1$ = 0
accu$ = 32
left$ = 4256
right$ = 4264
dst$ = 4272
shift$ = 4280
mul_clip_matrix_32x32_avx2 PROC

; 816  : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rsi
	mov	QWORD PTR [rsp+24], rdi
	mov	QWORD PTR [rsp+32], r14
	push	rbp
	mov	eax, 4240				; 00001090H
	call	__chkstk
	sub	rsp, rax
	vmovaps	XMMWORD PTR [rsp+4224], xmm6
	vmovaps	XMMWORD PTR [rsp+4208], xmm7
	vmovaps	XMMWORD PTR [rsp+4192], xmm8
	lea	rbp, QWORD PTR [rsp+64]
	and	rbp, -32				; ffffffffffffffe0H
	mov	rsi, rcx

; 817  :   const int32_t add    = 1 << (shift - 1);

	lea	eax, DWORD PTR [r9-1]
	mov	ecx, 1
	mov	r14, r8
	shlx	eax, ecx, eax

; 818  :   const __m256i debias = _mm256_set1_epi32(add);

	vmovd	xmm0, eax
	vpbroadcastd ymm0, xmm0
	vmovdqu	YMMWORD PTR debias$1$[rbp], ymm0
	vpxor	xmm0, xmm0, xmm0
	mov	rbx, rdx

; 819  : 
; 820  :   const uint32_t *l_32  = (const uint32_t *)left;
; 821  :   const __m256i  *r_v   = (const __m256i *)right;
; 822  :         __m256i  *dst_v = (      __m256i *)dst;
; 823  : 
; 824  :   __m256i accu[128] = {_mm256_setzero_si256()};

	lea	rcx, QWORD PTR accu$[rbp+32]
	vmovdqu	YMMWORD PTR accu$[rbp], ymm0
	mov	edi, r9d
	xor	edx, edx
	mov	r8d, 4064				; 00000fe0H
	vzeroupper
	call	memset
	xor	r8d, r8d
	add	rbx, 64					; 00000040H
	lea	r9d, QWORD PTR [r8+8]
	npad	10
$LL4@mul_clip_m:

; 828  :     const __m256i r0 = r_v[j + 0];

	vmovdqu	ymm1, YMMWORD PTR [rbx-64]

; 829  :     const __m256i r1 = r_v[j + 1];
; 830  :     const __m256i r2 = r_v[j + 2];
; 831  :     const __m256i r3 = r_v[j + 3];
; 832  : 
; 833  :     __m256i r02l   = _mm256_unpacklo_epi16(r0, r2);

	vpunpcklwd ymm5, ymm1, YMMWORD PTR [rbx]

; 834  :     __m256i r02h   = _mm256_unpackhi_epi16(r0, r2);

	vpunpckhwd ymm3, ymm1, YMMWORD PTR [rbx]
	vmovdqu	ymm4, YMMWORD PTR [rbx-32]

; 835  :     __m256i r13l   = _mm256_unpacklo_epi16(r1, r3);

	vpunpcklwd ymm1, ymm4, YMMWORD PTR [rbx+32]

; 836  :     __m256i r13h   = _mm256_unpackhi_epi16(r1, r3);

	vpunpckhwd ymm0, ymm4, YMMWORD PTR [rbx+32]
	mov	rcx, r8
	lea	rax, QWORD PTR accu$[rbp+288]
	shr	rcx, 2
	add	rcx, 32					; 00000020H

; 837  : 
; 838  :     __m256i r02_07 = _mm256_permute2x128_si256(r02l, r02h, 0x20);

	vperm2i128 ymm6, ymm5, ymm3, 32			; 00000020H

; 839  :     __m256i r02_8f = _mm256_permute2x128_si256(r02l, r02h, 0x31);

	vperm2i128 ymm5, ymm5, ymm3, 49			; 00000031H

; 840  : 
; 841  :     __m256i r13_07 = _mm256_permute2x128_si256(r13l, r13h, 0x20);

	vperm2i128 ymm7, ymm1, ymm0, 32			; 00000020H
	lea	rdx, QWORD PTR [rsi+rcx*4]
	mov	rcx, r9

; 842  :     __m256i r13_8f = _mm256_permute2x128_si256(r13l, r13h, 0x31);

	vperm2i128 ymm8, ymm1, ymm0, 49			; 00000031H
	npad	3
$LL7@mul_clip_m:

; 843  : 
; 844  :     for (i = 0; i < 32; i += 2) {

	lea	rax, QWORD PTR [rax+512]

; 845  :       size_t acc_base = i << 2;
; 846  : 
; 847  :       uint32_t curr_e    = l_32[(i + 0) * (32 / 2) + (j >> 2)];
; 848  :       uint32_t curr_o    = l_32[(i + 1) * (32 / 2) + (j >> 2)];
; 849  : 
; 850  :       __m256i even       = _mm256_set1_epi32(curr_e);

	vpbroadcastd ymm2, DWORD PTR [rdx-128]

; 851  :       __m256i odd        = _mm256_set1_epi32(curr_o);

	vpbroadcastd ymm3, DWORD PTR [rdx-64]
	vpbroadcastd ymm4, DWORD PTR [rdx+64]
	lea	rdx, QWORD PTR [rdx+256]

; 852  : 
; 853  :       __m256i p_e0       = _mm256_madd_epi16(even, r02_07);

	vpmaddwd ymm0, ymm2, ymm6

; 854  :       __m256i p_e1       = _mm256_madd_epi16(even, r02_8f);
; 855  :       __m256i p_e2       = _mm256_madd_epi16(even, r13_07);
; 856  :       __m256i p_e3       = _mm256_madd_epi16(even, r13_8f);
; 857  : 
; 858  :       __m256i p_o0       = _mm256_madd_epi16(odd,  r02_07);
; 859  :       __m256i p_o1       = _mm256_madd_epi16(odd,  r02_8f);
; 860  :       __m256i p_o2       = _mm256_madd_epi16(odd,  r13_07);
; 861  :       __m256i p_o3       = _mm256_madd_epi16(odd,  r13_8f);
; 862  : 
; 863  :       accu[acc_base + 0] = _mm256_add_epi32 (p_e0, accu[acc_base + 0]);

	vpaddd	ymm1, ymm0, YMMWORD PTR [rax-800]
	vmovdqu	YMMWORD PTR [rax-800], ymm1
	vpmaddwd ymm0, ymm2, ymm5

; 864  :       accu[acc_base + 1] = _mm256_add_epi32 (p_e1, accu[acc_base + 1]);

	vpaddd	ymm1, ymm0, YMMWORD PTR [rax-768]
	vmovdqu	YMMWORD PTR [rax-768], ymm1
	vpmaddwd ymm0, ymm2, ymm7

; 865  :       accu[acc_base + 2] = _mm256_add_epi32 (p_e2, accu[acc_base + 2]);

	vpaddd	ymm1, ymm0, YMMWORD PTR [rax-736]
	vmovdqu	YMMWORD PTR [rax-736], ymm1
	vpmaddwd ymm0, ymm2, ymm8

; 866  :       accu[acc_base + 3] = _mm256_add_epi32 (p_e3, accu[acc_base + 3]);

	vpaddd	ymm1, ymm0, YMMWORD PTR [rax-704]
	vmovdqu	YMMWORD PTR [rax-704], ymm1
	vpmaddwd ymm0, ymm3, ymm6

; 867  : 
; 868  :       accu[acc_base + 4] = _mm256_add_epi32 (p_o0, accu[acc_base + 4]);

	vpaddd	ymm1, ymm0, YMMWORD PTR [rax-672]
	vmovdqu	YMMWORD PTR [rax-672], ymm1
	vpmaddwd ymm0, ymm3, ymm5

; 869  :       accu[acc_base + 5] = _mm256_add_epi32 (p_o1, accu[acc_base + 5]);

	vpaddd	ymm1, ymm0, YMMWORD PTR [rax-640]
	vmovdqu	YMMWORD PTR [rax-640], ymm1
	vpmaddwd ymm0, ymm3, ymm7

; 870  :       accu[acc_base + 6] = _mm256_add_epi32 (p_o2, accu[acc_base + 6]);

	vpaddd	ymm1, ymm0, YMMWORD PTR [rax-608]
	vmovdqu	YMMWORD PTR [rax-608], ymm1
	vpmaddwd ymm0, ymm3, ymm8

; 871  :       accu[acc_base + 7] = _mm256_add_epi32 (p_o3, accu[acc_base + 7]);

	vpaddd	ymm1, ymm0, YMMWORD PTR [rax-576]
	vpbroadcastd ymm3, DWORD PTR [rdx-256]
	vmovdqu	YMMWORD PTR [rax-576], ymm1
	vpmaddwd ymm0, ymm3, ymm6
	vpaddd	ymm1, ymm0, YMMWORD PTR [rax-544]
	vmovdqu	YMMWORD PTR [rax-544], ymm1
	vpmaddwd ymm1, ymm3, ymm7
	vpmaddwd ymm2, ymm3, ymm5
	vpaddd	ymm0, ymm2, YMMWORD PTR [rax-512]
	vmovdqu	YMMWORD PTR [rax-512], ymm0
	vpaddd	ymm0, ymm1, YMMWORD PTR [rax-480]
	vmovdqu	YMMWORD PTR [rax-480], ymm0
	vpmaddwd ymm1, ymm3, ymm8
	vpaddd	ymm0, ymm1, YMMWORD PTR [rax-448]
	vmovdqu	YMMWORD PTR [rax-448], ymm0
	vpmaddwd ymm1, ymm4, ymm6
	vpaddd	ymm0, ymm1, YMMWORD PTR [rax-416]
	vmovdqu	YMMWORD PTR [rax-416], ymm0
	vpmaddwd ymm1, ymm4, ymm5
	vpaddd	ymm0, ymm1, YMMWORD PTR [rax-384]
	vmovdqu	YMMWORD PTR [rax-384], ymm0
	vpmaddwd ymm1, ymm4, ymm7
	vpaddd	ymm0, ymm1, YMMWORD PTR [rax-352]
	vmovdqu	YMMWORD PTR [rax-352], ymm0
	vpmaddwd ymm1, ymm4, ymm8
	vpaddd	ymm0, ymm1, YMMWORD PTR [rax-320]
	vmovdqu	YMMWORD PTR [rax-320], ymm0
	sub	rcx, 1
	jne	$LL7@mul_clip_m

; 825  :   size_t i, j;
; 826  : 
; 827  :   for (j = 0; j < 64; j += 4) {

	add	r8, 4
	sub	rbx, -128				; ffffffffffffff80H
	cmp	r8, 64					; 00000040H
	jb	$LL4@mul_clip_m

; 872  :     }
; 873  :   }
; 874  : 
; 875  :   for (i = 0; i < 32; i++) {

	vmovdqu	ymm5, YMMWORD PTR debias$1$[rbp]
	lea	rax, QWORD PTR accu$[rbp+256]
	lea	rcx, QWORD PTR [r14+224]
	npad	4
$LL10@mul_clip_m:
	lea	rcx, QWORD PTR [rcx+256]

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm2, ymm5, YMMWORD PTR [rax-256]
	vpaddd	ymm3, ymm5, YMMWORD PTR [rax-224]

; 872  :     }
; 873  :   }
; 874  : 
; 875  :   for (i = 0; i < 32; i++) {

	lea	rax, QWORD PTR [rax+512]

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, edi
	vpsrad	ymm4, ymm2, xmm1
	vmovd	xmm1, edi
	vpsrad	ymm2, ymm3, xmm1

; 876  :     size_t acc_base = i << 2;
; 877  :     size_t dst_base = i << 1;
; 878  : 
; 879  :     __m256i q0  = truncate_avx2(accu[acc_base + 0], debias, shift);
; 880  :     __m256i q1  = truncate_avx2(accu[acc_base + 1], debias, shift);
; 881  :     __m256i q2  = truncate_avx2(accu[acc_base + 2], debias, shift);
; 882  :     __m256i q3  = truncate_avx2(accu[acc_base + 3], debias, shift);
; 883  : 
; 884  :     __m256i h01 = _mm256_packs_epi32(q0, q1);

	vpackssdw ymm0, ymm4, ymm2

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm2, ymm5, YMMWORD PTR [rax-704]

; 886  : 
; 887  :             h01 = _mm256_permute4x64_epi64(h01, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm3, ymm0, 216				; 000000d8H

; 889  : 
; 890  :     _mm256_store_si256(dst_v + dst_base + 0, h01);

	vmovdqu	YMMWORD PTR [rcx-480], ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm5, YMMWORD PTR [rax-672]

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm0, edi
	vpsrad	ymm4, ymm2, xmm0
	vmovd	xmm0, edi
	vpsrad	ymm2, ymm3, xmm0

; 885  :     __m256i h23 = _mm256_packs_epi32(q2, q3);

	vpackssdw ymm1, ymm4, ymm2

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm2, ymm5, YMMWORD PTR [rax-640]

; 888  :             h23 = _mm256_permute4x64_epi64(h23, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm3, ymm1, 216				; 000000d8H

; 891  :     _mm256_store_si256(dst_v + dst_base + 1, h23);

	vmovdqu	YMMWORD PTR [rcx-448], ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm5, YMMWORD PTR [rax-608]

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, edi
	vpsrad	ymm4, ymm2, xmm1
	vmovd	xmm1, edi
	vpsrad	ymm2, ymm3, xmm1

; 876  :     size_t acc_base = i << 2;
; 877  :     size_t dst_base = i << 1;
; 878  : 
; 879  :     __m256i q0  = truncate_avx2(accu[acc_base + 0], debias, shift);
; 880  :     __m256i q1  = truncate_avx2(accu[acc_base + 1], debias, shift);
; 881  :     __m256i q2  = truncate_avx2(accu[acc_base + 2], debias, shift);
; 882  :     __m256i q3  = truncate_avx2(accu[acc_base + 3], debias, shift);
; 883  : 
; 884  :     __m256i h01 = _mm256_packs_epi32(q0, q1);

	vpackssdw ymm0, ymm4, ymm2

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm2, ymm5, YMMWORD PTR [rax-576]

; 886  : 
; 887  :             h01 = _mm256_permute4x64_epi64(h01, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm3, ymm0, 216				; 000000d8H

; 889  : 
; 890  :     _mm256_store_si256(dst_v + dst_base + 0, h01);

	vmovdqu	YMMWORD PTR [rcx-416], ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm5, YMMWORD PTR [rax-544]

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm0, edi
	vpsrad	ymm4, ymm2, xmm0
	vmovd	xmm0, edi
	vpsrad	ymm2, ymm3, xmm0

; 885  :     __m256i h23 = _mm256_packs_epi32(q2, q3);

	vpackssdw ymm1, ymm4, ymm2

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm2, ymm5, YMMWORD PTR [rax-480]

; 888  :             h23 = _mm256_permute4x64_epi64(h23, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm3, ymm1, 216				; 000000d8H

; 891  :     _mm256_store_si256(dst_v + dst_base + 1, h23);

	vmovdqu	YMMWORD PTR [rcx-384], ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm5, YMMWORD PTR [rax-512]

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, edi
	vpsrad	ymm4, ymm2, xmm1
	vmovd	xmm1, edi
	vpsrad	ymm2, ymm3, xmm1

; 876  :     size_t acc_base = i << 2;
; 877  :     size_t dst_base = i << 1;
; 878  : 
; 879  :     __m256i q0  = truncate_avx2(accu[acc_base + 0], debias, shift);
; 880  :     __m256i q1  = truncate_avx2(accu[acc_base + 1], debias, shift);
; 881  :     __m256i q2  = truncate_avx2(accu[acc_base + 2], debias, shift);
; 882  :     __m256i q3  = truncate_avx2(accu[acc_base + 3], debias, shift);
; 883  : 
; 884  :     __m256i h01 = _mm256_packs_epi32(q0, q1);

	vpackssdw ymm0, ymm2, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm2, ymm5, YMMWORD PTR [rax-416]

; 886  : 
; 887  :             h01 = _mm256_permute4x64_epi64(h01, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm3, ymm0, 216				; 000000d8H

; 889  : 
; 890  :     _mm256_store_si256(dst_v + dst_base + 0, h01);

	vmovdqu	YMMWORD PTR [rcx-352], ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm5, YMMWORD PTR [rax-448]

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm0, edi
	vpsrad	ymm4, ymm2, xmm0
	vmovd	xmm0, edi
	vpsrad	ymm2, ymm3, xmm0

; 885  :     __m256i h23 = _mm256_packs_epi32(q2, q3);

	vpackssdw ymm1, ymm2, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm2, ymm5, YMMWORD PTR [rax-352]

; 888  :             h23 = _mm256_permute4x64_epi64(h23, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm3, ymm1, 216				; 000000d8H

; 891  :     _mm256_store_si256(dst_v + dst_base + 1, h23);

	vmovdqu	YMMWORD PTR [rcx-320], ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm5, YMMWORD PTR [rax-384]

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm1, edi
	vpsrad	ymm4, ymm2, xmm1
	vmovd	xmm1, edi
	vpsrad	ymm2, ymm3, xmm1

; 876  :     size_t acc_base = i << 2;
; 877  :     size_t dst_base = i << 1;
; 878  : 
; 879  :     __m256i q0  = truncate_avx2(accu[acc_base + 0], debias, shift);
; 880  :     __m256i q1  = truncate_avx2(accu[acc_base + 1], debias, shift);
; 881  :     __m256i q2  = truncate_avx2(accu[acc_base + 2], debias, shift);
; 882  :     __m256i q3  = truncate_avx2(accu[acc_base + 3], debias, shift);
; 883  : 
; 884  :     __m256i h01 = _mm256_packs_epi32(q0, q1);

	vpackssdw ymm0, ymm2, ymm4

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm2, ymm5, YMMWORD PTR [rax-288]

; 886  : 
; 887  :             h01 = _mm256_permute4x64_epi64(h01, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm3, ymm0, 216				; 000000d8H

; 889  : 
; 890  :     _mm256_store_si256(dst_v + dst_base + 0, h01);

	vmovdqu	YMMWORD PTR [rcx-288], ymm3

; 71   :   __m256i truncable = _mm256_add_epi32 (v,         debias);

	vpaddd	ymm3, ymm5, YMMWORD PTR [rax-320]

; 72   :   return              _mm256_srai_epi32(truncable, shift);

	vmovd	xmm0, edi
	vpsrad	ymm4, ymm2, xmm0
	vmovd	xmm0, edi
	vpsrad	ymm2, ymm3, xmm0

; 885  :     __m256i h23 = _mm256_packs_epi32(q2, q3);

	vpackssdw ymm1, ymm2, ymm4

; 888  :             h23 = _mm256_permute4x64_epi64(h23, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm3, ymm1, 216				; 000000d8H

; 891  :     _mm256_store_si256(dst_v + dst_base + 1, h23);

	vmovdqu	YMMWORD PTR [rcx-256], ymm3
	sub	r9, 1
	jne	$LL10@mul_clip_m
	vzeroupper

; 892  :   }
; 893  : }

	lea	r11, QWORD PTR [rsp+4240]
	mov	rbx, QWORD PTR [r11+16]
	mov	rsi, QWORD PTR [r11+24]
	mov	rdi, QWORD PTR [r11+32]
	mov	r14, QWORD PTR [r11+40]
	vmovaps	xmm6, XMMWORD PTR [r11-16]
	vmovaps	xmm7, XMMWORD PTR [r11-32]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	mov	rsp, r11
	pop	rbp
	ret	0
mul_clip_matrix_32x32_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
tmp$ = 0
bitdepth$ = 2160
input$ = 2168
output$ = 2176
matrix_dct_32x32_avx2 PROC

; 938  : TRANSFORM(dct, 32);

	mov	QWORD PTR [rsp+8], rbx
	push	rbp
	sub	rsp, 2144				; 00000860H
	lea	rbp, QWORD PTR [rsp+96]
	and	rbp, -64				; ffffffffffffffc0H
	mov	rax, rdx
	movsx	r9d, cl
	mov	rbx, r8
	lea	rdx, OFFSET FLAT:kvz_g_dct_32_t
	sub	r9d, 4
	lea	r8, QWORD PTR tmp$[rbp]
	mov	rcx, rax
	call	mul_clip_matrix_32x32_avx2
	mov	r9d, 11
	lea	rdx, QWORD PTR tmp$[rbp]
	mov	r8, rbx
	lea	rcx, OFFSET FLAT:kvz_g_dct_32
	call	mul_clip_matrix_32x32_avx2
	mov	rbx, QWORD PTR [rsp+2160]
	add	rsp, 2144				; 00000860H
	pop	rbp
	ret	0
matrix_dct_32x32_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\dct-avx2.c
_TEXT	SEGMENT
tmp$ = 0
bitdepth$ = 2160
input$ = 2168
output$ = 2176
matrix_idct_32x32_avx2 PROC

; 939  : ITRANSFORM(dct, 32);

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rdi
	push	rbp
	sub	rsp, 2144				; 00000860H
	lea	rbp, QWORD PTR [rsp+96]
	and	rbp, -64				; ffffffffffffffc0H
	mov	rdi, r8
	movsx	ebx, cl
	lea	r8, QWORD PTR tmp$[rbp]
	mov	r9d, 7
	lea	rcx, OFFSET FLAT:kvz_g_dct_32_t
	call	mul_clip_matrix_32x32_avx2
	mov	r9d, 20
	lea	rdx, OFFSET FLAT:kvz_g_dct_32
	sub	r9d, ebx
	lea	rcx, QWORD PTR tmp$[rbp]
	mov	r8, rdi
	call	mul_clip_matrix_32x32_avx2
	lea	r11, QWORD PTR [rsp+2144]
	mov	rbx, QWORD PTR [r11+16]
	mov	rdi, QWORD PTR [r11+24]
	mov	rsp, r11
	pop	rbp
	ret	0
matrix_idct_32x32_avx2 ENDP
_TEXT	ENDS
END
