; Listing generated by Microsoft (R) Optimizing Compiler Version 19.26.28806.0 

include listing.inc

INCLUDELIB OLDNAMES

cbf_masks DW	01fH
	DW	0fH
	DW	07H
	DW	03H
	DW	01H
	ORG $+6
g_sig_last_scan_16x16 DD 00H
	DD	04H
	DD	01H
	DD	08H
	DD	05H
	DD	02H
	DD	0cH
	DD	09H
	DD	06H
	DD	03H
	DD	0dH
	DD	0aH
	DD	07H
	DD	0eH
	DD	0bH
	DD	0fH
g_group_idx DB	00H
	DB	01H
	DB	02H
	DB	03H
	DB	04H
	DB	04H
	DB	05H
	DB	05H
	DB	06H
	DB	06H
	DB	06H
	DB	06H
	DB	07H
	DB	07H
	DB	07H
	DB	07H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
strategies_to_select DQ FLAT:$SG4294946736
	DQ	FLAT:kvz_array_checksum
	DQ	FLAT:$SG4294946735
	DQ	FLAT:kvz_array_md5
	DQ	FLAT:$SG4294946734
	DQ	FLAT:kvz_reg_sad
	DQ	FLAT:$SG4294946733
	DQ	FLAT:kvz_sad_4x4
	DQ	FLAT:$SG4294946732
	DQ	FLAT:kvz_sad_8x8
	DQ	FLAT:$SG4294946731
	DQ	FLAT:kvz_sad_16x16
	DQ	FLAT:$SG4294946730
	DQ	FLAT:kvz_sad_32x32
	DQ	FLAT:$SG4294946729
	DQ	FLAT:kvz_sad_64x64
	DQ	FLAT:$SG4294946728
	DQ	FLAT:kvz_satd_4x4
	DQ	FLAT:$SG4294946727
	DQ	FLAT:kvz_satd_8x8
	DQ	FLAT:$SG4294946726
	DQ	FLAT:kvz_satd_16x16
	DQ	FLAT:$SG4294946725
	DQ	FLAT:kvz_satd_32x32
	DQ	FLAT:$SG4294946724
	DQ	FLAT:kvz_satd_64x64
	DQ	FLAT:$SG4294946723
	DQ	FLAT:kvz_satd_any_size
	DQ	FLAT:$SG4294946722
	DQ	FLAT:kvz_sad_4x4_dual
	DQ	FLAT:$SG4294946721
	DQ	FLAT:kvz_sad_8x8_dual
	DQ	FLAT:$SG4294946720
	DQ	FLAT:kvz_sad_16x16_dual
	DQ	FLAT:$SG4294946719
	DQ	FLAT:kvz_sad_32x32_dual
	DQ	FLAT:$SG4294946718
	DQ	FLAT:kvz_sad_64x64_dual
	DQ	FLAT:$SG4294946717
	DQ	FLAT:kvz_satd_4x4_dual
	DQ	FLAT:$SG4294946716
	DQ	FLAT:kvz_satd_8x8_dual
	DQ	FLAT:$SG4294946715
	DQ	FLAT:kvz_satd_16x16_dual
	DQ	FLAT:$SG4294946714
	DQ	FLAT:kvz_satd_32x32_dual
	DQ	FLAT:$SG4294946713
	DQ	FLAT:kvz_satd_64x64_dual
	DQ	FLAT:$SG4294946712
	DQ	FLAT:kvz_satd_any_size_quad
	DQ	FLAT:$SG4294946711
	DQ	FLAT:kvz_pixels_calc_ssd
	DQ	FLAT:$SG4294946710
	DQ	FLAT:kvz_bipred_average
	DQ	FLAT:$SG4294946709
	DQ	FLAT:kvz_get_optimized_sad
	DQ	FLAT:$SG4294946708
	DQ	FLAT:kvz_ver_sad
	DQ	FLAT:$SG4294946707
	DQ	FLAT:kvz_hor_sad
	DQ	FLAT:$SG4294946706
	DQ	FLAT:kvz_pixel_var
	DQ	FLAT:$SG4294946705
	DQ	FLAT:kvz_fast_forward_dst_4x4
	DQ	FLAT:$SG4294946704
	DQ	FLAT:kvz_dct_4x4
	DQ	FLAT:$SG4294946703
	DQ	FLAT:kvz_dct_8x8
	DQ	FLAT:$SG4294946702
	DQ	FLAT:kvz_dct_16x16
	DQ	FLAT:$SG4294946701
	DQ	FLAT:kvz_dct_32x32
	DQ	FLAT:$SG4294946700
	DQ	FLAT:kvz_fast_inverse_dst_4x4
	DQ	FLAT:$SG4294946699
	DQ	FLAT:kvz_idct_4x4
	DQ	FLAT:$SG4294946698
	DQ	FLAT:kvz_idct_8x8
	DQ	FLAT:$SG4294946697
	DQ	FLAT:kvz_idct_16x16
	DQ	FLAT:$SG4294946696
	DQ	FLAT:kvz_idct_32x32
	DQ	FLAT:$SG4294946695
	DQ	FLAT:kvz_filter_hpel_blocks_hor_ver_luma
	DQ	FLAT:$SG4294946694
	DQ	FLAT:kvz_filter_hpel_blocks_diag_luma
	DQ	FLAT:$SG4294946693
	DQ	FLAT:kvz_filter_qpel_blocks_hor_ver_luma
	DQ	FLAT:$SG4294946692
	DQ	FLAT:kvz_filter_qpel_blocks_diag_luma
	DQ	FLAT:$SG4294946691
	DQ	FLAT:kvz_sample_quarterpel_luma
	DQ	FLAT:$SG4294946690
	DQ	FLAT:kvz_sample_octpel_chroma
	DQ	FLAT:$SG4294946689
	DQ	FLAT:kvz_sample_quarterpel_luma_hi
	DQ	FLAT:$SG4294946688
	DQ	FLAT:kvz_sample_octpel_chroma_hi
	DQ	FLAT:$SG4294946687
	DQ	FLAT:kvz_get_extended_block
	DQ	FLAT:$SG4294946686
	DQ	FLAT:kvz_quant
	DQ	FLAT:$SG4294946685
	DQ	FLAT:kvz_quantize_residual
	DQ	FLAT:$SG4294946684
	DQ	FLAT:kvz_dequant
	DQ	FLAT:$SG4294946683
	DQ	FLAT:kvz_coeff_abs_sum
	DQ	FLAT:$SG4294946682
	DQ	FLAT:kvz_fast_coeff_cost
	DQ	FLAT:$SG4294946681
	DQ	FLAT:kvz_angular_pred
	DQ	FLAT:$SG4294946680
	DQ	FLAT:kvz_intra_pred_planar
	DQ	FLAT:$SG4294946679
	DQ	FLAT:kvz_intra_pred_filtered_dc
	DQ	FLAT:$SG4294946678
	DQ	FLAT:kvz_sao_edge_ddistortion
	DQ	FLAT:$SG4294946677
	DQ	FLAT:kvz_calc_sao_edge_dir
	DQ	FLAT:$SG4294946676
	DQ	FLAT:kvz_sao_reconstruct_color
	DQ	FLAT:$SG4294946675
	DQ	FLAT:kvz_sao_band_ddistortion
	DQ	FLAT:$SG4294946674
	DQ	FLAT:kvz_encode_coeff_nxn
	DQ	0000000000000000H
	DQ	0000000000000000H
g_sig_last_scan_32x32 DD 00H
	DD	08H
	DD	01H
	DD	010H
	DD	09H
	DD	02H
	DD	018H
	DD	011H
	DD	0aH
	DD	03H
	DD	020H
	DD	019H
	DD	012H
	DD	0bH
	DD	04H
	DD	028H
	DD	021H
	DD	01aH
	DD	013H
	DD	0cH
	DD	05H
	DD	030H
	DD	029H
	DD	022H
	DD	01bH
	DD	014H
	DD	0dH
	DD	06H
	DD	038H
	DD	031H
	DD	02aH
	DD	023H
	DD	01cH
	DD	015H
	DD	0eH
	DD	07H
	DD	039H
	DD	032H
	DD	02bH
	DD	024H
	DD	01dH
	DD	016H
	DD	0fH
	DD	03aH
	DD	033H
	DD	02cH
	DD	025H
	DD	01eH
	DD	017H
	DD	03bH
	DD	034H
	DD	02dH
	DD	026H
	DD	01fH
	DD	03cH
	DD	035H
	DD	02eH
	DD	027H
	DD	03dH
	DD	036H
	DD	02fH
	DD	03eH
	DD	037H
	DD	03fH
g_min_in_group DB 00H
	DB	01H
	DB	02H
	DB	03H
	DB	04H
	DB	06H
	DB	08H
	DB	0cH
	DB	010H
	DB	018H
	ORG $+6
default_fast_coeff_cost_wts DD 03e282e88r	; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e26c094r			; 0.162844
	DD	04081ca43r			; 4.05594
	DD	04064203ar			; 3.56447
	DD	040db915ar			; 6.86149
	DD	03e03d189r			; 0.128729
	DD	04089fbafr			; 4.31197
	DD	0407c5771r			; 3.94284
	DD	040ddeed2r			; 6.9354
	DD	03de33ce6r			; 0.110956
	DD	0408ddcb1r			; 4.43319
	DD	0407c8738r			; 3.94575
	DD	040dc1618r			; 6.8777
	DD	03dc29cfer			; 0.095026
	DD	0408f7938r			; 4.48355
	DD	0408636aar			; 4.19417
	DD	040d90260r			; 6.78154
	DD	03d99b1b8r			; 0.075046
	DD	04094474cr			; 4.6337
	DD	04082b1b6r			; 4.08419
	DD	040d65aeer			; 6.6986
	DD	03d56bca5r			; 0.052426
	DD	0409ef37er			; 4.96722
	DD	04080dee8r			; 4.02721
	DD	040d19306r			; 6.5492
	DD	03d24bcaer			; 0.040219
	DD	040a489car			; 5.14182
	DD	0407ee3bdr			; 3.98265
	DD	040cec513r			; 6.46156
	DD	03d0fba88r			; 0.03509
	DD	040a628e7r			; 5.19249
	DD	040752e49r			; 3.83095
	DD	040cd642ar			; 6.41848
	DD	03cf47d80r			; 0.029845
	DD	040a6c5d0r			; 5.21165
	DD	040743073r			; 3.81546
	DD	040cb0dd8r			; 6.34544
	DD	03cc0b136r			; 0.023522
	DD	040aa4f92r			; 5.32221
	DD	040744224r			; 3.81654
	DD	040cb8aaar			; 6.36068
	DD	03cae87d3r			; 0.021305
	DD	040a73ac3r			; 5.22592
	DD	04075eeccr			; 3.8427
	DD	040ca6cd9r			; 6.32579
	DD	03c821294r			; 0.015878
	DD	040a5dbe0r			; 5.18309
	DD	0407d2f27r			; 3.956
	DD	040ca8cbdr			; 6.32968
	DD	03c2ae297r			; 0.01043
	DD	040a32ce4r			; 5.09923
	DD	04085a85fr			; 4.1768
	DD	040c9c5d6r			; 6.3054
	DD	03c0a2a91r			; 0.008433
	DD	040a0f7der			; 5.03026
	DD	040879a50r			; 4.23759
	DD	040c8a4eer			; 6.27013
	DD	03bd4fdf4r			; 0.0065
	DD	0409f0412r			; 4.96925
	DD	0408adc57r			; 4.3394
	DD	040c6f870r			; 6.21783
	DD	03ba18373r			; 0.004929
	DD	0409d8d50r			; 4.9235
	DD	0408e283fr			; 4.44241
	DD	040c5df6cr			; 6.18352
	DD	03b73775cr			; 0.003715
	DD	0409d4c75r			; 4.91558
	DD	0408dbb1br			; 4.42909
	DD	040c4029fr			; 6.12532
	DD	03b4a70d2r			; 0.003089
	DD	0409c48f7r			; 4.88391
	DD	040920260r			; 4.56279
	DD	040c5019dr			; 6.15645
	DD	03b219c9dr			; 0.002466
	DD	0409c31abr			; 4.88106
	DD	040942800r			; 4.62988
	DD	040c49088r			; 6.14264
	DD	03b0e25c8r			; 0.002169
	DD	0409c3d62r			; 4.88249
	DD	04094ae99r			; 4.64631
	DD	040c415d1r			; 6.12766
	DD	03b26dacbr			; 0.002546
	DD	040996304r			; 4.79334
	DD	0409acc16r			; 4.83741
	DD	040c6606br			; 6.19927
	DD	03aac3a86r			; 0.001314
	DD	04099e220r			; 4.80885
	DD	0409a81bdr			; 4.82834
	DD	040c7ca3cr			; 6.24344
	DD	03a9741d1r			; 0.001154
	DD	0409b9a72r			; 4.8626
	DD	0409b19aar			; 4.84688
	DD	040c693a5r			; 6.20552
	DD	03a80f990r			; 0.000984
	DD	0409bb993r			; 4.8664
	DD	0409b7fa2r			; 4.85933
	DD	040c7b565r			; 6.24089
	DD	03a551f82r			; 0.000813
	DD	0409b698ar			; 4.85663
	DD	0409d95bar			; 4.92453
	DD	040c963a4r			; 6.29341
	DD	03a91c087r			; 0.001112
	DD	04099419er			; 4.78926
	DD	040a050f0r			; 5.00988
	DD	040cddf8fr			; 6.43354
	DD	03a10b418r			; 0.000552
	DD	04098580ar			; 4.76075
	DD	040a2e4f1r			; 5.09045
	DD	040d32e1fr			; 6.59938
	DD	039ccff22r			; 0.000391
	DD	0409ec42cr			; 4.96145
	DD	040a38d95r			; 5.11103
	DD	040d8342fr			; 6.75637
	DD	039ae1049r			; 0.000332
	DD	0409f63f8r			; 4.98095
	DD	040a46b89r			; 5.13813
	DD	040dbc1e8r			; 6.86742
	DD	03952c387r			; 0.000201
	DD	040a5d297r			; 5.18196
	DD	04097af64r			; 4.74016
	DD	040cec07dr			; 6.461
	DD	0397ba882r			; 0.00024
	DD	040a5eeb7r			; 5.18539
	DD	0409bfeb0r			; 4.87484
	DD	040da3603r			; 6.81909
	DD	03908509cr			; 0.00013
	DD	040a8a6b5r			; 5.27035
	DD	040977eacr			; 4.73421
	DD	040da708fr			; 6.82624
	DD	038da1a93r			; 0.000104
	DD	040abe6e8r			; 5.37194
	DD	040930af4r			; 4.59509
	DD	040d5189ar			; 6.65925
	DD	038ae1049r			; 8.3e-05
	DD	040ab9581r			; 5.362
	DD	04093c250r			; 4.61747
	DD	040dacf03r			; 6.83777
	DD	03890b418r			; 6.9e-05
	DD	040a926e3r			; 5.286
	DD	0409828e7r			; 4.75499
	DD	040e516e1r			; 7.15904
	DD	0384d8559r			; 4.9e-05
	DD	040afa18cr			; 5.48847
	DD	0408cace9r			; 4.39611
	DD	040d74682r			; 6.72736
	DD	038734507r			; 5.8e-05
	DD	0409eafa3r			; 4.95894
	DD	040929321r			; 4.58046
	DD	040cf49a5r			; 6.47774
	DD	037eae18br			; 2.8e-05
	DD	040b0ae1br			; 5.52125
	DD	0408e1885r			; 4.44049
	DD	040e68f80r			; 7.20502
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	0379f6230r			; 1.9e-05
	DD	040b9f5d8r			; 5.81126
	DD	0408cc582r			; 4.39911
	DD	040eac30dr			; 7.33631
g_sao_edge_offsets DD 0ffffffffH
	DD	00H
	DD	01H
	DD	00H
	DD	00H
	DD	0ffffffffH
	DD	00H
	DD	01H
	DD	0ffffffffH
	DD	0ffffffffH
	DD	01H
	DD	01H
	DD	01H
	DD	0ffffffffH
	DD	0ffffffffH
	DD	01H
g_sig_last_scan_cg DQ FLAT:g_sig_last_scan_8x8
	DQ	FLAT:g_sig_last_scan_8x8+16
	DQ	FLAT:g_sig_last_scan_8x8+32
	DQ	FLAT:g_sig_last_scan_8x8
	DQ	FLAT:g_sig_last_scan_8x8+16
	DQ	FLAT:g_sig_last_scan_8x8+32
	DQ	FLAT:g_sig_last_scan_16x16
	DQ	0000000000000000H
	DQ	0000000000000000H
	DQ	FLAT:g_sig_last_scan_32x32
	DQ	0000000000000000H
	DQ	0000000000000000H
g_sig_last_scan_8x8 DD 00H
	DD	02H
	DD	01H
	DD	03H
	DD	00H
	DD	01H
	DD	02H
	DD	03H
	DD	00H
	DD	02H
	DD	01H
	DD	03H
	ORG $+3
$SG4294946686 DB 'quant', 00H
	ORG $+7
$SG4294946736 DB 'array_checksum', 00H
	ORG $+1
$SG4294946735 DB 'array_md5', 00H
	ORG $+6
$SG4294946734 DB 'reg_sad', 00H
$SG4294946733 DB 'sad_4x4', 00H
$SG4294946732 DB 'sad_8x8', 00H
$SG4294946731 DB 'sad_16x16', 00H
	ORG $+6
$SG4294946730 DB 'sad_32x32', 00H
	ORG $+6
$SG4294946729 DB 'sad_64x64', 00H
	ORG $+6
$SG4294946728 DB 'satd_4x4', 00H
	ORG $+7
$SG4294946727 DB 'satd_8x8', 00H
	ORG $+7
$SG4294946726 DB 'satd_16x16', 00H
	ORG $+5
$SG4294946725 DB 'satd_32x32', 00H
	ORG $+5
$SG4294946724 DB 'satd_64x64', 00H
	ORG $+5
$SG4294946723 DB 'satd_any_size', 00H
	ORG $+2
$SG4294946722 DB 'sad_4x4_dual', 00H
	ORG $+3
$SG4294946721 DB 'sad_8x8_dual', 00H
	ORG $+3
$SG4294946720 DB 'sad_16x16_dual', 00H
	ORG $+1
$SG4294946719 DB 'sad_32x32_dual', 00H
	ORG $+1
$SG4294946718 DB 'sad_64x64_dual', 00H
	ORG $+1
$SG4294946717 DB 'satd_4x4_dual', 00H
	ORG $+2
$SG4294946716 DB 'satd_8x8_dual', 00H
	ORG $+2
$SG4294946715 DB 'satd_16x16_dual', 00H
$SG4294946714 DB 'satd_32x32_dual', 00H
$SG4294946713 DB 'satd_64x64_dual', 00H
$SG4294946712 DB 'satd_any_size_quad', 00H
	ORG $+5
$SG4294946711 DB 'pixels_calc_ssd', 00H
$SG4294946710 DB 'bipred_average', 00H
	ORG $+1
$SG4294946709 DB 'get_optimized_sad', 00H
	ORG $+6
$SG4294946708 DB 'ver_sad', 00H
$SG4294946707 DB 'hor_sad', 00H
$SG4294946706 DB 'pixel_var', 00H
	ORG $+6
$SG4294946705 DB 'fast_forward_dst_4x4', 00H
	ORG $+3
$SG4294946704 DB 'dct_4x4', 00H
$SG4294946703 DB 'dct_8x8', 00H
$SG4294946702 DB 'dct_16x16', 00H
	ORG $+6
$SG4294946701 DB 'dct_32x32', 00H
	ORG $+6
$SG4294946700 DB 'fast_inverse_dst_4x4', 00H
	ORG $+3
$SG4294946699 DB 'idct_4x4', 00H
	ORG $+7
$SG4294946698 DB 'idct_8x8', 00H
	ORG $+7
$SG4294946697 DB 'idct_16x16', 00H
	ORG $+5
$SG4294946696 DB 'idct_32x32', 00H
	ORG $+5
$SG4294946695 DB 'filter_hpel_blocks_hor_ver_luma', 00H
$SG4294946694 DB 'filter_hpel_blocks_diag_luma', 00H
	ORG $+3
$SG4294946693 DB 'filter_qpel_blocks_hor_ver_luma', 00H
$SG4294946692 DB 'filter_qpel_blocks_diag_luma', 00H
	ORG $+3
$SG4294946691 DB 'sample_quarterpel_luma', 00H
	ORG $+1
$SG4294946690 DB 'sample_octpel_chroma', 00H
	ORG $+3
$SG4294946689 DB 'sample_quarterpel_luma_hi', 00H
	ORG $+6
$SG4294946688 DB 'sample_octpel_chroma_hi', 00H
$SG4294946687 DB 'get_extended_block', 00H
	ORG $+5
$SG4294946685 DB 'quantize_residual', 00H
	ORG $+6
$SG4294946684 DB 'dequant', 00H
$SG4294946683 DB 'coeff_abs_sum', 00H
	ORG $+2
$SG4294946682 DB 'fast_coeff_cost', 00H
$SG4294946681 DB 'angular_pred', 00H
	ORG $+3
$SG4294946680 DB 'intra_pred_planar', 00H
	ORG $+6
$SG4294946679 DB 'intra_pred_filtered_dc', 00H
	ORG $+1
$SG4294946678 DB 'sao_edge_ddistortion', 00H
	ORG $+3
$SG4294946677 DB 'calc_sao_edge_dir', 00H
	ORG $+6
$SG4294946676 DB 'sao_reconstruct_color', 00H
	ORG $+2
$SG4294946675 DB 'sao_band_ddistortion', 00H
	ORG $+3
$SG4294946674 DB 'encode_coeff_nxn', 00H
PUBLIC	kvz_encode_coeff_nxn_avx2
PUBLIC	kvz_strategy_register_encode_avx2
pdata	SEGMENT
$pdata$kvz_encode_coeff_nxn_avx2 DD imagerel $LN127
	DD	imagerel $LN127+3337
	DD	imagerel $unwind$kvz_encode_coeff_nxn_avx2
$pdata$kvz_context_get_sig_ctx_inc_16x16b DD imagerel kvz_context_get_sig_ctx_inc_16x16b
	DD	imagerel kvz_context_get_sig_ctx_inc_16x16b+602
	DD	imagerel $unwind$kvz_context_get_sig_ctx_inc_16x16b
$pdata$scanord_read_vector DD imagerel scanord_read_vector
	DD	imagerel scanord_read_vector+283
	DD	imagerel $unwind$scanord_read_vector
$pdata$kvz_strategy_register_encode_avx2 DD imagerel $LN4
	DD	imagerel $LN4+49
	DD	imagerel $unwind$kvz_strategy_register_encode_avx2
xdata	SEGMENT
$unwind$kvz_encode_coeff_nxn_avx2 DD 01f9201H
	DD	0240f886H
	DD	0241e87dH
	DD	0242d874H
	DD	0243c86bH
	DD	0244b862H
	DD	0245a859H
	DD	02469850H
	DD	02478847H
	DD	0248783eH
	DD	02496835H
	DD	049e342cH
	DD	0494012cH
	DD	0e01df01fH
	DD	0c019d01bH
	DD	060167017H
	DD	05015H
$unwind$kvz_context_get_sig_ctx_inc_16x16b DD 0195301H
	DD	0af84aH
	DD	0be842H
	DD	0cd83aH
	DD	0dc832H
	DD	0eb82dH
	DD	0fa828H
	DD	0109823H
	DD	011881eH
	DD	0127819H
	DD	0136814H
	DD	02a340fH
	DD	028010fH
	DD	05008H
$unwind$scanord_read_vector DD 020a01H
	DD	019010aH
$unwind$kvz_strategy_register_encode_avx2 DD 010401H
	DD	06204H
	ORG $+2
$SG4294946670 DB 'encode_coeff_nxn', 00H
	ORG $+2
$SG4294946671 DB 'avx2', 00H
$SG4294946672 DB 's', 00H, 'c', 00H, 'a', 00H, 'n', 00H, '_', 00H, 'c', 00H
	DB	'g', 00H, '_', 00H, 'l', 00H, 'a', 00H, 's', 00H, 't', 00H, ' '
	DB	00H, '>', 00H, '=', 00H, ' ', 00H, '0', 00H, 00H, 00H
	ORG $+4
$SG4294946673 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'e', 00H, 'n', 00H, 'c', 00H, 'o', 00H, 'd'
	DB	00H, 'e', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'i', 00H
	DB	'n', 00H, 'g', 00H, '_', 00H, 't', 00H, 'r', 00H, 'e', 00H, 'e'
	DB	00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H, '2', 00H, '.', 00H
	DB	'c', 00H, 00H, 00H
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c
_TEXT	SEGMENT
opaque$ = 64
bitdepth$dead$ = 72
kvz_strategy_register_encode_avx2 PROC

; 609  : {

$LN4:
	sub	rsp, 56					; 00000038H

; 610  :   bool success = true;
; 611  : 
; 612  : #if COMPILE_INTEL_AVX2
; 613  :   success &= kvz_strategyselector_register(opaque, "encode_coeff_nxn", "avx2", 40, &kvz_encode_coeff_nxn_avx2);

	lea	rax, OFFSET FLAT:kvz_encode_coeff_nxn_avx2
	mov	r9d, 40					; 00000028H
	lea	r8, OFFSET FLAT:$SG4294946671
	mov	QWORD PTR [rsp+32], rax
	lea	rdx, OFFSET FLAT:$SG4294946670
	call	kvz_strategyselector_register
	and	eax, 1

; 614  : #endif
; 615  : 
; 616  :   return success;
; 617  : }

	add	rsp, 56					; 00000038H
	ret	0
kvz_strategy_register_encode_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\extras\crypto.h
_TEXT	SEGMENT
hdl$dead$ = 8
num_bits$dead$ = 16
kvz_crypto_get_key PROC

; 66   :   return 0;

	xor	eax, eax

; 67   : }

	ret	0
kvz_crypto_get_key ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
_TEXT	SEGMENT
blend_masks$ = 0
low128_shuffle_masks$ = 48
invec_rearr_masks_lower$ = 96
invec_rearr_masks_upper$ = 144
coeffs$ = 208
scan$ = 216
scan_mode$ = 224
subpos$ = 232
width$ = 240
result_vecs$ = 248
n_bufs$dead$ = 256
scanord_read_vector PROC

; 71   : {

	mov	rax, rsp
	sub	rsp, 200				; 000000c8H

; 72   :   // For vectorized reordering of coef and q_coef
; 73   :   const __m128i low128_shuffle_masks[3] = {

	vmovdqu	xmm2, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	vmovdqu	xmm0, XMMWORD PTR __xmm@030209080f0e070601000d0c05040b0a

; 74   :     _mm_setr_epi8(10,11,  4, 5, 12,13,  0, 1,  6, 7, 14,15,  8, 9,  2, 3),
; 75   :     _mm_setr_epi8( 0, 1,  2, 3,  4, 5,  6, 7,  8, 9, 10,11, 12,13, 14,15),
; 76   :     _mm_setr_epi8( 4, 5,  6, 7,  0, 1,  2, 3, 12,13, 14,15,  8, 9, 10,11),
; 77   :   };
; 78   : 
; 79   :   const __m128i blend_masks[3] = {

	vmovdqu	xmm1, XMMWORD PTR __xmm@ffffffff00000000ffff000000000000
	vmovdqu	XMMWORD PTR low128_shuffle_masks$[rsp], xmm0
	vmovdqu	xmm0, XMMWORD PTR __xmm@0b0a09080f0e0d0c0302010007060504

; 80   :     _mm_setr_epi16( 0,  0,  0, -1,  0,  0, -1, -1),
; 81   :     _mm_setr_epi16( 0,  0,  0,  0,  0,  0,  0,  0),
; 82   :     _mm_setr_epi16( 0,  0, -1, -1,  0,  0, -1, -1),
; 83   :   };
; 84   : 
; 85   :   const __m128i invec_rearr_masks_upper[3] = {

	vmovdqu	XMMWORD PTR [rax-40], xmm2

; 86   :     _mm_setr_epi8( 0, 1,  8, 9,  2, 3,  6, 7, 10,11,  4, 5, 12,13, 14,15),
; 87   :     _mm_setr_epi8( 0, 1,  2, 3,  4, 5,  6, 7,  8, 9, 10,11, 12,13, 14,15),
; 88   :     _mm_setr_epi8( 0, 1,  8, 9,  4, 5, 12,13,  2, 3, 10,11,  6, 7, 14,15),
; 89   :   };
; 90   : 
; 91   :   const __m128i invec_rearr_masks_lower[3] = {

	vmovdqu	XMMWORD PTR [rax-88], xmm2
	vmovdqu	XMMWORD PTR blend_masks$[rsp], xmm1
	vmovdqu	xmm1, XMMWORD PTR __xmm@ffffffff00000000ffffffff00000000
	vmovdqu	XMMWORD PTR blend_masks$[rsp+32], xmm1
	vmovdqu	xmm1, XMMWORD PTR __xmm@0f0e07060b0a03020d0c050409080100
	vmovdqu	XMMWORD PTR [rax-24], xmm1
	vmovdqu	xmm1, XMMWORD PTR __xmm@0b0a03020f0e0706090801000d0c0504
	vmovdqu	XMMWORD PTR [rax-72], xmm1
	vmovdqu	XMMWORD PTR low128_shuffle_masks$[rsp+16], xmm2
	vmovdqu	XMMWORD PTR [rax-120], xmm0
	vpxor	xmm0, xmm0, xmm0
	vmovdqu	XMMWORD PTR blend_masks$[rsp+16], xmm0
	vmovdqu	xmm0, XMMWORD PTR __xmm@0f0e0d0c05040b0a0706030209080100
	vmovdqu	XMMWORD PTR [rax-56], xmm0
	vmovdqu	xmm0, XMMWORD PTR __xmm@0b0a090805040f0e0302010007060d0c
	vmovdqu	XMMWORD PTR [rax-104], xmm0

; 92   :     _mm_setr_epi8(12,13,  6, 7,  0, 1,  2, 3, 14,15,  4, 5,  8, 9, 10,11),
; 93   :     _mm_setr_epi8( 0, 1,  2, 3,  4, 5,  6, 7,  8, 9, 10,11, 12,13, 14,15),
; 94   :     _mm_setr_epi8( 4, 5, 12,13,  0, 1,  8, 9,  6, 7, 14,15,  2, 3, 10,11),
; 95   :   };
; 96   : 
; 97   :   const size_t row_offsets[4] = {

	movsxd	rax, r9d

; 98   :     scan[subpos] + width * 0,
; 99   :     scan[subpos] + width * 1,
; 100  :     scan[subpos] + width * 2,
; 101  :     scan[subpos] + width * 3,
; 102  :   };
; 103  : 
; 104  :   for (int i = 0; i < n_bufs; i++) {
; 105  :     const int16_t *__restrict coeff = coeffs[i];
; 106  : 
; 107  :     // NOTE: Upper means "higher in pixel order inside block", which implies
; 108  :     // lower addresses (note the difference: HIGH and LOW vs UPPER and LOWER),
; 109  :     // so upper 128b vector actually becomes the lower part of a 256-bit coeff
; 110  :     // vector and lower vector the higher part!
; 111  :     __m128d coeffs_d_upper;
; 112  :     __m128d coeffs_d_lower;
; 113  : 
; 114  :     __m128i coeffs_upper;
; 115  :     __m128i coeffs_lower;
; 116  : 
; 117  :     __m128i coeffs_rearr1_upper;
; 118  :     __m128i coeffs_rearr1_lower;
; 119  : 
; 120  :     __m128i coeffs_rearr2_upper;
; 121  :     __m128i coeffs_rearr2_lower;
; 122  : 
; 123  :     // Zeroing these is actually unnecessary, but the compiler will whine
; 124  :     // about uninitialized values otherwise
; 125  :     coeffs_d_upper = _mm_setzero_pd();
; 126  :     coeffs_d_lower = _mm_setzero_pd();
; 127  : 
; 128  :     coeffs_d_upper = _mm_loadl_pd(coeffs_d_upper, (double *)(coeff + row_offsets[0]));
; 129  :     coeffs_d_upper = _mm_loadh_pd(coeffs_d_upper, (double *)(coeff + row_offsets[1]));
; 130  : 
; 131  :     coeffs_d_lower = _mm_loadl_pd(coeffs_d_lower, (double *)(coeff + row_offsets[2]));
; 132  :     coeffs_d_lower = _mm_loadh_pd(coeffs_d_lower, (double *)(coeff + row_offsets[3]));
; 133  : 
; 134  :     coeffs_upper   = _mm_castpd_si128(coeffs_d_upper);
; 135  :     coeffs_lower   = _mm_castpd_si128(coeffs_d_lower);
; 136  : 
; 137  :     coeffs_lower   = _mm_shuffle_epi8(coeffs_lower, low128_shuffle_masks[scan_mode]);

	movsx	r10, r8b
	mov	r8, QWORD PTR [rcx]
	add	r10, r10
	mov	r9d, DWORD PTR [rdx+rax*4]
	mov	edx, DWORD PTR width$[rsp]

; 138  : 
; 139  :     coeffs_rearr1_upper = _mm_blendv_epi8(coeffs_upper, coeffs_lower, blend_masks[scan_mode]);
; 140  :     coeffs_rearr1_lower = _mm_blendv_epi8(coeffs_lower, coeffs_upper, blend_masks[scan_mode]);

	vmovdqu	xmm0, XMMWORD PTR blend_masks$[rsp+r10*8]
	vmovddup xmm3, QWORD PTR [r8+r9*2]
	lea	eax, DWORD PTR [r9+rdx]
	vmovhpd	xmm3, xmm3, QWORD PTR [r8+rax*2]
	lea	eax, DWORD PTR [r9+rdx*2]
	vmovddup xmm1, QWORD PTR [r8+rax*2]
	lea	eax, DWORD PTR [r9+rdx*2]
	add	edx, eax

; 141  : 
; 142  :     coeffs_rearr2_upper = _mm_shuffle_epi8(coeffs_rearr1_upper, invec_rearr_masks_upper[scan_mode]);
; 143  :     coeffs_rearr2_lower = _mm_shuffle_epi8(coeffs_rearr1_lower, invec_rearr_masks_lower[scan_mode]);
; 144  : 
; 145  :     // The Intel Intrinsics Guide talks about _mm256_setr_m128i but my headers
; 146  :     // lack such an instruction. What it does is essentially this anyway.
; 147  :     result_vecs[i] = _mm256_inserti128_si256(_mm256_castsi128_si256(coeffs_rearr2_upper),

	mov	rax, QWORD PTR result_vecs$[rsp]
	vmovhpd	xmm1, xmm1, QWORD PTR [r8+rdx*2]
	vpshufb	xmm2, xmm1, XMMWORD PTR low128_shuffle_masks$[rsp+r10*8]
	vpblendvb xmm1, xmm2, xmm3, xmm0
	vpshufb	xmm4, xmm1, XMMWORD PTR invec_rearr_masks_lower$[rsp+r10*8]
	vpblendvb xmm2, xmm3, xmm2, xmm0
	vpshufb	xmm3, xmm2, XMMWORD PTR invec_rearr_masks_upper$[rsp+r10*8]
	vinserti128 ymm0, ymm3, xmm4, 1
	vmovdqu	YMMWORD PTR [rax], ymm0
	vzeroupper

; 148  :                                              coeffs_rearr2_lower,
; 149  :                                              1);
; 150  :   }
; 151  : }

	add	rsp, 200				; 000000c8H
	ret	0
scanord_read_vector ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
_TEXT	SEGMENT
ints$ = 8
first$ = 16
last$ = 24
get_first_last_nz_int16 PROC

; 155  : {

	vpxor	xmm1, xmm1, xmm1

; 156  :   // Note that nonzero_bytes will always have both bytes set for a set word
; 157  :   // even if said word only had one of its bytes set, because we're doing 16
; 158  :   // bit wide comparisons. No big deal, just shift results to the right by one
; 159  :   // bit to have the results represent indexes of first set words, not bytes.
; 160  :   // Another note, it has to use right shift instead of division to preserve
; 161  :   // behavior on an all-zero vector (-1 / 2 == 0, but -1 >> 1 == -1)
; 162  :   const __m256i zero = _mm256_setzero_si256();
; 163  : 
; 164  :   __m256i zeros = _mm256_cmpeq_epi16(ints, zero);

	vpcmpeqw ymm1, ymm1, YMMWORD PTR [rcx]

; 165  :   uint32_t nonzero_bytes = ~((uint32_t)_mm256_movemask_epi8(zeros));

	vpmovmskb ecx, ymm1
	not	ecx

; 166  :   *first = (    (int32_t)_tzcnt_u32(nonzero_bytes)) >> 1;

	tzcnt	eax, ecx
	sar	eax, 1
	mov	DWORD PTR [rdx], eax

; 167  :   *last = (31 - (int32_t)_lzcnt_u32(nonzero_bytes)) >> 1;

	lzcnt	eax, ecx
	mov	ecx, 31
	sub	ecx, eax
	sar	ecx, 1
	mov	DWORD PTR [r8], ecx
	vzeroupper

; 168  : }

	ret	0
get_first_last_nz_int16 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c
_TEXT	SEGMENT
a$ = 8
b$ = 16
u32vec_cmpgt_epu2 PROC

; 59   :   uint32_t a_gt_b          = _andn_u32(b, a);

	andn	r8d, edx, ecx

; 60   :   uint32_t a_ne_b          = a ^ b;

	xor	ecx, edx

; 61   :   uint32_t a_gt_b_sh       = a_gt_b << 1;

	lea	eax, DWORD PTR [r8+r8]

; 62   :   uint32_t lobit_tiebrk_hi = _andn_u32(a_ne_b, a_gt_b_sh);

	andn	eax, ecx, eax

; 63   :   uint32_t res             = a_gt_b | lobit_tiebrk_hi;

	or	eax, r8d

; 64   :   return res;
; 65   : }

	ret	0
u32vec_cmpgt_epu2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c
_TEXT	SEGMENT
src$ = 8
pack_16x16b_to_16x2b PROC

; 69   :   /*
; 70   :    * For each 16-bit element in src:
; 71   :    * ABCD EFGH IJKL MNOP Original elements
; 72   :    * 0000 0000 0000 00XY Element clipped to [0, 3] using _mm256_min_epu16
; 73   :    * 0000 000X Y000 0000 Shift word to align LSBs across byte boundary
; 74   :    * 0000 0001 1000 0000 Comparison mask to be compared against
; 75   :    * XXXX XXXX YYYY YYYY Comparison result, for movemask
; 76   :    */
; 77   :   const __m256i threes  = _mm256_set1_epi16   (3);

	vmovdqu	ymm1, YMMWORD PTR __ymm@0003000300030003000300030003000300030003000300030003000300030003

; 78   :   const __m256i cmpmask = _mm256_slli_epi16   (threes, 7); // 0x0180 (avoid set1)

	vpsllw	ymm3, ymm1, 7

; 79   : 
; 80   :   __m256i  clipped      = _mm256_min_epu16    (src, threes);

	vpminuw	ymm1, ymm1, YMMWORD PTR [rcx]

; 81   :   __m256i  shifted      = _mm256_slli_epi16   (clipped, 7);

	vpsllw	ymm2, ymm1, 7

; 82   :   __m256i  cmpres       = _mm256_cmpeq_epi8   (shifted, cmpmask);

	vpcmpeqb ymm3, ymm2, ymm3

; 83   :   uint32_t result       = _mm256_movemask_epi8(cmpres);

	vpmovmskb eax, ymm3
	vzeroupper

; 84   : 
; 85   :   return result;
; 86   : }

	ret	0
pack_16x16b_to_16x2b ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c
_TEXT	SEGMENT
pattern_sig_ctxs_eq_zero$1$ = 0
ctx_ind_map$ = 32
pattern_sig_ctx$ = 336
scan_idx$ = 344
pos_xs$ = 352
pos_ys$ = 360
block_type$ = 368
texture_type$ = 376
kvz_context_get_sig_ctx_inc_16x16b PROC

; 102  : {

	mov	rax, rsp
	mov	QWORD PTR [rax+8], rbx
	push	rbp
	sub	rsp, 320				; 00000140H
	vmovaps	XMMWORD PTR [rax-24], xmm6
	vmovaps	XMMWORD PTR [rax-40], xmm7
	vmovaps	XMMWORD PTR [rax-56], xmm8
	vmovaps	XMMWORD PTR [rax-72], xmm9
	vmovaps	XMMWORD PTR [rax-88], xmm10
	vmovaps	XMMWORD PTR [rax-104], xmm11
	vmovaps	XMMWORD PTR [rax-120], xmm12
	vmovaps	XMMWORD PTR [rax-136], xmm13
	vmovaps	XMMWORD PTR [rax-152], xmm14
	vmovaps	XMMWORD PTR [rax-168], xmm15
	lea	rbp, QWORD PTR [rsp+32]
	and	rbp, -32				; ffffffffffffffe0H

; 103  :   const __m256i zero   = _mm256_set1_epi8(0);
; 104  :   const __m256i ff     = _mm256_set1_epi8(0xff);
; 105  : 
; 106  :   const __m256i ones   = _mm256_set1_epi16(1);
; 107  :   const __m256i twos   = _mm256_set1_epi16(2);
; 108  :   const __m256i threes = _mm256_set1_epi16(3);
; 109  : 
; 110  :   const __m256i ctx_ind_map[3] = {

	vmovdqu	ymm0, YMMWORD PTR __ymm@0008000800080005000800070005000400060007000400030006000100020000
	vmovdqu	ymm1, YMMWORD PTR __ymm@0008000800070007000800080006000600050004000300020005000400010000
	vmovdqu	ymm4, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vmovdqu	ymm2, YMMWORD PTR __ymm@0002000200020002000200020002000200020002000200020002000200020002
	vmovdqu	ymm6, YMMWORD PTR __ymm@0003000300030003000300030003000300030003000300030003000300030003
	vmovdqu	YMMWORD PTR ctx_ind_map$[rbp], ymm0
	vmovdqu	ymm0, YMMWORD PTR __ymm@0008000800050005000800080004000400070006000300010007000600020000
	mov	r11, r9
	mov	rbx, r8

; 111  :     _mm256_setr_epi16(
; 112  :         0, 2, 1, 6,
; 113  :         3, 4, 7, 6,
; 114  :         4, 5, 7, 8,
; 115  :         5, 8, 8, 8
; 116  :     ),
; 117  :     _mm256_setr_epi16(
; 118  :         0, 1, 4, 5,
; 119  :         2, 3, 4, 5,
; 120  :         6, 6, 8, 8,
; 121  :         7, 7, 8, 8
; 122  :     ),
; 123  :     _mm256_setr_epi16(
; 124  :         0, 2, 6, 7,
; 125  :         1, 3, 6, 7,
; 126  :         4, 4, 8, 8,
; 127  :         5, 5, 8, 8
; 128  :     ),
; 129  :   };
; 130  : 
; 131  :   int16_t offset;
; 132  :   if (block_type == 3)

	mov	r9d, DWORD PTR block_type$[rsp]
	mov	r10d, ecx
	movsx	r8d, BYTE PTR texture_type$[rsp]
	vmovdqu	YMMWORD PTR ctx_ind_map$[rbp+64], ymm0
	vpxor	xmm3, xmm3, xmm3
	vmovdqu	YMMWORD PTR ctx_ind_map$[rbp+32], ymm1
	cmp	r9d, 3
	jne	SHORT $LN2@kvz_contex

; 133  :     if (scan_idx == SCAN_DIAG)
; 134  :       offset = 9;
; 135  :     else
; 136  :       offset = 15;

	test	edx, edx
	mov	ecx, 9
	mov	eax, 15
	cmovne	cx, ax
	jmp	SHORT $LN7@kvz_contex
$LN2@kvz_contex:

; 137  :   else
; 138  :     if (texture_type == 0)
; 139  :       offset = 21;

	mov	ecx, 21
	test	r8b, r8b

; 140  :     else
; 141  :       offset = 12;

	mov	eax, 12
	cmove	ax, cx
	movzx	ecx, ax
$LN7@kvz_contex:

; 142  : 
; 143  :   __m256i offsets = _mm256_set1_epi16(offset);
; 144  : 
; 145  :   // This will only ever be compared to 0, 1 and 2, so it's fine to cast down
; 146  :   // to 16b (and it should never be above 3 anyways)
; 147  :   __m256i pattern_sig_ctxs = _mm256_set1_epi16((int16_t)(MIN(0xffff, pattern_sig_ctx)));
; 148  :   __m256i pattern_sig_ctxs_eq_zero = _mm256_cmpeq_epi16(pattern_sig_ctxs, zero);
; 149  :   __m256i pattern_sig_ctxs_eq_one  = _mm256_cmpeq_epi16(pattern_sig_ctxs, ones);
; 150  :   __m256i pattern_sig_ctxs_eq_two  = _mm256_cmpeq_epi16(pattern_sig_ctxs, twos);
; 151  : 
; 152  :   __m256i pattern_sig_ctxs_eq_1or2 = _mm256_or_si256 (pattern_sig_ctxs_eq_one,
; 153  :                                                       pattern_sig_ctxs_eq_two);
; 154  :   __m256i pattern_sig_ctxs_lt3     = _mm256_or_si256 (pattern_sig_ctxs_eq_1or2,
; 155  :                                                       pattern_sig_ctxs_eq_zero);
; 156  :   __m256i pattern_sig_ctxs_other   = _mm256_xor_si256(pattern_sig_ctxs_lt3,
; 157  :                                                       ff);
; 158  :   __m256i x_plus_y        = _mm256_add_epi16  (pos_xs,   pos_ys);
; 159  :   __m256i x_plus_y_zero   = _mm256_cmpeq_epi16(x_plus_y, zero);   // All these should be 0, preempts block_type_two rule
; 160  : 
; 161  :   __m256i texture_types = _mm256_set1_epi16((int16_t)texture_type);
; 162  : 
; 163  :   __m256i block_types     = _mm256_set1_epi16((int16_t)block_type);
; 164  :   __m256i block_type_two  = _mm256_cmpeq_epi16(block_types, twos);   // All these should be ctx_ind_map[4 * pos_y + pos_x];
; 165  :   __m256i bt2_vals        = ctx_ind_map[scan_idx];
; 166  :   __m256i bt2_vals_masked = _mm256_and_si256(bt2_vals, block_type_two);
; 167  : 
; 168  :   __m256i pos_xs_in_subset = _mm256_and_si256(pos_xs, threes);

	vmovdqu	ymm15, YMMWORD PTR [rbx]

; 169  :   __m256i pos_ys_in_subset = _mm256_and_si256(pos_ys, threes);

	vmovdqu	ymm13, YMMWORD PTR [r11]
	mov	eax, -1
	cmp	r10d, 65535				; 0000ffffH
	vpand	ymm8, ymm15, ymm6
	cmovg	r10w, ax
	movsx	eax, r10w
	vmovd	xmm0, eax
	vpbroadcastw ymm0, xmm0
	vpcmpeqw ymm5, ymm0, ymm3
	vpcmpeqw ymm11, ymm0, ymm2
	vpcmpeqw ymm0, ymm0, ymm4
	vpor	ymm12, ymm0, ymm11
	vpand	ymm7, ymm13, ymm6
	movsx	eax, r9w
	vmovd	xmm0, eax
	vpbroadcastw ymm0, xmm0
	vpcmpeqw ymm14, ymm0, ymm2
	vpxor	xmm2, xmm2, xmm2
	vmovdqu	YMMWORD PTR pattern_sig_ctxs_eq_zero$1$[rbp], ymm5

; 170  : 
; 171  :   __m256i cg_pos_xs        = _mm256_srli_epi16(pos_xs, 2);
; 172  :   __m256i cg_pos_ys        = _mm256_srli_epi16(pos_ys, 2);
; 173  :   __m256i cg_pos_xysums    = _mm256_add_epi16 (cg_pos_xs, cg_pos_ys);
; 174  : 
; 175  :   __m256i pos_xy_sums_in_subset = _mm256_add_epi16(pos_xs_in_subset, pos_ys_in_subset);

	vpaddw	ymm9, ymm7, ymm8
	mov	eax, edx
	shl	rax, 5
	vpsrlw	ymm1, ymm15, 2
	vmovdqu	ymm0, YMMWORD PTR ctx_ind_map$[rbp+rax]
	vpand	ymm10, ymm0, ymm14
	vmovd	xmm0, r8d
	vpbroadcastw ymm0, xmm0

; 176  : 
; 177  :   /*
; 178  :    * if (pattern_sig_ctx == 0) {
; 179  :    *   switch (pos_x_in_subset + pos_y_in_subset) {
; 180  :    *   case 0:
; 181  :    *     cnt = 2;
; 182  :    *     break;
; 183  :    *   case 1:
; 184  :    *   case 2:
; 185  :    *     cnt = 1;
; 186  :    *     break;
; 187  :    *   default:
; 188  :    *     cnt = 0;
; 189  :    *   }
; 190  :    * }
; 191  :    *
; 192  :    * Equivalent to:
; 193  :    *
; 194  :    * if (pattern_sig_ctx == 0) {
; 195  :    *   subamt = cnt <= 1 ? 1 : 0;
; 196  :    *   pxyis_max3 = min(3, pos_x_in_subset + pos_y_in_subset);
; 197  :    *   cnt = (3 - pxyis_max3) - subamt;
; 198  :    * }
; 199  :    */
; 200  :   __m256i pxyis_lte_1     = _mm256_cmpgt_epi16(twos,                  pos_xy_sums_in_subset);
; 201  :   __m256i subamts         = _mm256_and_si256  (pxyis_lte_1,           ones);
; 202  :   __m256i pxyis_max3      = _mm256_min_epu16  (pos_xy_sums_in_subset, threes);
; 203  :   __m256i cnts_tmp        = _mm256_sub_epi16  (threes,                pxyis_max3);
; 204  :   __m256i cnts_sig_ctx_0  = _mm256_sub_epi16  (cnts_tmp,              subamts);
; 205  :   __m256i cnts_sc0_masked = _mm256_and_si256  (cnts_sig_ctx_0,        pattern_sig_ctxs_eq_zero);
; 206  : 
; 207  :   /*
; 208  :    * if (pattern_sig_ctx == 1 || pattern_sig_ctx == 2) {
; 209  :    *   if (pattern_sig_ctx == 1)
; 210  :    *     subtrahend = pos_y_in_subset;
; 211  :    *   else
; 212  :    *     subtrahend = pos_x_in_subset;
; 213  :    *   cnt = 2 - min(2, subtrahend);
; 214  :    * }
; 215  :    */
; 216  :   __m256i pos_operands_ctx_1or2 = _mm256_blendv_epi8(pos_ys_in_subset,
; 217  :                                                      pos_xs_in_subset,
; 218  :                                                      pattern_sig_ctxs_eq_two);
; 219  : 
; 220  :   __m256i pos_operands_max2     = _mm256_min_epu16  (pos_operands_ctx_1or2, twos);
; 221  :   __m256i cnts_sig_ctx_1or2     = _mm256_sub_epi16  (twos,                  pos_operands_max2);
; 222  :   __m256i cnts_sc12_masked      = _mm256_and_si256  (cnts_sig_ctx_1or2,     pattern_sig_ctxs_eq_1or2);
; 223  : 
; 224  :   /*
; 225  :    * if (pattern_sig_ctx > 2)
; 226  :    *   cnt = 2;
; 227  :    */
; 228  :   __m256i cnts_scother_masked = _mm256_and_si256(twos, pattern_sig_ctxs_other);
; 229  : 
; 230  :   // Select correct count
; 231  :   __m256i cnts_sc012_masked   = _mm256_or_si256 (cnts_sc0_masked,     cnts_sc12_masked);
; 232  :   __m256i cnts                = _mm256_or_si256 (cnts_scother_masked, cnts_sc012_masked);
; 233  : 
; 234  :   // Compute final values
; 235  :   __m256i textype_eq_0     = _mm256_cmpeq_epi16(texture_types, zero);

	vpcmpeqw ymm3, ymm0, ymm3
	vpsrlw	ymm0, ymm13, 2
	vpaddw	ymm1, ymm0, ymm1

; 236  :   __m256i cg_pos_sums_gt_0 = _mm256_cmpgt_epi16(cg_pos_xysums, zero);

	vpcmpgtw ymm2, ymm1, ymm2

; 237  :   __m256i tmpcond          = _mm256_and_si256  (textype_eq_0,  cg_pos_sums_gt_0);

	vpand	ymm3, ymm2, ymm3

; 238  :   __m256i tmp              = _mm256_and_si256  (tmpcond,       threes);

	vpand	ymm0, ymm3, ymm6
	vmovdqu	ymm3, YMMWORD PTR __ymm@0002000200020002000200020002000200020002000200020002000200020002
	vpor	ymm1, ymm12, ymm5
	vpxor	ymm2, ymm1, YMMWORD PTR __ymm@ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
	vpand	ymm5, ymm2, ymm3
	movsx	eax, cx
	vmovd	xmm4, eax
	vpbroadcastw ymm4, xmm4

; 239  :   __m256i tmp_with_offsets = _mm256_add_epi16  (tmp,           offsets);

	vpaddw	ymm6, ymm0, ymm4
	vpblendvb ymm0, ymm7, ymm8, ymm11
	vpminuw	ymm1, ymm0, ymm3
	vpsubw	ymm2, ymm3, ymm1
	vmovdqu	ymm1, YMMWORD PTR __ymm@0003000300030003000300030003000300030003000300030003000300030003
	vpminuw	ymm0, ymm9, ymm1
	vpsubw	ymm3, ymm1, ymm0
	vmovdqu	ymm1, YMMWORD PTR __ymm@0002000200020002000200020002000200020002000200020002000200020002
	vpand	ymm4, ymm2, ymm12
	vpcmpgtw ymm1, ymm1, ymm9
	vpand	ymm2, ymm1, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vpsubw	ymm0, ymm3, ymm2
	vpand	ymm3, ymm0, YMMWORD PTR pattern_sig_ctxs_eq_zero$1$[rbp]
	vpor	ymm1, ymm3, ymm4
	vpor	ymm2, ymm1, ymm5

; 240  :   __m256i rv_noshortcirc   = _mm256_add_epi16  (cnts,          tmp_with_offsets);

	vpaddw	ymm0, ymm2, ymm6

; 241  : 
; 242  :   // Ol' sprite mask method works here!
; 243  :   __m256i rv1 = _mm256_andnot_si256(block_type_two, rv_noshortcirc);

	vpandn	ymm3, ymm14, ymm0
	vpxor	xmm0, xmm0, xmm0
	vpaddw	ymm1, ymm13, ymm15
	vpcmpeqw ymm0, ymm1, ymm0

; 244  :   __m256i rv2 = _mm256_or_si256    (rv1,            bt2_vals_masked);

	vpor	ymm4, ymm3, ymm10

; 245  :   __m256i rv  = _mm256_andnot_si256(x_plus_y_zero,  rv2);

	vpandn	ymm0, ymm0, ymm4

; 246  :   return rv;
; 247  : }

	lea	r11, QWORD PTR [rsp+320]
	mov	rbx, QWORD PTR [r11+16]
	vmovaps	xmm6, XMMWORD PTR [r11-16]
	vmovaps	xmm7, XMMWORD PTR [r11-32]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm9, XMMWORD PTR [r11-64]
	vmovaps	xmm10, XMMWORD PTR [r11-80]
	vmovaps	xmm11, XMMWORD PTR [r11-96]
	vmovaps	xmm12, XMMWORD PTR [r11-112]
	vmovaps	xmm13, XMMWORD PTR [r11-128]
	vmovaps	xmm14, XMMWORD PTR [r11-144]
	vmovaps	xmm15, XMMWORD PTR [r11-160]
	mov	rsp, r11
	pop	rbp
	ret	0
kvz_context_get_sig_ctx_inc_16x16b ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\context.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\context.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\context.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c
_TEXT	SEGMENT
$T1 = 0
base_levels$1$ = 4
coeffs_gt1_bits$1$ = 4
sub_pos$1$ = 4
encode_decisions$1$ = 8
coeff_signs$1$ = 8
be_valid$1$ = 12
c1$1$ = 16
scan_cg_last$1$ = 20
tv3218 = 24
tv3206 = 24
go_rice_param$1$ = 24
width$1$ = 28
i$1$ = 32
scan_pos_sig$1$ = 36
coeffs_gt2_bits$1$ = 40
coeff_sign_buf$1$ = 40
tv3204 = 40
log2_block_size$1$ = 44
first_nz_pos_in_cg$1$ = 48
num_non_zero$1$ = 56
tv3205 = 56
tv3177 = 56
sigs$1$ = 64
tv3234 = 64
first_c2_flag_idx$1$ = 68
esc_flags$1$ = 68
num_blk_side$1$ = 72
last_nz_pos_in_cg$1$ = 80
c1s_nextiter$1$ = 80
scan_cg$1$ = 80
tv3192 = 88
tv3174 = 88
scan$1$ = 96
encoder$1$ = 104
$T2 = 112
tv3223 = 120
tv3195 = 128
abs_coeff$ = 192
low128_shuffle_masks$3 = 256
abs_coeff_buf_sb$ = 256
coeffs_r$4 = 256
tv3200 = 320
ctx_sig_buf$ = 320
encode_sig_coeff_flags_inv$1$ = 352
invec_rearr_masks_lower$5 = 352
sigs_inv$1$ = 416
invec_rearr_masks_upper$6 = 416
coeffs_negative$1$ = 480
blend_masks$7 = 480
ctx_ind_map$8 = 544
sig_coeffgroup_nzs$ = 640
coeff_reord$ = 896
state$ = 9440
cabac$ = 9448
coeff$ = 9456
width$ = 9464
type$ = 9472
scan_mode$ = 9480
tr_skip$ = 9488
kvz_encode_coeff_nxn_avx2 PROC

; 256  : {

$LN127:
	mov	QWORD PTR [rsp+24], rbx
	mov	BYTE PTR [rsp+32], r9b
	mov	QWORD PTR [rsp+16], rdx
	mov	QWORD PTR [rsp+8], rcx
	push	rbp
	push	rsi
	push	rdi
	push	r12
	push	r13
	push	r14
	push	r15
	mov	eax, 9376				; 000024a0H
	call	__chkstk
	sub	rsp, rax
	vmovaps	XMMWORD PTR [rsp+9360], xmm6
	vmovaps	XMMWORD PTR [rsp+9344], xmm7
	vmovaps	XMMWORD PTR [rsp+9328], xmm8
	vmovaps	XMMWORD PTR [rsp+9312], xmm9
	vmovaps	XMMWORD PTR [rsp+9296], xmm10
	vmovaps	XMMWORD PTR [rsp+9280], xmm11
	vmovaps	XMMWORD PTR [rsp+9264], xmm12
	vmovaps	XMMWORD PTR [rsp+9248], xmm13
	vmovaps	XMMWORD PTR [rsp+9232], xmm14
	vmovaps	XMMWORD PTR [rsp+9216], xmm15
	lea	rbp, QWORD PTR [rsp+128]
	and	rbp, -64				; ffffffffffffffc0H

; 257  :   const encoder_control_t * const encoder = state->encoder_control;

	mov	rsi, QWORD PTR [rcx]

; 258  :   int c1 = 1;

	mov	eax, 1
	mov	rbx, r8
	mov	QWORD PTR encoder$1$[rbp], rsi
	mov	rdi, rdx
	mov	DWORD PTR c1$1$[rbp], eax

; 259  :   uint8_t last_coeff_x = 0;
; 260  :   uint8_t last_coeff_y = 0;
; 261  :   int32_t i;
; 262  :   uint32_t sig_coeffgroup_nzs[8 * 8] = { 0 };

	xor	edx, edx
	movzx	r12d, r9b
	mov	r8d, 256				; 00000100H
	lea	rcx, QWORD PTR sig_coeffgroup_nzs$[rbp]
	mov	r15d, eax
	call	memset

; 263  : 
; 264  :   int8_t be_valid = encoder->cfg.signhide_enable;

	movzx	eax, BYTE PTR [rsi+52]

; 265  :   int32_t scan_pos_sig;
; 266  :   uint32_t go_rice_param = 0;
; 267  :   uint32_t ctx_sig;
; 268  : 
; 269  :   // CONSTANTS
; 270  :   const uint32_t num_blk_side    = width >> TR_MIN_LOG2_SIZE;
; 271  :   const uint32_t log2_block_size = kvz_g_convert_to_bit[width] + 2;

	lea	r10, OFFSET FLAT:__ImageBase

; 272  :   const uint32_t *scan           =

	movsx	rsi, BYTE PTR scan_mode$[rsp]
	mov	r8d, r12d
	movsx	r9d, BYTE PTR kvz_g_convert_to_bit[r10+r12]
	mov	edx, r12d
	add	r9d, 2
	mov	BYTE PTR be_valid$1$[rbp], al
	shr	r8d, 2
	mov	r13d, r12d
	lea	rax, QWORD PTR [rsi+rsi*4]
	mov	DWORD PTR log2_block_size$1$[rbp], r9d

; 273  :     kvz_g_sig_last_scan[scan_mode][log2_block_size - 1];
; 274  :   const uint32_t *scan_cg = g_sig_last_scan_cg[log2_block_size - 2][scan_mode];
; 275  :   const uint32_t num_blocks = num_blk_side * num_blk_side;

	mov	r11d, r8d
	mov	DWORD PTR num_blk_side$1$[rbp], r8d
	lea	r14d, DWORD PTR [r9-2]
	imul	r11d, r8d

; 276  : 
; 277  :   const __m256i zero = _mm256_set1_epi8(0);
; 278  :   const __m256i ones = _mm256_set1_epi16(1);
; 279  :   const __m256i twos = _mm256_set1_epi16(2);
; 280  : 
; 281  :   // Init base contexts according to block type
; 282  :   cabac_ctx_t *base_coeff_group_ctx = &(cabac->ctx.cu_sig_coeff_group_model[type]);

	movzx	r8d, BYTE PTR type$[rsp]
	lea	ecx, DWORD PTR [r9-1]
	add	rcx, rax
	mov	DWORD PTR width$1$[rbp], edx

; 283  :   cabac_ctx_t *baseCtx           = (type == 0) ? &(cabac->ctx.cu_sig_model_luma[0]) :
; 284  :                                  &(cabac->ctx.cu_sig_model_chroma[0]);
; 285  : 
; 286  :   // Scan all coeff groups to find out which of them have coeffs.
; 287  :   // Populate sig_coeffgroup_nzs with that info.
; 288  : 
; 289  :   // NOTE: Modified the functionality a bit, sig_coeffgroup_flag used to be
; 290  :   // 1 if true and 0 if false, now it's "undefined but nonzero" if true and
; 291  :   // 0 if false (not actually undefined, it's a bitmask representing the
; 292  :   // significant coefficients' position in the group which in itself could
; 293  :   // be useful information)
; 294  :   int32_t scan_cg_last = -1;

	mov	r9d, 103				; 00000067H
	mov	QWORD PTR tv3192[rbp], rsi
	vpxor	xmm2, xmm2, xmm2
	mov	DWORD PTR scan_cg_last$1$[rbp], -1
	mov	rax, QWORD PTR kvz_g_sig_last_scan[r10+rcx*8]
	mov	QWORD PTR scan$1$[rbp], rax
	lea	rax, QWORD PTR [rsi+r14*2]
	mov	ecx, r14d
	add	rcx, rax
	lea	rax, QWORD PTR [rdi+r8]
	mov	QWORD PTR tv3223[rbp], rax
	test	r8b, r8b
	mov	eax, 76					; 0000004cH
	cmovne	eax, r9d
	xor	r9d, r9d
	mov	rcx, QWORD PTR g_sig_last_scan_cg[r10+rcx*8]
	add	rax, rdi
	mov	QWORD PTR scan_cg$1$[rbp], rcx
	mov	QWORD PTR tv3195[rbp], rax

; 295  : 
; 296  :   for (int32_t i = 0; i < num_blocks; i++) {

	test	r11d, r11d
	je	$LN119@kvz_encode

; 297  :     const uint32_t cg_id = scan_cg[i];
; 298  :     const uint32_t n_xbits = log2_block_size - 2; // How many lowest bits of scan_cg represent X coord

	mov	eax, r15d
	lea	edi, DWORD PTR [rdx+rdx*2]
	shlx	r15d, eax, r14d
	lea	eax, DWORD PTR [rdx-1]
	add	r12d, r12d
	mov	esi, eax
	mov	r10, rcx
	npad	12
$LL4@kvz_encode:

; 299  :     const uint32_t cg_x = cg_id & ((1 << n_xbits) - 1);

	mov	ecx, DWORD PTR [r10]
	lea	eax, DWORD PTR [r15-1]
	and	eax, ecx
	lea	r10, QWORD PTR [r10+4]

; 300  :     const uint32_t cg_y = cg_id >> n_xbits;

	shrx	r8d, ecx, r14d
	imul	r8d, edx

; 301  : 
; 302  :     const uint32_t cg_pos = cg_y * width * 4 + cg_x * 4;
; 303  :     const uint32_t cg_pos_y = (cg_pos >> log2_block_size) >> TR_MIN_LOG2_SIZE;
; 304  :     const uint32_t cg_pos_x = (cg_pos & (width - 1)) >> TR_MIN_LOG2_SIZE;

	mov	ecx, esi
	shr	ecx, 2
	add	r8d, eax
	shl	r8d, 2

; 305  :     const uint32_t idx = cg_pos_x + cg_pos_y * num_blk_side;
; 306  : 
; 307  :     __m128d coeffs_d_upper = _mm_setzero_pd();
; 308  :     __m128d coeffs_d_lower = _mm_setzero_pd();
; 309  :     __m128i coeffs_upper;
; 310  :     __m128i coeffs_lower;
; 311  :     __m256i cur_coeffs;
; 312  : 
; 313  :     coeffs_d_upper = _mm_loadl_pd(coeffs_d_upper, (double *)(coeff + cg_pos + 0 * width));

	vmovddup xmm0, QWORD PTR [rbx+r8*2]

; 314  :     coeffs_d_upper = _mm_loadh_pd(coeffs_d_upper, (double *)(coeff + cg_pos + 1 * width));

	lea	rax, QWORD PTR [r8+r13]
	vmovhpd	xmm0, xmm0, QWORD PTR [rbx+rax*2]

; 315  :     coeffs_d_lower = _mm_loadl_pd(coeffs_d_lower, (double *)(coeff + cg_pos + 2 * width));

	lea	rax, QWORD PTR [r12+r8]
	vmovddup xmm1, QWORD PTR [rbx+rax*2]

; 316  :     coeffs_d_lower = _mm_loadh_pd(coeffs_d_lower, (double *)(coeff + cg_pos + 3 * width));

	lea	rax, QWORD PTR [rdi+r8]
	vmovhpd	xmm1, xmm1, QWORD PTR [rbx+rax*2]
	mov	eax, r8d
	shr	eax, 2
	and	ecx, eax
	mov	eax, DWORD PTR log2_block_size$1$[rbp]
	shrx	eax, r8d, eax
	shr	eax, 2
	imul	eax, DWORD PTR num_blk_side$1$[rbp]

; 317  : 
; 318  :     coeffs_upper = _mm_castpd_si128(coeffs_d_upper);
; 319  :     coeffs_lower = _mm_castpd_si128(coeffs_d_lower);
; 320  : 
; 321  :     cur_coeffs = _mm256_insertf128_si256(_mm256_castsi128_si256(coeffs_upper),

	vinsertf128 ymm1, ymm0, xmm1, 1

; 322  :                                          coeffs_lower,
; 323  :                                          1);
; 324  : 
; 325  :     __m256i coeffs_zero = _mm256_cmpeq_epi16(cur_coeffs, zero);

	vpcmpeqw ymm2, ymm1, ymm2

; 326  : 
; 327  :     uint32_t nz_coeffs_2b = ~((uint32_t)_mm256_movemask_epi8(coeffs_zero));

	vpmovmskb edx, ymm2
	not	edx
	add	ecx, eax
	mov	eax, r9d
	test	edx, edx
	vpxor	xmm2, xmm2, xmm2
	cmove	eax, DWORD PTR scan_cg_last$1$[rbp]
	inc	r9d

; 328  :     sig_coeffgroup_nzs[idx] = nz_coeffs_2b;

	mov	DWORD PTR sig_coeffgroup_nzs$[rbp+rcx*4], edx
	mov	edx, r13d
	mov	DWORD PTR scan_cg_last$1$[rbp], eax
	cmp	r9d, r11d
	jb	$LL4@kvz_encode

; 329  : 
; 330  :     if (nz_coeffs_2b)
; 331  :       scan_cg_last = i;
; 332  :   }
; 333  :   // Rest of the code assumes at least one non-zero coeff.
; 334  :   assert(scan_cg_last >= 0);

	mov	rdi, QWORD PTR cabac$[rsp]
	mov	r15d, DWORD PTR c1$1$[rbp]
	mov	rsi, QWORD PTR tv3192[rbp]
	movzx	r12d, BYTE PTR width$[rsp]

; 467  :           cabac->cur_ctx = &baseCtx[ctx_sig];
; 468  :           CABAC_BIN(cabac, curr_sig, "sig_coeff_flag");

	mov	DWORD PTR scan_cg_last$1$[rbp], eax
	test	eax, eax
	jns	SHORT $LN44@kvz_encode
$LN119@kvz_encode:

; 329  : 
; 330  :     if (nz_coeffs_2b)
; 331  :       scan_cg_last = i;
; 332  :   }
; 333  :   // Rest of the code assumes at least one non-zero coeff.
; 334  :   assert(scan_cg_last >= 0);

	mov	r8d, 334				; 0000014eH
	lea	rdx, OFFSET FLAT:$SG4294946673
	lea	rcx, OFFSET FLAT:$SG4294946672
	vzeroupper
	call	QWORD PTR __imp__wassert
	mov	eax, DWORD PTR scan_cg_last$1$[rbp]
$LN44@kvz_encode:

; 341  :     for (int32_t i = 0; i <= scan_cg_last; i++) {

	movsxd	rcx, eax
	test	eax, eax
	js	$LN121@kvz_encode
	mov	edx, DWORD PTR width$1$[rbp]
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 142  :     coeffs_rearr2_upper = _mm_shuffle_epi8(coeffs_rearr1_upper, invec_rearr_masks_upper[scan_mode]);

	lea	r8, QWORD PTR invec_rearr_masks_upper$6[rbp]
	vmovdqu	xmm7, XMMWORD PTR __xmm@030209080f0e070601000d0c05040b0a
	vmovdqu	xmm5, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	vmovdqu	xmm8, XMMWORD PTR __xmm@0b0a09080f0e0d0c0302010007060504
	vmovdqu	xmm9, XMMWORD PTR __xmm@ffffffff00000000ffff000000000000
	vmovdqu	xmm11, XMMWORD PTR __xmm@ffffffff00000000ffffffff00000000
	vmovdqu	xmm12, XMMWORD PTR __xmm@0f0e0d0c05040b0a0706030209080100
	vmovdqu	xmm13, XMMWORD PTR __xmm@0f0e07060b0a03020d0c050409080100
	vmovdqu	xmm14, XMMWORD PTR __xmm@0b0a090805040f0e0302010007060d0c
	vmovdqu	xmm15, XMMWORD PTR __xmm@0b0a03020f0e0706090801000d0c0504

; 133  : 
; 134  :     coeffs_upper   = _mm_castpd_si128(coeffs_d_upper);
; 135  :     coeffs_lower   = _mm_castpd_si128(coeffs_d_lower);
; 136  : 
; 137  :     coeffs_lower   = _mm_shuffle_epi8(coeffs_lower, low128_shuffle_masks[scan_mode]);

	mov	r9, QWORD PTR scan$1$[rbp]
	lea	eax, DWORD PTR [rdx+rdx]
	shl	rsi, 4
	lea	r11, QWORD PTR low128_shuffle_masks$3[rbp]
	mov	DWORD PTR tv3206[rbp], eax

; 138  : 
; 139  :     coeffs_rearr1_upper = _mm_blendv_epi8(coeffs_upper, coeffs_lower, blend_masks[scan_mode]);
; 140  :     coeffs_rearr1_lower = _mm_blendv_epi8(coeffs_lower, coeffs_upper, blend_masks[scan_mode]);

	lea	r14, QWORD PTR blend_masks$7[rbp]
	mov	r15d, DWORD PTR tv3206[rbp]

; 143  :     coeffs_rearr2_lower = _mm_shuffle_epi8(coeffs_rearr1_lower, invec_rearr_masks_lower[scan_mode]);

	lea	r13, QWORD PTR invec_rearr_masks_lower$5[rbp]
	add	r11, rsi
	lea	eax, DWORD PTR [rdx+rdx*2]

; 138  : 
; 139  :     coeffs_rearr1_upper = _mm_blendv_epi8(coeffs_upper, coeffs_lower, blend_masks[scan_mode]);
; 140  :     coeffs_rearr1_lower = _mm_blendv_epi8(coeffs_lower, coeffs_upper, blend_masks[scan_mode]);

	add	r14, rsi
	lea	r10, QWORD PTR [rcx+1]

; 143  :     coeffs_rearr2_lower = _mm_shuffle_epi8(coeffs_rearr1_lower, invec_rearr_masks_lower[scan_mode]);

	add	r13, rsi
	mov	edi, eax

; 142  :     coeffs_rearr2_upper = _mm_shuffle_epi8(coeffs_rearr1_upper, invec_rearr_masks_upper[scan_mode]);

	add	rsi, r8
	mov	r12d, edx
	vpxor	xmm10, xmm10, xmm10

; 133  : 
; 134  :     coeffs_upper   = _mm_castpd_si128(coeffs_d_upper);
; 135  :     coeffs_lower   = _mm_castpd_si128(coeffs_d_lower);
; 136  : 
; 137  :     coeffs_lower   = _mm_shuffle_epi8(coeffs_lower, low128_shuffle_masks[scan_mode]);

	lea	r8, QWORD PTR coeff_reord$[rbp]
	npad	6
$LL7@kvz_encode:

; 74   :     _mm_setr_epi8(10,11,  4, 5, 12,13,  0, 1,  6, 7, 14,15,  8, 9,  2, 3),
; 75   :     _mm_setr_epi8( 0, 1,  2, 3,  4, 5,  6, 7,  8, 9, 10,11, 12,13, 14,15),
; 76   :     _mm_setr_epi8( 4, 5,  6, 7,  0, 1,  2, 3, 12,13, 14,15,  8, 9, 10,11),
; 77   :   };
; 78   : 
; 79   :   const __m128i blend_masks[3] = {
; 80   :     _mm_setr_epi16( 0,  0,  0, -1,  0,  0, -1, -1),
; 81   :     _mm_setr_epi16( 0,  0,  0,  0,  0,  0,  0,  0),
; 82   :     _mm_setr_epi16( 0,  0, -1, -1,  0,  0, -1, -1),
; 83   :   };
; 84   : 
; 85   :   const __m128i invec_rearr_masks_upper[3] = {
; 86   :     _mm_setr_epi8( 0, 1,  8, 9,  2, 3,  6, 7, 10,11,  4, 5, 12,13, 14,15),
; 87   :     _mm_setr_epi8( 0, 1,  2, 3,  4, 5,  6, 7,  8, 9, 10,11, 12,13, 14,15),
; 88   :     _mm_setr_epi8( 0, 1,  8, 9,  4, 5, 12,13,  2, 3, 10,11,  6, 7, 14,15),
; 89   :   };
; 90   : 
; 91   :   const __m128i invec_rearr_masks_lower[3] = {
; 92   :     _mm_setr_epi8(12,13,  6, 7,  0, 1,  2, 3, 14,15,  4, 5,  8, 9, 10,11),
; 93   :     _mm_setr_epi8( 0, 1,  2, 3,  4, 5,  6, 7,  8, 9, 10,11, 12,13, 14,15),
; 94   :     _mm_setr_epi8( 4, 5, 12,13,  0, 1,  8, 9,  6, 7, 14,15,  2, 3, 10,11),
; 95   :   };
; 96   : 
; 97   :   const size_t row_offsets[4] = {

	mov	edx, DWORD PTR [r9]
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c

; 341  :     for (int32_t i = 0; i <= scan_cg_last; i++) {

	lea	r8, QWORD PTR [r8+32]
	lea	r9, QWORD PTR [r9+64]
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 73   :   const __m128i low128_shuffle_masks[3] = {

	vmovdqu	XMMWORD PTR low128_shuffle_masks$3[rbp], xmm7
	vmovdqu	XMMWORD PTR low128_shuffle_masks$3[rbp+16], xmm5

; 74   :     _mm_setr_epi8(10,11,  4, 5, 12,13,  0, 1,  6, 7, 14,15,  8, 9,  2, 3),
; 75   :     _mm_setr_epi8( 0, 1,  2, 3,  4, 5,  6, 7,  8, 9, 10,11, 12,13, 14,15),
; 76   :     _mm_setr_epi8( 4, 5,  6, 7,  0, 1,  2, 3, 12,13, 14,15,  8, 9, 10,11),
; 77   :   };
; 78   : 
; 79   :   const __m128i blend_masks[3] = {
; 80   :     _mm_setr_epi16( 0,  0,  0, -1,  0,  0, -1, -1),
; 81   :     _mm_setr_epi16( 0,  0,  0,  0,  0,  0,  0,  0),
; 82   :     _mm_setr_epi16( 0,  0, -1, -1,  0,  0, -1, -1),
; 83   :   };
; 84   : 
; 85   :   const __m128i invec_rearr_masks_upper[3] = {
; 86   :     _mm_setr_epi8( 0, 1,  8, 9,  2, 3,  6, 7, 10,11,  4, 5, 12,13, 14,15),
; 87   :     _mm_setr_epi8( 0, 1,  2, 3,  4, 5,  6, 7,  8, 9, 10,11, 12,13, 14,15),
; 88   :     _mm_setr_epi8( 0, 1,  8, 9,  4, 5, 12,13,  2, 3, 10,11,  6, 7, 14,15),
; 89   :   };
; 90   : 
; 91   :   const __m128i invec_rearr_masks_lower[3] = {
; 92   :     _mm_setr_epi8(12,13,  6, 7,  0, 1,  2, 3, 14,15,  4, 5,  8, 9, 10,11),
; 93   :     _mm_setr_epi8( 0, 1,  2, 3,  4, 5,  6, 7,  8, 9, 10,11, 12,13, 14,15),
; 94   :     _mm_setr_epi8( 4, 5, 12,13,  0, 1,  8, 9,  6, 7, 14,15,  2, 3, 10,11),
; 95   :   };
; 96   : 
; 97   :   const size_t row_offsets[4] = {

	lea	eax, DWORD PTR [rdx+r12]

; 98   :     scan[subpos] + width * 0,
; 99   :     scan[subpos] + width * 1,
; 100  :     scan[subpos] + width * 2,
; 101  :     scan[subpos] + width * 3,
; 102  :   };
; 103  : 
; 104  :   for (int i = 0; i < n_bufs; i++) {
; 105  :     const int16_t *__restrict coeff = coeffs[i];
; 106  : 
; 107  :     // NOTE: Upper means "higher in pixel order inside block", which implies
; 108  :     // lower addresses (note the difference: HIGH and LOW vs UPPER and LOWER),
; 109  :     // so upper 128b vector actually becomes the lower part of a 256-bit coeff
; 110  :     // vector and lower vector the higher part!
; 111  :     __m128d coeffs_d_upper;
; 112  :     __m128d coeffs_d_lower;
; 113  : 
; 114  :     __m128i coeffs_upper;
; 115  :     __m128i coeffs_lower;
; 116  : 
; 117  :     __m128i coeffs_rearr1_upper;
; 118  :     __m128i coeffs_rearr1_lower;
; 119  : 
; 120  :     __m128i coeffs_rearr2_upper;
; 121  :     __m128i coeffs_rearr2_lower;
; 122  : 
; 123  :     // Zeroing these is actually unnecessary, but the compiler will whine
; 124  :     // about uninitialized values otherwise
; 125  :     coeffs_d_upper = _mm_setzero_pd();
; 126  :     coeffs_d_lower = _mm_setzero_pd();
; 127  : 
; 128  :     coeffs_d_upper = _mm_loadl_pd(coeffs_d_upper, (double *)(coeff + row_offsets[0]));

	vmovddup xmm3, QWORD PTR [rbx+rdx*2]

; 129  :     coeffs_d_upper = _mm_loadh_pd(coeffs_d_upper, (double *)(coeff + row_offsets[1]));

	vmovhpd	xmm3, xmm3, QWORD PTR [rbx+rax*2]
	lea	eax, DWORD PTR [rdx+r15]

; 130  : 
; 131  :     coeffs_d_lower = _mm_loadl_pd(coeffs_d_lower, (double *)(coeff + row_offsets[2]));

	vmovddup xmm1, QWORD PTR [rbx+rax*2]
	lea	eax, DWORD PTR [rdx+rdi]

; 132  :     coeffs_d_lower = _mm_loadh_pd(coeffs_d_lower, (double *)(coeff + row_offsets[3]));

	vmovhpd	xmm1, xmm1, QWORD PTR [rbx+rax*2]
	vmovdqu	XMMWORD PTR low128_shuffle_masks$3[rbp+32], xmm8

; 133  : 
; 134  :     coeffs_upper   = _mm_castpd_si128(coeffs_d_upper);
; 135  :     coeffs_lower   = _mm_castpd_si128(coeffs_d_lower);
; 136  : 
; 137  :     coeffs_lower   = _mm_shuffle_epi8(coeffs_lower, low128_shuffle_masks[scan_mode]);

	vpshufb	xmm2, xmm1, XMMWORD PTR [r11]
	vmovdqu	XMMWORD PTR blend_masks$7[rbp], xmm9
	vmovdqu	XMMWORD PTR blend_masks$7[rbp+16], xmm10
	vmovdqu	XMMWORD PTR blend_masks$7[rbp+32], xmm11

; 138  : 
; 139  :     coeffs_rearr1_upper = _mm_blendv_epi8(coeffs_upper, coeffs_lower, blend_masks[scan_mode]);
; 140  :     coeffs_rearr1_lower = _mm_blendv_epi8(coeffs_lower, coeffs_upper, blend_masks[scan_mode]);

	vmovdqu	xmm0, XMMWORD PTR [r14]
	vpblendvb xmm1, xmm2, xmm3, xmm0
	vpblendvb xmm2, xmm3, xmm2, xmm0
	vmovdqu	XMMWORD PTR invec_rearr_masks_upper$6[rbp], xmm12
	vmovdqu	XMMWORD PTR invec_rearr_masks_upper$6[rbp+16], xmm5
	vmovdqu	XMMWORD PTR invec_rearr_masks_upper$6[rbp+32], xmm13

; 142  :     coeffs_rearr2_upper = _mm_shuffle_epi8(coeffs_rearr1_upper, invec_rearr_masks_upper[scan_mode]);

	vpshufb	xmm3, xmm2, XMMWORD PTR [rsi]
	vmovdqu	XMMWORD PTR invec_rearr_masks_lower$5[rbp], xmm14
	vmovdqu	XMMWORD PTR invec_rearr_masks_lower$5[rbp+16], xmm5
	vmovdqu	XMMWORD PTR invec_rearr_masks_lower$5[rbp+32], xmm15

; 143  :     coeffs_rearr2_lower = _mm_shuffle_epi8(coeffs_rearr1_lower, invec_rearr_masks_lower[scan_mode]);

	vpshufb	xmm4, xmm1, XMMWORD PTR [r13]

; 144  : 
; 145  :     // The Intel Intrinsics Guide talks about _mm256_setr_m128i but my headers
; 146  :     // lack such an instruction. What it does is essentially this anyway.
; 147  :     result_vecs[i] = _mm256_inserti128_si256(_mm256_castsi128_si256(coeffs_rearr2_upper),

	vinserti128 ymm0, ymm3, xmm4, 1
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c

; 344  :       _mm256_store_si256((__m256i *)(coeff_reord + subpos), coeffs_r);

	vmovdqu	YMMWORD PTR [r8-32], ymm0
	sub	r10, 1
	jne	$LL7@kvz_encode

; 341  :     for (int32_t i = 0; i <= scan_cg_last; i++) {

	mov	rdi, QWORD PTR cabac$[rsp]
	mov	r15d, DWORD PTR c1$1$[rbp]
	movzx	r12d, BYTE PTR width$[rsp]
	jmp	SHORT $LN6@kvz_encode
$LN121@kvz_encode:
	vmovdqu	ymm0, YMMWORD PTR coeffs_r$4[rbp]
$LN6@kvz_encode:

; 345  :     }
; 346  : 
; 347  :     // Find the last coeff by going backwards in scan order. With cmpeq_epi16
; 348  :     // and movemask, we can generate a dword with 16 2-bit masks that are 11
; 349  :     // for zero words in the coeff vector, and 00 for nonzero words. By
; 350  :     // inverting the bits and counting leading zeros, we can determine the
; 351  :     // number of zero bytes in the vector counting from high to low memory
; 352  :     // addresses; subtract that from 31 and divide by 2 to get the offset of
; 353  :     // the last nonzero word.
; 354  :     uint32_t baseaddr = scan_cg_last * 16;

	movsxd	r14, DWORD PTR scan_cg_last$1$[rbp]

; 355  :     __m256i cur_coeffs_zeros = _mm256_cmpeq_epi16(coeffs_r, zero);
; 356  :     uint32_t nz_bytes = ~(_mm256_movemask_epi8(cur_coeffs_zeros));
; 357  :     scan_pos_last = baseaddr + ((31 - _lzcnt_u32(nz_bytes)) >> 1);

	mov	r13d, 31
	vpxor	xmm2, xmm2, xmm2
	vpcmpeqw ymm0, ymm0, ymm2
	vpmovmskb eax, ymm0
	not	eax
	lzcnt	ecx, eax
	sub	r13d, ecx
	mov	eax, r14d

; 358  :     pos_last = scan[scan_pos_last];

	mov	rcx, QWORD PTR scan$1$[rbp]
	shr	r13d, 1
	shl	eax, 4
	add	r13d, eax
	mov	esi, DWORD PTR [rcx+r13*4]

; 359  :   }
; 360  : 
; 361  :   // transform skip flag
; 362  :   if(width == 4 && encoder->cfg.trskip_enable) {

	cmp	r12b, 4
	jne	SHORT $LN21@kvz_encode
	mov	rax, QWORD PTR encoder$1$[rbp]
	cmp	DWORD PTR [rax+72], 0
	je	SHORT $LN21@kvz_encode

; 363  :     cabac->cur_ctx = (type == 0) ? &(cabac->ctx.transform_skip_model_luma) : &(cabac->ctx.transform_skip_model_chroma);
; 364  :     CABAC_BIN(cabac, tr_skip, "transform_skip_flag");

	movsx	edx, BYTE PTR tr_skip$[rsp]
	xor	eax, eax
	cmp	BYTE PTR type$[rsp], al
	mov	rcx, rdi
	setne	al
	add	rax, 222				; 000000deH
	add	rax, rdi
	mov	QWORD PTR [rdi], rax
	vzeroupper
	call	kvz_cabac_encode_bin
$LN21@kvz_encode:

; 365  :   }
; 366  : 
; 367  :   last_coeff_x = pos_last & (width - 1);
; 368  :   last_coeff_y = (uint8_t)(pos_last >> log2_block_size);

	mov	eax, DWORD PTR log2_block_size$1$[rbp]
	lea	edx, DWORD PTR [r12-1]
	shrx	r8d, esi, eax

; 369  : 
; 370  :   // Code last_coeff_x and last_coeff_y
; 371  :   kvz_encode_last_significant_xy(cabac,

	movzx	eax, BYTE PTR scan_mode$[rsp]
	and	dl, sil
	mov	BYTE PTR [rsp+48], al
	movzx	r9d, r12b
	movzx	eax, BYTE PTR type$[rsp]
	mov	rcx, rdi
	mov	BYTE PTR [rsp+40], al
	mov	BYTE PTR [rsp+32], r12b
	vzeroupper
	call	kvz_encode_last_significant_xy

; 372  :                                  last_coeff_x,
; 373  :                                  last_coeff_y,
; 374  :                                  width,
; 375  :                                  width,
; 376  :                                  type,
; 377  :                                  scan_mode);
; 378  : 
; 379  :   scan_pos_sig = scan_pos_last;
; 380  : 
; 381  :   ALIGNED(64) uint16_t abs_coeff[16];
; 382  :   ALIGNED(32) uint16_t abs_coeff_buf_sb[16];
; 383  :   ALIGNED(32) int16_t pos_ys_buf[16];
; 384  :   ALIGNED(32) int16_t pos_xs_buf[16];
; 385  :   ALIGNED(32) int16_t ctx_sig_buf[16];
; 386  : 
; 387  :   abs_coeff[0] = abs(coeff[pos_last]);

	movzx	r11d, WORD PTR [rbx+rsi*2]

; 388  :   uint32_t coeff_signs  = (coeff[pos_last] < 0);
; 389  :   int32_t num_non_zero = 1;
; 390  :   int32_t last_nz_pos_in_cg  = scan_pos_sig;
; 391  :   int32_t first_nz_pos_in_cg = scan_pos_sig;
; 392  :   scan_pos_sig--;

	lea	r9d, DWORD PTR [r13-1]
	movsx	eax, r11w
	mov	ecx, 1
	shr	r11d, 15
	cdq
	xor	eax, edx
	mov	DWORD PTR coeff_signs$1$[rbp], r11d
	sub	eax, edx
	mov	DWORD PTR num_non_zero$1$[rbp], ecx
	mov	WORD PTR abs_coeff$[rbp], ax
	mov	r12d, ecx
	mov	DWORD PTR first_nz_pos_in_cg$1$[rbp], r13d

; 393  : 
; 394  :   // significant_coeff_flag
; 395  :   for (i = scan_cg_last; i >= 0; i--) {

	mov	r11d, r14d
	mov	DWORD PTR scan_pos_sig$1$[rbp], r9d
	mov	DWORD PTR i$1$[rbp], r14d
	test	r14d, r14d
	js	$LN9@kvz_encode
	mov	QWORD PTR $T2[rbp], rcx

; 103  :   const __m256i zero   = _mm256_set1_epi8(0);

	mov	rcx, QWORD PTR scan_cg$1$[rbp]
	lea	rax, QWORD PTR [rcx+r14*4]
	mov	QWORD PTR tv3174[rbp], rax
	npad	2
$LL10@kvz_encode:

; 396  :     int32_t sub_pos        = i << 4; // LOG2_SCAN_SET_SIZE;
; 397  :     int32_t cg_blk_pos     = scan_cg[i];

	movsxd	r10, DWORD PTR [rax]

; 398  :     int32_t cg_pos_y       = cg_blk_pos / num_blk_side;

	xor	edx, edx
	mov	eax, r10d

; 399  :     int32_t cg_pos_x       = cg_blk_pos - (cg_pos_y * num_blk_side);
; 400  : 
; 401  :     go_rice_param = 0;

	xor	r14d, r14d
	div	DWORD PTR num_blk_side$1$[rbp]
	mov	r8d, r11d
	mov	DWORD PTR go_rice_param$1$[rbp], r14d
	shl	r8d, 4
	mov	ecx, eax
	imul	ecx, DWORD PTR num_blk_side$1$[rbp]
	mov	ebx, r10d
	mov	DWORD PTR sub_pos$1$[rbp], r8d
	mov	esi, eax
	sub	ebx, ecx

; 402  : 
; 403  :     if (i == scan_cg_last || i == 0) {

	cmp	r11d, DWORD PTR scan_cg_last$1$[rbp]
	je	$LN24@kvz_encode
	test	r11d, r11d
	je	$LN24@kvz_encode

; 406  :       uint32_t sig_coeff_group   = (sig_coeffgroup_nzs[cg_blk_pos] != 0);

	mov	eax, DWORD PTR sig_coeffgroup_nzs$[rbp+r10*4]
	xor	r11d, r11d
; File F:\open_codec_learn_2021\kvazaar-master\src\context.c

; 322  :   width >>= 2;

	mov	edx, DWORD PTR width$1$[rbp]
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c

; 406  :       uint32_t sig_coeff_group   = (sig_coeffgroup_nzs[cg_blk_pos] != 0);

	test	eax, eax
	mov	DWORD PTR tv3204[rbp], eax
	setne	r11b
; File F:\open_codec_learn_2021\kvazaar-master\src\context.c

; 322  :   width >>= 2;

	shr	edx, 2
	xor	r8d, r8d
	xor	r9d, r9d

; 323  :   if (pos_x < (uint32_t)width - 1) uiRight = (sig_coeff_group_flag[pos_y * width + pos_x + 1] != 0);

	lea	r10d, DWORD PTR [rdx-1]
	cmp	ebx, r10d
	jae	SHORT $LN65@kvz_encode
	mov	eax, edx
	imul	eax, esi
	inc	eax
	add	eax, ebx
	cmp	DWORD PTR sig_coeffgroup_nzs$[rbp+rax*4], r8d
	setne	r8b
$LN65@kvz_encode:

; 324  :   if (pos_y < (uint32_t)width - 1) uiLower = (sig_coeff_group_flag[(pos_y  + 1 ) * width + pos_x] != 0);

	cmp	esi, r10d
	jae	SHORT $LN66@kvz_encode
	lea	eax, DWORD PTR [rsi+1]
	imul	eax, edx
	add	eax, ebx
	cmp	DWORD PTR sig_coeffgroup_nzs$[rbp+rax*4], r9d
	setne	r9b
$LN66@kvz_encode:

; 325  : 
; 326  :   return uiRight || uiLower;

	test	r8d, r8d
	jne	SHORT $LN68@kvz_encode
	lea	eax, QWORD PTR [r8+72]
	test	r9d, r9d
	je	SHORT $LN69@kvz_encode
$LN68@kvz_encode:
	mov	eax, 73					; 00000049H
$LN69@kvz_encode:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c

; 409  :       cabac->cur_ctx = &base_coeff_group_ctx[ctx_sig];

	add	rax, QWORD PTR tv3223[rbp]

; 410  :       CABAC_BIN(cabac, sig_coeff_group, "coded_sub_block_flag");

	mov	edx, r11d
	mov	QWORD PTR [rdi], rax
	mov	rcx, rdi
	vzeroupper
	call	kvz_cabac_encode_bin

; 411  :     }
; 412  : 
; 413  :     if (sig_coeffgroup_nzs[cg_blk_pos]) {

	cmp	DWORD PTR tv3204[rbp], r14d
	jne	SHORT $LN122@kvz_encode

; 475  :         }
; 476  :       }
; 477  :     } else {
; 478  :       scan_pos_sig = sub_pos - 1;

	mov	eax, DWORD PTR sub_pos$1$[rbp]
	dec	eax
	mov	DWORD PTR scan_pos_sig$1$[rbp], eax
	jmp	$LN125@kvz_encode
$LN24@kvz_encode:

; 404  :       sig_coeffgroup_nzs[cg_blk_pos] = 1;

	mov	DWORD PTR sig_coeffgroup_nzs$[rbp+r10*4], 1

; 411  :     }
; 412  : 
; 413  :     if (sig_coeffgroup_nzs[cg_blk_pos]) {

	jmp	SHORT $LN120@kvz_encode
$LN122@kvz_encode:
	mov	r8d, DWORD PTR sub_pos$1$[rbp]
	mov	r9d, DWORD PTR scan_pos_sig$1$[rbp]
$LN120@kvz_encode:
; File F:\open_codec_learn_2021\kvazaar-master\src\context.c

; 344  :   if (width == 4) return -1;

	mov	eax, DWORD PTR width$1$[rbp]
	xor	r11d, r11d
	xor	r10d, r10d
	cmp	eax, 4
	jne	SHORT $LN71@kvz_encode
	lea	edx, QWORD PTR [rax-5]
	jmp	SHORT $LN70@kvz_encode
$LN71@kvz_encode:

; 345  : 
; 346  :   width >>= 2;

	mov	edx, eax
	shr	edx, 2

; 347  :   if (pos_x < (uint32_t)width - 1) sigRight = (sig_coeff_group_flag[pos_y * width + pos_x + 1] != 0);

	lea	r13d, DWORD PTR [rdx-1]
	cmp	ebx, r13d
	jae	SHORT $LN72@kvz_encode
	mov	eax, edx
	imul	eax, esi
	inc	eax
	add	eax, ebx
	cmp	DWORD PTR sig_coeffgroup_nzs$[rbp+rax*4], r10d
	setne	r11b
$LN72@kvz_encode:

; 348  :   if (pos_y < (uint32_t)width - 1) sigLower = (sig_coeff_group_flag[(pos_y  + 1 ) * width + pos_x] != 0);

	cmp	esi, r13d
	jae	SHORT $LN73@kvz_encode
	lea	eax, DWORD PTR [rsi+1]
	imul	eax, edx
	add	eax, ebx
	cmp	DWORD PTR sig_coeffgroup_nzs$[rbp+rax*4], r10d
	setne	r10b
$LN73@kvz_encode:

; 349  : 
; 350  :   return sigRight + (sigLower<<1);

	lea	edx, DWORD PTR [r11+r10*2]
$LN70@kvz_encode:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c

; 434  :       __m256i blk_poses_tmp = _mm256_packs_epi32(blk_poses_lo, blk_poses_hi);

	mov	r10, QWORD PTR scan$1$[rbp]
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 167  :   *last = (31 - (int32_t)_lzcnt_u32(nonzero_bytes)) >> 1;

	mov	r13d, 31
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c

; 420  :       const __m128i log2_block_size_128 = _mm_cvtsi32_si128(log2_block_size);

	mov	ecx, DWORD PTR log2_block_size$1$[rbp]

; 424  :       __m256i is = _mm256_set1_epi16(i);

	mov	ebx, DWORD PTR i$1$[rbp]

; 132  :   if (block_type == 3)

	movzx	esi, BYTE PTR type$[rsp]

; 421  : 
; 422  :       __m256i coeffs = _mm256_load_si256((__m256i *)(coeff_reord + sub_pos));

	movsxd	rax, r8d
	vpxor	xmm6, xmm6, xmm6
	vmovd	xmm5, ecx
	vmovdqu	ymm0, YMMWORD PTR coeff_reord$[rbp+rax*2]

; 423  :       __m256i sigs_inv = _mm256_cmpeq_epi16(coeffs, zero);

	vpcmpeqw ymm1, ymm0, ymm6

; 425  :       __m256i is_zero = _mm256_cmpeq_epi16(is, zero);
; 426  :       __m256i coeffs_negative = _mm256_cmpgt_epi16(zero, coeffs);
; 427  : 
; 428  :       __m256i masked_coeffs = _mm256_andnot_si256(sigs_inv, coeffs);

	vpandn	ymm4, ymm1, ymm0
	vpcmpgtw ymm2, ymm6, ymm0

; 434  :       __m256i blk_poses_tmp = _mm256_packs_epi32(blk_poses_lo, blk_poses_hi);

	vmovdqu	ymm0, YMMWORD PTR [r10+rax*4]
	vmovdqu	YMMWORD PTR sigs_inv$1$[rbp], ymm1
	vpackssdw ymm1, ymm0, YMMWORD PTR [r10+rax*4+32]

; 447  : 
; 448  :       __m256i ctx_sigs = kvz_context_get_sig_ctx_inc_16x16b(pattern_sig_ctx, scan_mode, pos_xs, pos_ys,

	movsx	r10d, BYTE PTR scan_mode$[rsp]
	vpermq	ymm3, ymm1, 216				; 000000d8H
	movsx	eax, bx
	vmovd	xmm0, eax
	vpbroadcastw ymm0, xmm0
	mov	eax, 65535				; 0000ffffH
	vmovd	xmm1, eax
	vmovdqu	YMMWORD PTR coeffs_negative$1$[rbp], ymm2
	vpcmpeqw ymm2, ymm0, ymm6
	vpandn	ymm0, ymm2, ymm1
	vmovdqu	YMMWORD PTR encode_sig_coeff_flags_inv$1$[rbp], ymm0
	vpxor	xmm0, xmm0, xmm0
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 164  :   __m256i zeros = _mm256_cmpeq_epi16(ints, zero);

	vpcmpeqw ymm1, ymm4, ymm0

; 165  :   uint32_t nonzero_bytes = ~((uint32_t)_mm256_movemask_epi8(zeros));

	vpmovmskb eax, ymm1
	not	eax

; 166  :   *first = (    (int32_t)_tzcnt_u32(nonzero_bytes)) >> 1;

	tzcnt	r11d, eax
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c

; 429  :       __m256i abs_coeffs = _mm256_abs_epi16(masked_coeffs);

	vpabsw	ymm0, ymm4
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 167  :   *last = (31 - (int32_t)_lzcnt_u32(nonzero_bytes)) >> 1;

	lzcnt	eax, eax
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c

; 437  :       __m256i pos_ys = _mm256_srl_epi16(blk_poses, log2_block_size_128);

	vpsrlw	ymm15, ymm3, xmm5

; 438  :       __m256i pos_xs = _mm256_sub_epi16(blk_poses, _mm256_sll_epi16(pos_ys, log2_block_size_128));

	vpsllw	ymm1, ymm15, xmm5

; 108  :   const __m256i threes = _mm256_set1_epi16(3);

	vmovdqu	ymm5, YMMWORD PTR __ymm@0003000300030003000300030003000300030003000300030003000300030003

; 439  : 
; 440  :       _mm256_store_si256((__m256i *)pos_ys_buf, pos_ys);
; 441  :       _mm256_store_si256((__m256i *)pos_xs_buf, pos_xs);
; 442  : 
; 443  :       __m256i encode_sig_coeff_flags_inv = _mm256_andnot_si256(is_zero, coeff_pos_zero);
; 444  : 
; 445  :       get_first_last_nz_int16(masked_coeffs, &first_nz_pos_in_cg, &last_nz_pos_in_cg);
; 446  :       _mm256_store_si256((__m256i *)abs_coeff_buf_sb, abs_coeffs);

	vmovdqu	YMMWORD PTR abs_coeff_buf_sb$[rbp], ymm0

; 110  :   const __m256i ctx_ind_map[3] = {

	vmovdqu	ymm0, YMMWORD PTR __ymm@0008000800080005000800070005000400060007000400030006000100020000
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 167  :   *last = (31 - (int32_t)_lzcnt_u32(nonzero_bytes)) >> 1;

	sub	r13d, eax
	sar	r11d, 1
	sar	r13d, 1
	mov	DWORD PTR first_nz_pos_in_cg$1$[rbp], r11d
	mov	DWORD PTR last_nz_pos_in_cg$1$[rbp], r13d
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\encode_coding_tree-avx2.c

; 438  :       __m256i pos_xs = _mm256_sub_epi16(blk_poses, _mm256_sll_epi16(pos_ys, log2_block_size_128));

	vpsubw	ymm2, ymm3, ymm1

; 110  :   const __m256i ctx_ind_map[3] = {

	vmovdqu	ymm1, YMMWORD PTR __ymm@0008000800070007000800080006000600050004000300020005000400010000
	vmovdqu	YMMWORD PTR ctx_ind_map$8[rbp], ymm0
	vmovdqu	ymm0, YMMWORD PTR __ymm@0008000800050005000800080004000400070006000300010007000600020000

; 438  :       __m256i pos_xs = _mm256_sub_epi16(blk_poses, _mm256_sll_epi16(pos_ys, log2_block_size_128));

	vmovdqu	YMMWORD PTR tv3200[rbp], ymm2

; 110  :   const __m256i ctx_ind_map[3] = {

	vmovdqu	YMMWORD PTR ctx_ind_map$8[rbp+32], ymm1
	vmovdqu	YMMWORD PTR ctx_ind_map$8[rbp+64], ymm0

; 111  :     _mm256_setr_epi16(
; 112  :         0, 2, 1, 6,
; 113  :         3, 4, 7, 6,
; 114  :         4, 5, 7, 8,
; 115  :         5, 8, 8, 8
; 116  :     ),
; 117  :     _mm256_setr_epi16(
; 118  :         0, 1, 4, 5,
; 119  :         2, 3, 4, 5,
; 120  :         6, 6, 8, 8,
; 121  :         7, 7, 8, 8
; 122  :     ),
; 123  :     _mm256_setr_epi16(
; 124  :         0, 2, 6, 7,
; 125  :         1, 3, 6, 7,
; 126  :         4, 4, 8, 8,
; 127  :         5, 5, 8, 8
; 128  :     ),
; 129  :   };
; 130  : 
; 131  :   int16_t offset;
; 132  :   if (block_type == 3)

	cmp	ecx, 3
	jne	SHORT $LN78@kvz_encode

; 133  :     if (scan_idx == SCAN_DIAG)
; 134  :       offset = 9;
; 135  :     else
; 136  :       offset = 15;

	test	r10d, r10d
	mov	ecx, 9
	mov	eax, 15
	cmovne	cx, ax
	jmp	SHORT $LN83@kvz_encode
$LN78@kvz_encode:

; 137  :   else
; 138  :     if (texture_type == 0)

	mov	ecx, 21
	test	sil, sil

; 139  :       offset = 21;
; 140  :     else
; 141  :       offset = 12;

	mov	eax, 12
	cmove	ax, cx
	movzx	ecx, ax
$LN83@kvz_encode:

; 160  : 
; 161  :   __m256i texture_types = _mm256_set1_epi16((int16_t)texture_type);
; 162  : 
; 163  :   __m256i block_types     = _mm256_set1_epi16((int16_t)block_type);
; 164  :   __m256i block_type_two  = _mm256_cmpeq_epi16(block_types, twos);   // All these should be ctx_ind_map[4 * pos_y + pos_x];
; 165  :   __m256i bt2_vals        = ctx_ind_map[scan_idx];
; 166  :   __m256i bt2_vals_masked = _mm256_and_si256(bt2_vals, block_type_two);
; 167  : 
; 168  :   __m256i pos_xs_in_subset = _mm256_and_si256(pos_xs, threes);

	vpand	ymm8, ymm5, ymm2
	movsx	eax, dx
	vmovd	xmm0, eax
	movsx	eax, WORD PTR log2_block_size$1$[rbp]
	vpbroadcastw ymm0, xmm0
	vpcmpeqw ymm11, ymm0, YMMWORD PTR __ymm@0002000200020002000200020002000200020002000200020002000200020002
	vpcmpeqw ymm14, ymm0, ymm6
	vpcmpeqw ymm0, ymm0, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vpor	ymm12, ymm0, ymm11
	vmovd	xmm0, eax
	vpbroadcastw ymm0, xmm0
	vpcmpeqw ymm13, ymm0, YMMWORD PTR __ymm@0002000200020002000200020002000200020002000200020002000200020002

; 169  :   __m256i pos_ys_in_subset = _mm256_and_si256(pos_ys, threes);

	vpand	ymm7, ymm5, ymm15

; 170  : 
; 171  :   __m256i cg_pos_xs        = _mm256_srli_epi16(pos_xs, 2);
; 172  :   __m256i cg_pos_ys        = _mm256_srli_epi16(pos_ys, 2);
; 173  :   __m256i cg_pos_xysums    = _mm256_add_epi16 (cg_pos_xs, cg_pos_ys);
; 174  : 
; 175  :   __m256i pos_xy_sums_in_subset = _mm256_add_epi16(pos_xs_in_subset, pos_ys_in_subset);

	vpaddw	ymm9, ymm7, ymm8
	mov	eax, r10d
	shl	rax, 5
	vpsrlw	ymm1, ymm15, 2
	vmovdqu	ymm0, YMMWORD PTR ctx_ind_map$8[rbp+rax]
	movsx	eax, cx
	vpand	ymm10, ymm0, ymm13
	vmovd	xmm4, eax
	movsx	eax, sil
	vpbroadcastw ymm4, xmm4
	vmovd	xmm0, eax
	vpbroadcastw ymm0, xmm0

; 176  : 
; 177  :   /*
; 178  :    * if (pattern_sig_ctx == 0) {
; 179  :    *   switch (pos_x_in_subset + pos_y_in_subset) {
; 180  :    *   case 0:
; 181  :    *     cnt = 2;
; 182  :    *     break;
; 183  :    *   case 1:
; 184  :    *   case 2:
; 185  :    *     cnt = 1;
; 186  :    *     break;
; 187  :    *   default:
; 188  :    *     cnt = 0;
; 189  :    *   }
; 190  :    * }
; 191  :    *
; 192  :    * Equivalent to:
; 193  :    *
; 194  :    * if (pattern_sig_ctx == 0) {
; 195  :    *   subamt = cnt <= 1 ? 1 : 0;
; 196  :    *   pxyis_max3 = min(3, pos_x_in_subset + pos_y_in_subset);
; 197  :    *   cnt = (3 - pxyis_max3) - subamt;
; 198  :    * }
; 199  :    */
; 200  :   __m256i pxyis_lte_1     = _mm256_cmpgt_epi16(twos,                  pos_xy_sums_in_subset);
; 201  :   __m256i subamts         = _mm256_and_si256  (pxyis_lte_1,           ones);
; 202  :   __m256i pxyis_max3      = _mm256_min_epu16  (pos_xy_sums_in_subset, threes);
; 203  :   __m256i cnts_tmp        = _mm256_sub_epi16  (threes,                pxyis_max3);
; 204  :   __m256i cnts_sig_ctx_0  = _mm256_sub_epi16  (cnts_tmp,              subamts);
; 205  :   __m256i cnts_sc0_masked = _mm256_and_si256  (cnts_sig_ctx_0,        pattern_sig_ctxs_eq_zero);
; 206  : 
; 207  :   /*
; 208  :    * if (pattern_sig_ctx == 1 || pattern_sig_ctx == 2) {
; 209  :    *   if (pattern_sig_ctx == 1)
; 210  :    *     subtrahend = pos_y_in_subset;
; 211  :    *   else
; 212  :    *     subtrahend = pos_x_in_subset;
; 213  :    *   cnt = 2 - min(2, subtrahend);
; 214  :    * }
; 215  :    */
; 216  :   __m256i pos_operands_ctx_1or2 = _mm256_blendv_epi8(pos_ys_in_subset,
; 217  :                                                      pos_xs_in_subset,
; 218  :                                                      pattern_sig_ctxs_eq_two);
; 219  : 
; 220  :   __m256i pos_operands_max2     = _mm256_min_epu16  (pos_operands_ctx_1or2, twos);
; 221  :   __m256i cnts_sig_ctx_1or2     = _mm256_sub_epi16  (twos,                  pos_operands_max2);
; 222  :   __m256i cnts_sc12_masked      = _mm256_and_si256  (cnts_sig_ctx_1or2,     pattern_sig_ctxs_eq_1or2);
; 223  : 
; 224  :   /*
; 225  :    * if (pattern_sig_ctx > 2)
; 226  :    *   cnt = 2;
; 227  :    */
; 228  :   __m256i cnts_scother_masked = _mm256_and_si256(twos, pattern_sig_ctxs_other);
; 229  : 
; 230  :   // Select correct count
; 231  :   __m256i cnts_sc012_masked   = _mm256_or_si256 (cnts_sc0_masked,     cnts_sc12_masked);
; 232  :   __m256i cnts                = _mm256_or_si256 (cnts_scother_masked, cnts_sc012_masked);
; 233  : 
; 234  :   // Compute final values
; 235  :   __m256i textype_eq_0     = _mm256_cmpeq_epi16(texture_types, zero);

	vpcmpeqw ymm3, ymm0, ymm6
	vpsrlw	ymm0, ymm2, 2
	vpaddw	ymm1, ymm0, ymm1

; 236  :   __m256i cg_pos_sums_gt_0 = _mm256_cmpgt_epi16(cg_pos_xysums, zero);

	vpcmpgtw ymm2, ymm1, ymm6

; 237  :   __m256i tmpcond          = _mm256_and_si256  (textype_eq_0,  cg_pos_sums_gt_0);

	vpand	ymm3, ymm2, ymm3

; 238  :   __m256i tmp              = _mm256_and_si256  (tmpcond,       threes);

	vpand	ymm0, ymm3, ymm5
	vmovdqu	ymm3, YMMWORD PTR __ymm@0003000300030003000300030003000300030003000300030003000300030003

; 239  :   __m256i tmp_with_offsets = _mm256_add_epi16  (tmp,           offsets);

	vpaddw	ymm6, ymm0, ymm4
	vpblendvb ymm0, ymm7, ymm8, ymm11
	vmovdqu	ymm7, YMMWORD PTR __ymm@0002000200020002000200020002000200020002000200020002000200020002
	vmovdqu	ymm8, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vpor	ymm1, ymm12, ymm14
	vpxor	ymm2, ymm1, YMMWORD PTR __ymm@ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
	vpand	ymm5, ymm2, YMMWORD PTR __ymm@0002000200020002000200020002000200020002000200020002000200020002
	vpminuw	ymm1, ymm0, ymm7
	vpsubw	ymm2, ymm7, ymm1
	vpand	ymm4, ymm2, ymm12
	vpminuw	ymm0, ymm9, ymm3
	vpsubw	ymm3, ymm3, ymm0
	vpcmpgtw ymm1, ymm7, ymm9
	vpand	ymm2, ymm1, ymm8
	vpsubw	ymm0, ymm3, ymm2
	vpand	ymm3, ymm0, ymm14
	vpor	ymm1, ymm3, ymm4
	vpor	ymm2, ymm1, ymm5
	vpaddw	ymm1, ymm15, YMMWORD PTR tv3200[rbp]

; 240  :   __m256i rv_noshortcirc   = _mm256_add_epi16  (cnts,          tmp_with_offsets);

	vpaddw	ymm0, ymm2, ymm6

; 241  : 
; 242  :   // Ol' sprite mask method works here!
; 243  :   __m256i rv1 = _mm256_andnot_si256(block_type_two, rv_noshortcirc);

	vpandn	ymm3, ymm13, ymm0
	vpxor	xmm2, xmm2, xmm2

; 148  :   __m256i pattern_sig_ctxs_eq_zero = _mm256_cmpeq_epi16(pattern_sig_ctxs, zero);
; 149  :   __m256i pattern_sig_ctxs_eq_one  = _mm256_cmpeq_epi16(pattern_sig_ctxs, ones);
; 150  :   __m256i pattern_sig_ctxs_eq_two  = _mm256_cmpeq_epi16(pattern_sig_ctxs, twos);
; 151  : 
; 152  :   __m256i pattern_sig_ctxs_eq_1or2 = _mm256_or_si256 (pattern_sig_ctxs_eq_one,
; 153  :                                                       pattern_sig_ctxs_eq_two);
; 154  :   __m256i pattern_sig_ctxs_lt3     = _mm256_or_si256 (pattern_sig_ctxs_eq_1or2,
; 155  :                                                       pattern_sig_ctxs_eq_zero);
; 156  :   __m256i pattern_sig_ctxs_other   = _mm256_xor_si256(pattern_sig_ctxs_lt3,
; 157  :                                                       ff);
; 158  :   __m256i x_plus_y        = _mm256_add_epi16  (pos_xs,   pos_ys);
; 159  :   __m256i x_plus_y_zero   = _mm256_cmpeq_epi16(x_plus_y, zero);   // All these should be 0, preempts block_type_two rule

	vpcmpeqw ymm0, ymm1, ymm2

; 449  :                                              log2_block_size, type);
; 450  : 
; 451  :       _mm256_store_si256((__m256i *)ctx_sig_buf, ctx_sigs);
; 452  : 
; 453  :       uint32_t esc_flags = ~(_mm256_movemask_epi8(encode_sig_coeff_flags_inv));

	vmovdqu	ymm1, YMMWORD PTR encode_sig_coeff_flags_inv$1$[rbp]

; 244  :   __m256i rv2 = _mm256_or_si256    (rv1,            bt2_vals_masked);

	vpor	ymm4, ymm3, ymm10

; 245  :   __m256i rv  = _mm256_andnot_si256(x_plus_y_zero,  rv2);

	vpandn	ymm2, ymm0, ymm4

; 454  :       uint32_t sigs = ~(_mm256_movemask_epi8(sigs_inv));

	vmovdqu	ymm0, YMMWORD PTR sigs_inv$1$[rbp]
	vpmovmskb r10d, ymm0
	not	r10d
	mov	DWORD PTR sigs$1$[rbp], r10d

; 455  :       uint32_t coeff_sign_buf = _mm256_movemask_epi8(coeffs_negative);

	vmovdqu	ymm0, YMMWORD PTR coeffs_negative$1$[rbp]
	vpmovmskb edx, ymm1
	not	edx
	mov	DWORD PTR esc_flags$1$[rbp], edx
	vpmovmskb eax, ymm0
	mov	DWORD PTR coeff_sign_buf$1$[rbp], eax
	vmovdqu	YMMWORD PTR ctx_sig_buf$[rbp], ymm2

; 456  : 
; 457  :       for (; scan_pos_sig >= sub_pos; scan_pos_sig--) {

	cmp	r9d, r8d
	jl	$LN26@kvz_encode

; 147  :   __m256i pattern_sig_ctxs = _mm256_set1_epi16((int16_t)(MIN(0xffff, pattern_sig_ctx)));

	mov	rax, QWORD PTR $T2[rbp]
	lea	rcx, QWORD PTR abs_coeff$[rbp]
	mov	r14d, DWORD PTR coeff_sign_buf$1$[rbp]
	lea	rcx, QWORD PTR [rcx+rax*2]
	mov	eax, r9d
	sub	eax, r8d
	mov	QWORD PTR tv3177[rbp], rcx
	lea	ebx, DWORD PTR [rax*2+1]
	npad	11
$LL13@kvz_encode:

; 458  :         uint32_t id = scan_pos_sig - sub_pos;
; 459  :         uint32_t shift = (id << 1) + 1;
; 460  : 
; 461  :         uint32_t curr_sig = (sigs >> shift) & 1;

	shrx	esi, r10d, ebx

; 462  :         uint32_t curr_esc_flag = (esc_flags >> shift) & 1;
; 463  :         uint32_t curr_coeff_sign = (coeff_sign_buf >> shift) & 1;

	shrx	r15d, r14d, ebx
	mov	r13d, r9d
	and	esi, 1
	shrx	eax, edx, ebx
	and	r15d, 1
	sub	r13d, r8d
	and	eax, 1

; 464  : 
; 465  :         if (curr_esc_flag | num_non_zero) {

	or	eax, r12d
	je	SHORT $LN111@kvz_encode

; 466  :           ctx_sig = ctx_sig_buf[id];

	movsx	ecx, WORD PTR ctx_sig_buf$[rbp+r13*2]

; 467  :           cabac->cur_ctx = &baseCtx[ctx_sig];
; 468  :           CABAC_BIN(cabac, curr_sig, "sig_coeff_flag");

	mov	edx, esi
	add	rcx, QWORD PTR tv3195[rbp]
	mov	QWORD PTR [rdi], rcx
	mov	rcx, rdi
	vzeroupper
	call	kvz_cabac_encode_bin
	mov	rcx, QWORD PTR tv3177[rbp]
	mov	r8d, DWORD PTR sub_pos$1$[rbp]
	mov	r9d, DWORD PTR scan_pos_sig$1$[rbp]
	mov	edx, DWORD PTR esc_flags$1$[rbp]
	mov	r10d, DWORD PTR sigs$1$[rbp]
$LN111@kvz_encode:

; 469  :         }
; 470  : 
; 471  :         if (curr_sig) {

	test	esi, esi
	je	SHORT $LN11@kvz_encode

; 472  :           abs_coeff[num_non_zero]  = abs_coeff_buf_sb[id];
; 473  :           coeff_signs              = 2 * coeff_signs + curr_coeff_sign;

	mov	r11d, DWORD PTR coeff_signs$1$[rbp]

; 474  :           num_non_zero++;

	inc	r12d
	movzx	eax, WORD PTR abs_coeff_buf_sb$[rbp+r13*2]
	mov	WORD PTR [rcx], ax
	add	rcx, 2
	mov	QWORD PTR tv3177[rbp], rcx
	lea	r11d, DWORD PTR [r15+r11*2]
	mov	DWORD PTR coeff_signs$1$[rbp], r11d
$LN11@kvz_encode:

; 456  : 
; 457  :       for (; scan_pos_sig >= sub_pos; scan_pos_sig--) {

	dec	r9d
	sub	ebx, 2
	mov	DWORD PTR scan_pos_sig$1$[rbp], r9d
	cmp	r9d, r8d
	jge	$LL13@kvz_encode
	mov	r14d, DWORD PTR go_rice_param$1$[rbp]
	mov	r15d, DWORD PTR c1$1$[rbp]
	mov	r13d, DWORD PTR last_nz_pos_in_cg$1$[rbp]
	mov	DWORD PTR num_non_zero$1$[rbp], r12d
$LN125@kvz_encode:

; 479  :     }
; 480  : 
; 481  :     if (num_non_zero > 0) {

	movzx	esi, BYTE PTR type$[rsp]
	mov	ebx, DWORD PTR i$1$[rbp]
	mov	r11d, DWORD PTR first_nz_pos_in_cg$1$[rbp]
	vmovdqu	ymm8, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vmovdqu	ymm7, YMMWORD PTR __ymm@0002000200020002000200020002000200020002000200020002000200020002
$LN26@kvz_encode:
	test	r12d, r12d
	jle	$LN18@kvz_encode

; 482  :       bool sign_hidden = last_nz_pos_in_cg - first_nz_pos_in_cg >= 4 /* SBH_THRESHOLD */

	sub	r13d, r11d
	cmp	r13d, 4
	jl	SHORT $LN47@kvz_encode
	mov	rax, QWORD PTR encoder$1$[rbp]
	mov	BYTE PTR $T1[rbp], 1
	cmp	DWORD PTR [rax+2392], 0
	je	SHORT $LN48@kvz_encode
$LN47@kvz_encode:
	mov	BYTE PTR $T1[rbp], 0
$LN48@kvz_encode:

; 483  :                          && !encoder->cfg.lossless;
; 484  :       uint32_t ctx_set  = (i > 0 && type == 0) ? 2 : 0;

	test	ebx, ebx
	je	SHORT $LN49@kvz_encode
	test	sil, sil
	jne	SHORT $LN49@kvz_encode
	mov	eax, 2
	jmp	SHORT $LN50@kvz_encode
$LN49@kvz_encode:
	xor	eax, eax
$LN50@kvz_encode:

; 485  :       cabac_ctx_t *base_ctx_mod;
; 486  :       int32_t num_c1_flag, first_c2_flag_idx, idx;
; 487  : 
; 488  :       __m256i abs_coeffs = _mm256_load_si256((__m256i *)abs_coeff);
; 489  :       __m256i coeffs_gt1 = _mm256_cmpgt_epi16(abs_coeffs, ones);

	vmovdqu	ymm1, YMMWORD PTR abs_coeff$[rbp]

; 490  :       __m256i coeffs_gt2 = _mm256_cmpgt_epi16(abs_coeffs, twos);
; 491  :       uint32_t coeffs_gt1_bits = _mm256_movemask_epi8(coeffs_gt1);
; 492  :       uint32_t coeffs_gt2_bits = _mm256_movemask_epi8(coeffs_gt2);
; 493  : 
; 494  :       if (c1 == 0) {
; 495  :         ctx_set++;
; 496  :       }
; 497  : 
; 498  :       base_ctx_mod     = (type == 0) ? &(cabac->ctx.cu_one_model_luma[4 * ctx_set]) :

	test	r15d, r15d

; 499  :                          &(cabac->ctx.cu_one_model_chroma[4 * ctx_set]);
; 500  :       num_c1_flag      = MIN(num_non_zero, C1FLAG_NUMBER);

	mov	r13d, 8
	vpcmpgtw ymm0, ymm1, ymm8
	vpmovmskb ebx, ymm0
	vpcmpgtw ymm1, ymm1, ymm7
	vpmovmskb ecx, ymm1
	mov	DWORD PTR coeffs_gt2_bits$1$[rbp], ecx
	mov	r15d, 178				; 000000b2H
	lea	ecx, DWORD PTR [rax+1]
	mov	DWORD PTR coeffs_gt1_bits$1$[rbp], ebx
	cmovne	ecx, eax
	test	sil, sil
	mov	DWORD PTR tv3234[rbp], ecx
	mov	eax, 194				; 000000c2H
	cmovne	r15d, eax
	lea	ecx, DWORD PTR [rcx*4]
	lea	rax, QWORD PTR [rdi+rcx]
	add	r15, rax

; 501  :       first_c2_flag_idx = -1;
; 502  : 
; 503  : 
; 504  :       /*
; 505  :        * c1s_pattern is 16 base-4 numbers: 3, 3, 3, ... , 3, 2 (c1 will never
; 506  :        * be less than 0 or greater than 3, so two bits per iter are enough).
; 507  :        * It's essentially the values that c1 will be for the next iteration as
; 508  :        * long as we have not encountered any >1 symbols. Count how long run of
; 509  :        * such symbols there is in the beginning of this CG, and zero all c1's
; 510  :        * that are located at or after the first >1 symbol.
; 511  :        */
; 512  :       const uint32_t c1s_pattern = 0xfffffffe;
; 513  :       uint32_t n_nongt1_bits = _tzcnt_u32(coeffs_gt1_bits);
; 514  :       uint32_t c1s_nextiter  = _bzhi_u32(c1s_pattern, n_nongt1_bits);

	mov	eax, -2					; fffffffeH
	cmp	r12d, r13d
	cmovl	r13d, r12d
	xor	esi, esi
	tzcnt	ecx, ebx
	bzhi	eax, eax, ecx

; 515  :       first_c2_flag_idx      = n_nongt1_bits >> 1;

	shr	ecx, 1
	mov	DWORD PTR first_c2_flag_idx$1$[rbp], ecx

; 516  : 
; 517  :       c1 = 1;

	mov	ecx, 1
	mov	DWORD PTR c1s_nextiter$1$[rbp], eax
	mov	eax, ecx
	mov	DWORD PTR c1$1$[rbp], eax

; 518  :       for (idx = 0; idx < num_c1_flag; idx++) {

	test	r13d, r13d
	jle	$LN32@kvz_encode
	mov	r14d, DWORD PTR c1s_nextiter$1$[rbp]
	mov	r12d, ebx
	npad	13
$LL16@kvz_encode:

; 519  :         uint32_t shift = idx << 1;

	lea	ebx, DWORD PTR [rsi+rsi]

; 520  :         uint32_t symbol = (coeffs_gt1_bits >> shift) & 1;
; 521  : 
; 522  :         cabac->cur_ctx = &base_ctx_mod[c1];

	add	rax, r15
	shrx	edx, r12d, ebx
	and	edx, 1
	mov	QWORD PTR [rdi], rax

; 523  :         CABAC_BIN(cabac, symbol, "coeff_abs_level_greater1_flag");

	mov	rcx, rdi
	vzeroupper
	call	kvz_cabac_encode_bin

; 524  : 
; 525  :         c1 = (c1s_nextiter >> shift) & 3;

	shrx	eax, r14d, ebx
	and	eax, 3
	inc	esi
	cmp	esi, r13d
	jl	SHORT $LL16@kvz_encode

; 526  :       }
; 527  : 
; 528  :       if (c1 == 0) {

	mov	r14d, DWORD PTR go_rice_param$1$[rbp]
	mov	r12d, DWORD PTR num_non_zero$1$[rbp]
	mov	DWORD PTR c1$1$[rbp], eax
	test	eax, eax
	jne	SHORT $LN123@kvz_encode

; 529  :         base_ctx_mod = (type == 0) ? &(cabac->ctx.cu_abs_model_luma[ctx_set]) :
; 530  :                        &(cabac->ctx.cu_abs_model_chroma[ctx_set]);
; 531  : 
; 532  :         if (first_c2_flag_idx != -1) {

	cmp	BYTE PTR type$[rsp], r14b
	mov	ecx, 206				; 000000ceH
	mov	eax, 202				; 000000caH
	cmovne	eax, ecx
	mov	ecx, DWORD PTR tv3234[rbp]
	add	rax, rdi
	add	rcx, rax

; 533  :           uint32_t shift = (first_c2_flag_idx << 1) + 1;

	mov	eax, DWORD PTR first_c2_flag_idx$1$[rbp]

; 534  :           uint8_t symbol = (coeffs_gt2_bits >> shift) & 1;
; 535  :           cabac->cur_ctx = &base_ctx_mod[0];

	mov	QWORD PTR [rdi], rcx

; 536  : 
; 537  :           CABAC_BIN(cabac, symbol, "coeff_abs_level_greater2_flag");

	mov	rcx, rdi
	lea	eax, DWORD PTR [rax*2+1]
	shrx	edx, DWORD PTR coeffs_gt2_bits$1$[rbp], eax
	and	edx, 1
	call	kvz_cabac_encode_bin
$LN123@kvz_encode:

; 538  :         }
; 539  :       }
; 540  :       int32_t shiftamt = (be_valid && sign_hidden) ? 1 : 0;

	mov	ebx, DWORD PTR coeffs_gt1_bits$1$[rbp]
	mov	ecx, 1
$LN32@kvz_encode:
	cmp	BYTE PTR be_valid$1$[rbp], 0
	je	SHORT $LN55@kvz_encode
	cmp	BYTE PTR $T1[rbp], 0
	je	SHORT $LN55@kvz_encode
	mov	eax, ecx
	jmp	SHORT $LN56@kvz_encode
$LN55@kvz_encode:
	xor	eax, eax
$LN56@kvz_encode:

; 541  :       int32_t nnz = num_non_zero - shiftamt;
; 542  :       coeff_signs >>= shiftamt;

	shrx	edx, DWORD PTR coeff_signs$1$[rbp], eax
	mov	r8d, r12d

; 543  :       if (!cabac->only_count) {
; 544  :         if (encoder->cfg.crypto_features & KVZ_CRYPTO_TRANSF_COEFF_SIGNS) {
; 545  :           coeff_signs ^= kvz_crypto_get_key(state->crypto_hdl, nnz);
; 546  :         }
; 547  :       }
; 548  :       CABAC_BINS_EP(cabac, coeff_signs, nnz, "coeff_sign_flag");

	mov	rcx, rdi
	sub	r8d, eax
	vzeroupper
	call	kvz_cabac_encode_bins_ep

; 549  : 
; 550  :       if (c1 == 0 || num_non_zero > C1FLAG_NUMBER) {

	mov	r15d, DWORD PTR c1$1$[rbp]
	test	r15d, r15d
	je	SHORT $LN36@kvz_encode
	cmp	r12d, 8
	jle	$LN18@kvz_encode
$LN36@kvz_encode:

; 77   :   const __m256i threes  = _mm256_set1_epi16   (3);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0003000300030003000300030003000300030003000300030003000300030003

; 551  : 
; 552  :         const __m256i ones        = _mm256_set1_epi16(1);
; 553  : 
; 554  :         __m256i abs_coeffs_gt1    = _mm256_cmpgt_epi16  (abs_coeffs, ones);
; 555  :         uint32_t acgt1_bits       = _mm256_movemask_epi8(abs_coeffs_gt1);
; 556  :         uint32_t first_acgt1_bpos = _tzcnt_u32(acgt1_bits);

	tzcnt	eax, ebx

; 557  : 
; 558  :         uint32_t abs_coeffs_base4 = pack_16x16b_to_16x2b(abs_coeffs);
; 559  : 
; 560  :         const uint32_t ones_base4 = 0x55555555;
; 561  :         const uint32_t twos_base4 = 0xaaaaaaaa;
; 562  : 
; 563  :         const uint32_t c1flag_number_mask_inv = 0xffffffff << (C1FLAG_NUMBER << 1);
; 564  :         const uint32_t c1flag_number_mask     = ~c1flag_number_mask_inv;
; 565  : 
; 566  :         // The addition will not overflow between 2-bit atoms because
; 567  :         // first_coeff2s will only be 1 or 0, and the other addend is 2
; 568  :         uint32_t first_coeff2s    = _bzhi_u32(ones_base4, first_acgt1_bpos + 2);

	add	eax, 2

; 573  : 
; 574  :         uint32_t encode_decisions = u32vec_cmpgt_epu2(base_levels, abs_coeffs_base4);
; 575  : 
; 576  :         for (idx = 0; idx < num_non_zero; idx++) {

	movsxd	r12, r12d
	mov	ecx, 1431655765				; 55555555H

; 77   :   const __m256i threes  = _mm256_set1_epi16   (3);

	xor	r13d, r13d

; 557  : 
; 558  :         uint32_t abs_coeffs_base4 = pack_16x16b_to_16x2b(abs_coeffs);
; 559  : 
; 560  :         const uint32_t ones_base4 = 0x55555555;
; 561  :         const uint32_t twos_base4 = 0xaaaaaaaa;
; 562  : 
; 563  :         const uint32_t c1flag_number_mask_inv = 0xffffffff << (C1FLAG_NUMBER << 1);
; 564  :         const uint32_t c1flag_number_mask     = ~c1flag_number_mask_inv;
; 565  : 
; 566  :         // The addition will not overflow between 2-bit atoms because
; 567  :         // first_coeff2s will only be 1 or 0, and the other addend is 2
; 568  :         uint32_t first_coeff2s    = _bzhi_u32(ones_base4, first_acgt1_bpos + 2);

	bzhi	eax, ecx, eax

; 569  :         uint32_t base_levels      = first_coeff2s + twos_base4;

	sub	eax, 21846				; 00005556H

; 77   :   const __m256i threes  = _mm256_set1_epi16   (3);

	xor	esi, esi

; 78   :   const __m256i cmpmask = _mm256_slli_epi16   (threes, 7); // 0x0180 (avoid set1)

	vpsllw	ymm2, ymm0, 7

; 79   : 
; 80   :   __m256i  clipped      = _mm256_min_epu16    (src, threes);

	vpminuw	ymm0, ymm0, YMMWORD PTR abs_coeff$[rbp]

; 570  : 
; 571  :         base_levels &= c1flag_number_mask;

	movzx	r9d, ax

; 572  :         base_levels |= (ones_base4 & c1flag_number_mask_inv);

	or	r9d, 1431633920				; 55550000H

; 81   :   __m256i  shifted      = _mm256_slli_epi16   (clipped, 7);

	vpsllw	ymm1, ymm0, 7

; 82   :   __m256i  cmpres       = _mm256_cmpeq_epi8   (shifted, cmpmask);

	vpcmpeqb ymm2, ymm1, ymm2

; 83   :   uint32_t result       = _mm256_movemask_epi8(cmpres);

	vpmovmskb r8d, ymm2

; 59   :   uint32_t a_gt_b          = _andn_u32(b, a);

	andn	edx, r8d, r9d

; 60   :   uint32_t a_ne_b          = a ^ b;

	mov	ecx, r9d

; 572  :         base_levels |= (ones_base4 & c1flag_number_mask_inv);

	mov	DWORD PTR base_levels$1$[rbp], r9d

; 60   :   uint32_t a_ne_b          = a ^ b;

	xor	ecx, r8d

; 61   :   uint32_t a_gt_b_sh       = a_gt_b << 1;

	lea	eax, DWORD PTR [rdx+rdx]

; 62   :   uint32_t lobit_tiebrk_hi = _andn_u32(a_ne_b, a_gt_b_sh);

	andn	ecx, ecx, eax

; 63   :   uint32_t res             = a_gt_b | lobit_tiebrk_hi;

	or	ecx, edx
	mov	DWORD PTR encode_decisions$1$[rbp], ecx

; 573  : 
; 574  :         uint32_t encode_decisions = u32vec_cmpgt_epu2(base_levels, abs_coeffs_base4);
; 575  : 
; 576  :         for (idx = 0; idx < num_non_zero; idx++) {

	test	r12, r12
	jle	$LN18@kvz_encode

; 467  :           cabac->cur_ctx = &baseCtx[ctx_sig];
; 468  :           CABAC_BIN(cabac, curr_sig, "sig_coeff_flag");

	lea	ebx, QWORD PTR [rsi+1]
	npad	2
$LL19@kvz_encode:

; 577  : 
; 578  :           uint32_t shift = idx << 1;

	lea	eax, DWORD PTR [r13*2]

; 579  :           uint32_t dont_encode_curr = (encode_decisions >> shift);
; 580  :           int16_t base_level        = (base_levels      >> shift) & 3;

	shrx	edx, r9d, eax
	and	dx, 3
	shrx	eax, ecx, eax

; 581  : 
; 582  :           uint16_t curr_abs_coeff = abs_coeff[idx];
; 583  : 
; 584  :           if (!(dont_encode_curr & 2)) {

	test	al, 2
	jne	SHORT $LN17@kvz_encode

; 585  :             uint16_t level_diff = curr_abs_coeff - base_level;

	movzx	r15d, WORD PTR abs_coeff$[rbp+rsi*2]
	movzx	ecx, r15w
	sub	cx, dx

; 586  :             if (!cabac->only_count && (encoder->cfg.crypto_features & KVZ_CRYPTO_TRANSF_COEFFS)) {

	cmp	BYTE PTR [rdi+28], 0
	jne	SHORT $LN38@kvz_encode
	mov	rax, QWORD PTR encoder$1$[rbp]
	test	BYTE PTR [rax+2372], 4
	je	SHORT $LN38@kvz_encode

; 587  :               kvz_cabac_write_coeff_remain_encry(state, cabac, level_diff, go_rice_param, base_level);

	movzx	eax, dx
	mov	r9d, r14d
	movzx	r8d, cx
	mov	rdx, rdi
	mov	rcx, QWORD PTR state$[rsp]
	mov	DWORD PTR [rsp+32], eax
	vzeroupper
	call	kvz_cabac_write_coeff_remain_encry

; 588  :             } else {

	jmp	SHORT $LN39@kvz_encode
$LN38@kvz_encode:

; 589  :               kvz_cabac_write_coeff_remain(cabac, level_diff, go_rice_param);

	movzx	edx, cx
	mov	r8d, r14d
	mov	rcx, rdi
	vzeroupper
	call	kvz_cabac_write_coeff_remain
$LN39@kvz_encode:

; 590  :             }
; 591  : 
; 592  :             if (curr_abs_coeff > 3 * (1 << go_rice_param)) {

	mov	r9d, DWORD PTR base_levels$1$[rbp]
	lea	ecx, DWORD PTR [rbx+rbx*2]
	cmp	r15d, ecx
	mov	ecx, DWORD PTR encode_decisions$1$[rbp]
	jle	SHORT $LN17@kvz_encode

; 593  :               go_rice_param = MIN(go_rice_param + 1, 4);

	inc	r14d
	cmp	r14d, 4
	jae	SHORT $LN57@kvz_encode
	rol	ebx, 1
	jmp	SHORT $LN17@kvz_encode
$LN57@kvz_encode:
	mov	ebx, 16
	lea	r14d, QWORD PTR [rbx-12]
$LN17@kvz_encode:

; 573  : 
; 574  :         uint32_t encode_decisions = u32vec_cmpgt_epu2(base_levels, abs_coeffs_base4);
; 575  : 
; 576  :         for (idx = 0; idx < num_non_zero; idx++) {

	inc	r13d
	inc	rsi
	cmp	rsi, r12
	jl	$LL19@kvz_encode
	mov	r15d, DWORD PTR c1$1$[rbp]
$LN18@kvz_encode:

; 393  : 
; 394  :   // significant_coeff_flag
; 395  :   for (i = scan_cg_last; i >= 0; i--) {

	mov	r11d, DWORD PTR i$1$[rbp]

; 594  :             }
; 595  :           }
; 596  : 
; 597  :         }
; 598  :       }
; 599  :     }
; 600  :     last_nz_pos_in_cg = -1;
; 601  :     first_nz_pos_in_cg = 16;
; 602  :     num_non_zero = 0;

	xor	eax, eax
	mov	r9d, DWORD PTR scan_pos_sig$1$[rbp]
	mov	r12d, eax
	mov	DWORD PTR num_non_zero$1$[rbp], eax
	dec	r11d
	mov	QWORD PTR $T2[rbp], rax
	mov	r13d, -1

; 603  :     coeff_signs = 0;

	mov	DWORD PTR coeff_signs$1$[rbp], eax
	mov	rax, QWORD PTR tv3174[rbp]
	sub	rax, 4
	mov	DWORD PTR first_nz_pos_in_cg$1$[rbp], 16
	mov	DWORD PTR i$1$[rbp], r11d
	mov	QWORD PTR tv3174[rbp], rax
	test	r11d, r11d
	jns	$LL10@kvz_encode
$LN9@kvz_encode:
	vzeroupper

; 604  :   }
; 605  : }

	lea	r11, QWORD PTR [rsp+9376]
	mov	rbx, QWORD PTR [r11+80]
	vmovaps	xmm6, XMMWORD PTR [r11-16]
	vmovaps	xmm7, XMMWORD PTR [r11-32]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm9, XMMWORD PTR [r11-64]
	vmovaps	xmm10, XMMWORD PTR [r11-80]
	vmovaps	xmm11, XMMWORD PTR [r11-96]
	vmovaps	xmm12, XMMWORD PTR [r11-112]
	vmovaps	xmm13, XMMWORD PTR [r11-128]
	vmovaps	xmm14, XMMWORD PTR [r11-144]
	vmovaps	xmm15, XMMWORD PTR [r11-160]
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r13
	pop	r12
	pop	rdi
	pop	rsi
	pop	rbp
	ret	0
kvz_encode_coeff_nxn_avx2 ENDP
_TEXT	ENDS
END
