; Listing generated by Microsoft (R) Optimizing Compiler Version 19.26.28806.0 

include listing.inc

INCLUDELIB OLDNAMES

cbf_masks DW	01fH
	DW	0fH
	DW	07H
	DW	03H
	DW	01H
	ORG $+6
g_sig_last_scan_16x16 DD 00H
	DD	04H
	DD	01H
	DD	08H
	DD	05H
	DD	02H
	DD	0cH
	DD	09H
	DD	06H
	DD	03H
	DD	0dH
	DD	0aH
	DD	07H
	DD	0eH
	DD	0bH
	DD	0fH
g_group_idx DB	00H
	DB	01H
	DB	02H
	DB	03H
	DB	04H
	DB	04H
	DB	05H
	DB	05H
	DB	06H
	DB	06H
	DB	06H
	DB	06H
	DB	07H
	DB	07H
	DB	07H
	DB	07H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
strategies_to_select DQ FLAT:$SG4294951385
	DQ	FLAT:kvz_array_checksum
	DQ	FLAT:$SG4294951384
	DQ	FLAT:kvz_array_md5
	DQ	FLAT:$SG4294951383
	DQ	FLAT:kvz_reg_sad
	DQ	FLAT:$SG4294951382
	DQ	FLAT:kvz_sad_4x4
	DQ	FLAT:$SG4294951381
	DQ	FLAT:kvz_sad_8x8
	DQ	FLAT:$SG4294951380
	DQ	FLAT:kvz_sad_16x16
	DQ	FLAT:$SG4294951379
	DQ	FLAT:kvz_sad_32x32
	DQ	FLAT:$SG4294951378
	DQ	FLAT:kvz_sad_64x64
	DQ	FLAT:$SG4294951377
	DQ	FLAT:kvz_satd_4x4
	DQ	FLAT:$SG4294951376
	DQ	FLAT:kvz_satd_8x8
	DQ	FLAT:$SG4294951375
	DQ	FLAT:kvz_satd_16x16
	DQ	FLAT:$SG4294951374
	DQ	FLAT:kvz_satd_32x32
	DQ	FLAT:$SG4294951373
	DQ	FLAT:kvz_satd_64x64
	DQ	FLAT:$SG4294951372
	DQ	FLAT:kvz_satd_any_size
	DQ	FLAT:$SG4294951371
	DQ	FLAT:kvz_sad_4x4_dual
	DQ	FLAT:$SG4294951370
	DQ	FLAT:kvz_sad_8x8_dual
	DQ	FLAT:$SG4294951369
	DQ	FLAT:kvz_sad_16x16_dual
	DQ	FLAT:$SG4294951368
	DQ	FLAT:kvz_sad_32x32_dual
	DQ	FLAT:$SG4294951367
	DQ	FLAT:kvz_sad_64x64_dual
	DQ	FLAT:$SG4294951366
	DQ	FLAT:kvz_satd_4x4_dual
	DQ	FLAT:$SG4294951365
	DQ	FLAT:kvz_satd_8x8_dual
	DQ	FLAT:$SG4294951364
	DQ	FLAT:kvz_satd_16x16_dual
	DQ	FLAT:$SG4294951363
	DQ	FLAT:kvz_satd_32x32_dual
	DQ	FLAT:$SG4294951362
	DQ	FLAT:kvz_satd_64x64_dual
	DQ	FLAT:$SG4294951361
	DQ	FLAT:kvz_satd_any_size_quad
	DQ	FLAT:$SG4294951360
	DQ	FLAT:kvz_pixels_calc_ssd
	DQ	FLAT:$SG4294951359
	DQ	FLAT:kvz_bipred_average
	DQ	FLAT:$SG4294951358
	DQ	FLAT:kvz_get_optimized_sad
	DQ	FLAT:$SG4294951357
	DQ	FLAT:kvz_ver_sad
	DQ	FLAT:$SG4294951356
	DQ	FLAT:kvz_hor_sad
	DQ	FLAT:$SG4294951355
	DQ	FLAT:kvz_pixel_var
	DQ	FLAT:$SG4294951354
	DQ	FLAT:kvz_fast_forward_dst_4x4
	DQ	FLAT:$SG4294951353
	DQ	FLAT:kvz_dct_4x4
	DQ	FLAT:$SG4294951352
	DQ	FLAT:kvz_dct_8x8
	DQ	FLAT:$SG4294951351
	DQ	FLAT:kvz_dct_16x16
	DQ	FLAT:$SG4294951350
	DQ	FLAT:kvz_dct_32x32
	DQ	FLAT:$SG4294951349
	DQ	FLAT:kvz_fast_inverse_dst_4x4
	DQ	FLAT:$SG4294951348
	DQ	FLAT:kvz_idct_4x4
	DQ	FLAT:$SG4294951347
	DQ	FLAT:kvz_idct_8x8
	DQ	FLAT:$SG4294951346
	DQ	FLAT:kvz_idct_16x16
	DQ	FLAT:$SG4294951345
	DQ	FLAT:kvz_idct_32x32
	DQ	FLAT:$SG4294951344
	DQ	FLAT:kvz_filter_hpel_blocks_hor_ver_luma
	DQ	FLAT:$SG4294951343
	DQ	FLAT:kvz_filter_hpel_blocks_diag_luma
	DQ	FLAT:$SG4294951342
	DQ	FLAT:kvz_filter_qpel_blocks_hor_ver_luma
	DQ	FLAT:$SG4294951341
	DQ	FLAT:kvz_filter_qpel_blocks_diag_luma
	DQ	FLAT:$SG4294951340
	DQ	FLAT:kvz_sample_quarterpel_luma
	DQ	FLAT:$SG4294951339
	DQ	FLAT:kvz_sample_octpel_chroma
	DQ	FLAT:$SG4294951338
	DQ	FLAT:kvz_sample_quarterpel_luma_hi
	DQ	FLAT:$SG4294951337
	DQ	FLAT:kvz_sample_octpel_chroma_hi
	DQ	FLAT:$SG4294951336
	DQ	FLAT:kvz_get_extended_block
	DQ	FLAT:$SG4294951335
	DQ	FLAT:kvz_quant
	DQ	FLAT:$SG4294951334
	DQ	FLAT:kvz_quantize_residual
	DQ	FLAT:$SG4294951333
	DQ	FLAT:kvz_dequant
	DQ	FLAT:$SG4294951332
	DQ	FLAT:kvz_coeff_abs_sum
	DQ	FLAT:$SG4294951331
	DQ	FLAT:kvz_fast_coeff_cost
	DQ	FLAT:$SG4294951330
	DQ	FLAT:kvz_angular_pred
	DQ	FLAT:$SG4294951329
	DQ	FLAT:kvz_intra_pred_planar
	DQ	FLAT:$SG4294951328
	DQ	FLAT:kvz_intra_pred_filtered_dc
	DQ	FLAT:$SG4294951327
	DQ	FLAT:kvz_sao_edge_ddistortion
	DQ	FLAT:$SG4294951326
	DQ	FLAT:kvz_calc_sao_edge_dir
	DQ	FLAT:$SG4294951325
	DQ	FLAT:kvz_sao_reconstruct_color
	DQ	FLAT:$SG4294951324
	DQ	FLAT:kvz_sao_band_ddistortion
	DQ	FLAT:$SG4294951323
	DQ	FLAT:kvz_encode_coeff_nxn
	DQ	0000000000000000H
	DQ	0000000000000000H
g_sig_last_scan_32x32 DD 00H
	DD	08H
	DD	01H
	DD	010H
	DD	09H
	DD	02H
	DD	018H
	DD	011H
	DD	0aH
	DD	03H
	DD	020H
	DD	019H
	DD	012H
	DD	0bH
	DD	04H
	DD	028H
	DD	021H
	DD	01aH
	DD	013H
	DD	0cH
	DD	05H
	DD	030H
	DD	029H
	DD	022H
	DD	01bH
	DD	014H
	DD	0dH
	DD	06H
	DD	038H
	DD	031H
	DD	02aH
	DD	023H
	DD	01cH
	DD	015H
	DD	0eH
	DD	07H
	DD	039H
	DD	032H
	DD	02bH
	DD	024H
	DD	01dH
	DD	016H
	DD	0fH
	DD	03aH
	DD	033H
	DD	02cH
	DD	025H
	DD	01eH
	DD	017H
	DD	03bH
	DD	034H
	DD	02dH
	DD	026H
	DD	01fH
	DD	03cH
	DD	035H
	DD	02eH
	DD	027H
	DD	03dH
	DD	036H
	DD	02fH
	DD	03eH
	DD	037H
	DD	03fH
g_min_in_group DB 00H
	DB	01H
	DB	02H
	DB	03H
	DB	04H
	DB	06H
	DB	08H
	DB	0cH
	DB	010H
	DB	018H
	ORG $+6
default_fast_coeff_cost_wts DD 03e282e88r	; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e26c094r			; 0.162844
	DD	04081ca43r			; 4.05594
	DD	04064203ar			; 3.56447
	DD	040db915ar			; 6.86149
	DD	03e03d189r			; 0.128729
	DD	04089fbafr			; 4.31197
	DD	0407c5771r			; 3.94284
	DD	040ddeed2r			; 6.9354
	DD	03de33ce6r			; 0.110956
	DD	0408ddcb1r			; 4.43319
	DD	0407c8738r			; 3.94575
	DD	040dc1618r			; 6.8777
	DD	03dc29cfer			; 0.095026
	DD	0408f7938r			; 4.48355
	DD	0408636aar			; 4.19417
	DD	040d90260r			; 6.78154
	DD	03d99b1b8r			; 0.075046
	DD	04094474cr			; 4.6337
	DD	04082b1b6r			; 4.08419
	DD	040d65aeer			; 6.6986
	DD	03d56bca5r			; 0.052426
	DD	0409ef37er			; 4.96722
	DD	04080dee8r			; 4.02721
	DD	040d19306r			; 6.5492
	DD	03d24bcaer			; 0.040219
	DD	040a489car			; 5.14182
	DD	0407ee3bdr			; 3.98265
	DD	040cec513r			; 6.46156
	DD	03d0fba88r			; 0.03509
	DD	040a628e7r			; 5.19249
	DD	040752e49r			; 3.83095
	DD	040cd642ar			; 6.41848
	DD	03cf47d80r			; 0.029845
	DD	040a6c5d0r			; 5.21165
	DD	040743073r			; 3.81546
	DD	040cb0dd8r			; 6.34544
	DD	03cc0b136r			; 0.023522
	DD	040aa4f92r			; 5.32221
	DD	040744224r			; 3.81654
	DD	040cb8aaar			; 6.36068
	DD	03cae87d3r			; 0.021305
	DD	040a73ac3r			; 5.22592
	DD	04075eeccr			; 3.8427
	DD	040ca6cd9r			; 6.32579
	DD	03c821294r			; 0.015878
	DD	040a5dbe0r			; 5.18309
	DD	0407d2f27r			; 3.956
	DD	040ca8cbdr			; 6.32968
	DD	03c2ae297r			; 0.01043
	DD	040a32ce4r			; 5.09923
	DD	04085a85fr			; 4.1768
	DD	040c9c5d6r			; 6.3054
	DD	03c0a2a91r			; 0.008433
	DD	040a0f7der			; 5.03026
	DD	040879a50r			; 4.23759
	DD	040c8a4eer			; 6.27013
	DD	03bd4fdf4r			; 0.0065
	DD	0409f0412r			; 4.96925
	DD	0408adc57r			; 4.3394
	DD	040c6f870r			; 6.21783
	DD	03ba18373r			; 0.004929
	DD	0409d8d50r			; 4.9235
	DD	0408e283fr			; 4.44241
	DD	040c5df6cr			; 6.18352
	DD	03b73775cr			; 0.003715
	DD	0409d4c75r			; 4.91558
	DD	0408dbb1br			; 4.42909
	DD	040c4029fr			; 6.12532
	DD	03b4a70d2r			; 0.003089
	DD	0409c48f7r			; 4.88391
	DD	040920260r			; 4.56279
	DD	040c5019dr			; 6.15645
	DD	03b219c9dr			; 0.002466
	DD	0409c31abr			; 4.88106
	DD	040942800r			; 4.62988
	DD	040c49088r			; 6.14264
	DD	03b0e25c8r			; 0.002169
	DD	0409c3d62r			; 4.88249
	DD	04094ae99r			; 4.64631
	DD	040c415d1r			; 6.12766
	DD	03b26dacbr			; 0.002546
	DD	040996304r			; 4.79334
	DD	0409acc16r			; 4.83741
	DD	040c6606br			; 6.19927
	DD	03aac3a86r			; 0.001314
	DD	04099e220r			; 4.80885
	DD	0409a81bdr			; 4.82834
	DD	040c7ca3cr			; 6.24344
	DD	03a9741d1r			; 0.001154
	DD	0409b9a72r			; 4.8626
	DD	0409b19aar			; 4.84688
	DD	040c693a5r			; 6.20552
	DD	03a80f990r			; 0.000984
	DD	0409bb993r			; 4.8664
	DD	0409b7fa2r			; 4.85933
	DD	040c7b565r			; 6.24089
	DD	03a551f82r			; 0.000813
	DD	0409b698ar			; 4.85663
	DD	0409d95bar			; 4.92453
	DD	040c963a4r			; 6.29341
	DD	03a91c087r			; 0.001112
	DD	04099419er			; 4.78926
	DD	040a050f0r			; 5.00988
	DD	040cddf8fr			; 6.43354
	DD	03a10b418r			; 0.000552
	DD	04098580ar			; 4.76075
	DD	040a2e4f1r			; 5.09045
	DD	040d32e1fr			; 6.59938
	DD	039ccff22r			; 0.000391
	DD	0409ec42cr			; 4.96145
	DD	040a38d95r			; 5.11103
	DD	040d8342fr			; 6.75637
	DD	039ae1049r			; 0.000332
	DD	0409f63f8r			; 4.98095
	DD	040a46b89r			; 5.13813
	DD	040dbc1e8r			; 6.86742
	DD	03952c387r			; 0.000201
	DD	040a5d297r			; 5.18196
	DD	04097af64r			; 4.74016
	DD	040cec07dr			; 6.461
	DD	0397ba882r			; 0.00024
	DD	040a5eeb7r			; 5.18539
	DD	0409bfeb0r			; 4.87484
	DD	040da3603r			; 6.81909
	DD	03908509cr			; 0.00013
	DD	040a8a6b5r			; 5.27035
	DD	040977eacr			; 4.73421
	DD	040da708fr			; 6.82624
	DD	038da1a93r			; 0.000104
	DD	040abe6e8r			; 5.37194
	DD	040930af4r			; 4.59509
	DD	040d5189ar			; 6.65925
	DD	038ae1049r			; 8.3e-05
	DD	040ab9581r			; 5.362
	DD	04093c250r			; 4.61747
	DD	040dacf03r			; 6.83777
	DD	03890b418r			; 6.9e-05
	DD	040a926e3r			; 5.286
	DD	0409828e7r			; 4.75499
	DD	040e516e1r			; 7.15904
	DD	0384d8559r			; 4.9e-05
	DD	040afa18cr			; 5.48847
	DD	0408cace9r			; 4.39611
	DD	040d74682r			; 6.72736
	DD	038734507r			; 5.8e-05
	DD	0409eafa3r			; 4.95894
	DD	040929321r			; 4.58046
	DD	040cf49a5r			; 6.47774
	DD	037eae18br			; 2.8e-05
	DD	040b0ae1br			; 5.52125
	DD	0408e1885r			; 4.44049
	DD	040e68f80r			; 7.20502
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	0379f6230r			; 1.9e-05
	DD	040b9f5d8r			; 5.81126
	DD	0408cc582r			; 4.39911
	DD	040eac30dr			; 7.33631
g_sao_edge_offsets DD 0ffffffffH
	DD	00H
	DD	01H
	DD	00H
	DD	00H
	DD	0ffffffffH
	DD	00H
	DD	01H
	DD	0ffffffffH
	DD	0ffffffffH
	DD	01H
	DD	01H
	DD	01H
	DD	0ffffffffH
	DD	0ffffffffH
	DD	01H
g_sig_last_scan_cg DQ FLAT:g_sig_last_scan_8x8
	DQ	FLAT:g_sig_last_scan_8x8+16
	DQ	FLAT:g_sig_last_scan_8x8+32
	DQ	FLAT:g_sig_last_scan_8x8
	DQ	FLAT:g_sig_last_scan_8x8+16
	DQ	FLAT:g_sig_last_scan_8x8+32
	DQ	FLAT:g_sig_last_scan_16x16
	DQ	0000000000000000H
	DQ	0000000000000000H
	DQ	FLAT:g_sig_last_scan_32x32
	DQ	0000000000000000H
	DQ	0000000000000000H
g_sig_last_scan_8x8 DD 00H
	DD	02H
	DD	01H
	DD	03H
	DD	00H
	DD	01H
	DD	02H
	DD	03H
	DD	00H
	DD	02H
	DD	01H
	DD	03H
	ORG $+3
$SG4294951335 DB 'quant', 00H
	ORG $+7
$SG4294951385 DB 'array_checksum', 00H
	ORG $+1
$SG4294951384 DB 'array_md5', 00H
	ORG $+6
$SG4294951383 DB 'reg_sad', 00H
$SG4294951382 DB 'sad_4x4', 00H
$SG4294951381 DB 'sad_8x8', 00H
$SG4294951380 DB 'sad_16x16', 00H
	ORG $+6
$SG4294951379 DB 'sad_32x32', 00H
	ORG $+6
$SG4294951378 DB 'sad_64x64', 00H
	ORG $+6
$SG4294951377 DB 'satd_4x4', 00H
	ORG $+7
$SG4294951376 DB 'satd_8x8', 00H
	ORG $+7
$SG4294951375 DB 'satd_16x16', 00H
	ORG $+5
$SG4294951374 DB 'satd_32x32', 00H
	ORG $+5
$SG4294951373 DB 'satd_64x64', 00H
	ORG $+5
$SG4294951372 DB 'satd_any_size', 00H
	ORG $+2
$SG4294951371 DB 'sad_4x4_dual', 00H
	ORG $+3
$SG4294951370 DB 'sad_8x8_dual', 00H
	ORG $+3
$SG4294951369 DB 'sad_16x16_dual', 00H
	ORG $+1
$SG4294951368 DB 'sad_32x32_dual', 00H
	ORG $+1
$SG4294951367 DB 'sad_64x64_dual', 00H
	ORG $+1
$SG4294951366 DB 'satd_4x4_dual', 00H
	ORG $+2
$SG4294951365 DB 'satd_8x8_dual', 00H
	ORG $+2
$SG4294951364 DB 'satd_16x16_dual', 00H
$SG4294951363 DB 'satd_32x32_dual', 00H
$SG4294951362 DB 'satd_64x64_dual', 00H
$SG4294951361 DB 'satd_any_size_quad', 00H
	ORG $+5
$SG4294951360 DB 'pixels_calc_ssd', 00H
$SG4294951359 DB 'bipred_average', 00H
	ORG $+1
$SG4294951358 DB 'get_optimized_sad', 00H
	ORG $+6
$SG4294951357 DB 'ver_sad', 00H
$SG4294951356 DB 'hor_sad', 00H
$SG4294951355 DB 'pixel_var', 00H
	ORG $+6
$SG4294951354 DB 'fast_forward_dst_4x4', 00H
	ORG $+3
$SG4294951353 DB 'dct_4x4', 00H
$SG4294951352 DB 'dct_8x8', 00H
$SG4294951351 DB 'dct_16x16', 00H
	ORG $+6
$SG4294951350 DB 'dct_32x32', 00H
	ORG $+6
$SG4294951349 DB 'fast_inverse_dst_4x4', 00H
	ORG $+3
$SG4294951348 DB 'idct_4x4', 00H
	ORG $+7
$SG4294951347 DB 'idct_8x8', 00H
	ORG $+7
$SG4294951346 DB 'idct_16x16', 00H
	ORG $+5
$SG4294951345 DB 'idct_32x32', 00H
	ORG $+5
$SG4294951344 DB 'filter_hpel_blocks_hor_ver_luma', 00H
$SG4294951343 DB 'filter_hpel_blocks_diag_luma', 00H
	ORG $+3
$SG4294951342 DB 'filter_qpel_blocks_hor_ver_luma', 00H
$SG4294951341 DB 'filter_qpel_blocks_diag_luma', 00H
	ORG $+3
$SG4294951340 DB 'sample_quarterpel_luma', 00H
	ORG $+1
$SG4294951339 DB 'sample_octpel_chroma', 00H
	ORG $+3
$SG4294951338 DB 'sample_quarterpel_luma_hi', 00H
	ORG $+6
$SG4294951337 DB 'sample_octpel_chroma_hi', 00H
$SG4294951336 DB 'get_extended_block', 00H
	ORG $+5
$SG4294951334 DB 'quantize_residual', 00H
	ORG $+6
$SG4294951333 DB 'dequant', 00H
$SG4294951332 DB 'coeff_abs_sum', 00H
	ORG $+2
$SG4294951331 DB 'fast_coeff_cost', 00H
$SG4294951330 DB 'angular_pred', 00H
	ORG $+3
$SG4294951329 DB 'intra_pred_planar', 00H
	ORG $+6
$SG4294951328 DB 'intra_pred_filtered_dc', 00H
	ORG $+1
$SG4294951327 DB 'sao_edge_ddistortion', 00H
	ORG $+3
$SG4294951326 DB 'calc_sao_edge_dir', 00H
	ORG $+6
$SG4294951325 DB 'sao_reconstruct_color', 00H
	ORG $+2
$SG4294951324 DB 'sao_band_ddistortion', 00H
	ORG $+3
$SG4294951323 DB 'encode_coeff_nxn', 00H
PUBLIC	kvz_reg_sad_avx2
PUBLIC	kvz_strategy_register_picture_avx2
pdata	SEGMENT
$pdata$pixel_var_avx2_largebuf DD imagerel pixel_var_avx2_largebuf
	DD	imagerel pixel_var_avx2_largebuf+26
	DD	imagerel $unwind$pixel_var_avx2_largebuf
$pdata$1$pixel_var_avx2_largebuf DD imagerel pixel_var_avx2_largebuf+26
	DD	imagerel pixel_var_avx2_largebuf+200
	DD	imagerel $chain$1$pixel_var_avx2_largebuf
$pdata$2$pixel_var_avx2_largebuf DD imagerel pixel_var_avx2_largebuf+200
	DD	imagerel pixel_var_avx2_largebuf+345
	DD	imagerel $chain$2$pixel_var_avx2_largebuf
$pdata$3$pixel_var_avx2_largebuf DD imagerel pixel_var_avx2_largebuf+345
	DD	imagerel pixel_var_avx2_largebuf+397
	DD	imagerel $chain$3$pixel_var_avx2_largebuf
$pdata$4$pixel_var_avx2_largebuf DD imagerel pixel_var_avx2_largebuf+397
	DD	imagerel pixel_var_avx2_largebuf+617
	DD	imagerel $chain$4$pixel_var_avx2_largebuf
$pdata$hor_sad_avx2 DD imagerel hor_sad_avx2
	DD	imagerel hor_sad_avx2+301
	DD	imagerel $unwind$hor_sad_avx2
$pdata$13$hor_sad_avx2 DD imagerel hor_sad_avx2+301
	DD	imagerel hor_sad_avx2+735
	DD	imagerel $chain$13$hor_sad_avx2
$pdata$14$hor_sad_avx2 DD imagerel hor_sad_avx2+735
	DD	imagerel hor_sad_avx2+806
	DD	imagerel $chain$14$hor_sad_avx2
$pdata$16$hor_sad_avx2 DD imagerel hor_sad_avx2+806
	DD	imagerel hor_sad_avx2+2104
	DD	imagerel $chain$16$hor_sad_avx2
$pdata$17$hor_sad_avx2 DD imagerel hor_sad_avx2+2104
	DD	imagerel hor_sad_avx2+2635
	DD	imagerel $chain$17$hor_sad_avx2
$pdata$ver_sad_avx2 DD imagerel ver_sad_avx2
	DD	imagerel ver_sad_avx2+22
	DD	imagerel $unwind$ver_sad_avx2
$pdata$1$ver_sad_avx2 DD imagerel ver_sad_avx2+22
	DD	imagerel ver_sad_avx2+53
	DD	imagerel $chain$1$ver_sad_avx2
$pdata$3$ver_sad_avx2 DD imagerel ver_sad_avx2+53
	DD	imagerel ver_sad_avx2+181
	DD	imagerel $chain$3$ver_sad_avx2
$pdata$4$ver_sad_avx2 DD imagerel ver_sad_avx2+181
	DD	imagerel ver_sad_avx2+246
	DD	imagerel $chain$4$ver_sad_avx2
$pdata$6$ver_sad_avx2 DD imagerel ver_sad_avx2+246
	DD	imagerel ver_sad_avx2+457
	DD	imagerel $chain$6$ver_sad_avx2
$pdata$8$ver_sad_avx2 DD imagerel ver_sad_avx2+457
	DD	imagerel ver_sad_avx2+551
	DD	imagerel $chain$8$ver_sad_avx2
$pdata$10$ver_sad_avx2 DD imagerel ver_sad_avx2+551
	DD	imagerel ver_sad_avx2+738
	DD	imagerel $chain$10$ver_sad_avx2
$pdata$12$ver_sad_avx2 DD imagerel ver_sad_avx2+738
	DD	imagerel ver_sad_avx2+757
	DD	imagerel $chain$12$ver_sad_avx2
$pdata$bipred_average_avx2 DD imagerel bipred_average_avx2
	DD	imagerel bipred_average_avx2+8121
	DD	imagerel $unwind$bipred_average_avx2
$pdata$bipred_average_px_im_avx2 DD imagerel bipred_average_px_im_avx2
	DD	imagerel bipred_average_px_im_avx2+2574
	DD	imagerel $unwind$bipred_average_px_im_avx2
$pdata$0$bipred_average_px_im_avx2 DD imagerel bipred_average_px_im_avx2+2574
	DD	imagerel bipred_average_px_im_avx2+2802
	DD	imagerel $chain$0$bipred_average_px_im_avx2
$pdata$1$bipred_average_px_im_avx2 DD imagerel bipred_average_px_im_avx2+2802
	DD	imagerel bipred_average_px_im_avx2+3773
	DD	imagerel $chain$1$bipred_average_px_im_avx2
$pdata$bipred_average_px_im_template_avx2 DD imagerel bipred_average_px_im_template_avx2
	DD	imagerel bipred_average_px_im_template_avx2+628
	DD	imagerel $unwind$bipred_average_px_im_template_avx2
$pdata$3$bipred_average_px_im_template_avx2 DD imagerel bipred_average_px_im_template_avx2+628
	DD	imagerel bipred_average_px_im_template_avx2+960
	DD	imagerel $chain$3$bipred_average_px_im_template_avx2
$pdata$4$bipred_average_px_im_template_avx2 DD imagerel bipred_average_px_im_template_avx2+960
	DD	imagerel bipred_average_px_im_template_avx2+1411
	DD	imagerel $chain$4$bipred_average_px_im_template_avx2
$pdata$bipred_average_im_im_avx2 DD imagerel bipred_average_im_im_avx2
	DD	imagerel bipred_average_im_im_avx2+2514
	DD	imagerel $unwind$bipred_average_im_im_avx2
$pdata$0$bipred_average_im_im_avx2 DD imagerel bipred_average_im_im_avx2+2514
	DD	imagerel bipred_average_im_im_avx2+2733
	DD	imagerel $chain$0$bipred_average_im_im_avx2
$pdata$1$bipred_average_im_im_avx2 DD imagerel bipred_average_im_im_avx2+2733
	DD	imagerel bipred_average_im_im_avx2+3669
	DD	imagerel $chain$1$bipred_average_im_im_avx2
$pdata$bipred_average_im_im_template_avx2 DD imagerel bipred_average_im_im_template_avx2
	DD	imagerel bipred_average_im_im_template_avx2+610
	DD	imagerel $unwind$bipred_average_im_im_template_avx2
$pdata$3$bipred_average_im_im_template_avx2 DD imagerel bipred_average_im_im_template_avx2+610
	DD	imagerel bipred_average_im_im_template_avx2+918
	DD	imagerel $chain$3$bipred_average_im_im_template_avx2
$pdata$4$bipred_average_im_im_template_avx2 DD imagerel bipred_average_im_im_template_avx2+918
	DD	imagerel bipred_average_im_im_template_avx2+1350
	DD	imagerel $chain$4$bipred_average_im_im_template_avx2
$pdata$bipred_average_px_px_avx2 DD imagerel bipred_average_px_px_avx2
	DD	imagerel bipred_average_px_px_avx2+2405
	DD	imagerel $unwind$bipred_average_px_px_avx2
$pdata$bipred_average_px_px_template_avx2 DD imagerel bipred_average_px_px_template_avx2
	DD	imagerel bipred_average_px_px_template_avx2+842
	DD	imagerel $unwind$bipred_average_px_px_template_avx2
$pdata$pixels_calc_ssd_avx2 DD imagerel pixels_calc_ssd_avx2
	DD	imagerel pixels_calc_ssd_avx2+51
	DD	imagerel $unwind$pixels_calc_ssd_avx2
$pdata$4$pixels_calc_ssd_avx2 DD imagerel pixels_calc_ssd_avx2+51
	DD	imagerel pixels_calc_ssd_avx2+347
	DD	imagerel $chain$4$pixels_calc_ssd_avx2
$pdata$5$pixels_calc_ssd_avx2 DD imagerel pixels_calc_ssd_avx2+347
	DD	imagerel pixels_calc_ssd_avx2+515
	DD	imagerel $chain$5$pixels_calc_ssd_avx2
$pdata$satd_any_size_quad_avx2 DD imagerel satd_any_size_quad_avx2
	DD	imagerel satd_any_size_quad_avx2+748
	DD	imagerel $unwind$satd_any_size_quad_avx2
$pdata$satd_8bit_64x64_dual_avx2 DD imagerel satd_8bit_64x64_dual_avx2
	DD	imagerel satd_8bit_64x64_dual_avx2+187
	DD	imagerel $unwind$satd_8bit_64x64_dual_avx2
$pdata$satd_8bit_32x32_dual_avx2 DD imagerel satd_8bit_32x32_dual_avx2
	DD	imagerel satd_8bit_32x32_dual_avx2+187
	DD	imagerel $unwind$satd_8bit_32x32_dual_avx2
$pdata$satd_8bit_16x16_dual_avx2 DD imagerel satd_8bit_16x16_dual_avx2
	DD	imagerel satd_8bit_16x16_dual_avx2+187
	DD	imagerel $unwind$satd_8bit_16x16_dual_avx2
$pdata$satd_8bit_8x8_dual_avx2 DD imagerel satd_8bit_8x8_dual_avx2
	DD	imagerel satd_8bit_8x8_dual_avx2+94
	DD	imagerel $unwind$satd_8bit_8x8_dual_avx2
$pdata$satd_any_size_8bit_avx2 DD imagerel satd_any_size_8bit_avx2
	DD	imagerel satd_any_size_8bit_avx2+821
	DD	imagerel $unwind$satd_any_size_8bit_avx2
$pdata$satd_64x64_8bit_avx2 DD imagerel satd_64x64_8bit_avx2
	DD	imagerel satd_64x64_8bit_avx2+134
	DD	imagerel $unwind$satd_64x64_8bit_avx2
$pdata$satd_32x32_8bit_avx2 DD imagerel satd_32x32_8bit_avx2
	DD	imagerel satd_32x32_8bit_avx2+134
	DD	imagerel $unwind$satd_32x32_8bit_avx2
$pdata$satd_16x16_8bit_avx2 DD imagerel satd_16x16_8bit_avx2
	DD	imagerel satd_16x16_8bit_avx2+118
	DD	imagerel $unwind$satd_16x16_8bit_avx2
$pdata$satd_8x8_subblock_quad_avx2 DD imagerel satd_8x8_subblock_quad_avx2
	DD	imagerel satd_8x8_subblock_quad_avx2+149
	DD	imagerel $unwind$satd_8x8_subblock_quad_avx2
$pdata$satd_8x8_subblock_8bit_avx2 DD imagerel satd_8x8_subblock_8bit_avx2
	DD	imagerel satd_8x8_subblock_8bit_avx2+1074
	DD	imagerel $unwind$satd_8x8_subblock_8bit_avx2
$pdata$kvz_satd_8bit_8x8_general_dual_avx2 DD imagerel kvz_satd_8bit_8x8_general_dual_avx2
	DD	imagerel kvz_satd_8bit_8x8_general_dual_avx2+1222
	DD	imagerel $unwind$kvz_satd_8bit_8x8_general_dual_avx2
$pdata$hor_transform_block_dual_avx2 DD imagerel hor_transform_block_dual_avx2
	DD	imagerel hor_transform_block_dual_avx2+540
	DD	imagerel $unwind$hor_transform_block_dual_avx2
$pdata$hor_transform_block_avx2 DD imagerel hor_transform_block_avx2
	DD	imagerel hor_transform_block_avx2+513
	DD	imagerel $unwind$hor_transform_block_avx2
$pdata$diff_blocks_dual_avx2 DD imagerel diff_blocks_dual_avx2
	DD	imagerel diff_blocks_dual_avx2+463
	DD	imagerel $unwind$diff_blocks_dual_avx2
$pdata$sum_block_dual_avx2 DD imagerel sum_block_dual_avx2
	DD	imagerel sum_block_dual_avx2+235
	DD	imagerel $unwind$sum_block_dual_avx2
$pdata$ver_transform_block_dual_avx2 DD imagerel ver_transform_block_dual_avx2
	DD	imagerel ver_transform_block_dual_avx2+272
	DD	imagerel $unwind$ver_transform_block_dual_avx2
$pdata$ver_transform_block_avx2 DD imagerel ver_transform_block_avx2
	DD	imagerel ver_transform_block_avx2+239
	DD	imagerel $unwind$ver_transform_block_avx2
$pdata$hor_transform_row_dual_avx2 DD imagerel hor_transform_row_dual_avx2
	DD	imagerel hor_transform_row_dual_avx2+105
	DD	imagerel $unwind$hor_transform_row_dual_avx2
$pdata$hor_transform_row_avx2 DD imagerel hor_transform_row_avx2
	DD	imagerel hor_transform_row_avx2+102
	DD	imagerel $unwind$hor_transform_row_avx2
$pdata$satd_8bit_4x4_dual_avx2 DD imagerel satd_8bit_4x4_dual_avx2
	DD	imagerel satd_8bit_4x4_dual_avx2+230
	DD	imagerel $unwind$satd_8bit_4x4_dual_avx2
$pdata$sad_8bit_16x16_avx2 DD imagerel sad_8bit_16x16_avx2
	DD	imagerel sad_8bit_16x16_avx2+174
	DD	imagerel $unwind$sad_8bit_16x16_avx2
$pdata$inline_8bit_sad_16x16_avx2 DD imagerel inline_8bit_sad_16x16_avx2
	DD	imagerel inline_8bit_sad_16x16_avx2+149
	DD	imagerel $unwind$inline_8bit_sad_16x16_avx2
$pdata$kvz_reg_sad_avx2 DD imagerel $LN165
	DD	imagerel $LN165+928
	DD	imagerel $unwind$kvz_reg_sad_avx2
$pdata$5$kvz_reg_sad_avx2 DD imagerel $LN165+928
	DD	imagerel $LN165+2385
	DD	imagerel $chain$5$kvz_reg_sad_avx2
$pdata$6$kvz_reg_sad_avx2 DD imagerel $LN165+2385
	DD	imagerel $LN165+2420
	DD	imagerel $chain$6$kvz_reg_sad_avx2
$pdata$hor_sad_avx2_w32 DD imagerel hor_sad_avx2_w32
	DD	imagerel hor_sad_avx2_w32+368
	DD	imagerel $unwind$hor_sad_avx2_w32
$pdata$reg_sad_w64 DD imagerel reg_sad_w64
	DD	imagerel reg_sad_w64+298
	DD	imagerel $unwind$reg_sad_w64
$pdata$reg_sad_w32 DD imagerel reg_sad_w32
	DD	imagerel reg_sad_w32+13
	DD	imagerel $unwind$reg_sad_w32
$pdata$1$reg_sad_w32 DD imagerel reg_sad_w32+13
	DD	imagerel reg_sad_w32+62
	DD	imagerel $chain$1$reg_sad_w32
$pdata$2$reg_sad_w32 DD imagerel reg_sad_w32+62
	DD	imagerel reg_sad_w32+207
	DD	imagerel $chain$2$reg_sad_w32
$pdata$3$reg_sad_w32 DD imagerel reg_sad_w32+207
	DD	imagerel reg_sad_w32+222
	DD	imagerel $chain$3$reg_sad_w32
$pdata$4$reg_sad_w32 DD imagerel reg_sad_w32+222
	DD	imagerel reg_sad_w32+293
	DD	imagerel $chain$4$reg_sad_w32
$pdata$hor_sad_sse41_arbitrary DD imagerel hor_sad_sse41_arbitrary
	DD	imagerel hor_sad_sse41_arbitrary+413
	DD	imagerel $unwind$hor_sad_sse41_arbitrary
$pdata$0$hor_sad_sse41_arbitrary DD imagerel hor_sad_sse41_arbitrary+413
	DD	imagerel hor_sad_sse41_arbitrary+487
	DD	imagerel $chain$0$hor_sad_sse41_arbitrary
$pdata$2$hor_sad_sse41_arbitrary DD imagerel hor_sad_sse41_arbitrary+487
	DD	imagerel hor_sad_sse41_arbitrary+1712
	DD	imagerel $chain$2$hor_sad_sse41_arbitrary
$pdata$3$hor_sad_sse41_arbitrary DD imagerel hor_sad_sse41_arbitrary+1712
	DD	imagerel hor_sad_sse41_arbitrary+2197
	DD	imagerel $chain$3$hor_sad_sse41_arbitrary
$pdata$hor_sad_sse41_w16 DD imagerel hor_sad_sse41_w16
	DD	imagerel hor_sad_sse41_w16+47
	DD	imagerel $unwind$hor_sad_sse41_w16
$pdata$1$hor_sad_sse41_w16 DD imagerel hor_sad_sse41_w16+47
	DD	imagerel hor_sad_sse41_w16+139
	DD	imagerel $chain$1$hor_sad_sse41_w16
$pdata$2$hor_sad_sse41_w16 DD imagerel hor_sad_sse41_w16+139
	DD	imagerel hor_sad_sse41_w16+325
	DD	imagerel $chain$2$hor_sad_sse41_w16
$pdata$3$hor_sad_sse41_w16 DD imagerel hor_sad_sse41_w16+325
	DD	imagerel hor_sad_sse41_w16+340
	DD	imagerel $chain$3$hor_sad_sse41_w16
$pdata$4$hor_sad_sse41_w16 DD imagerel hor_sad_sse41_w16+340
	DD	imagerel hor_sad_sse41_w16+415
	DD	imagerel $chain$4$hor_sad_sse41_w16
$pdata$hor_sad_sse41_w8 DD imagerel hor_sad_sse41_w8
	DD	imagerel hor_sad_sse41_w8+27
	DD	imagerel $unwind$hor_sad_sse41_w8
$pdata$1$hor_sad_sse41_w8 DD imagerel hor_sad_sse41_w8+27
	DD	imagerel hor_sad_sse41_w8+145
	DD	imagerel $chain$1$hor_sad_sse41_w8
$pdata$4$hor_sad_sse41_w8 DD imagerel hor_sad_sse41_w8+145
	DD	imagerel hor_sad_sse41_w8+344
	DD	imagerel $chain$4$hor_sad_sse41_w8
$pdata$5$hor_sad_sse41_w8 DD imagerel hor_sad_sse41_w8+344
	DD	imagerel hor_sad_sse41_w8+359
	DD	imagerel $chain$5$hor_sad_sse41_w8
$pdata$6$hor_sad_sse41_w8 DD imagerel hor_sad_sse41_w8+359
	DD	imagerel hor_sad_sse41_w8+434
	DD	imagerel $chain$6$hor_sad_sse41_w8
$pdata$hor_sad_sse41_w4 DD imagerel hor_sad_sse41_w4
	DD	imagerel hor_sad_sse41_w4+98
	DD	imagerel $unwind$hor_sad_sse41_w4
$pdata$0$hor_sad_sse41_w4 DD imagerel hor_sad_sse41_w4+98
	DD	imagerel hor_sad_sse41_w4+164
	DD	imagerel $chain$0$hor_sad_sse41_w4
$pdata$1$hor_sad_sse41_w4 DD imagerel hor_sad_sse41_w4+164
	DD	imagerel hor_sad_sse41_w4+331
	DD	imagerel $chain$1$hor_sad_sse41_w4
$pdata$2$hor_sad_sse41_w4 DD imagerel hor_sad_sse41_w4+331
	DD	imagerel hor_sad_sse41_w4+343
	DD	imagerel $chain$2$hor_sad_sse41_w4
$pdata$3$hor_sad_sse41_w4 DD imagerel hor_sad_sse41_w4+343
	DD	imagerel hor_sad_sse41_w4+431
	DD	imagerel $chain$3$hor_sad_sse41_w4
$pdata$ver_sad_arbitrary DD imagerel ver_sad_arbitrary
	DD	imagerel ver_sad_arbitrary+525
	DD	imagerel $unwind$ver_sad_arbitrary
$pdata$ver_sad_w16 DD imagerel ver_sad_w16
	DD	imagerel ver_sad_w16+186
	DD	imagerel $unwind$ver_sad_w16
$pdata$ver_sad_w8 DD imagerel ver_sad_w8
	DD	imagerel ver_sad_w8+195
	DD	imagerel $unwind$ver_sad_w8
$pdata$ver_sad_w4 DD imagerel ver_sad_w4
	DD	imagerel ver_sad_w4+7
	DD	imagerel $unwind$ver_sad_w4
$pdata$1$ver_sad_w4 DD imagerel ver_sad_w4+7
	DD	imagerel ver_sad_w4+48
	DD	imagerel $chain$1$ver_sad_w4
$pdata$2$ver_sad_w4 DD imagerel ver_sad_w4+48
	DD	imagerel ver_sad_w4+142
	DD	imagerel $chain$2$ver_sad_w4
$pdata$3$ver_sad_w4 DD imagerel ver_sad_w4+142
	DD	imagerel ver_sad_w4+156
	DD	imagerel $chain$3$ver_sad_w4
$pdata$4$ver_sad_w4 DD imagerel ver_sad_w4+156
	DD	imagerel ver_sad_w4+217
	DD	imagerel $chain$4$ver_sad_w4
$pdata$reg_sad_arbitrary DD imagerel reg_sad_arbitrary
	DD	imagerel reg_sad_arbitrary+134
	DD	imagerel $unwind$reg_sad_arbitrary
$pdata$0$reg_sad_arbitrary DD imagerel reg_sad_arbitrary+134
	DD	imagerel reg_sad_arbitrary+439
	DD	imagerel $chain$0$reg_sad_arbitrary
$pdata$1$reg_sad_arbitrary DD imagerel reg_sad_arbitrary+439
	DD	imagerel reg_sad_arbitrary+465
	DD	imagerel $chain$1$reg_sad_arbitrary
$pdata$2$reg_sad_arbitrary DD imagerel reg_sad_arbitrary+465
	DD	imagerel reg_sad_arbitrary+662
	DD	imagerel $chain$2$reg_sad_arbitrary
$pdata$3$reg_sad_arbitrary DD imagerel reg_sad_arbitrary+662
	DD	imagerel reg_sad_arbitrary+752
	DD	imagerel $chain$3$reg_sad_arbitrary
$pdata$reg_sad_w24 DD imagerel reg_sad_w24
	DD	imagerel reg_sad_w24+39
	DD	imagerel $unwind$reg_sad_w24
$pdata$0$reg_sad_w24 DD imagerel reg_sad_w24+39
	DD	imagerel reg_sad_w24+66
	DD	imagerel $chain$0$reg_sad_w24
$pdata$1$reg_sad_w24 DD imagerel reg_sad_w24+66
	DD	imagerel reg_sad_w24+187
	DD	imagerel $chain$1$reg_sad_w24
$pdata$2$reg_sad_w24 DD imagerel reg_sad_w24+187
	DD	imagerel reg_sad_w24+197
	DD	imagerel $chain$2$reg_sad_w24
$pdata$3$reg_sad_w24 DD imagerel reg_sad_w24+197
	DD	imagerel reg_sad_w24+281
	DD	imagerel $chain$3$reg_sad_w24
$pdata$reg_sad_w16 DD imagerel reg_sad_w16
	DD	imagerel reg_sad_w16+14
	DD	imagerel $unwind$reg_sad_w16
$pdata$1$reg_sad_w16 DD imagerel reg_sad_w16+14
	DD	imagerel reg_sad_w16+60
	DD	imagerel $chain$1$reg_sad_w16
$pdata$2$reg_sad_w16 DD imagerel reg_sad_w16+60
	DD	imagerel reg_sad_w16+210
	DD	imagerel $chain$2$reg_sad_w16
$pdata$3$reg_sad_w16 DD imagerel reg_sad_w16+210
	DD	imagerel reg_sad_w16+225
	DD	imagerel $chain$3$reg_sad_w16
$pdata$4$reg_sad_w16 DD imagerel reg_sad_w16+225
	DD	imagerel reg_sad_w16+296
	DD	imagerel $chain$4$reg_sad_w16
$pdata$reg_sad_w12 DD imagerel reg_sad_w12
	DD	imagerel reg_sad_w12+97
	DD	imagerel $unwind$reg_sad_w12
$pdata$reg_sad_w8 DD imagerel reg_sad_w8
	DD	imagerel reg_sad_w8+12
	DD	imagerel $unwind$reg_sad_w8
$pdata$1$reg_sad_w8 DD imagerel reg_sad_w8+12
	DD	imagerel reg_sad_w8+53
	DD	imagerel $chain$1$reg_sad_w8
$pdata$2$reg_sad_w8 DD imagerel reg_sad_w8+53
	DD	imagerel reg_sad_w8+189
	DD	imagerel $chain$2$reg_sad_w8
$pdata$3$reg_sad_w8 DD imagerel reg_sad_w8+189
	DD	imagerel reg_sad_w8+204
	DD	imagerel $chain$3$reg_sad_w8
$pdata$4$reg_sad_w8 DD imagerel reg_sad_w8+204
	DD	imagerel reg_sad_w8+265
	DD	imagerel $chain$4$reg_sad_w8
$pdata$reg_sad_w4 DD imagerel reg_sad_w4
	DD	imagerel reg_sad_w4+17
	DD	imagerel $unwind$reg_sad_w4
$pdata$1$reg_sad_w4 DD imagerel reg_sad_w4+17
	DD	imagerel reg_sad_w4+66
	DD	imagerel $chain$1$reg_sad_w4
$pdata$2$reg_sad_w4 DD imagerel reg_sad_w4+66
	DD	imagerel reg_sad_w4+211
	DD	imagerel $chain$2$reg_sad_w4
$pdata$3$reg_sad_w4 DD imagerel reg_sad_w4+211
	DD	imagerel reg_sad_w4+226
	DD	imagerel $chain$3$reg_sad_w4
$pdata$4$reg_sad_w4 DD imagerel reg_sad_w4+226
	DD	imagerel reg_sad_w4+299
	DD	imagerel $chain$4$reg_sad_w4
$pdata$kvz_strategy_register_picture_avx2 DD imagerel $LN5
	DD	imagerel $LN5+1051
	DD	imagerel $unwind$kvz_strategy_register_picture_avx2
xdata	SEGMENT
$unwind$pixel_var_avx2_largebuf DD 051201H
	DD	0a812H
	DD	03780aH
	DD	0a204H
$chain$1$pixel_var_avx2_largebuf DD 040c21H
	DD	01980cH
	DD	028806H
	DD	imagerel pixel_var_avx2_largebuf
	DD	imagerel pixel_var_avx2_largebuf+26
	DD	imagerel $unwind$pixel_var_avx2_largebuf
$chain$2$pixel_var_avx2_largebuf DD 020621H
	DD	046806H
	DD	imagerel pixel_var_avx2_largebuf+26
	DD	imagerel pixel_var_avx2_largebuf+200
	DD	imagerel $chain$1$pixel_var_avx2_largebuf
$chain$3$pixel_var_avx2_largebuf DD 021H
	DD	imagerel pixel_var_avx2_largebuf+26
	DD	imagerel pixel_var_avx2_largebuf+200
	DD	imagerel $chain$1$pixel_var_avx2_largebuf
$chain$4$pixel_var_avx2_largebuf DD 021H
	DD	imagerel pixel_var_avx2_largebuf
	DD	imagerel pixel_var_avx2_largebuf+26
	DD	imagerel $unwind$pixel_var_avx2_largebuf
$unwind$hor_sad_avx2 DD 031201H
	DD	0420112H
	DD	0300bH
$chain$13$hor_sad_avx2 DD 01cb221H
	DD	014f8b2H
	DD	017c8a9H
	DD	019a88dH
	DD	01a9870H
	DD	01b885eH
	DD	01c7855H
	DD	01d684cH
	DD	03cf443H
	DD	03de43bH
	DD	03ed42bH
	DD	03fc423H
	DD	040741bH
	DD	0416413H
	DD	0445408H
	DD	imagerel hor_sad_avx2
	DD	imagerel hor_sad_avx2+301
	DD	imagerel $unwind$hor_sad_avx2
$chain$14$hor_sad_avx2 DD 020921H
	DD	018b809H
	DD	imagerel hor_sad_avx2+301
	DD	imagerel hor_sad_avx2+735
	DD	imagerel $chain$13$hor_sad_avx2
$chain$16$hor_sad_avx2 DD 041521H
	DD	015e815H
	DD	016d809H
	DD	imagerel hor_sad_avx2+735
	DD	imagerel hor_sad_avx2+806
	DD	imagerel $chain$14$hor_sad_avx2
$chain$17$hor_sad_avx2 DD 021H
	DD	imagerel hor_sad_avx2+735
	DD	imagerel hor_sad_avx2+806
	DD	imagerel $chain$14$hor_sad_avx2
$unwind$ver_sad_avx2 DD 010401H
	DD	06204H
$chain$1$ver_sad_avx2 DD 040a21H
	DD	06740aH
	DD	083405H
	DD	imagerel ver_sad_avx2
	DD	imagerel ver_sad_avx2+22
	DD	imagerel $unwind$ver_sad_avx2
$chain$3$ver_sad_avx2 DD 040d21H
	DD	0a640dH
	DD	095405H
	DD	imagerel ver_sad_avx2+22
	DD	imagerel ver_sad_avx2+53
	DD	imagerel $chain$1$ver_sad_avx2
$chain$4$ver_sad_avx2 DD 021H
	DD	imagerel ver_sad_avx2+22
	DD	imagerel ver_sad_avx2+53
	DD	imagerel $chain$1$ver_sad_avx2
$chain$6$ver_sad_avx2 DD 040021H
	DD	067400H
	DD	083400H
	DD	imagerel ver_sad_avx2
	DD	imagerel ver_sad_avx2+22
	DD	imagerel $unwind$ver_sad_avx2
$chain$8$ver_sad_avx2 DD 040021H
	DD	067400H
	DD	083400H
	DD	imagerel ver_sad_avx2
	DD	imagerel ver_sad_avx2+22
	DD	imagerel $unwind$ver_sad_avx2
$chain$10$ver_sad_avx2 DD 040021H
	DD	067400H
	DD	083400H
	DD	imagerel ver_sad_avx2
	DD	imagerel ver_sad_avx2+22
	DD	imagerel $unwind$ver_sad_avx2
$chain$12$ver_sad_avx2 DD 040021H
	DD	067400H
	DD	083400H
	DD	imagerel ver_sad_avx2
	DD	imagerel ver_sad_avx2+22
	DD	imagerel $unwind$ver_sad_avx2
$unwind$bipred_average_avx2 DD 082301H
	DD	0f01f7223H
	DD	0d01be01dH
	DD	07017c019H
	DD	030156016H
$unwind$bipred_average_px_im_avx2 DD 0144901H
	DD	03a849H
	DD	049844H
	DD	05883cH
	DD	067834H
	DD	076821H
	DD	016741cH
	DD	015641cH
	DD	014341cH
	DD	0f015f21cH
	DD	0c011e013H
$chain$0$bipred_average_px_im_avx2 DD 020621H
	DD	02b806H
	DD	imagerel bipred_average_px_im_avx2
	DD	imagerel bipred_average_px_im_avx2+2574
	DD	imagerel $unwind$bipred_average_px_im_avx2
$chain$1$bipred_average_px_im_avx2 DD 021H
	DD	imagerel bipred_average_px_im_avx2
	DD	imagerel bipred_average_px_im_avx2+2574
	DD	imagerel $unwind$bipred_average_px_im_avx2
$unwind$bipred_average_px_im_template_avx2 DD 0149d01H
	DD	04989dH
	DD	067897H
	DD	076891H
	DD	05882fH
	DD	017c421H
	DD	0167421H
	DD	0156421H
	DD	0143421H
	DD	0f01af221H
	DD	0d016e018H
$chain$3$bipred_average_px_im_template_avx2 DD 020621H
	DD	03a806H
	DD	imagerel bipred_average_px_im_template_avx2
	DD	imagerel bipred_average_px_im_template_avx2+628
	DD	imagerel $unwind$bipred_average_px_im_template_avx2
$chain$4$bipred_average_px_im_template_avx2 DD 021H
	DD	imagerel bipred_average_px_im_template_avx2
	DD	imagerel bipred_average_px_im_template_avx2+628
	DD	imagerel $unwind$bipred_average_px_im_template_avx2
$unwind$bipred_average_im_im_avx2 DD 0144901H
	DD	03a849H
	DD	049844H
	DD	05883cH
	DD	067834H
	DD	076821H
	DD	016741cH
	DD	015641cH
	DD	014341cH
	DD	0f015f21cH
	DD	0c011e013H
$chain$0$bipred_average_im_im_avx2 DD 020621H
	DD	02b806H
	DD	imagerel bipred_average_im_im_avx2
	DD	imagerel bipred_average_im_im_avx2+2514
	DD	imagerel $unwind$bipred_average_im_im_avx2
$chain$1$bipred_average_im_im_avx2 DD 021H
	DD	imagerel bipred_average_im_im_avx2
	DD	imagerel bipred_average_im_im_avx2+2514
	DD	imagerel $unwind$bipred_average_im_im_avx2
$unwind$bipred_average_im_im_template_avx2 DD 0149a01H
	DD	03989aH
	DD	057894H
	DD	06688eH
	DD	04882cH
	DD	015c41eH
	DD	014741eH
	DD	013641eH
	DD	012341eH
	DD	0f01ad21eH
	DD	0d016e018H
$chain$3$bipred_average_im_im_template_avx2 DD 020621H
	DD	02a806H
	DD	imagerel bipred_average_im_im_template_avx2
	DD	imagerel bipred_average_im_im_template_avx2+610
	DD	imagerel $unwind$bipred_average_im_im_template_avx2
$chain$4$bipred_average_im_im_template_avx2 DD 021H
	DD	imagerel bipred_average_im_im_template_avx2
	DD	imagerel bipred_average_im_im_template_avx2+610
	DD	imagerel $unwind$bipred_average_im_im_template_avx2
$unwind$bipred_average_px_px_avx2 DD 0a1901H
	DD	0a7419H
	DD	096419H
	DD	083419H
	DD	0f0153219H
	DD	0c011e013H
$unwind$bipred_average_px_px_template_avx2 DD 0c1e01H
	DD	0bc41eH
	DD	0a741eH
	DD	09641eH
	DD	08341eH
	DD	0f01a321eH
	DD	0d016e018H
$unwind$pixels_calc_ssd_avx2 DD 031101H
	DD	0700d8211H
	DD	0600cH
$chain$4$pixels_calc_ssd_avx2 DD 0a1421H
	DD	06f414H
	DD	07e410H
	DD	08d40cH
	DD	0fc408H
	DD	0e3404H
	DD	imagerel pixels_calc_ssd_avx2
	DD	imagerel pixels_calc_ssd_avx2+51
	DD	imagerel $unwind$pixels_calc_ssd_avx2
$chain$5$pixels_calc_ssd_avx2 DD 021H
	DD	imagerel pixels_calc_ssd_avx2
	DD	imagerel pixels_calc_ssd_avx2+51
	DD	imagerel $unwind$pixels_calc_ssd_avx2
$unwind$satd_any_size_quad_avx2 DD 0a8901H
	DD	017e489H
	DD	019341dH
	DD	0f016f21dH
	DD	0c012d014H
	DD	0600f7010H
$unwind$satd_8bit_64x64_dual_avx2 DD 0a1701H
	DD	0115417H
	DD	0103417H
	DD	0f0139217H
	DD	0c00fe011H
	DD	0600c700dH
$unwind$satd_8bit_32x32_dual_avx2 DD 0a1701H
	DD	0115417H
	DD	0103417H
	DD	0f0139217H
	DD	0c00fe011H
	DD	0600c700dH
$unwind$satd_8bit_16x16_dual_avx2 DD 0a1701H
	DD	0115417H
	DD	0103417H
	DD	0f0139217H
	DD	0c00fe011H
	DD	0600c700dH
$unwind$satd_8bit_8x8_dual_avx2 DD 020601H
	DD	030029206H
$unwind$satd_any_size_8bit_avx2 DD 0e5f01H
	DD	010f45fH
	DD	011c457H
	DD	012644fH
	DD	01a3447H
	DD	013011aH
	DD	0d011e013H
	DD	0500e700fH
$unwind$satd_64x64_8bit_avx2 DD 0a1801H
	DD	0a6418H
	DD	095418H
	DD	083418H
	DD	0f0143218H
	DD	07010e012H
$unwind$satd_32x32_8bit_avx2 DD 0a1801H
	DD	0a6418H
	DD	095418H
	DD	083418H
	DD	0f0143218H
	DD	07010e012H
$unwind$satd_16x16_8bit_avx2 DD 0a1801H
	DD	0a6418H
	DD	095418H
	DD	083418H
	DD	0f0143218H
	DD	07010e012H
$unwind$satd_8x8_subblock_quad_avx2 DD 0a1901H
	DD	0d7419H
	DD	0c6419H
	DD	0b5419H
	DD	0a3419H
	DD	0e0157219H
$unwind$satd_8x8_subblock_8bit_avx2 DD 0167401H
	DD	02f874H
	DD	03e86eH
	DD	04d85bH
	DD	05c855H
	DD	06b84fH
	DD	07a847H
	DD	089841H
	DD	098833H
	DD	0a782dH
	DD	0b6827H
	DD	019010aH
$unwind$kvz_satd_8bit_8x8_general_dual_avx2 DD 0179001H
	DD	0f890H
	DD	01e88bH
	DD	02d885H
	DD	03c87fH
	DD	04b879H
	DD	05a864H
	DD	06985eH
	DD	07884eH
	DD	08783eH
	DD	096838H
	DD	014010bH
	DD	03004H
$unwind$hor_transform_block_dual_avx2 DD 052c01H
	DD	0782cH
	DD	016823H
	DD	04204H
$unwind$hor_transform_block_avx2 DD 052701H
	DD	07827H
	DD	01681eH
	DD	04204H
$unwind$diff_blocks_dual_avx2 DD 040a01H
	DD	02740aH
	DD	013405H
$unwind$sum_block_dual_avx2 DD 0b5d01H
	DD	0a85dH
	DD	019835H
	DD	028828H
	DD	037823H
	DD	04681eH
	DD	0a207H
$unwind$ver_transform_block_dual_avx2 DD 0d3601H
	DD	0b836H
	DD	01a831H
	DD	02982cH
	DD	038827H
	DD	04781dH
	DD	056818H
	DD	0c207H
$unwind$ver_transform_block_avx2 DD 0d2e01H
	DD	0b82eH
	DD	01a829H
	DD	029824H
	DD	03881fH
	DD	047815H
	DD	056810H
	DD	0c207H
$unwind$hor_transform_row_dual_avx2 DD 031a01H
	DD	0681aH
	DD	02204H
$unwind$hor_transform_row_avx2 DD 031501H
	DD	06815H
	DD	02204H
$unwind$satd_8bit_4x4_dual_avx2 DD 010401H
	DD	02204H
$unwind$sad_8bit_16x16_avx2 DD 036301H
	DD	06863H
	DD	02204H
$unwind$inline_8bit_sad_16x16_avx2 DD 035701H
	DD	06857H
	DD	02204H
$unwind$kvz_reg_sad_avx2 DD 0c3701H
	DD	08f437H
	DD	09e432H
	DD	0fd42dH
	DD	0ec428H
	DD	070049208H
	DD	030026003H
$chain$5$kvz_reg_sad_avx2 DD 040c21H
	DD	02780cH
	DD	036806H
	DD	imagerel $LN165
	DD	imagerel $LN165+928
	DD	imagerel $unwind$kvz_reg_sad_avx2
$chain$6$kvz_reg_sad_avx2 DD 021H
	DD	imagerel $LN165
	DD	imagerel $LN165+928
	DD	imagerel $unwind$kvz_reg_sad_avx2
$unwind$hor_sad_avx2_w32 DD 0107d01H
	DD	0987dH
	DD	018869H
	DD	027835H
	DD	03682bH
	DD	0c7415H
	DD	0b6415H
	DD	0a3415H
	DD	0e0117215H
$unwind$reg_sad_w64 DD 0c2901H
	DD	06829H
	DD	07e41aH
	DD	06741aH
	DD	05641aH
	DD	04341aH
	DD	0f016121aH
$unwind$reg_sad_w32 DD 040701H
	DD	063407H
	DD	060067007H
$chain$1$reg_sad_w32 DD 040d21H
	DD	05f40dH
	DD	03c405H
	DD	imagerel reg_sad_w32
	DD	imagerel reg_sad_w32+13
	DD	imagerel $unwind$reg_sad_w32
$chain$2$reg_sad_w32 DD 020521H
	DD	04e405H
	DD	imagerel reg_sad_w32+13
	DD	imagerel reg_sad_w32+62
	DD	imagerel $chain$1$reg_sad_w32
$chain$3$reg_sad_w32 DD 021H
	DD	imagerel reg_sad_w32+13
	DD	imagerel reg_sad_w32+62
	DD	imagerel $chain$1$reg_sad_w32
$chain$4$reg_sad_w32 DD 021H
	DD	imagerel reg_sad_w32
	DD	imagerel reg_sad_w32+13
	DD	imagerel $unwind$reg_sad_w32
$unwind$hor_sad_sse41_arbitrary DD 0197201H
	DD	010f872H
	DD	013c86aH
	DD	015a862H
	DD	0169852H
	DD	017884dH
	DD	0187848H
	DD	0196843H
	DD	03c3421H
	DD	0340121H
	DD	0e018f01aH
	DD	0c014d016H
	DD	060117012H
	DD	05010H
$chain$0$hor_sad_sse41_arbitrary DD 020921H
	DD	014b809H
	DD	imagerel hor_sad_sse41_arbitrary
	DD	imagerel hor_sad_sse41_arbitrary+413
	DD	imagerel $unwind$hor_sad_sse41_arbitrary
$chain$2$hor_sad_sse41_arbitrary DD 041521H
	DD	011e815H
	DD	012d809H
	DD	imagerel hor_sad_sse41_arbitrary+413
	DD	imagerel hor_sad_sse41_arbitrary+487
	DD	imagerel $chain$0$hor_sad_sse41_arbitrary
$chain$3$hor_sad_sse41_arbitrary DD 021H
	DD	imagerel hor_sad_sse41_arbitrary+413
	DD	imagerel hor_sad_sse41_arbitrary+487
	DD	imagerel $chain$0$hor_sad_sse41_arbitrary
$unwind$hor_sad_sse41_w16 DD 060a01H
	DD	08340aH
	DD	07008e00aH
	DD	050066007H
$chain$1$hor_sad_sse41_w16 DD 041521H
	DD	06d415H
	DD	05c405H
	DD	imagerel hor_sad_sse41_w16
	DD	imagerel hor_sad_sse41_w16+47
	DD	imagerel $unwind$hor_sad_sse41_w16
$chain$2$hor_sad_sse41_w16 DD 020521H
	DD	07f405H
	DD	imagerel hor_sad_sse41_w16+47
	DD	imagerel hor_sad_sse41_w16+139
	DD	imagerel $chain$1$hor_sad_sse41_w16
$chain$3$hor_sad_sse41_w16 DD 021H
	DD	imagerel hor_sad_sse41_w16+47
	DD	imagerel hor_sad_sse41_w16+139
	DD	imagerel $chain$1$hor_sad_sse41_w16
$chain$4$hor_sad_sse41_w16 DD 021H
	DD	imagerel hor_sad_sse41_w16
	DD	imagerel hor_sad_sse41_w16+47
	DD	imagerel $unwind$hor_sad_sse41_w16
$unwind$hor_sad_sse41_w8 DD 040801H
	DD	070041208H
	DD	030026003H
$chain$1$hor_sad_sse41_w8 DD 043221H
	DD	09f432H
	DD	07c405H
	DD	imagerel hor_sad_sse41_w8
	DD	imagerel hor_sad_sse41_w8+27
	DD	imagerel $unwind$hor_sad_sse41_w8
$chain$4$hor_sad_sse41_w8 DD 061421H
	DD	06814H
	DD	08e40fH
	DD	065405H
	DD	imagerel hor_sad_sse41_w8+27
	DD	imagerel hor_sad_sse41_w8+145
	DD	imagerel $chain$1$hor_sad_sse41_w8
$chain$5$hor_sad_sse41_w8 DD 021H
	DD	imagerel hor_sad_sse41_w8+27
	DD	imagerel hor_sad_sse41_w8+145
	DD	imagerel $chain$1$hor_sad_sse41_w8
$chain$6$hor_sad_sse41_w8 DD 021H
	DD	imagerel hor_sad_sse41_w8
	DD	imagerel hor_sad_sse41_w8+27
	DD	imagerel $unwind$hor_sad_sse41_w8
$unwind$hor_sad_sse41_w4 DD 0a5c01H
	DD	0685cH
	DD	0a3410H
	DD	0f00c1210H
	DD	07008e00aH
	DD	050066007H
$chain$0$hor_sad_sse41_w4 DD 020521H
	DD	08c405H
	DD	imagerel hor_sad_sse41_w4
	DD	imagerel hor_sad_sse41_w4+98
	DD	imagerel $unwind$hor_sad_sse41_w4
$chain$1$hor_sad_sse41_w4 DD 020521H
	DD	09d405H
	DD	imagerel hor_sad_sse41_w4+98
	DD	imagerel hor_sad_sse41_w4+164
	DD	imagerel $chain$0$hor_sad_sse41_w4
$chain$2$hor_sad_sse41_w4 DD 021H
	DD	imagerel hor_sad_sse41_w4+98
	DD	imagerel hor_sad_sse41_w4+164
	DD	imagerel $chain$0$hor_sad_sse41_w4
$chain$3$hor_sad_sse41_w4 DD 021H
	DD	imagerel hor_sad_sse41_w4
	DD	imagerel hor_sad_sse41_w4+98
	DD	imagerel $unwind$hor_sad_sse41_w4
$unwind$ver_sad_arbitrary DD 0e2901H
	DD	06829H
	DD	0b641cH
	DD	09541cH
	DD	08341cH
	DD	0f018121cH
	DD	0d014e016H
	DD	07010c012H
$unwind$ver_sad_w16 DD 020501H
	DD	013405H
$unwind$ver_sad_w8 DD 040a01H
	DD	02740aH
	DD	013405H
$unwind$ver_sad_w4 DD 010201H
	DD	03002H
$chain$1$ver_sad_w4 DD 040c21H
	DD	03640cH
	DD	025405H
	DD	imagerel ver_sad_w4
	DD	imagerel ver_sad_w4+7
	DD	imagerel $unwind$ver_sad_w4
$chain$2$ver_sad_w4 DD 020521H
	DD	047405H
	DD	imagerel ver_sad_w4+7
	DD	imagerel ver_sad_w4+48
	DD	imagerel $chain$1$ver_sad_w4
$chain$3$ver_sad_w4 DD 021H
	DD	imagerel ver_sad_w4+7
	DD	imagerel ver_sad_w4+48
	DD	imagerel $chain$1$ver_sad_w4
$chain$4$ver_sad_w4 DD 021H
	DD	imagerel ver_sad_w4
	DD	imagerel ver_sad_w4+7
	DD	imagerel $unwind$ver_sad_w4
$unwind$reg_sad_arbitrary DD 0a2c01H
	DD	01782cH
	DD	0e00b520fH
	DD	0c007d009H
	DD	060047005H
	DD	030025003H
$chain$0$reg_sad_arbitrary DD 020521H
	DD	0ef405H
	DD	imagerel reg_sad_arbitrary
	DD	imagerel reg_sad_arbitrary+134
	DD	imagerel $unwind$reg_sad_arbitrary
$chain$1$reg_sad_arbitrary DD 021H
	DD	imagerel reg_sad_arbitrary
	DD	imagerel reg_sad_arbitrary+134
	DD	imagerel $unwind$reg_sad_arbitrary
$chain$2$reg_sad_arbitrary DD 020621H
	DD	026806H
	DD	imagerel reg_sad_arbitrary+439
	DD	imagerel reg_sad_arbitrary+465
	DD	imagerel $chain$1$reg_sad_arbitrary
$chain$3$reg_sad_arbitrary DD 021H
	DD	imagerel reg_sad_arbitrary+439
	DD	imagerel reg_sad_arbitrary+465
	DD	imagerel $chain$1$reg_sad_arbitrary
$unwind$reg_sad_w24 DD 0a2301H
	DD	016823H
	DD	097415H
	DD	086415H
	DD	075415H
	DD	0e0113215H
$chain$0$reg_sad_w24 DD 020521H
	DD	063405H
	DD	imagerel reg_sad_w24
	DD	imagerel reg_sad_w24+39
	DD	imagerel $unwind$reg_sad_w24
$chain$1$reg_sad_w24 DD 020521H
	DD	07805H
	DD	imagerel reg_sad_w24+39
	DD	imagerel reg_sad_w24+66
	DD	imagerel $chain$0$reg_sad_w24
$chain$2$reg_sad_w24 DD 021H
	DD	imagerel reg_sad_w24+39
	DD	imagerel reg_sad_w24+66
	DD	imagerel $chain$0$reg_sad_w24
$chain$3$reg_sad_w24 DD 021H
	DD	imagerel reg_sad_w24
	DD	imagerel reg_sad_w24+39
	DD	imagerel $unwind$reg_sad_w24
$unwind$reg_sad_w16 DD 050801H
	DD	073408H
	DD	060077008H
	DD	05006H
$chain$1$reg_sad_w16 DD 040d21H
	DD	06f40dH
	DD	04c405H
	DD	imagerel reg_sad_w16
	DD	imagerel reg_sad_w16+14
	DD	imagerel $unwind$reg_sad_w16
$chain$2$reg_sad_w16 DD 020521H
	DD	05e405H
	DD	imagerel reg_sad_w16+14
	DD	imagerel reg_sad_w16+60
	DD	imagerel $chain$1$reg_sad_w16
$chain$3$reg_sad_w16 DD 021H
	DD	imagerel reg_sad_w16+14
	DD	imagerel reg_sad_w16+60
	DD	imagerel $chain$1$reg_sad_w16
$chain$4$reg_sad_w16 DD 021H
	DD	imagerel reg_sad_w16
	DD	imagerel reg_sad_w16+14
	DD	imagerel $unwind$reg_sad_w16
$unwind$reg_sad_w12 DD 040a01H
	DD	02740aH
	DD	013405H
$unwind$reg_sad_w8 DD 030601H
	DD	053406H
	DD	07006H
$chain$1$reg_sad_w8 DD 040d21H
	DD	04e40dH
	DD	036405H
	DD	imagerel reg_sad_w8
	DD	imagerel reg_sad_w8+12
	DD	imagerel $unwind$reg_sad_w8
$chain$2$reg_sad_w8 DD 020521H
	DD	025405H
	DD	imagerel reg_sad_w8+12
	DD	imagerel reg_sad_w8+53
	DD	imagerel $chain$1$reg_sad_w8
$chain$3$reg_sad_w8 DD 021H
	DD	imagerel reg_sad_w8+12
	DD	imagerel reg_sad_w8+53
	DD	imagerel $chain$1$reg_sad_w8
$chain$4$reg_sad_w8 DD 021H
	DD	imagerel reg_sad_w8
	DD	imagerel reg_sad_w8+12
	DD	imagerel $unwind$reg_sad_w8
$unwind$reg_sad_w4 DD 060a01H
	DD	08340aH
	DD	07008e00aH
	DD	050066007H
$chain$1$reg_sad_w4 DD 040d21H
	DD	06d40dH
	DD	05c405H
	DD	imagerel reg_sad_w4
	DD	imagerel reg_sad_w4+17
	DD	imagerel $unwind$reg_sad_w4
$chain$2$reg_sad_w4 DD 020521H
	DD	07f405H
	DD	imagerel reg_sad_w4+17
	DD	imagerel reg_sad_w4+66
	DD	imagerel $chain$1$reg_sad_w4
$chain$3$reg_sad_w4 DD 021H
	DD	imagerel reg_sad_w4+17
	DD	imagerel reg_sad_w4+66
	DD	imagerel $chain$1$reg_sad_w4
$chain$4$reg_sad_w4 DD 021H
	DD	imagerel reg_sad_w4
	DD	imagerel reg_sad_w4+17
	DD	imagerel $unwind$reg_sad_w4
$unwind$kvz_strategy_register_picture_avx2 DD 081401H
	DD	0a6414H
	DD	095414H
	DD	083414H
	DD	070105214H
	ORG $+4
$SG4294951235 DB 'pixel_var', 00H
	ORG $+2
$SG4294951236 DB 'avx2', 00H
	ORG $+7
$SG4294951237 DB 'hor_sad', 00H
$SG4294951238 DB 'avx2', 00H
	ORG $+3
$SG4294951239 DB 'ver_sad', 00H
$SG4294951240 DB 'avx2', 00H
	ORG $+3
$SG4294951241 DB 'get_optimized_sad', 00H
	ORG $+2
$SG4294951242 DB 'avx2', 00H
	ORG $+7
$SG4294951243 DB 'bipred_average', 00H
	ORG $+1
$SG4294951244 DB 'avx2', 00H
	ORG $+3
$SG4294951245 DB 'pixels_calc_ssd', 00H
$SG4294951246 DB 'avx2', 00H
	ORG $+3
$SG4294951247 DB 'satd_any_size_quad', 00H
	ORG $+1
$SG4294951248 DB 'avx2', 00H
	ORG $+7
$SG4294951249 DB 'satd_any_size', 00H
	ORG $+2
$SG4294951250 DB 'avx2', 00H
	ORG $+3
$SG4294951251 DB 'satd_64x64_dual', 00H
$SG4294951252 DB 'avx2', 00H
	ORG $+3
$SG4294951253 DB 'satd_32x32_dual', 00H
$SG4294951254 DB 'avx2', 00H
	ORG $+3
$SG4294951255 DB 'satd_16x16_dual', 00H
$SG4294951256 DB 'avx2', 00H
	ORG $+3
$SG4294951257 DB 'satd_8x8_dual', 00H
	ORG $+2
$SG4294951258 DB 'avx2', 00H
	ORG $+3
$SG4294951259 DB 'satd_4x4_dual', 00H
	ORG $+2
$SG4294951260 DB 'avx2', 00H
	ORG $+3
$SG4294951261 DB 'satd_64x64', 00H
	ORG $+1
$SG4294951262 DB 'avx2', 00H
	ORG $+7
$SG4294951263 DB 'satd_32x32', 00H
	ORG $+1
$SG4294951264 DB 'avx2', 00H
	ORG $+7
$SG4294951265 DB 'satd_16x16', 00H
	ORG $+1
$SG4294951266 DB 'avx2', 00H
	ORG $+7
$SG4294951267 DB 'satd_8x8', 00H
	ORG $+3
$SG4294951268 DB 'avx2', 00H
	ORG $+7
$SG4294951269 DB 'satd_4x4', 00H
	ORG $+3
$SG4294951270 DB 'avx2', 00H
	ORG $+7
$SG4294951271 DB 'sad_64x64', 00H
	ORG $+2
$SG4294951272 DB 'avx2', 00H
	ORG $+7
$SG4294951273 DB 'sad_32x32', 00H
	ORG $+2
$SG4294951274 DB 'avx2', 00H
	ORG $+7
$SG4294951275 DB 'sad_16x16', 00H
	ORG $+2
$SG4294951276 DB 'avx2', 00H
	ORG $+7
$SG4294951277 DB 'sad_8x8', 00H
$SG4294951278 DB 'avx2', 00H
	ORG $+3
$SG4294951279 DB 'reg_sad', 00H
$SG4294951280 DB 'avx2', 00H
	ORG $+3
$SG4294951281 DB '0', 00H, ' ', 00H, '&', 00H, '&', 00H, ' ', 00H, '"', 00H
	DB	'U', 00H, 'n', 00H, 'e', 00H, 'x', 00H, 'p', 00H, 'e', 00H, 'c'
	DB	00H, 't', 00H, 'e', 00H, 'd', 00H, ' ', 00H, 'b', 00H, 'l', 00H
	DB	'o', 00H, 'c', 00H, 'k', 00H, ' ', 00H, 'w', 00H, 'i', 00H, 'd'
	DB	00H, 't', 00H, 'h', 00H, '.', 00H, '"', 00H, 00H, 00H
	ORG $+2
$SG4294951282 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'p', 00H, 'i', 00H, 'c', 00H, 't', 00H, 'u'
	DB	00H, 'r', 00H, 'e', 00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '.', 00H, 'c', 00H, 00H, 00H
	ORG $+2
$SG4294951283 DB '0', 00H, ' ', 00H, '&', 00H, '&', 00H, ' ', 00H, '"', 00H
	DB	'U', 00H, 'n', 00H, 'e', 00H, 'x', 00H, 'p', 00H, 'e', 00H, 'c'
	DB	00H, 't', 00H, 'e', 00H, 'd', 00H, ' ', 00H, 'b', 00H, 'l', 00H
	DB	'o', 00H, 'c', 00H, 'k', 00H, ' ', 00H, 'w', 00H, 'i', 00H, 'd'
	DB	00H, 't', 00H, 'h', 00H, '.', 00H, '"', 00H, 00H, 00H
$SG4294951284 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'p', 00H, 'i', 00H, 'c', 00H, 't', 00H, 'u'
	DB	00H, 'r', 00H, 'e', 00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '.', 00H, 'c', 00H, 00H, 00H
	ORG $+2
$SG4294951285 DB '0', 00H, ' ', 00H, '&', 00H, '&', 00H, ' ', 00H, '"', 00H
	DB	'U', 00H, 'n', 00H, 'e', 00H, 'x', 00H, 'p', 00H, 'e', 00H, 'c'
	DB	00H, 't', 00H, 'e', 00H, 'd', 00H, ' ', 00H, 'b', 00H, 'l', 00H
	DB	'o', 00H, 'c', 00H, 'k', 00H, ' ', 00H, 'w', 00H, 'i', 00H, 'd'
	DB	00H, 't', 00H, 'h', 00H, '.', 00H, '"', 00H, 00H, 00H
$SG4294951286 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'p', 00H, 'i', 00H, 'c', 00H, 't', 00H, 'u'
	DB	00H, 'r', 00H, 'e', 00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '.', 00H, 'c', 00H, 00H, 00H
	ORG $+2
$SG4294951287 DB '0', 00H, ' ', 00H, '&', 00H, '&', 00H, ' ', 00H, '"', 00H
	DB	'U', 00H, 'n', 00H, 'e', 00H, 'x', 00H, 'p', 00H, 'e', 00H, 'c'
	DB	00H, 't', 00H, 'e', 00H, 'd', 00H, ' ', 00H, 'b', 00H, 'l', 00H
	DB	'o', 00H, 'c', 00H, 'k', 00H, ' ', 00H, 'w', 00H, 'i', 00H, 'd'
	DB	00H, 't', 00H, 'h', 00H, '.', 00H, '"', 00H, 00H, 00H
$SG4294951288 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'p', 00H, 'i', 00H, 'c', 00H, 't', 00H, 'u'
	DB	00H, 'r', 00H, 'e', 00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '.', 00H, 'c', 00H, 00H, 00H
$SG4294951292 DB '!', 00H, '(', 00H, 'p', 00H, 'u', 00H, '_', 00H, 'w', 00H
	DB	' ', 00H, '=', 00H, '=', 00H, ' ', 00H, '4', 00H, ' ', 00H, '&'
	DB	00H, '&', 00H, ' ', 00H, 'p', 00H, 'u', 00H, '_', 00H, 'h', 00H
	DB	' ', 00H, '=', 00H, '=', 00H, ' ', 00H, '4', 00H, ')', 00H, ' '
	DB	00H, '&', 00H, '&', 00H, ' ', 00H, '"', 00H, 'B', 00H, 'r', 00H
	DB	'a', 00H, 'n', 00H, 'c', 00H, 'h', 00H, ' ', 00H, 'f', 00H, 'o'
	DB	00H, 'r', 00H, ' ', 00H, '4', 00H, 'x', 00H, '4', 00H, ' ', 00H
	DB	'n', 00H, 'o', 00H, 't', 00H, ' ', 00H, 'y', 00H, 'e', 00H, 't'
	DB	00H, ' ', 00H, 'i', 00H, 'm', 00H, 'p', 00H, 'l', 00H, 'e', 00H
	DB	'm', 00H, 'e', 00H, 'n', 00H, 't', 00H, 'e', 00H, 'd', 00H, '.'
	DB	00H, '"', 00H, 00H, 00H
$SG4294951293 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'p', 00H, 'i', 00H, 'c', 00H, 't', 00H, 'u'
	DB	00H, 'r', 00H, 'e', 00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '.', 00H, 'c', 00H, 00H, 00H
	ORG $+2
$SG4294951295 DB '0', 00H, ' ', 00H, '&', 00H, '&', 00H, ' ', 00H, '"', 00H
	DB	'U', 00H, 'n', 00H, 'e', 00H, 'x', 00H, 'p', 00H, 'e', 00H, 'c'
	DB	00H, 't', 00H, 'e', 00H, 'd', 00H, ' ', 00H, 'b', 00H, 'l', 00H
	DB	'o', 00H, 'c', 00H, 'k', 00H, ' ', 00H, 'w', 00H, 'i', 00H, 'd'
	DB	00H, 't', 00H, 'h', 00H, '.', 00H, '"', 00H, 00H, 00H
	ORG $+6
$SG4294951296 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'p', 00H, 'i', 00H, 'c', 00H, 't', 00H, 'u'
	DB	00H, 'r', 00H, 'e', 00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '.', 00H, 'c', 00H, 00H, 00H
	ORG $+2
$SG4294951297 DB '0', 00H, ' ', 00H, '&', 00H, '&', 00H, ' ', 00H, '"', 00H
	DB	'U', 00H, 'n', 00H, 'e', 00H, 'x', 00H, 'p', 00H, 'e', 00H, 'c'
	DB	00H, 't', 00H, 'e', 00H, 'd', 00H, ' ', 00H, 'b', 00H, 'l', 00H
	DB	'o', 00H, 'c', 00H, 'k', 00H, ' ', 00H, 'w', 00H, 'i', 00H, 'd'
	DB	00H, 't', 00H, 'h', 00H, '.', 00H, '"', 00H, 00H, 00H
$SG4294951298 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'p', 00H, 'i', 00H, 'c', 00H, 't', 00H, 'u'
	DB	00H, 'r', 00H, 'e', 00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '.', 00H, 'c', 00H, 00H, 00H
	ORG $+2
$SG4294951299 DB '0', 00H, ' ', 00H, '&', 00H, '&', 00H, ' ', 00H, '"', 00H
	DB	'U', 00H, 'n', 00H, 'e', 00H, 'x', 00H, 'p', 00H, 'e', 00H, 'c'
	DB	00H, 't', 00H, 'e', 00H, 'd', 00H, ' ', 00H, 'b', 00H, 'l', 00H
	DB	'o', 00H, 'c', 00H, 'k', 00H, ' ', 00H, 'w', 00H, 'i', 00H, 'd'
	DB	00H, 't', 00H, 'h', 00H, '.', 00H, '"', 00H, 00H, 00H
$SG4294951300 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'p', 00H, 'i', 00H, 'c', 00H, 't', 00H, 'u'
	DB	00H, 'r', 00H, 'e', 00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '.', 00H, 'c', 00H, 00H, 00H
	ORG $+2
$SG4294951301 DB '0', 00H, ' ', 00H, '&', 00H, '&', 00H, ' ', 00H, '"', 00H
	DB	'U', 00H, 'n', 00H, 'e', 00H, 'x', 00H, 'p', 00H, 'e', 00H, 'c'
	DB	00H, 't', 00H, 'e', 00H, 'd', 00H, ' ', 00H, 'b', 00H, 'l', 00H
	DB	'o', 00H, 'c', 00H, 'k', 00H, ' ', 00H, 'w', 00H, 'i', 00H, 'd'
	DB	00H, 't', 00H, 'h', 00H, '.', 00H, '"', 00H, 00H, 00H
$SG4294951302 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'p', 00H, 'i', 00H, 'c', 00H, 't', 00H, 'u'
	DB	00H, 'r', 00H, 'e', 00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '.', 00H, 'c', 00H, 00H, 00H
	ORG $+5
$SG4294951306 DB '!', 00H, '(', 00H, 'p', 00H, 'u', 00H, '_', 00H, 'w', 00H
	DB	' ', 00H, '=', 00H, '=', 00H, ' ', 00H, '4', 00H, ' ', 00H, '&'
	DB	00H, '&', 00H, ' ', 00H, 'p', 00H, 'u', 00H, '_', 00H, 'h', 00H
	DB	' ', 00H, '=', 00H, '=', 00H, ' ', 00H, '4', 00H, ')', 00H, ' '
	DB	00H, '&', 00H, '&', 00H, ' ', 00H, '"', 00H, 'B', 00H, 'r', 00H
	DB	'a', 00H, 'n', 00H, 'c', 00H, 'h', 00H, ' ', 00H, 'f', 00H, 'o'
	DB	00H, 'r', 00H, ' ', 00H, '4', 00H, 'x', 00H, '4', 00H, ' ', 00H
	DB	'n', 00H, 'o', 00H, 't', 00H, ' ', 00H, 'y', 00H, 'e', 00H, 't'
	DB	00H, ' ', 00H, 'i', 00H, 'm', 00H, 'p', 00H, 'l', 00H, 'e', 00H
	DB	'm', 00H, 'e', 00H, 'n', 00H, 't', 00H, 'e', 00H, 'd', 00H, '.'
	DB	00H, '"', 00H, 00H, 00H
$SG4294951307 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'p', 00H, 'i', 00H, 'c', 00H, 't', 00H, 'u'
	DB	00H, 'r', 00H, 'e', 00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '.', 00H, 'c', 00H, 00H, 00H
	ORG $+2
$SG4294951309 DB '0', 00H, ' ', 00H, '&', 00H, '&', 00H, ' ', 00H, '"', 00H
	DB	'U', 00H, 'n', 00H, 'e', 00H, 'x', 00H, 'p', 00H, 'e', 00H, 'c'
	DB	00H, 't', 00H, 'e', 00H, 'd', 00H, ' ', 00H, 'b', 00H, 'l', 00H
	DB	'o', 00H, 'c', 00H, 'k', 00H, ' ', 00H, 'w', 00H, 'i', 00H, 'd'
	DB	00H, 't', 00H, 'h', 00H, '.', 00H, '"', 00H, 00H, 00H
$SG4294951310 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'p', 00H, 'i', 00H, 'c', 00H, 't', 00H, 'u'
	DB	00H, 'r', 00H, 'e', 00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '.', 00H, 'c', 00H, 00H, 00H
	ORG $+2
$SG4294951311 DB '0', 00H, ' ', 00H, '&', 00H, '&', 00H, ' ', 00H, '"', 00H
	DB	'U', 00H, 'n', 00H, 'e', 00H, 'x', 00H, 'p', 00H, 'e', 00H, 'c'
	DB	00H, 't', 00H, 'e', 00H, 'd', 00H, ' ', 00H, 'b', 00H, 'l', 00H
	DB	'o', 00H, 'c', 00H, 'k', 00H, ' ', 00H, 'w', 00H, 'i', 00H, 'd'
	DB	00H, 't', 00H, 'h', 00H, '.', 00H, '"', 00H, 00H, 00H
$SG4294951312 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'p', 00H, 'i', 00H, 'c', 00H, 't', 00H, 'u'
	DB	00H, 'r', 00H, 'e', 00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '.', 00H, 'c', 00H, 00H, 00H
	ORG $+2
$SG4294951313 DB '0', 00H, ' ', 00H, '&', 00H, '&', 00H, ' ', 00H, '"', 00H
	DB	'U', 00H, 'n', 00H, 'e', 00H, 'x', 00H, 'p', 00H, 'e', 00H, 'c'
	DB	00H, 't', 00H, 'e', 00H, 'd', 00H, ' ', 00H, 'b', 00H, 'l', 00H
	DB	'o', 00H, 'c', 00H, 'k', 00H, ' ', 00H, 'w', 00H, 'i', 00H, 'd'
	DB	00H, 't', 00H, 'h', 00H, '.', 00H, '"', 00H, 00H, 00H
$SG4294951314 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'p', 00H, 'i', 00H, 'c', 00H, 't', 00H, 'u'
	DB	00H, 'r', 00H, 'e', 00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '.', 00H, 'c', 00H, 00H, 00H
	ORG $+2
$SG4294951315 DB '0', 00H, ' ', 00H, '&', 00H, '&', 00H, ' ', 00H, '"', 00H
	DB	'U', 00H, 'n', 00H, 'e', 00H, 'x', 00H, 'p', 00H, 'e', 00H, 'c'
	DB	00H, 't', 00H, 'e', 00H, 'd', 00H, ' ', 00H, 'b', 00H, 'l', 00H
	DB	'o', 00H, 'c', 00H, 'k', 00H, ' ', 00H, 'w', 00H, 'i', 00H, 'd'
	DB	00H, 't', 00H, 'h', 00H, '.', 00H, '"', 00H, 00H, 00H
$SG4294951316 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'p', 00H, 'i', 00H, 'c', 00H, 't', 00H, 'u'
	DB	00H, 'r', 00H, 'e', 00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '.', 00H, 'c', 00H, 00H, 00H
$SG4294951320 DB '!', 00H, '(', 00H, 'p', 00H, 'u', 00H, '_', 00H, 'w', 00H
	DB	' ', 00H, '=', 00H, '=', 00H, ' ', 00H, '4', 00H, ' ', 00H, '&'
	DB	00H, '&', 00H, ' ', 00H, 'p', 00H, 'u', 00H, '_', 00H, 'h', 00H
	DB	' ', 00H, '=', 00H, '=', 00H, ' ', 00H, '4', 00H, ')', 00H, ' '
	DB	00H, '&', 00H, '&', 00H, ' ', 00H, '"', 00H, 'B', 00H, 'r', 00H
	DB	'a', 00H, 'n', 00H, 'c', 00H, 'h', 00H, ' ', 00H, 'f', 00H, 'o'
	DB	00H, 'r', 00H, ' ', 00H, '4', 00H, 'x', 00H, '4', 00H, ' ', 00H
	DB	'n', 00H, 'o', 00H, 't', 00H, ' ', 00H, 'y', 00H, 'e', 00H, 't'
	DB	00H, ' ', 00H, 'i', 00H, 'm', 00H, 'p', 00H, 'l', 00H, 'e', 00H
	DB	'm', 00H, 'e', 00H, 'n', 00H, 't', 00H, 'e', 00H, 'd', 00H, '.'
	DB	00H, '"', 00H, 00H, 00H
$SG4294951321 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'p', 00H, 'i', 00H, 'c', 00H, 't', 00H, 'u'
	DB	00H, 'r', 00H, 'e', 00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '.', 00H, 'c', 00H, 00H, 00H
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
opaque$ = 64
bitdepth$dead$ = 72
kvz_strategy_register_picture_avx2 PROC

; 1716 : {

$LN5:
	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rbp
	mov	QWORD PTR [rsp+24], rsi
	push	rdi
	sub	rsp, 48					; 00000030H

; 1717 :   bool success = true;
; 1718 : #if COMPILE_INTEL_AVX2
; 1719 : #if KVZ_BIT_DEPTH == 8
; 1720 :   // We don't actually use SAD for intra right now, other than 4x4 for
; 1721 :   // transform skip, but we might again one day and this is some of the
; 1722 :   // simplest code to look at for anyone interested in doing more
; 1723 :   // optimizations, so it's worth it to keep this maintained.
; 1724 :   if (bitdepth == 8){
; 1725 : 
; 1726 :     success &= kvz_strategyselector_register(opaque, "reg_sad", "avx2", 40, &kvz_reg_sad_avx2);

	lea	rax, OFFSET FLAT:kvz_reg_sad_avx2
	mov	r9d, 40					; 00000028H
	lea	r8, OFFSET FLAT:$SG4294951280
	mov	QWORD PTR [rsp+32], rax
	lea	rdx, OFFSET FLAT:$SG4294951279
	mov	rbp, rcx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 1727 :     success &= kvz_strategyselector_register(opaque, "sad_8x8", "avx2", 40, &sad_8bit_8x8_avx2);

	lea	r8, OFFSET FLAT:$SG4294951278
	lea	rax, OFFSET FLAT:sad_8bit_8x8_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294951277
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rbp
	and	ebx, 1
	call	kvz_strategyselector_register
	mov	edi, eax

; 1728 :     success &= kvz_strategyselector_register(opaque, "sad_16x16", "avx2", 40, &sad_8bit_16x16_avx2);

	lea	r8, OFFSET FLAT:$SG4294951276
	lea	rax, OFFSET FLAT:sad_8bit_16x16_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294951275
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rbp
	and	edi, ebx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 1729 :     success &= kvz_strategyselector_register(opaque, "sad_32x32", "avx2", 40, &sad_8bit_32x32_avx2);

	lea	r8, OFFSET FLAT:$SG4294951274
	lea	rax, OFFSET FLAT:sad_8bit_32x32_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294951273
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rbp
	and	ebx, edi
	call	kvz_strategyselector_register
	mov	edi, eax

; 1730 :     success &= kvz_strategyselector_register(opaque, "sad_64x64", "avx2", 40, &sad_8bit_64x64_avx2);

	lea	r8, OFFSET FLAT:$SG4294951272
	lea	rax, OFFSET FLAT:sad_8bit_64x64_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294951271
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rbp
	and	edi, ebx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 1731 : 
; 1732 :     success &= kvz_strategyselector_register(opaque, "satd_4x4", "avx2", 40, &satd_4x4_8bit_avx2);

	lea	r8, OFFSET FLAT:$SG4294951270
	lea	rax, OFFSET FLAT:satd_4x4_8bit_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294951269
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rbp
	and	ebx, edi
	call	kvz_strategyselector_register
	mov	edi, eax

; 1733 :     success &= kvz_strategyselector_register(opaque, "satd_8x8", "avx2", 40, &satd_8x8_8bit_avx2);

	lea	r8, OFFSET FLAT:$SG4294951268
	lea	rax, OFFSET FLAT:satd_8x8_8bit_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294951267
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rbp
	and	edi, ebx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 1734 :     success &= kvz_strategyselector_register(opaque, "satd_16x16", "avx2", 40, &satd_16x16_8bit_avx2);

	lea	r8, OFFSET FLAT:$SG4294951266
	lea	rax, OFFSET FLAT:satd_16x16_8bit_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294951265
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rbp
	and	ebx, edi
	call	kvz_strategyselector_register
	mov	edi, eax

; 1735 :     success &= kvz_strategyselector_register(opaque, "satd_32x32", "avx2", 40, &satd_32x32_8bit_avx2);

	lea	r8, OFFSET FLAT:$SG4294951264
	lea	rax, OFFSET FLAT:satd_32x32_8bit_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294951263
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rbp
	and	edi, ebx
	call	kvz_strategyselector_register
	mov	ebx, eax
	and	ebx, edi

; 1736 :     success &= kvz_strategyselector_register(opaque, "satd_64x64", "avx2", 40, &satd_64x64_8bit_avx2);

	lea	rax, OFFSET FLAT:satd_64x64_8bit_avx2
	mov	r9d, 40					; 00000028H
	lea	r8, OFFSET FLAT:$SG4294951262
	mov	QWORD PTR [rsp+32], rax
	lea	rdx, OFFSET FLAT:$SG4294951261
	mov	rcx, rbp
	call	kvz_strategyselector_register
	xor	esi, esi

; 1737 : 
; 1738 :     success &= kvz_strategyselector_register(opaque, "satd_4x4_dual", "avx2", 40, &satd_8bit_4x4_dual_avx2);

	lea	r8, OFFSET FLAT:$SG4294951260
	mov	ecx, esi
	lea	rdx, OFFSET FLAT:$SG4294951259
	mov	edi, eax
	test	ebx, ebx
	lea	rax, OFFSET FLAT:satd_8bit_4x4_dual_avx2
	setne	cl
	mov	QWORD PTR [rsp+32], rax
	and	edi, ecx
	lea	r9d, QWORD PTR [rsi+40]
	mov	rcx, rbp
	call	kvz_strategyselector_register
	mov	ebx, eax

; 1739 :     success &= kvz_strategyselector_register(opaque, "satd_8x8_dual", "avx2", 40, &satd_8bit_8x8_dual_avx2);

	lea	r9d, QWORD PTR [rsi+40]
	lea	rax, OFFSET FLAT:satd_8bit_8x8_dual_avx2
	mov	rcx, rbp
	lea	r8, OFFSET FLAT:$SG4294951258
	mov	QWORD PTR [rsp+32], rax
	lea	rdx, OFFSET FLAT:$SG4294951257
	and	ebx, edi
	call	kvz_strategyselector_register
	mov	edi, eax

; 1740 :     success &= kvz_strategyselector_register(opaque, "satd_16x16_dual", "avx2", 40, &satd_8bit_16x16_dual_avx2);

	lea	r9d, QWORD PTR [rsi+40]
	lea	rax, OFFSET FLAT:satd_8bit_16x16_dual_avx2
	mov	rcx, rbp
	lea	r8, OFFSET FLAT:$SG4294951256
	mov	QWORD PTR [rsp+32], rax
	lea	rdx, OFFSET FLAT:$SG4294951255
	and	edi, ebx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 1741 :     success &= kvz_strategyselector_register(opaque, "satd_32x32_dual", "avx2", 40, &satd_8bit_32x32_dual_avx2);

	lea	r9d, QWORD PTR [rsi+40]
	lea	rax, OFFSET FLAT:satd_8bit_32x32_dual_avx2
	mov	rcx, rbp
	lea	r8, OFFSET FLAT:$SG4294951254
	mov	QWORD PTR [rsp+32], rax
	lea	rdx, OFFSET FLAT:$SG4294951253
	and	ebx, edi
	call	kvz_strategyselector_register
	mov	edi, eax

; 1742 :     success &= kvz_strategyselector_register(opaque, "satd_64x64_dual", "avx2", 40, &satd_8bit_64x64_dual_avx2);

	lea	r9d, QWORD PTR [rsi+40]
	lea	rax, OFFSET FLAT:satd_8bit_64x64_dual_avx2
	mov	rcx, rbp
	lea	r8, OFFSET FLAT:$SG4294951252
	mov	QWORD PTR [rsp+32], rax
	lea	rdx, OFFSET FLAT:$SG4294951251
	and	edi, ebx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 1743 :     success &= kvz_strategyselector_register(opaque, "satd_any_size", "avx2", 40, &satd_any_size_8bit_avx2);

	lea	r9d, QWORD PTR [rsi+40]
	lea	rax, OFFSET FLAT:satd_any_size_8bit_avx2
	mov	rcx, rbp
	lea	r8, OFFSET FLAT:$SG4294951250
	mov	QWORD PTR [rsp+32], rax
	lea	rdx, OFFSET FLAT:$SG4294951249
	and	ebx, edi
	call	kvz_strategyselector_register
	mov	edi, eax

; 1744 :     success &= kvz_strategyselector_register(opaque, "satd_any_size_quad", "avx2", 40, &satd_any_size_quad_avx2);

	lea	r9d, QWORD PTR [rsi+40]
	lea	rax, OFFSET FLAT:satd_any_size_quad_avx2
	mov	rcx, rbp
	lea	r8, OFFSET FLAT:$SG4294951248
	mov	QWORD PTR [rsp+32], rax
	lea	rdx, OFFSET FLAT:$SG4294951247
	and	edi, ebx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 1745 : 
; 1746 :     success &= kvz_strategyselector_register(opaque, "pixels_calc_ssd", "avx2", 40, &pixels_calc_ssd_avx2);

	lea	r9d, QWORD PTR [rsi+40]
	lea	rax, OFFSET FLAT:pixels_calc_ssd_avx2
	and	ebx, edi
	mov	QWORD PTR [rsp+32], rax
	lea	r8, OFFSET FLAT:$SG4294951246
	lea	rdx, OFFSET FLAT:$SG4294951245
	mov	rcx, rbp
	call	kvz_strategyselector_register
	mov	edi, eax

; 1747 :     success &= kvz_strategyselector_register(opaque, "bipred_average", "avx2", 40, &bipred_average_avx2);

	lea	r8, OFFSET FLAT:$SG4294951244
	lea	rax, OFFSET FLAT:bipred_average_avx2
	test	ebx, ebx
	mov	r9d, 40					; 00000028H
	mov	QWORD PTR [rsp+32], rax
	setne	sil
	lea	rdx, OFFSET FLAT:$SG4294951243
	mov	rcx, rbp
	and	edi, esi
	call	kvz_strategyselector_register
	mov	ebx, eax

; 1748 :     success &= kvz_strategyselector_register(opaque, "get_optimized_sad", "avx2", 40, &get_optimized_sad_avx2);

	lea	r8, OFFSET FLAT:$SG4294951242
	lea	rax, OFFSET FLAT:get_optimized_sad_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294951241
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rbp
	and	ebx, edi
	call	kvz_strategyselector_register
	mov	edi, eax

; 1749 :     success &= kvz_strategyselector_register(opaque, "ver_sad", "avx2", 40, &ver_sad_avx2);

	lea	r8, OFFSET FLAT:$SG4294951240
	lea	rax, OFFSET FLAT:ver_sad_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294951239
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rbp
	and	edi, ebx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 1750 :     success &= kvz_strategyselector_register(opaque, "hor_sad", "avx2", 40, &hor_sad_avx2);

	lea	r8, OFFSET FLAT:$SG4294951238
	lea	rax, OFFSET FLAT:hor_sad_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294951237
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rbp
	and	ebx, edi
	call	kvz_strategyselector_register
	mov	edi, eax

; 1751 : 
; 1752 :     success &= kvz_strategyselector_register(opaque, "pixel_var", "avx2", 40, &pixel_var_avx2);

	lea	r8, OFFSET FLAT:$SG4294951236
	lea	rax, OFFSET FLAT:pixel_var_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294951235
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rbp
	and	edi, ebx
	call	kvz_strategyselector_register

; 1753 : 
; 1754 :   }
; 1755 : #endif // KVZ_BIT_DEPTH == 8
; 1756 : #endif
; 1757 :   return success;
; 1758 : }

	mov	rbx, QWORD PTR [rsp+64]
	and	eax, edi
	mov	rbp, QWORD PTR [rsp+72]
	mov	rsi, QWORD PTR [rsp+80]
	add	rsp, 48					; 00000030H
	pop	rdi
	ret	0
kvz_strategy_register_picture_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
data1$ = 8
data2$ = 16
height$ = 24
stride1$ = 32
stride2$ = 40
reg_sad_w0 PROC

; 47   :   return 0;

	xor	eax, eax

; 48   : }

	ret	0
reg_sad_w0 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
data1$ = 40
data2$ = 48
height$ = 56
stride1$ = 64
stride2$ = 72
reg_sad_w4 PROC

; 53   : {

	mov	QWORD PTR [rsp+32], rbx
	push	rbp
	push	rsi
	push	rdi
	push	r14

; 55   :   int32_t y;
; 56   : 
; 57   :   const int32_t height_fourline_groups = height & ~3;
; 58   :   const int32_t height_residual_lines  = height &  3;
; 59   : 
; 60   :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r14d, DWORD PTR stride2$[rsp]
	xor	eax, eax
	mov	QWORD PTR [rsp+40], r12
	mov	edi, r9d
	mov	QWORD PTR [rsp+48], r13
	mov	r12d, r8d
	mov	r13d, r8d
	and	r12d, -4
	and	r13d, 3
	mov	ebx, r8d
	mov	rsi, rdx
	mov	rbp, rcx
	vpxor	xmm5, xmm5, xmm5
	test	r12d, r12d
	jle	$LN3@reg_sad_w4

; 54   :   __m128i sse_inc = _mm_setzero_si128();

	mov	QWORD PTR [rsp+56], r15
	mov	r15d, 2
	npad	3
$LL4@reg_sad_w4:

; 61   :     __m128i a = _mm_cvtsi32_si128(*(uint32_t *)(data1 + y * stride1));
; 62   :     __m128i b = _mm_cvtsi32_si128(*(uint32_t *)(data2 + y * stride2));
; 63   : 
; 64   :     a = _mm_insert_epi32(a, *(const uint32_t *)(data1 + (y + 1) * stride1), 1);

	lea	r11d, DWORD PTR [r15-1]
	mov	ecx, eax
	imul	ecx, edi

; 65   :     b = _mm_insert_epi32(b, *(const uint32_t *)(data2 + (y + 1) * stride2), 1);
; 66   :     a = _mm_insert_epi32(a, *(const uint32_t *)(data1 + (y + 2) * stride1), 2);
; 67   :     b = _mm_insert_epi32(b, *(const uint32_t *)(data2 + (y + 2) * stride2), 2);
; 68   :     a = _mm_insert_epi32(a, *(const uint32_t *)(data1 + (y + 3) * stride1), 3);

	lea	r10d, DWORD PTR [r15+1]
	mov	r8d, r15d
	mov	edx, r11d
	imul	r8d, edi
	mov	r9d, r10d
	imul	edx, edi
	imul	r11d, r14d
	imul	r9d, edi

; 69   :     b = _mm_insert_epi32(b, *(const uint32_t *)(data2 + (y + 3) * stride2), 3);

	imul	r10d, r14d
	vmovd	xmm0, DWORD PTR [rcx+rbp]
	mov	ecx, eax
	add	eax, 4
	imul	ecx, r14d
	vpinsrd	xmm1, xmm0, DWORD PTR [rdx+rbp], 1
	vpinsrd	xmm2, xmm1, DWORD PTR [r8+rbp], 2
	vpinsrd	xmm4, xmm2, DWORD PTR [r9+rbp], 3
	mov	r8d, r15d
	add	r15d, 4
	imul	r8d, r14d
	vmovd	xmm0, DWORD PTR [rcx+rsi]
	vpinsrd	xmm1, xmm0, DWORD PTR [r11+rsi], 1
	vpinsrd	xmm2, xmm1, DWORD PTR [r8+rsi], 2
	vpinsrd	xmm3, xmm2, DWORD PTR [r10+rsi], 3

; 70   : 
; 71   :     __m128i curr_sads = _mm_sad_epu8(a, b);

	vpsadbw	xmm0, xmm4, xmm3

; 72   :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm5, xmm0, xmm5
	cmp	eax, r12d
	jl	SHORT $LL4@reg_sad_w4

; 55   :   int32_t y;
; 56   : 
; 57   :   const int32_t height_fourline_groups = height & ~3;
; 58   :   const int32_t height_residual_lines  = height &  3;
; 59   : 
; 60   :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r15, QWORD PTR [rsp+56]
$LN3@reg_sad_w4:
	mov	r12, QWORD PTR [rsp+40]

; 73   :   }
; 74   :   if (height_residual_lines) {

	test	r13d, r13d
	mov	r13, QWORD PTR [rsp+48]
	je	SHORT $LN6@reg_sad_w4

; 75   :     for (; y < height; y++) {

	cmp	eax, ebx
	jge	SHORT $LN6@reg_sad_w4
	npad	10
$LL7@reg_sad_w4:

; 76   :       __m128i a = _mm_cvtsi32_si128(*(const uint32_t *)(data1 + y * stride1));

	mov	ecx, eax
	imul	ecx, edi
	vmovd	xmm1, DWORD PTR [rcx+rbp]

; 77   :       __m128i b = _mm_cvtsi32_si128(*(const uint32_t *)(data2 + y * stride2));

	mov	ecx, eax
	inc	eax
	imul	ecx, r14d
	vmovd	xmm0, DWORD PTR [rcx+rsi]

; 78   : 
; 79   :       __m128i curr_sads = _mm_sad_epu8(a, b);

	vpsadbw	xmm1, xmm1, xmm0

; 80   :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm5, xmm1, xmm5
	cmp	eax, ebx
	jl	SHORT $LL7@reg_sad_w4
$LN6@reg_sad_w4:

; 81   :     }
; 82   :   }
; 83   :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 84   :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 85   : 
; 86   :   return _mm_cvtsi128_si32(sad);
; 87   : }

	mov	rbx, QWORD PTR [rsp+64]
	vpshufd	xmm0, xmm5, 78				; 0000004eH
	vpaddq	xmm1, xmm0, xmm5
	vmovd	eax, xmm1
	pop	r14
	pop	rdi
	pop	rsi
	pop	rbp
	ret	0
reg_sad_w4 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
data1$ = 16
data2$ = 24
height$ = 32
stride1$ = 40
stride2$ = 48
reg_sad_w8 PROC

; 92   : {

	mov	QWORD PTR [rsp+32], rbx
	push	rdi

; 94   :   int32_t y;
; 95   : 
; 96   :   const int32_t height_fourline_groups = height & ~3;
; 97   :   const int32_t height_residual_lines  = height &  3;
; 98   : 
; 99   :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	edi, DWORD PTR stride2$[rsp]
	xor	eax, eax
	mov	QWORD PTR [rsp+24], rsi
	mov	r10d, r8d
	mov	QWORD PTR [rsp+32], r14
	mov	esi, r8d
	mov	r14d, r8d
	and	esi, -4
	and	r14d, 3
	mov	rbx, rcx
	vpxor	xmm4, xmm4, xmm4
	test	esi, esi
	jle	$LN3@reg_sad_w8

; 93   :   __m128i sse_inc = _mm_setzero_si128();

	mov	QWORD PTR [rsp+16], rbp
	mov	ebp, 2
	npad	1
$LL4@reg_sad_w8:

; 100  :     __m128d a_d = _mm_setzero_pd();
; 101  :     __m128d b_d = _mm_setzero_pd();
; 102  :     __m128d c_d = _mm_setzero_pd();
; 103  :     __m128d d_d = _mm_setzero_pd();
; 104  : 
; 105  :     a_d = _mm_loadl_pd(a_d, (const double *)(data1 + (y + 0) * stride1));

	mov	ecx, eax

; 106  :     b_d = _mm_loadl_pd(b_d, (const double *)(data2 + (y + 0) * stride2));
; 107  :     a_d = _mm_loadh_pd(a_d, (const double *)(data1 + (y + 1) * stride1));

	lea	r8d, DWORD PTR [rbp-1]
	imul	ecx, r9d
	vmovddup xmm2, QWORD PTR [rcx+rbx]
	mov	ecx, r8d

; 108  :     b_d = _mm_loadh_pd(b_d, (const double *)(data2 + (y + 1) * stride2));

	imul	r8d, edi
	imul	ecx, r9d
	vmovhpd	xmm2, xmm2, QWORD PTR [rcx+rbx]
	mov	ecx, eax
	add	eax, 4
	imul	ecx, edi
	vmovddup xmm0, QWORD PTR [rcx+rdx]
	vmovhpd	xmm0, xmm0, QWORD PTR [r8+rdx]

; 109  : 
; 110  :     c_d = _mm_loadl_pd(c_d, (const double *)(data1 + (y + 2) * stride1));

	mov	ecx, ebp

; 111  :     d_d = _mm_loadl_pd(d_d, (const double *)(data2 + (y + 2) * stride2));
; 112  :     c_d = _mm_loadh_pd(c_d, (const double *)(data1 + (y + 3) * stride1));

	lea	r8d, DWORD PTR [rbp+1]
	imul	ecx, r9d

; 113  :     d_d = _mm_loadh_pd(d_d, (const double *)(data2 + (y + 3) * stride2));
; 114  : 
; 115  :     __m128i a = _mm_castpd_si128(a_d);
; 116  :     __m128i b = _mm_castpd_si128(b_d);
; 117  :     __m128i c = _mm_castpd_si128(c_d);
; 118  :     __m128i d = _mm_castpd_si128(d_d);
; 119  : 
; 120  :     __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	vpsadbw	xmm0, xmm2, xmm0

; 121  :     __m128i curr_sads_cd = _mm_sad_epu8(c, d);
; 122  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm2, xmm0, xmm4
	vmovddup xmm3, QWORD PTR [rcx+rbx]
	mov	ecx, r8d
	imul	r8d, edi
	imul	ecx, r9d
	vmovhpd	xmm3, xmm3, QWORD PTR [rcx+rbx]
	mov	ecx, ebp
	add	ebp, 4
	imul	ecx, edi
	vmovddup xmm1, QWORD PTR [rcx+rdx]
	vmovhpd	xmm1, xmm1, QWORD PTR [r8+rdx]
	vpsadbw	xmm1, xmm3, xmm1

; 123  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vpaddq	xmm4, xmm2, xmm1
	cmp	eax, esi
	jl	SHORT $LL4@reg_sad_w8

; 94   :   int32_t y;
; 95   : 
; 96   :   const int32_t height_fourline_groups = height & ~3;
; 97   :   const int32_t height_residual_lines  = height &  3;
; 98   : 
; 99   :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	rbp, QWORD PTR [rsp+16]
$LN3@reg_sad_w8:
	mov	rsi, QWORD PTR [rsp+24]

; 124  :   }
; 125  :   if (height_residual_lines) {

	test	r14d, r14d
	mov	r14, QWORD PTR [rsp+32]
	je	SHORT $LN6@reg_sad_w8

; 126  :     for (; y < height; y++) {

	cmp	eax, r10d
	jge	SHORT $LN6@reg_sad_w8
$LL7@reg_sad_w8:

; 127  :       __m128i a = _mm_loadl_epi64((__m128i *)(data1 + y * stride1));

	mov	ecx, eax
	imul	ecx, r9d
	vmovq	xmm1, QWORD PTR [rcx+rbx]

; 128  :       __m128i b = _mm_loadl_epi64((__m128i *)(data2 + y * stride2));

	mov	ecx, eax
	inc	eax
	imul	ecx, edi
	vmovq	xmm0, QWORD PTR [rcx+rdx]

; 129  : 
; 130  :       __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	vpsadbw	xmm1, xmm1, xmm0

; 131  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm4, xmm1, xmm4
	cmp	eax, r10d
	jl	SHORT $LL7@reg_sad_w8
$LN6@reg_sad_w8:

; 132  :     }
; 133  :   }
; 134  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 135  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 136  : 
; 137  :   return _mm_cvtsi128_si32(sad);
; 138  : }

	mov	rbx, QWORD PTR [rsp+40]
	vpshufd	xmm0, xmm4, 78				; 0000004eH
	vpaddq	xmm1, xmm0, xmm4
	vmovd	eax, xmm1
	pop	rdi
	ret	0
reg_sad_w8 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
data1$ = 8
data2$ = 16
height$ = 24
stride1$ = 32
stride2$ = 40
reg_sad_w12 PROC

; 143  : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rdi

; 144  :   __m128i sse_inc = _mm_setzero_si128();

	xor	eax, eax
	mov	r11d, r9d
	mov	rdi, rcx
	vpxor	xmm2, xmm2, xmm2

; 145  :   int32_t y;
; 146  :   for (y = 0; y < height; y++) {

	test	r8d, r8d
	jle	SHORT $LN3@reg_sad_w1
	mov	r10d, DWORD PTR stride2$[rsp]
$LL4@reg_sad_w1:

; 147  :     __m128i a = _mm_loadu_si128((const __m128i *)(data1 + y * stride1));

	mov	r9d, eax

; 148  :     __m128i b = _mm_loadu_si128((const __m128i *)(data2 + y * stride2));

	mov	ecx, eax
	imul	r9d, r11d
	inc	eax
	imul	ecx, r10d
	vmovdqu	xmm1, XMMWORD PTR [r9+rdi]

; 149  : 
; 150  :     __m128i b_masked  = _mm_blend_epi16(a, b, 0x3f);

	vpblendw xmm0, xmm1, XMMWORD PTR [rcx+rdx], 63	; 0000003fH

; 151  :     __m128i curr_sads = _mm_sad_epu8   (a, b_masked);

	vpsadbw	xmm1, xmm0, xmm1

; 152  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm2, xmm1, xmm2
	cmp	eax, r8d
	jl	SHORT $LL4@reg_sad_w1
$LN3@reg_sad_w1:

; 153  :   }
; 154  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 155  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 156  :   return _mm_cvtsi128_si32(sad);
; 157  : }

	mov	rbx, QWORD PTR [rsp+8]
	mov	rdi, QWORD PTR [rsp+16]
	vpshufd	xmm0, xmm2, 78				; 0000004eH
	vpaddq	xmm1, xmm0, xmm2
	vmovd	eax, xmm1
	ret	0
reg_sad_w12 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
data1$ = 32
data2$ = 40
height$ = 48
stride1$ = 56
stride2$ = 64
reg_sad_w16 PROC

; 162  : {

	mov	QWORD PTR [rsp+32], rbx
	push	rbp
	push	rsi
	push	rdi

; 164  :   int32_t y;
; 165  : 
; 166  :   const int32_t height_fourline_groups = height & ~3;
; 167  :   const int32_t height_residual_lines  = height &  3;
; 168  : 
; 169  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	ebp, DWORD PTR stride2$[rsp]
	xor	eax, eax
	mov	QWORD PTR [rsp+32], r12
	mov	ebx, r9d
	mov	QWORD PTR [rsp+48], r15
	mov	r12d, r8d
	mov	r15d, r8d
	and	r12d, 3
	and	r15d, -4
	mov	r11d, r8d
	mov	rsi, rcx
	vpxor	xmm2, xmm2, xmm2
	test	r15d, r15d
	jle	$LN3@reg_sad_w1

; 163  :   __m128i sse_inc = _mm_setzero_si128();

	mov	QWORD PTR [rsp+40], r14
	mov	r14d, 2
	npad	9
$LL4@reg_sad_w1:

; 170  :     __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1));
; 171  :     __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2));

	mov	ecx, eax

; 172  :     __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1));

	lea	r9d, DWORD PTR [r14-1]
	imul	ecx, ebp

; 173  :     __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2));
; 174  :     __m128i e = _mm_loadu_si128((const __m128i *)(data1 + (y + 2) * stride1));
; 175  :     __m128i f = _mm_loadu_si128((const __m128i *)(data2 + (y + 2) * stride2));
; 176  :     __m128i g = _mm_loadu_si128((const __m128i *)(data1 + (y + 3) * stride1));

	lea	r10d, DWORD PTR [r14+1]
	mov	r8d, eax
	add	eax, 4
	imul	r8d, ebx

; 177  :     __m128i h = _mm_loadu_si128((const __m128i *)(data2 + (y + 3) * stride2));
; 178  : 
; 179  :     __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	vmovdqu	xmm0, XMMWORD PTR [r8+rsi]
	vpsadbw	xmm1, xmm0, XMMWORD PTR [rcx+rdx]

; 180  :     __m128i curr_sads_cd = _mm_sad_epu8(c, d);
; 181  :     __m128i curr_sads_ef = _mm_sad_epu8(e, f);
; 182  :     __m128i curr_sads_gh = _mm_sad_epu8(g, h);
; 183  : 
; 184  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm2, xmm1, xmm2
	mov	ecx, r9d
	mov	r8d, r14d
	imul	ecx, ebx
	imul	r9d, ebp
	imul	r8d, ebx
	vmovdqu	xmm0, XMMWORD PTR [rcx+rsi]
	vpsadbw	xmm1, xmm0, XMMWORD PTR [r9+rdx]
	vmovdqu	xmm0, XMMWORD PTR [r8+rsi]
	mov	ecx, r14d
	add	r14d, 4
	imul	ecx, ebp

; 185  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vpaddq	xmm3, xmm2, xmm1
	vpsadbw	xmm1, xmm0, XMMWORD PTR [rcx+rdx]
	mov	ecx, r10d
	imul	r10d, ebp
	imul	ecx, ebx

; 186  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	vpaddq	xmm2, xmm3, xmm1
	vmovdqu	xmm0, XMMWORD PTR [rcx+rsi]
	vpsadbw	xmm1, xmm0, XMMWORD PTR [r10+rdx]

; 187  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	vpaddq	xmm2, xmm2, xmm1
	cmp	eax, r15d
	jl	SHORT $LL4@reg_sad_w1

; 164  :   int32_t y;
; 165  : 
; 166  :   const int32_t height_fourline_groups = height & ~3;
; 167  :   const int32_t height_residual_lines  = height &  3;
; 168  : 
; 169  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r14, QWORD PTR [rsp+40]
$LN3@reg_sad_w1:
	mov	r15, QWORD PTR [rsp+48]

; 188  :   }
; 189  :   if (height_residual_lines) {

	test	r12d, r12d
	mov	r12, QWORD PTR [rsp+32]
	je	SHORT $LN6@reg_sad_w1

; 190  :     for (; y < height; y++) {

	cmp	eax, r11d
	jge	SHORT $LN6@reg_sad_w1
	npad	10
$LL7@reg_sad_w1:

; 191  :       __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1));

	mov	r8d, eax

; 192  :       __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2));

	mov	ecx, eax
	imul	r8d, ebx
	inc	eax
	imul	ecx, ebp

; 193  : 
; 194  :       __m128i curr_sads = _mm_sad_epu8(a, b);

	vmovdqu	xmm0, XMMWORD PTR [r8+rsi]
	vpsadbw	xmm1, xmm0, XMMWORD PTR [rcx+rdx]

; 195  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm2, xmm1, xmm2
	cmp	eax, r11d
	jl	SHORT $LL7@reg_sad_w1
$LN6@reg_sad_w1:

; 196  :     }
; 197  :   }
; 198  : 
; 199  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 200  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 201  :   return _mm_cvtsi128_si32(sad);
; 202  : }

	mov	rbx, QWORD PTR [rsp+56]
	vpshufd	xmm0, xmm2, 78				; 0000004eH
	vpaddq	xmm1, xmm0, xmm2
	vmovd	eax, xmm1
	pop	rdi
	pop	rsi
	pop	rbp
	ret	0
reg_sad_w16 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
data1$ = 48
data2$ = 56
height$ = 64
stride1$ = 72
stride2$ = 80
reg_sad_w24 PROC

; 207  : {

	mov	QWORD PTR [rsp+16], rbp
	mov	QWORD PTR [rsp+24], rsi
	mov	QWORD PTR [rsp+32], rdi
	push	r14
	sub	rsp, 32					; 00000020H

; 208  :   __m128i sse_inc = _mm_setzero_si128();
; 209  :   int32_t y;
; 210  : 
; 211  :   const int32_t height_doublelines = height & ~1;
; 212  :   const int32_t height_parity      = height &  1;
; 213  : 
; 214  :   for (y = 0; y < height_doublelines; y += 2) {

	mov	r14d, DWORD PTR stride2$[rsp]
	mov	r11d, r8d
	vmovaps	XMMWORD PTR [rsp+16], xmm6
	and	r11d, 1
	mov	QWORD PTR [rsp+48], rbx
	xor	eax, eax
	mov	ebx, r8d
	mov	edi, r9d
	and	ebx, -2
	mov	rbp, rcx
	vpxor	xmm6, xmm6, xmm6
	test	ebx, ebx
	jle	SHORT $LN3@reg_sad_w2
	vmovaps	XMMWORD PTR [rsp], xmm7
	npad	9
$LL4@reg_sad_w2:

; 215  :     __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1));
; 216  :     __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2));
; 217  :     __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1));
; 218  :     __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2));
; 219  : 
; 220  :     __m128d e_d = _mm_setzero_pd();
; 221  :     __m128d f_d = _mm_setzero_pd();
; 222  : 
; 223  :     e_d = _mm_loadl_pd(e_d, (const double *)(data1 + (y + 0) * stride1 + 16));
; 224  :     f_d = _mm_loadl_pd(f_d, (const double *)(data2 + (y + 0) * stride2 + 16));
; 225  :     e_d = _mm_loadh_pd(e_d, (const double *)(data1 + (y + 1) * stride1 + 16));

	lea	r8d, DWORD PTR [rax+1]
	mov	r10d, eax
	mov	ecx, r8d
	imul	r10d, edi
	imul	ecx, edi

; 226  :     f_d = _mm_loadh_pd(f_d, (const double *)(data2 + (y + 1) * stride2 + 16));

	imul	r8d, r14d
	mov	r9d, ecx

; 227  : 
; 228  :     __m128i e = _mm_castpd_si128(e_d);
; 229  :     __m128i f = _mm_castpd_si128(f_d);
; 230  : 
; 231  :     __m128i curr_sads_1 = _mm_sad_epu8(a, b);

	vmovdqu	xmm0, XMMWORD PTR [r10+rbp]
	vmovddup xmm5, QWORD PTR [r10+rbp+16]
	vmovhpd	xmm5, xmm5, QWORD PTR [rcx+rbp+16]

; 232  :     __m128i curr_sads_2 = _mm_sad_epu8(c, d);

	vmovdqu	xmm2, XMMWORD PTR [r9+rbp]
	mov	ecx, eax
	add	eax, 2
	imul	ecx, r14d
	vpsadbw	xmm1, xmm0, XMMWORD PTR [rcx+rdx]
	vmovddup xmm4, QWORD PTR [rcx+rdx+16]
	vmovhpd	xmm4, xmm4, QWORD PTR [r8+rdx+16]
	vpsadbw	xmm0, xmm2, XMMWORD PTR [r8+rdx]

; 233  :     __m128i curr_sads_3 = _mm_sad_epu8(e, f);
; 234  : 
; 235  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_1);

	vpaddq	xmm3, xmm1, xmm6

; 236  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_2);

	vpaddq	xmm3, xmm3, xmm0
	vpsadbw	xmm1, xmm5, xmm4

; 237  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_3);

	vpaddq	xmm6, xmm3, xmm1
	cmp	eax, ebx
	jl	SHORT $LL4@reg_sad_w2

; 208  :   __m128i sse_inc = _mm_setzero_si128();
; 209  :   int32_t y;
; 210  : 
; 211  :   const int32_t height_doublelines = height & ~1;
; 212  :   const int32_t height_parity      = height &  1;
; 213  : 
; 214  :   for (y = 0; y < height_doublelines; y += 2) {

	vmovaps	xmm7, XMMWORD PTR [rsp]
$LN3@reg_sad_w2:
	mov	rbx, QWORD PTR [rsp+48]

; 238  :   }
; 239  :   if (height_parity) {

	test	r11d, r11d
	je	SHORT $LN5@reg_sad_w2

; 240  :     __m128i a = _mm_loadu_si128   ((const __m128i *)(data1 + y * stride1));
; 241  :     __m128i b = _mm_loadu_si128   ((const __m128i *)(data2 + y * stride2));
; 242  :     __m128i c = _mm_loadl_epi64   ((const __m128i *)(data1 + y * stride1 + 16));

	mov	ecx, eax
	imul	ecx, edi

; 243  :     __m128i d = _mm_loadl_epi64   ((const __m128i *)(data2 + y * stride2 + 16));

	imul	eax, r14d
	vmovq	xmm1, QWORD PTR [rcx+rbp+16]
	vmovq	xmm0, QWORD PTR [rax+rdx+16]

; 244  : 
; 245  :     __m128i curr_sads_1 = _mm_sad_epu8(a, b);
; 246  :     __m128i curr_sads_2 = _mm_sad_epu8(c, d);

	vpsadbw	xmm3, xmm1, xmm0
	vmovdqu	xmm1, XMMWORD PTR [rcx+rbp]
	vpsadbw	xmm2, xmm1, XMMWORD PTR [rax+rdx]

; 247  : 
; 248  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_1);

	vpaddq	xmm0, xmm2, xmm6

; 249  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_2);

	vpaddq	xmm6, xmm3, xmm0
$LN5@reg_sad_w2:

; 250  :   }
; 251  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 252  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 253  :   return _mm_cvtsi128_si32(sad);
; 254  : }

	mov	rbp, QWORD PTR [rsp+56]
	mov	rsi, QWORD PTR [rsp+64]
	mov	rdi, QWORD PTR [rsp+72]
	vpshufd	xmm0, xmm6, 78				; 0000004eH
	vpaddq	xmm1, xmm0, xmm6
	vmovaps	xmm6, XMMWORD PTR [rsp+16]
	vmovd	eax, xmm1
	add	rsp, 32					; 00000020H
	pop	r14
	ret	0
reg_sad_w24 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
tv1022 = 0
tv1081 = 8
data1$ = 112
data2$ = 120
tv1023 = 128
width$ = 128
tv1024 = 136
height$ = 136
stride1$ = 144
stride2$ = 152
reg_sad_arbitrary PROC

; 259  : {

	push	rbx
	push	rbp
	push	rsi
	push	rdi
	push	r12
	push	r13
	push	r14
	sub	rsp, 48					; 00000030H

; 260  :   int32_t y, x;
; 261  :   __m128i sse_inc = _mm_setzero_si128();
; 262  :   
; 263  :   // Bytes in block in 128-bit blocks per each scanline, and remainder
; 264  :   const int32_t width_xmms             = width  & ~15;

	mov	esi, DWORD PTR stride2$[rsp]

; 265  :   const int32_t width_residual_pixels  = width  &  15;
; 266  : 
; 267  :   const int32_t height_fourline_groups = height & ~3;

	mov	r13d, r9d
	mov	ebp, DWORD PTR stride1$[rsp]

; 268  :   const int32_t height_residual_lines  = height &  3;

	mov	r11d, r9d

; 269  : 
; 270  :   const __m128i rds    = _mm_set1_epi8 (width_residual_pixels);
; 271  :   const __m128i ns     = _mm_setr_epi8 (0,  1,  2,  3,  4,  5,  6,  7,
; 272  :                                         8,  9,  10, 11, 12, 13, 14, 15);
; 273  :   const __m128i rdmask = _mm_cmpgt_epi8(rds, ns);
; 274  : 
; 275  :   for (x = 0; x < width_xmms; x += 16) {

	xor	r14d, r14d
	vmovaps	XMMWORD PTR [rsp+16], xmm7
	mov	r12d, r9d
	and	r11d, 3
	mov	eax, r8d
	movsxd	r9, r8d
	and	eax, 15
	mov	DWORD PTR tv1023[rsp], r11d
	mov	DWORD PTR tv1024[rsp], eax
	lea	r10d, QWORD PTR [r14+2]
	movsx	eax, al
	and	r13d, -4
	vmovd	xmm1, eax
	vpbroadcastb xmm1, xmm1
	vpcmpgtb xmm7, xmm1, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	and	r9, -16
	mov	rbx, rdx
	mov	rdi, rcx
	mov	QWORD PTR tv1022[rsp], r9
	vpxor	xmm3, xmm3, xmm3
	mov	eax, r14d
	jle	$LN3@reg_sad_ar

; 295  :     }
; 296  :     if (height_residual_lines) {

	lea	rax, QWORD PTR [r9-1]
	mov	QWORD PTR [rsp+112], r15
	shr	rax, 4
	mov	r11d, r14d
	inc	eax
	shl	eax, 4
	mov	QWORD PTR tv1081[rsp], rax
	npad	4
$LL4@reg_sad_ar:

; 276  :     for (y = 0; y < height_fourline_groups; y += 4) {

	mov	eax, r14d
	test	r13d, r13d
	jle	$LN6@reg_sad_ar
	mov	r15d, r10d
	npad	1
$LL7@reg_sad_ar:

; 277  :       __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1 + x));

	mov	r8d, eax

; 278  :       __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2 + x));
; 279  :       __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1 + x));

	lea	r9d, DWORD PTR [r15-1]
	imul	r8d, ebp

; 280  :       __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2 + x));
; 281  :       __m128i e = _mm_loadu_si128((const __m128i *)(data1 + (y + 2) * stride1 + x));
; 282  :       __m128i f = _mm_loadu_si128((const __m128i *)(data2 + (y + 2) * stride2 + x));
; 283  :       __m128i g = _mm_loadu_si128((const __m128i *)(data1 + (y + 3) * stride1 + x));

	lea	r10d, DWORD PTR [r15+1]
	mov	edx, eax
	add	eax, 4
	imul	edx, esi
	add	r8, r11
	add	rdx, r11

; 284  :       __m128i h = _mm_loadu_si128((const __m128i *)(data2 + (y + 3) * stride2 + x));
; 285  : 
; 286  :       __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	vmovdqu	xmm0, XMMWORD PTR [r8+rdi]
	mov	r8d, r15d
	vpsadbw	xmm1, xmm0, XMMWORD PTR [rdx+rbx]

; 287  :       __m128i curr_sads_cd = _mm_sad_epu8(c, d);
; 288  :       __m128i curr_sads_ef = _mm_sad_epu8(e, f);
; 289  :       __m128i curr_sads_gh = _mm_sad_epu8(g, h);
; 290  : 
; 291  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm2, xmm1, xmm3
	mov	edx, r9d
	imul	r8d, ebp
	imul	edx, ebp
	imul	r9d, esi
	add	r8, r11
	add	rdx, r11
	mov	ecx, r9d
	add	rcx, r11
	vmovdqu	xmm0, XMMWORD PTR [rdx+rdi]
	mov	edx, r15d
	add	r15d, 4
	vpsadbw	xmm1, xmm0, XMMWORD PTR [rcx+rbx]
	vmovdqu	xmm0, XMMWORD PTR [r8+rdi]
	imul	edx, esi

; 292  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vpaddq	xmm3, xmm2, xmm1
	add	rdx, r11
	vpsadbw	xmm1, xmm0, XMMWORD PTR [rdx+rbx]
	mov	edx, r10d
	imul	r10d, esi
	imul	edx, ebp

; 293  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	vpaddq	xmm2, xmm3, xmm1
	mov	ecx, r10d
	add	rdx, r11
	add	rcx, r11
	vmovdqu	xmm0, XMMWORD PTR [rdx+rdi]
	vpsadbw	xmm1, xmm0, XMMWORD PTR [rcx+rbx]

; 294  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	vpaddq	xmm3, xmm2, xmm1
	cmp	eax, r13d
	jl	$LL7@reg_sad_ar
	mov	r9, QWORD PTR tv1022[rsp]
	mov	r10d, 2
$LN6@reg_sad_ar:

; 295  :     }
; 296  :     if (height_residual_lines) {

	cmp	DWORD PTR tv1023[rsp], r14d
	je	SHORT $LN2@reg_sad_ar

; 297  :       for (; y < height; y++) {

	cmp	eax, r12d
	jge	SHORT $LN2@reg_sad_ar
	npad	10
$LL10@reg_sad_ar:

; 298  :         __m128i a = _mm_loadu_si128((const __m128i *)(data1 + y * stride1 + x));

	mov	r8d, eax

; 299  :         __m128i b = _mm_loadu_si128((const __m128i *)(data2 + y * stride2 + x));

	mov	edx, eax
	imul	r8d, ebp
	inc	eax
	imul	edx, esi
	add	r8, r11
	add	rdx, r11

; 300  : 
; 301  :         __m128i curr_sads = _mm_sad_epu8(a, b);

	vmovdqu	xmm0, XMMWORD PTR [r8+rdi]
	vpsadbw	xmm1, xmm0, XMMWORD PTR [rdx+rbx]

; 302  : 
; 303  :         sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm3, xmm1, xmm3
	cmp	eax, r12d
	jl	SHORT $LL10@reg_sad_ar
$LN2@reg_sad_ar:

; 269  : 
; 270  :   const __m128i rds    = _mm_set1_epi8 (width_residual_pixels);
; 271  :   const __m128i ns     = _mm_setr_epi8 (0,  1,  2,  3,  4,  5,  6,  7,
; 272  :                                         8,  9,  10, 11, 12, 13, 14, 15);
; 273  :   const __m128i rdmask = _mm_cmpgt_epi8(rds, ns);
; 274  : 
; 275  :   for (x = 0; x < width_xmms; x += 16) {

	add	r11, 16
	cmp	r11, r9
	jl	$LL4@reg_sad_ar
	mov	rax, QWORD PTR tv1081[rsp]
	mov	r11d, DWORD PTR tv1023[rsp]
	mov	r15, QWORD PTR [rsp+112]
$LN3@reg_sad_ar:

; 304  :       }
; 305  :     }
; 306  :   }
; 307  : 
; 308  :   if (width_residual_pixels) {

	cmp	DWORD PTR tv1024[rsp], r14d
	je	$LN15@reg_sad_ar

; 309  :     for (y = 0; y < height_fourline_groups; y += 4) {

	movsxd	r9, eax
	test	r13d, r13d
	jle	$LN12@reg_sad_ar
	vmovaps	XMMWORD PTR [rsp+32], xmm6
	npad	9
$LL13@reg_sad_ar:

; 310  :       __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1 + x));

	mov	ecx, r14d

; 311  :       __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2 + x));
; 312  :       __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1 + x));

	lea	edx, DWORD PTR [r10-1]
	imul	ecx, ebp

; 313  :       __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2 + x));
; 314  :       __m128i e = _mm_loadu_si128((const __m128i *)(data1 + (y + 2) * stride1 + x));
; 315  :       __m128i f = _mm_loadu_si128((const __m128i *)(data2 + (y + 2) * stride2 + x));
; 316  :       __m128i g = _mm_loadu_si128((const __m128i *)(data1 + (y + 3) * stride1 + x));

	lea	r8d, DWORD PTR [r10+1]
	add	rcx, r9
	vmovdqu	xmm2, XMMWORD PTR [rcx+rdi]
	mov	ecx, edx
	imul	edx, esi
	imul	ecx, ebp
	mov	eax, edx
	add	rcx, r9
	add	rax, r9
	vmovdqu	xmm4, XMMWORD PTR [rcx+rdi]
	mov	ecx, r10d
	imul	ecx, ebp
	add	rcx, r9
	vmovdqu	xmm5, XMMWORD PTR [rcx+rdi]
	mov	ecx, r8d

; 317  :       __m128i h = _mm_loadu_si128((const __m128i *)(data2 + (y + 3) * stride2 + x));

	imul	r8d, esi
	imul	ecx, ebp
	add	rcx, r9
	vmovdqu	xmm6, XMMWORD PTR [rcx+rdi]
	mov	ecx, r14d
	add	r14d, 4
	imul	ecx, esi
	add	rcx, r9

; 318  : 
; 319  :       __m128i b_masked     = _mm_blendv_epi8(a, b, rdmask);

	vpblendvb xmm1, xmm2, XMMWORD PTR [rcx+rbx], xmm7

; 320  :       __m128i d_masked     = _mm_blendv_epi8(c, d, rdmask);
; 321  :       __m128i f_masked     = _mm_blendv_epi8(e, f, rdmask);
; 322  :       __m128i h_masked     = _mm_blendv_epi8(g, h, rdmask);
; 323  : 
; 324  :       __m128i curr_sads_ab = _mm_sad_epu8   (a, b_masked);

	vpsadbw	xmm2, xmm1, xmm2
	vpblendvb xmm1, xmm4, XMMWORD PTR [rax+rbx], xmm7

; 325  :       __m128i curr_sads_cd = _mm_sad_epu8   (c, d_masked);
; 326  :       __m128i curr_sads_ef = _mm_sad_epu8   (e, f_masked);
; 327  :       __m128i curr_sads_gh = _mm_sad_epu8   (g, h_masked);
; 328  : 
; 329  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm3, xmm2, xmm3
	vpsadbw	xmm2, xmm1, xmm4

; 330  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vpaddq	xmm4, xmm3, xmm2
	mov	ecx, r10d
	mov	eax, r8d
	imul	ecx, esi
	add	rax, r9
	add	r10d, 4
	add	rcx, r9
	vpblendvb xmm1, xmm5, XMMWORD PTR [rcx+rbx], xmm7
	vpsadbw	xmm2, xmm1, xmm5
	vpblendvb xmm1, xmm6, XMMWORD PTR [rax+rbx], xmm7

; 331  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	vpaddq	xmm3, xmm4, xmm2
	vpsadbw	xmm2, xmm1, xmm6

; 332  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	vpaddq	xmm3, xmm3, xmm2
	cmp	r14d, r13d
	jl	$LL13@reg_sad_ar

; 309  :     for (y = 0; y < height_fourline_groups; y += 4) {

	vmovaps	xmm6, XMMWORD PTR [rsp+32]
$LN12@reg_sad_ar:

; 333  :     }
; 334  :     if (height_residual_lines) {

	test	r11d, r11d
	je	SHORT $LN15@reg_sad_ar

; 335  :       for (; y < height; y++) {

	cmp	r14d, r12d
	jge	SHORT $LN15@reg_sad_ar
$LL16@reg_sad_ar:

; 336  :         __m128i a = _mm_loadu_si128((const __m128i *)(data1 + y * stride1 + x));

	mov	ecx, r14d
	imul	ecx, ebp
	add	rcx, r9
	vmovdqu	xmm2, XMMWORD PTR [rcx+rdi]

; 337  :         __m128i b = _mm_loadu_si128((const __m128i *)(data2 + y * stride2 + x));

	mov	ecx, r14d
	inc	r14d
	imul	ecx, esi
	add	rcx, r9

; 338  : 
; 339  :         __m128i b_masked  = _mm_blendv_epi8(a, b, rdmask);

	vpblendvb xmm1, xmm2, XMMWORD PTR [rcx+rbx], xmm7

; 340  :         __m128i curr_sads = _mm_sad_epu8   (a, b_masked);

	vpsadbw	xmm2, xmm1, xmm2

; 341  : 
; 342  :         sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm3, xmm2, xmm3
	cmp	r14d, r12d
	jl	SHORT $LL16@reg_sad_ar
$LN15@reg_sad_ar:

; 343  :       }
; 344  :     }
; 345  :   }
; 346  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 347  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 348  : 
; 349  :   return _mm_cvtsi128_si32(sad);
; 350  : }

	vmovaps	xmm7, XMMWORD PTR [rsp+16]
	vpshufd	xmm0, xmm3, 78				; 0000004eH
	vpaddq	xmm1, xmm0, xmm3
	vmovd	eax, xmm1
	add	rsp, 48					; 00000030H
	pop	r14
	pop	r13
	pop	r12
	pop	rdi
	pop	rsi
	pop	rbp
	pop	rbx
	ret	0
reg_sad_arbitrary ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
pic_data$ = 16
ref_data$ = 24
height$ = 32
stride$ = 40
ver_sad_w4 PROC

; 354  : {

	push	rbx

; 355  :   __m128i ref_row = _mm_set1_epi32(*(const uint32_t *)ref_data);

	vpbroadcastd xmm5, DWORD PTR [rdx]
	mov	QWORD PTR [rsp+16], rbp

; 356  :   __m128i sse_inc = _mm_setzero_si128();
; 357  :   int32_t y;
; 358  : 
; 359  :   const int32_t height_fourline_groups = height & ~3;
; 360  :   const int32_t height_residual_lines  = height &  3;
; 361  : 
; 362  :   for (y = 0; y < height_fourline_groups; y += 4) {

	xor	eax, eax
	mov	QWORD PTR [rsp+24], rsi
	mov	ebp, r8d
	mov	esi, r8d
	and	ebp, 3
	and	esi, -4
	mov	r10d, r9d
	mov	ebx, r8d
	mov	r11, rcx
	vpxor	xmm4, xmm4, xmm4
	test	esi, esi
	jle	SHORT $LN3@ver_sad_w4

; 355  :   __m128i ref_row = _mm_set1_epi32(*(const uint32_t *)ref_data);

	mov	QWORD PTR [rsp+32], rdi
	mov	edi, 2
	npad	6
$LL4@ver_sad_w4:

; 363  :     __m128i a = _mm_cvtsi32_si128(*(uint32_t *)(pic_data + y * stride));

	mov	ecx, eax

; 364  : 
; 365  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 1) * stride), 1);

	lea	edx, DWORD PTR [rdi-1]

; 366  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 2) * stride), 2);

	mov	r8d, edi
	imul	ecx, r10d

; 367  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 3) * stride), 3);

	lea	r9d, DWORD PTR [rdi+1]
	imul	edx, r10d
	imul	r8d, r10d
	add	eax, 4
	imul	r9d, r10d
	add	edi, 4
	vmovd	xmm0, DWORD PTR [rcx+r11]
	vpinsrd	xmm1, xmm0, DWORD PTR [rdx+r11], 1
	vpinsrd	xmm2, xmm1, DWORD PTR [r8+r11], 2
	vpinsrd	xmm3, xmm2, DWORD PTR [r9+r11], 3

; 368  : 
; 369  :     __m128i curr_sads = _mm_sad_epu8(a, ref_row);

	vpsadbw	xmm0, xmm3, xmm5

; 370  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm4, xmm0, xmm4
	cmp	eax, esi
	jl	SHORT $LL4@ver_sad_w4

; 356  :   __m128i sse_inc = _mm_setzero_si128();
; 357  :   int32_t y;
; 358  : 
; 359  :   const int32_t height_fourline_groups = height & ~3;
; 360  :   const int32_t height_residual_lines  = height &  3;
; 361  : 
; 362  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	rdi, QWORD PTR [rsp+32]
$LN3@ver_sad_w4:
	mov	rsi, QWORD PTR [rsp+24]

; 371  :   }
; 372  :   if (height_residual_lines) {

	test	ebp, ebp
	mov	rbp, QWORD PTR [rsp+16]
	je	SHORT $LN6@ver_sad_w4

; 373  :     // Only pick the last dword, because we're comparing single dwords (lines)
; 374  :     ref_row = _mm_bsrli_si128(ref_row, 12);

	vpsrldq	xmm2, xmm5, 12

; 375  : 
; 376  :     for (; y < height; y++) {

	cmp	eax, ebx
	jge	SHORT $LN6@ver_sad_w4
	npad	11
$LL7@ver_sad_w4:

; 377  :       __m128i a = _mm_cvtsi32_si128(*(const uint32_t *)(pic_data + y * stride));

	mov	ecx, eax
	inc	eax
	imul	ecx, r10d
	vmovd	xmm0, DWORD PTR [rcx+r11]

; 378  : 
; 379  :       __m128i curr_sads = _mm_sad_epu8(a, ref_row);

	vpsadbw	xmm1, xmm0, xmm2

; 380  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm4, xmm1, xmm4
	cmp	eax, ebx
	jl	SHORT $LL7@ver_sad_w4
$LN6@ver_sad_w4:

; 381  :     }
; 382  :   }
; 383  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm4, 78				; 0000004eH

; 384  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vpaddq	xmm1, xmm0, xmm4

; 385  : 
; 386  :   return _mm_cvtsi128_si32(sad);

	vmovd	eax, xmm1

; 387  : }

	pop	rbx
	ret	0
ver_sad_w4 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
pic_data$ = 8
ref_data$ = 16
height$ = 24
stride$ = 32
ver_sad_w8 PROC

; 391  : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rdi

; 392  :   const __m128i ref_row = _mm_set1_epi64x(*(const uint64_t *)ref_data);

	vpbroadcastq xmm3, QWORD PTR [rdx]

; 393  :   __m128i sse_inc = _mm_setzero_si128();
; 394  :   int32_t y;
; 395  : 
; 396  :   const int32_t height_fourline_groups = height & ~3;

	mov	ebx, r8d

; 397  :   const int32_t height_residual_lines  = height &  3;

	mov	edi, r8d
	and	ebx, -4
	and	edi, 3

; 398  : 
; 399  :   for (y = 0; y < height_fourline_groups; y += 4) {

	xor	eax, eax
	mov	r10, rcx
	vpxor	xmm2, xmm2, xmm2
	test	ebx, ebx
	jle	SHORT $LN3@ver_sad_w8

; 392  :   const __m128i ref_row = _mm_set1_epi64x(*(const uint64_t *)ref_data);

	mov	r11d, 2
	npad	2
$LL4@ver_sad_w8:

; 400  :     __m128d a_d = _mm_setzero_pd();
; 401  :     __m128d c_d = _mm_setzero_pd();
; 402  : 
; 403  :     a_d = _mm_loadl_pd(a_d, (const double *)(pic_data + (y + 0) * stride));

	mov	ecx, eax
	add	eax, 4
	imul	ecx, r9d
	vmovddup xmm0, QWORD PTR [rcx+r10]

; 404  :     a_d = _mm_loadh_pd(a_d, (const double *)(pic_data + (y + 1) * stride));

	lea	ecx, DWORD PTR [r11-1]
	imul	ecx, r9d
	vmovhpd	xmm0, xmm0, QWORD PTR [rcx+r10]

; 405  : 
; 406  :     c_d = _mm_loadl_pd(c_d, (const double *)(pic_data + (y + 2) * stride));

	mov	ecx, r11d
	imul	ecx, r9d

; 407  :     c_d = _mm_loadh_pd(c_d, (const double *)(pic_data + (y + 3) * stride));
; 408  : 
; 409  :     __m128i a = _mm_castpd_si128(a_d);
; 410  :     __m128i c = _mm_castpd_si128(c_d);
; 411  : 
; 412  :     __m128i curr_sads_ab = _mm_sad_epu8(a, ref_row);

	vpsadbw	xmm0, xmm0, xmm3

; 413  :     __m128i curr_sads_cd = _mm_sad_epu8(c, ref_row);
; 414  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm2, xmm0, xmm2
	vmovddup xmm1, QWORD PTR [rcx+r10]
	lea	ecx, DWORD PTR [r11+1]
	add	r11d, 4
	imul	ecx, r9d
	vmovhpd	xmm1, xmm1, QWORD PTR [rcx+r10]
	vpsadbw	xmm1, xmm1, xmm3

; 415  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vpaddq	xmm2, xmm2, xmm1
	cmp	eax, ebx
	jl	SHORT $LL4@ver_sad_w8
$LN3@ver_sad_w8:

; 416  :   }
; 417  :   if (height_residual_lines) {

	test	edi, edi
	je	SHORT $LN6@ver_sad_w8

; 418  :     __m128i b = _mm_move_epi64(ref_row);

	vmovq	xmm3, xmm3

; 419  : 
; 420  :     for (; y < height; y++) {

	cmp	eax, r8d
	jge	SHORT $LN6@ver_sad_w8
	npad	3
$LL7@ver_sad_w8:

; 421  :       __m128i a = _mm_loadl_epi64((__m128i *)(pic_data + y * stride));

	mov	ecx, eax
	inc	eax
	imul	ecx, r9d
	vmovq	xmm0, QWORD PTR [rcx+r10]

; 422  : 
; 423  :       __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	vpsadbw	xmm1, xmm0, xmm3

; 424  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm2, xmm1, xmm2
	cmp	eax, r8d
	jl	SHORT $LL7@ver_sad_w8
$LN6@ver_sad_w8:

; 425  :     }
; 426  :   }
; 427  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 428  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 429  : 
; 430  :   return _mm_cvtsi128_si32(sad);
; 431  : }

	mov	rbx, QWORD PTR [rsp+8]
	mov	rdi, QWORD PTR [rsp+16]
	vpshufd	xmm0, xmm2, 78				; 0000004eH
	vpaddq	xmm1, xmm0, xmm2
	vmovd	eax, xmm1
	ret	0
ver_sad_w8 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
pic_data$ = 8
ref_data$ = 16
height$ = 24
stride$ = 32
ver_sad_w12 PROC

; 436  :   const __m128i ref_row = _mm_loadu_si128((__m128i *)ref_data);

	vmovdqu	xmm3, XMMWORD PTR [rdx]
	xor	edx, edx
	vpxor	xmm2, xmm2, xmm2

; 437  :   __m128i sse_inc = _mm_setzero_si128();
; 438  :   int32_t y;
; 439  : 
; 440  :   for (y = 0; y < height; y++) {

	test	r8d, r8d
	jle	SHORT $LN3@ver_sad_w1
	npad	1
$LL4@ver_sad_w1:

; 441  :     __m128i a = _mm_loadu_si128((const __m128i *)(pic_data + y * stride));

	mov	eax, edx
	inc	edx
	imul	eax, r9d

; 442  : 
; 443  :     __m128i a_masked  = _mm_blend_epi16(ref_row, a, 0x3f);

	vpblendw xmm0, xmm3, XMMWORD PTR [rax+rcx], 63	; 0000003fH

; 444  :     __m128i curr_sads = _mm_sad_epu8   (ref_row, a_masked);

	vpsadbw	xmm1, xmm3, xmm0

; 445  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm2, xmm1, xmm2
	cmp	edx, r8d
	jl	SHORT $LL4@ver_sad_w1
$LN3@ver_sad_w1:

; 446  :   }
; 447  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm2, 78				; 0000004eH

; 448  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vpaddq	xmm1, xmm0, xmm2

; 449  :   return _mm_cvtsi128_si32(sad);

	vmovd	eax, xmm1

; 450  : }

	ret	0
ver_sad_w12 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
pic_data$ = 8
ref_data$ = 16
height$ = 24
stride$ = 32
ver_sad_w16 PROC

; 454  : {

	mov	QWORD PTR [rsp+8], rbx

; 455  :   const __m128i ref_row = _mm_loadu_si128((__m128i *)ref_data);

	vmovdqu	xmm4, XMMWORD PTR [rdx]

; 456  :   __m128i sse_inc       = _mm_setzero_si128();
; 457  :   int32_t y;
; 458  : 
; 459  :   const int32_t height_fourline_groups = height & ~3;

	mov	r11d, r8d

; 460  :   const int32_t height_residual_lines  = height &  3;

	mov	ebx, r8d
	and	r11d, -4
	and	ebx, 3

; 461  : 
; 462  :   for (y = 0; y < height_fourline_groups; y += 4) {

	xor	eax, eax
	mov	r10, rcx
	vpxor	xmm2, xmm2, xmm2
	test	r11d, r11d
	jle	SHORT $LN3@ver_sad_w1

; 455  :   const __m128i ref_row = _mm_loadu_si128((__m128i *)ref_data);

	lea	edx, QWORD PTR [rax+2]
	npad	9
$LL4@ver_sad_w1:

; 463  :     __m128i pic_row_1   = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * stride));

	mov	ecx, eax
	add	eax, 4
	imul	ecx, r9d

; 464  :     __m128i pic_row_2   = _mm_loadu_si128((__m128i *)(pic_data + (y + 1) * stride));
; 465  :     __m128i pic_row_3   = _mm_loadu_si128((__m128i *)(pic_data + (y + 2) * stride));
; 466  :     __m128i pic_row_4   = _mm_loadu_si128((__m128i *)(pic_data + (y + 3) * stride));
; 467  : 
; 468  :     __m128i curr_sads_1 = _mm_sad_epu8   (pic_row_1, ref_row);

	vpsadbw	xmm1, xmm4, XMMWORD PTR [rcx+r10]

; 469  :     __m128i curr_sads_2 = _mm_sad_epu8   (pic_row_2, ref_row);
; 470  :     __m128i curr_sads_3 = _mm_sad_epu8   (pic_row_3, ref_row);
; 471  :     __m128i curr_sads_4 = _mm_sad_epu8   (pic_row_4, ref_row);
; 472  : 
; 473  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_1);

	vpaddq	xmm2, xmm1, xmm2
	lea	ecx, DWORD PTR [rdx-1]
	imul	ecx, r9d
	vpsadbw	xmm1, xmm4, XMMWORD PTR [rcx+r10]
	mov	ecx, edx
	imul	ecx, r9d

; 474  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_2);

	vpaddq	xmm3, xmm2, xmm1
	vpsadbw	xmm1, xmm4, XMMWORD PTR [rcx+r10]
	lea	ecx, DWORD PTR [rdx+1]
	add	edx, 4
	imul	ecx, r9d

; 475  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_3);

	vpaddq	xmm2, xmm3, xmm1
	vpsadbw	xmm1, xmm4, XMMWORD PTR [rcx+r10]

; 476  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_4);

	vpaddq	xmm2, xmm2, xmm1
	cmp	eax, r11d
	jl	SHORT $LL4@ver_sad_w1
$LN3@ver_sad_w1:

; 477  :   }
; 478  :   if (height_residual_lines) {

	test	ebx, ebx
	je	SHORT $LN6@ver_sad_w1

; 479  :     for (; y < height; y++) {

	cmp	eax, r8d
	jge	SHORT $LN6@ver_sad_w1
	npad	10
$LL7@ver_sad_w1:

; 480  :       __m128i pic_row   = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * stride));

	mov	ecx, eax
	inc	eax
	imul	ecx, r9d

; 481  :       __m128i curr_sads = _mm_sad_epu8   (pic_row, ref_row);

	vpsadbw	xmm1, xmm4, XMMWORD PTR [rcx+r10]

; 482  : 
; 483  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm2, xmm1, xmm2
	cmp	eax, r8d
	jl	SHORT $LL7@ver_sad_w1
$LN6@ver_sad_w1:

; 484  :     }
; 485  :   }
; 486  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 487  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 488  : 
; 489  :   return _mm_cvtsi128_si32(sad);
; 490  : }

	mov	rbx, QWORD PTR [rsp+8]
	vpshufd	xmm0, xmm2, 78				; 0000004eH
	vpaddq	xmm1, xmm0, xmm2
	vmovd	eax, xmm1
	ret	0
ver_sad_w16 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
pic_data$ = 64
ref_data$ = 72
tv835 = 80
width$ = 80
height$ = 88
stride$ = 96
ver_sad_arbitrary PROC

; 494  : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rbp
	mov	QWORD PTR [rsp+32], rsi
	push	rdi
	push	r12
	push	r13
	push	r14
	push	r15
	sub	rsp, 16

; 495  :   int32_t y, x;
; 496  :   __m128i sse_inc = _mm_setzero_si128();
; 497  : 
; 498  :   // Bytes in block in 128-bit blocks per each scanline, and remainder
; 499  :   const int32_t width_xmms             = width  & ~15;
; 500  :   const int32_t width_residual_pixels  = width  &  15;
; 501  : 
; 502  :   const int32_t height_fourline_groups = height & ~3;
; 503  :   const int32_t height_residual_lines  = height &  3;
; 504  : 
; 505  :   const __m128i rds    = _mm_set1_epi8 (width_residual_pixels);
; 506  :   const __m128i ns     = _mm_setr_epi8 (0,  1,  2,  3,  4,  5,  6,  7,
; 507  :                                         8,  9,  10, 11, 12, 13, 14, 15);
; 508  :   const __m128i rdmask = _mm_cmpgt_epi8(rds, ns);
; 509  : 
; 510  :   for (x = 0; x < width_xmms; x += 16) {

	xor	ebx, ebx
	movsxd	r15, r8d
	mov	esi, r9d
	vmovaps	XMMWORD PTR [rsp], xmm6
	mov	r12d, r9d
	mov	eax, r8d
	and	eax, 15
	and	esi, -4
	mov	DWORD PTR tv835[rsp], eax
	lea	r14d, QWORD PTR [rbx+2]
	movsx	eax, al
	and	r12d, 3
	mov	edi, r9d
	mov	r9d, DWORD PTR stride$[rsp]
	mov	r13, rdx
	mov	r10, rcx
	mov	ebp, ebx
	vmovd	xmm1, eax
	vpbroadcastb xmm1, xmm1
	vpcmpgtb xmm6, xmm1, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	vpxor	xmm3, xmm3, xmm3
	and	r15, -16
	jle	$LN3@ver_sad_ar

; 527  :     }
; 528  :     if (height_residual_lines) {

	lea	rbp, QWORD PTR [r15-1]
	mov	r8d, ebx
	shr	rbp, 4
	inc	ebp
	shl	ebp, 4
$LL4@ver_sad_ar:

; 511  :     const __m128i ref_row = _mm_loadu_si128((__m128i *)(ref_data + x));

	mov	eax, ebx
	vmovdqu	xmm4, XMMWORD PTR [r8+r13]

; 512  :     for (y = 0; y < height_fourline_groups; y += 4) {

	test	esi, esi
	jle	SHORT $LN6@ver_sad_ar

; 511  :     const __m128i ref_row = _mm_loadu_si128((__m128i *)(ref_data + x));

	mov	r11d, r14d
$LL7@ver_sad_ar:

; 513  :       __m128i a = _mm_loadu_si128((const __m128i *)(pic_data + (y + 0) * stride + x));

	mov	edx, eax
	add	eax, 4
	imul	edx, r9d
	add	rdx, r8

; 514  :       __m128i c = _mm_loadu_si128((const __m128i *)(pic_data + (y + 1) * stride + x));
; 515  :       __m128i e = _mm_loadu_si128((const __m128i *)(pic_data + (y + 2) * stride + x));
; 516  :       __m128i g = _mm_loadu_si128((const __m128i *)(pic_data + (y + 3) * stride + x));
; 517  : 
; 518  :       __m128i curr_sads_ab = _mm_sad_epu8(ref_row, a);

	vpsadbw	xmm0, xmm4, XMMWORD PTR [rdx+rcx]

; 519  :       __m128i curr_sads_cd = _mm_sad_epu8(ref_row, c);
; 520  :       __m128i curr_sads_ef = _mm_sad_epu8(ref_row, e);
; 521  :       __m128i curr_sads_gh = _mm_sad_epu8(ref_row, g);
; 522  : 
; 523  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm1, xmm0, xmm3
	lea	edx, DWORD PTR [r11-1]
	imul	edx, r9d
	add	rdx, r8
	vpsadbw	xmm0, xmm4, XMMWORD PTR [rdx+rcx]
	mov	edx, r11d
	imul	edx, r9d

; 524  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vpaddq	xmm2, xmm1, xmm0
	add	rdx, r8
	vpsadbw	xmm0, xmm4, XMMWORD PTR [rdx+rcx]
	lea	edx, DWORD PTR [r11+1]
	add	r11d, 4
	imul	edx, r9d

; 525  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	vpaddq	xmm1, xmm2, xmm0
	add	rdx, r8
	vpsadbw	xmm0, xmm4, XMMWORD PTR [rdx+rcx]

; 526  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	vpaddq	xmm3, xmm1, xmm0
	cmp	eax, esi
	jl	SHORT $LL7@ver_sad_ar
$LN6@ver_sad_ar:

; 527  :     }
; 528  :     if (height_residual_lines) {

	test	r12d, r12d
	je	SHORT $LN2@ver_sad_ar

; 529  :       for (; y < height; y++) {

	cmp	eax, edi
	jge	SHORT $LN2@ver_sad_ar
	npad	13
$LL10@ver_sad_ar:

; 530  :         __m128i a = _mm_loadu_si128((const __m128i *)(pic_data + y * stride + x));

	mov	edx, eax
	inc	eax
	imul	edx, r9d
	add	rdx, r8

; 531  : 
; 532  :         __m128i curr_sads = _mm_sad_epu8(a, ref_row);

	vpsadbw	xmm1, xmm4, XMMWORD PTR [rdx+rcx]

; 533  : 
; 534  :         sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm3, xmm1, xmm3
	cmp	eax, edi
	jl	SHORT $LL10@ver_sad_ar
$LN2@ver_sad_ar:

; 495  :   int32_t y, x;
; 496  :   __m128i sse_inc = _mm_setzero_si128();
; 497  : 
; 498  :   // Bytes in block in 128-bit blocks per each scanline, and remainder
; 499  :   const int32_t width_xmms             = width  & ~15;
; 500  :   const int32_t width_residual_pixels  = width  &  15;
; 501  : 
; 502  :   const int32_t height_fourline_groups = height & ~3;
; 503  :   const int32_t height_residual_lines  = height &  3;
; 504  : 
; 505  :   const __m128i rds    = _mm_set1_epi8 (width_residual_pixels);
; 506  :   const __m128i ns     = _mm_setr_epi8 (0,  1,  2,  3,  4,  5,  6,  7,
; 507  :                                         8,  9,  10, 11, 12, 13, 14, 15);
; 508  :   const __m128i rdmask = _mm_cmpgt_epi8(rds, ns);
; 509  : 
; 510  :   for (x = 0; x < width_xmms; x += 16) {

	add	r8, 16
	cmp	r8, r15
	jl	$LL4@ver_sad_ar
$LN3@ver_sad_ar:

; 535  :       }
; 536  :     }
; 537  :   }
; 538  : 
; 539  :   if (width_residual_pixels) {

	cmp	DWORD PTR tv835[rsp], ebx
	je	$LN15@ver_sad_ar

; 540  :     const __m128i ref_row = _mm_loadu_si128((__m128i *)(ref_data + x));

	movsxd	rdx, ebp
	vmovdqu	xmm5, XMMWORD PTR [rdx+r13]

; 541  :     for (y = 0; y < height_fourline_groups; y += 4) {

	test	esi, esi
	jle	SHORT $LN12@ver_sad_ar
	npad	4
$LL13@ver_sad_ar:

; 542  :       __m128i a = _mm_loadu_si128((const __m128i *)(pic_data + (y + 0) * stride + x));

	mov	ecx, ebx
	add	ebx, 4
	imul	ecx, r9d
	add	rcx, rdx

; 543  :       __m128i c = _mm_loadu_si128((const __m128i *)(pic_data + (y + 1) * stride + x));
; 544  :       __m128i e = _mm_loadu_si128((const __m128i *)(pic_data + (y + 2) * stride + x));
; 545  :       __m128i g = _mm_loadu_si128((const __m128i *)(pic_data + (y + 3) * stride + x));
; 546  : 
; 547  :       __m128i a_masked     = _mm_blendv_epi8(ref_row, a, rdmask);

	vpblendvb xmm1, xmm5, XMMWORD PTR [rcx+r10], xmm6

; 548  :       __m128i c_masked     = _mm_blendv_epi8(ref_row, c, rdmask);
; 549  :       __m128i e_masked     = _mm_blendv_epi8(ref_row, e, rdmask);
; 550  :       __m128i g_masked     = _mm_blendv_epi8(ref_row, g, rdmask);
; 551  : 
; 552  :       __m128i curr_sads_ab = _mm_sad_epu8   (ref_row, a_masked);

	vpsadbw	xmm2, xmm5, xmm1

; 553  :       __m128i curr_sads_cd = _mm_sad_epu8   (ref_row, c_masked);
; 554  :       __m128i curr_sads_ef = _mm_sad_epu8   (ref_row, e_masked);
; 555  :       __m128i curr_sads_gh = _mm_sad_epu8   (ref_row, g_masked);
; 556  : 
; 557  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm3, xmm2, xmm3
	lea	ecx, DWORD PTR [r14-1]
	imul	ecx, r9d
	add	rcx, rdx
	vpblendvb xmm1, xmm5, XMMWORD PTR [rcx+r10], xmm6
	vpsadbw	xmm2, xmm5, xmm1

; 558  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vpaddq	xmm4, xmm3, xmm2
	mov	ecx, r14d
	imul	ecx, r9d
	add	rcx, rdx
	vpblendvb xmm1, xmm5, XMMWORD PTR [rcx+r10], xmm6
	lea	ecx, DWORD PTR [r14+1]
	add	r14d, 4
	vpsadbw	xmm2, xmm5, xmm1
	imul	ecx, r9d

; 559  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	vpaddq	xmm3, xmm4, xmm2
	add	rcx, rdx
	vpblendvb xmm1, xmm5, XMMWORD PTR [rcx+r10], xmm6
	vpsadbw	xmm2, xmm5, xmm1

; 560  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	vpaddq	xmm3, xmm3, xmm2
	cmp	ebx, esi
	jl	SHORT $LL13@ver_sad_ar
$LN12@ver_sad_ar:

; 561  :     }
; 562  :     if (height_residual_lines) {

	test	r12d, r12d
	je	SHORT $LN15@ver_sad_ar

; 563  :       for (; y < height; y++) {

	cmp	ebx, edi
	jge	SHORT $LN15@ver_sad_ar
	npad	7
$LL16@ver_sad_ar:

; 564  :         __m128i a = _mm_loadu_si128((const __m128i *)(pic_data + y * stride + x));

	mov	ecx, ebx
	inc	ebx
	imul	ecx, r9d
	add	rcx, rdx

; 565  : 
; 566  :         __m128i a_masked  = _mm_blendv_epi8(ref_row, a, rdmask);

	vpblendvb xmm1, xmm5, XMMWORD PTR [rcx+r10], xmm6

; 567  :         __m128i curr_sads = _mm_sad_epu8   (ref_row, a_masked);

	vpsadbw	xmm2, xmm5, xmm1

; 568  : 
; 569  :         sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm3, xmm2, xmm3
	cmp	ebx, edi
	jl	SHORT $LL16@ver_sad_ar
$LN15@ver_sad_ar:

; 570  :       }
; 571  :     }
; 572  :   }
; 573  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 574  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 575  : 
; 576  :   return _mm_cvtsi128_si32(sad);
; 577  : }

	mov	rbx, QWORD PTR [rsp+64]
	mov	rbp, QWORD PTR [rsp+72]
	mov	rsi, QWORD PTR [rsp+88]
	vmovaps	xmm6, XMMWORD PTR [rsp]
	vpshufd	xmm0, xmm3, 78				; 0000004eH
	vpaddq	xmm1, xmm0, xmm3
	vmovd	eax, xmm1
	add	rsp, 16
	pop	r15
	pop	r14
	pop	r13
	pop	r12
	pop	rdi
	ret	0
ver_sad_arbitrary ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
pic_data$ = 64
ref_data$ = 72
height$ = 80
pic_stride$ = 88
ref_stride$ = 96
left$ = 104
tv639 = 112
right$ = 112
hor_sad_sse41_w4 PROC

; 582  : {

	mov	QWORD PTR [rsp+24], rbx
	push	rbp
	push	rsi
	push	rdi
	push	r14
	push	r15
	sub	rsp, 16

; 584  :   const int32_t border_idx       = left ? left : right_border_idx;
; 585  : 
; 586  :   const __m128i ns               = _mm_setr_epi8(0,  1,  2,  3,  4,  5,  6,  7,
; 587  :                                                  8,  9,  10, 11, 12, 13, 14, 15);
; 588  : 
; 589  :   const int32_t border_idx_negative = border_idx >> 31;
; 590  :   const int32_t leftoff             = border_idx_negative | left;
; 591  : 
; 592  :   // Dualword (ie. line) base indexes, ie. the edges the lines read will be
; 593  :   // clamped towards
; 594  :   const __m128i dwbaseids   = _mm_setr_epi8(0, 0, 0, 0, 4, 4, 4, 4,
; 595  :                                             8, 8, 8, 8, 12, 12, 12, 12);
; 596  : 
; 597  :   __m128i right_border_idxs = _mm_set1_epi8((int8_t)right_border_idx);
; 598  :   __m128i left_128          = _mm_set1_epi8((int8_t)left);
; 599  : 
; 600  :   right_border_idxs         = _mm_add_epi8 (right_border_idxs, dwbaseids);
; 601  : 
; 602  :   __m128i mask_right        = _mm_min_epi8 (ns,         right_border_idxs);
; 603  :   __m128i mask1             = _mm_sub_epi8 (mask_right, left_128);
; 604  : 
; 605  :   const __m128i epol_mask   = _mm_max_epi8(mask1, dwbaseids);
; 606  : 
; 607  :   const int32_t height_fourline_groups = height & ~3;
; 608  :   const int32_t height_residual_lines  = height &  3;
; 609  : 
; 610  :   __m128i sse_inc = _mm_setzero_si128();
; 611  :   int32_t y;
; 612  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r14d, DWORD PTR ref_stride$[rsp]
	mov	r15d, r8d
	movsxd	r8, DWORD PTR left$[rsp]
	mov	r10d, 3
	sub	r10d, DWORD PTR right$[rsp]
	mov	edi, r9d
	movsx	eax, r10b
	mov	rsi, rdx
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vpaddb	xmm1, xmm0, XMMWORD PTR __xmm@0c0c0c0c080808080404040400000000
	vpminsb	xmm2, xmm1, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	movsx	eax, r8b
	mov	rbp, rcx
	vmovd	xmm0, eax
	vmovaps	XMMWORD PTR [rsp], xmm6
	mov	eax, r15d
	and	eax, 3
	mov	QWORD PTR [rsp+64], r12
	mov	r12d, r15d
	mov	DWORD PTR tv639[rsp], eax
	xor	eax, eax
	and	r12d, -4
	test	r8d, r8d
	vpbroadcastb xmm0, xmm0
	cmovne	r10d, r8d
	movsxd	rbx, r10d
	sar	rbx, 31
	or	rbx, r8
	vpsubb	xmm0, xmm2, xmm0
	vpmaxsb	xmm6, xmm0, XMMWORD PTR __xmm@0c0c0c0c080808080404040400000000
	vpxor	xmm5, xmm5, xmm5
	test	r12d, r12d
	jle	$LN3@hor_sad_ss

; 583  :   const int32_t right_border_idx = 3 - right;

	mov	QWORD PTR [rsp+72], r13
	mov	r13d, 2
	npad	1
$LL4@hor_sad_ss:

; 613  :     __m128i a = _mm_cvtsi32_si128(*(const uint32_t *)(pic_data + y * pic_stride));
; 614  :     __m128i b = _mm_cvtsi32_si128(*(const uint32_t *)(ref_data + y * ref_stride + leftoff));
; 615  : 
; 616  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 1) * pic_stride),           1);
; 617  :     b = _mm_insert_epi32(b, *(const uint32_t *)(ref_data + (y + 1) * ref_stride + leftoff), 1);
; 618  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 2) * pic_stride),           2);
; 619  :     b = _mm_insert_epi32(b, *(const uint32_t *)(ref_data + (y + 2) * ref_stride + leftoff), 2);
; 620  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 3) * pic_stride),           3);

	lea	r10d, DWORD PTR [r13+1]
	mov	r8d, r13d
	mov	r9d, r10d
	imul	r8d, edi
	imul	r9d, edi
	lea	r11d, DWORD PTR [r13-1]
	mov	edx, r11d

; 621  :     b = _mm_insert_epi32(b, *(const uint32_t *)(ref_data + (y + 3) * ref_stride + leftoff), 3);

	imul	r10d, r14d
	imul	edx, edi
	mov	ecx, eax
	imul	ecx, edi
	imul	r11d, r14d
	add	r10, rbx
	vmovd	xmm0, DWORD PTR [rcx+rbp]
	vpinsrd	xmm1, xmm0, DWORD PTR [rdx+rbp], 1
	vpinsrd	xmm2, xmm1, DWORD PTR [r8+rbp], 2
	vpinsrd	xmm4, xmm2, DWORD PTR [r9+rbp], 3
	mov	edx, eax
	mov	r8d, r11d
	mov	r9d, r13d
	imul	edx, r14d
	imul	r9d, r14d
	add	r8, rbx
	add	eax, 4
	add	r13d, 4
	add	rdx, rbx
	add	r9, rbx
	vmovd	xmm0, DWORD PTR [rdx+rsi]
	vpinsrd	xmm1, xmm0, DWORD PTR [r8+rsi], 1
	vpinsrd	xmm2, xmm1, DWORD PTR [r9+rsi], 2
	vpinsrd	xmm3, xmm2, DWORD PTR [r10+rsi], 3

; 622  : 
; 623  :     __m128i b_epol    = _mm_shuffle_epi8(b,       epol_mask);

	vpshufb	xmm0, xmm3, xmm6

; 624  :     __m128i curr_sads = _mm_sad_epu8    (a,       b_epol);

	vpsadbw	xmm1, xmm4, xmm0

; 625  :             sse_inc   = _mm_add_epi64   (sse_inc, curr_sads);

	vpaddq	xmm5, xmm1, xmm5
	cmp	eax, r12d
	jl	$LL4@hor_sad_ss

; 584  :   const int32_t border_idx       = left ? left : right_border_idx;
; 585  : 
; 586  :   const __m128i ns               = _mm_setr_epi8(0,  1,  2,  3,  4,  5,  6,  7,
; 587  :                                                  8,  9,  10, 11, 12, 13, 14, 15);
; 588  : 
; 589  :   const int32_t border_idx_negative = border_idx >> 31;
; 590  :   const int32_t leftoff             = border_idx_negative | left;
; 591  : 
; 592  :   // Dualword (ie. line) base indexes, ie. the edges the lines read will be
; 593  :   // clamped towards
; 594  :   const __m128i dwbaseids   = _mm_setr_epi8(0, 0, 0, 0, 4, 4, 4, 4,
; 595  :                                             8, 8, 8, 8, 12, 12, 12, 12);
; 596  : 
; 597  :   __m128i right_border_idxs = _mm_set1_epi8((int8_t)right_border_idx);
; 598  :   __m128i left_128          = _mm_set1_epi8((int8_t)left);
; 599  : 
; 600  :   right_border_idxs         = _mm_add_epi8 (right_border_idxs, dwbaseids);
; 601  : 
; 602  :   __m128i mask_right        = _mm_min_epi8 (ns,         right_border_idxs);
; 603  :   __m128i mask1             = _mm_sub_epi8 (mask_right, left_128);
; 604  : 
; 605  :   const __m128i epol_mask   = _mm_max_epi8(mask1, dwbaseids);
; 606  : 
; 607  :   const int32_t height_fourline_groups = height & ~3;
; 608  :   const int32_t height_residual_lines  = height &  3;
; 609  : 
; 610  :   __m128i sse_inc = _mm_setzero_si128();
; 611  :   int32_t y;
; 612  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r13, QWORD PTR [rsp+72]
$LN3@hor_sad_ss:

; 626  :   }
; 627  :   if (height_residual_lines) {

	cmp	DWORD PTR tv639[rsp], 0
	mov	r12, QWORD PTR [rsp+64]
	je	SHORT $LN6@hor_sad_ss

; 628  :     for (; y < height; y++) {

	cmp	eax, r15d
	jge	SHORT $LN6@hor_sad_ss
	npad	4
$LL7@hor_sad_ss:

; 629  :       __m128i a = _mm_cvtsi32_si128(*(const uint32_t *)(pic_data + y * pic_stride));
; 630  :       __m128i b = _mm_cvtsi32_si128(*(const uint32_t *)(ref_data + y * ref_stride + leftoff));

	mov	edx, eax
	mov	ecx, eax
	imul	edx, r14d
	inc	eax
	imul	ecx, edi
	add	rdx, rbx
	vmovd	xmm0, DWORD PTR [rdx+rsi]
	vmovd	xmm2, DWORD PTR [rcx+rbp]

; 631  : 
; 632  :       __m128i b_epol = _mm_shuffle_epi8(b, epol_mask);

	vpshufb	xmm1, xmm0, xmm6

; 633  :       __m128i curr_sads = _mm_sad_epu8 (a, b_epol);

	vpsadbw	xmm2, xmm2, xmm1

; 634  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm5, xmm2, xmm5
	cmp	eax, r15d
	jl	SHORT $LL7@hor_sad_ss
$LN6@hor_sad_ss:

; 635  :     }
; 636  :   }
; 637  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 638  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 639  : 
; 640  :   return _mm_cvtsi128_si32(sad);
; 641  : }

	mov	rbx, QWORD PTR [rsp+80]
	vmovaps	xmm6, XMMWORD PTR [rsp]
	vpshufd	xmm0, xmm5, 78				; 0000004eH
	vpaddq	xmm1, xmm0, xmm5
	vmovd	eax, xmm1
	add	rsp, 16
	pop	r15
	pop	r14
	pop	rdi
	pop	rsi
	pop	rbp
	ret	0
hor_sad_sse41_w4 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
pic_data$ = 48
ref_data$ = 56
height$ = 64
pic_stride$ = 72
ref_stride$ = 80
left$ = 88
right$ = 96
hor_sad_sse41_w8 PROC

; 646  : {

	push	rbx
	push	rsi
	push	rdi
	sub	rsp, 16

; 650  :   const int32_t border_idx       = left ? left : right_border_idx;

	mov	r10d, DWORD PTR left$[rsp]
	mov	eax, 7
	sub	eax, DWORD PTR right$[rsp]
	mov	r11d, r8d
	mov	esi, eax
	mov	QWORD PTR [rsp+56], r12

; 651  : 
; 652  :   const __m128i ns               = _mm_setr_epi8(0,  1,  2,  3,  4,  5,  6,  7,
; 653  :                                                  8,  9,  10, 11, 12, 13, 14, 15);
; 654  : 
; 655  :   // Quadword (ie. line) base indexes, ie. the edges the lines read will be
; 656  :   // clamped towards; higher qword (lower line) bytes tend towards 8 and lower
; 657  :   // qword (higher line) bytes towards 0
; 658  :   const __m128i qwbaseids   = _mm_setr_epi8(0, 0, 0, 0, 0, 0, 0, 0,
; 659  :                                             8, 8, 8, 8, 8, 8, 8, 8);
; 660  : 
; 661  :   // Dirty hack alert! If right == block_width (ie. the entire vector is
; 662  :   // outside the frame), move the block offset one pixel to the left (so
; 663  :   // that the leftmost pixel in vector is actually the valid border pixel
; 664  :   // from which we want to extrapolate), and use an epol mask that will
; 665  :   // simply stretch the pixel all over the vector.
; 666  :   //
; 667  :   // To avoid a branch here:
; 668  :   // The mask will be -1 (0xffffffff) for border_idx -1 and 0 for >= 0
; 669  :   const int32_t border_idx_negative = border_idx >> 31;
; 670  :   const int32_t leftoff             = border_idx_negative | left;
; 671  : 
; 672  :   __m128i right_border_idxs = _mm_set1_epi8((int8_t)right_border_idx);

	movsx	eax, al
	test	r10d, r10d
	vmovd	xmm0, eax
	cmovne	esi, r10d

; 673  :   __m128i left_128          = _mm_set1_epi8((int8_t)left);

	movsx	eax, r10b
	vpbroadcastb xmm0, xmm0

; 674  : 
; 675  :   right_border_idxs         = _mm_add_epi8 (right_border_idxs, qwbaseids);

	vpaddb	xmm1, xmm0, XMMWORD PTR __xmm@08080808080808080000000000000000

; 676  : 
; 677  :   // If we're straddling the left border, right_border_idx is 7 and the first
; 678  :   // operation does nothing. If right border, left is 0 and the second
; 679  :   // operation does nothing.
; 680  :   __m128i mask_right        = _mm_min_epi8 (ns,         right_border_idxs);

	vpminsb	xmm3, xmm1, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	mov	QWORD PTR [rsp+72], r15

; 681  :   __m128i mask1             = _mm_sub_epi8 (mask_right, left_128);
; 682  : 
; 683  :   // If right == 8 (we're completely outside the frame), right_border_idx is
; 684  :   // -1 and so is mask1. Clamp negative values to qwbaseid and as discussed
; 685  :   // earlier, adjust the load offset instead to load the "-1'st" pixels and
; 686  :   // using qwbaseids as the shuffle mask, broadcast it all over the rows.
; 687  :   const __m128i epol_mask = _mm_max_epi8(mask1, qwbaseids);
; 688  : 
; 689  :   const int32_t height_fourline_groups = height & ~3;
; 690  :   const int32_t height_residual_lines  = height &  3;

	mov	r12d, r8d
	vmovd	xmm2, eax
	sar	esi, 31
	mov	r15d, r8d
	or	esi, r10d
	and	r15d, -4

; 691  : 
; 692  :   __m128i sse_inc = _mm_setzero_si128();
; 693  :   int32_t y;
; 694  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r10d, DWORD PTR ref_stride$[rsp]
	and	r12d, 3
	vpbroadcastb xmm2, xmm2
	vpsubb	xmm0, xmm3, xmm2
	vpmaxsb	xmm5, xmm0, XMMWORD PTR __xmm@08080808080808080000000000000000
	xor	eax, eax
	mov	rbx, rdx
	mov	rdi, rcx
	vpxor	xmm3, xmm3, xmm3
	test	r15d, r15d
	jle	$LN16@hor_sad_ss

; 647  :   // right is the number of overhanging pixels in the vector, so it has to be
; 648  :   // handled this way to produce the index of last valid (border) pixel
; 649  :   const int32_t right_border_idx = 7 - right;

	mov	QWORD PTR [rsp+48], rbp
	mov	ebp, 2
	mov	QWORD PTR [rsp+64], r14
	vmovaps	XMMWORD PTR [rsp], xmm6
	movsxd	r14, esi
	npad	8
$LL4@hor_sad_ss:

; 695  :     __m128d a_d = _mm_setzero_pd();
; 696  :     __m128d b_d = _mm_setzero_pd();
; 697  :     __m128d c_d = _mm_setzero_pd();
; 698  :     __m128d d_d = _mm_setzero_pd();
; 699  : 
; 700  :     a_d = _mm_loadl_pd(a_d, (const double *)(pic_data + (y + 0) * pic_stride));

	mov	ecx, eax

; 701  :     b_d = _mm_loadl_pd(b_d, (const double *)(ref_data + (y + 0) * ref_stride + leftoff));
; 702  :     a_d = _mm_loadh_pd(a_d, (const double *)(pic_data + (y + 1) * pic_stride));

	lea	r8d, DWORD PTR [rbp-1]
	imul	ecx, r9d
	mov	edx, eax
	imul	edx, r10d
	add	eax, 4
	add	rdx, r14
	vmovddup xmm1, QWORD PTR [rcx+rdi]
	mov	ecx, r8d

; 703  :     b_d = _mm_loadh_pd(b_d, (const double *)(ref_data + (y + 1) * ref_stride + leftoff));

	imul	r8d, r10d
	vmovddup xmm0, QWORD PTR [rdx+rbx]
	imul	ecx, r9d

; 704  : 
; 705  :     c_d = _mm_loadl_pd(c_d, (const double *)(pic_data + (y + 2) * pic_stride));
; 706  :     d_d = _mm_loadl_pd(d_d, (const double *)(ref_data + (y + 2) * ref_stride + leftoff));

	mov	edx, ebp
	imul	edx, r10d
	add	rdx, r14
	vmovhpd	xmm1, xmm1, QWORD PTR [rcx+rdi]
	mov	ecx, r8d

; 707  :     c_d = _mm_loadh_pd(c_d, (const double *)(pic_data + (y + 3) * pic_stride));

	lea	r8d, DWORD PTR [rbp+1]
	vmovddup xmm2, QWORD PTR [rdx+rbx]
	add	rcx, r14
	vmovhpd	xmm0, xmm0, QWORD PTR [rcx+rbx]
	mov	ecx, ebp
	add	ebp, 4
	imul	ecx, r9d

; 708  :     d_d = _mm_loadh_pd(d_d, (const double *)(ref_data + (y + 3) * ref_stride + leftoff));
; 709  : 
; 710  :     __m128i a = _mm_castpd_si128(a_d);
; 711  :     __m128i b = _mm_castpd_si128(b_d);
; 712  :     __m128i c = _mm_castpd_si128(c_d);
; 713  :     __m128i d = _mm_castpd_si128(d_d);
; 714  : 
; 715  :     __m128i b_epol = _mm_shuffle_epi8(b, epol_mask);

	vpshufb	xmm0, xmm0, xmm5

; 716  :     __m128i d_epol = _mm_shuffle_epi8(d, epol_mask);
; 717  : 
; 718  :     __m128i curr_sads_ab = _mm_sad_epu8(a, b_epol);

	vpsadbw	xmm1, xmm1, xmm0

; 719  :     __m128i curr_sads_cd = _mm_sad_epu8(c, d_epol);
; 720  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm3, xmm1, xmm3
	vmovddup xmm4, QWORD PTR [rcx+rdi]
	mov	ecx, r8d
	imul	r8d, r10d
	imul	ecx, r9d
	vmovhpd	xmm4, xmm4, QWORD PTR [rcx+rdi]
	mov	ecx, r8d
	add	rcx, r14
	vmovhpd	xmm2, xmm2, QWORD PTR [rcx+rbx]
	vpshufb	xmm2, xmm2, xmm5
	vpsadbw	xmm0, xmm4, xmm2

; 721  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vpaddq	xmm3, xmm3, xmm0
	cmp	eax, r15d
	jl	$LL4@hor_sad_ss

; 691  : 
; 692  :   __m128i sse_inc = _mm_setzero_si128();
; 693  :   int32_t y;
; 694  :   for (y = 0; y < height_fourline_groups; y += 4) {

	vmovaps	xmm6, XMMWORD PTR [rsp]
	mov	r14, QWORD PTR [rsp+64]
	mov	rbp, QWORD PTR [rsp+48]
$LN16@hor_sad_ss:

; 722  :   }
; 723  :   if (height_residual_lines) {

	mov	r15, QWORD PTR [rsp+72]
	test	r12d, r12d
	mov	r12, QWORD PTR [rsp+56]
	je	SHORT $LN6@hor_sad_ss

; 724  :     for (; y < height; y++) {

	cmp	eax, r11d
	jge	SHORT $LN6@hor_sad_ss
	movsxd	r8, esi
	npad	1
$LL7@hor_sad_ss:

; 725  :       __m128i a = _mm_loadl_epi64((__m128i *)(pic_data + y * pic_stride));
; 726  :       __m128i b = _mm_loadl_epi64((__m128i *)(ref_data + y * ref_stride + leftoff));

	mov	edx, eax
	mov	ecx, eax
	imul	edx, r10d
	inc	eax
	imul	ecx, r9d
	add	rdx, r8
	vmovq	xmm0, QWORD PTR [rdx+rbx]
	vmovq	xmm2, QWORD PTR [rcx+rdi]

; 727  : 
; 728  :       __m128i b_epol = _mm_shuffle_epi8(b, epol_mask);

	vpshufb	xmm1, xmm0, xmm5

; 729  : 
; 730  :       __m128i curr_sads_ab = _mm_sad_epu8(a, b_epol);

	vpsadbw	xmm2, xmm2, xmm1

; 731  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm3, xmm2, xmm3
	cmp	eax, r11d
	jl	SHORT $LL7@hor_sad_ss
$LN6@hor_sad_ss:

; 732  :     }
; 733  :   }
; 734  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm3, 78				; 0000004eH

; 735  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vpaddq	xmm1, xmm0, xmm3

; 736  :   return _mm_cvtsi128_si32(sad);

	vmovd	eax, xmm1

; 737  : }

	add	rsp, 16
	pop	rdi
	pop	rsi
	pop	rbx
	ret	0
hor_sad_sse41_w8 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
pic_data$ = 40
ref_data$ = 48
height$ = 56
pic_stride$ = 64
ref_stride$ = 72
left$ = 80
right$ = 88
hor_sad_sse41_w16 PROC

; 759  : {

	mov	QWORD PTR [rsp+32], rbx
	push	rbp
	push	rsi
	push	rdi
	push	r14

; 763  :   const int32_t border_idx       = left ? left : right_border_idx;
; 764  : 
; 765  :   const __m128i ns               = _mm_setr_epi8(0,  1,  2,  3,  4,  5,  6,  7,
; 766  :                                                  8,  9,  10, 11, 12, 13, 14, 15);
; 767  :   const __m128i zero             = _mm_setzero_si128();
; 768  : 
; 769  :   // Dirty hack alert! If right == block_width (ie. the entire vector is
; 770  :   // outside the frame), move the block offset one pixel to the left (so
; 771  :   // that the leftmost pixel in vector is actually the valid border pixel
; 772  :   // from which we want to extrapolate), and use an epol mask that will
; 773  :   // simply stretch the pixel all over the vector.
; 774  :   //
; 775  :   // To avoid a branch here:
; 776  :   // The mask will be -1 (0xffffffff) for border_idx -1 and 0 for >= 0
; 777  :   const int32_t border_idx_negative = border_idx >> 31;
; 778  :   const int32_t leftoff             = border_idx_negative | left;
; 779  : 
; 780  :   __m128i right_border_idxs = _mm_set1_epi8((int8_t)right_border_idx);
; 781  :   __m128i left_128          = _mm_set1_epi8((int8_t)left);
; 782  : 
; 783  :   // If we're straddling the left border, right_border_idx is 15 and the first
; 784  :   // operation does nothing. If right border, left is 0 and the second
; 785  :   // operation does nothing.
; 786  :   __m128i mask_right        = _mm_min_epi8 (ns,         right_border_idxs);
; 787  :   __m128i mask1             = _mm_sub_epi8 (mask_right, left_128);
; 788  : 
; 789  :   // If right == 16 (we're completely outside the frame), right_border_idx is
; 790  :   // -1 and so is mask1. Clamp negative values to zero and as discussed
; 791  :   // earlier, adjust the load offset instead to load the "-1'st" pixel and
; 792  :   // using an all-zero shuffle mask, broadcast it all over the vector.
; 793  :   const __m128i epol_mask = _mm_max_epi8(mask1, zero);
; 794  : 
; 795  :   const int32_t height_fourline_groups = height & ~3;
; 796  :   const int32_t height_residual_lines  = height &  3;
; 797  : 
; 798  :   __m128i sse_inc = _mm_setzero_si128();
; 799  :   int32_t y;
; 800  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	ebp, DWORD PTR ref_stride$[rsp]
	mov	r10d, 15
	sub	r10d, DWORD PTR right$[rsp]
	mov	ebx, r9d
	movsxd	r9, DWORD PTR left$[rsp]
	mov	rdi, rdx
	movsx	eax, r10b
	mov	rsi, rcx
	vmovd	xmm1, eax
	mov	QWORD PTR [rsp+40], r12
	mov	r12d, r8d
	movsx	eax, r9b
	and	r12d, -4
	mov	QWORD PTR [rsp+48], r13
	mov	r13d, r8d
	vmovd	xmm0, eax
	xor	eax, eax
	and	r13d, 3
	test	r9d, r9d
	vpbroadcastb xmm1, xmm1
	vpminsb	xmm2, xmm1, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	cmovne	r10d, r9d
	movsxd	r11, r10d
	sar	r11, 31
	vpbroadcastb xmm0, xmm0
	or	r11, r9
	vpxor	xmm3, xmm3, xmm3
	vpsubb	xmm0, xmm2, xmm0
	vpmaxsb	xmm5, xmm0, xmm3
	test	r12d, r12d
	jle	$LN3@hor_sad_ss

; 760  :   // right is the number of overhanging pixels in the vector, so it has to be
; 761  :   // handled this way to produce the index of last valid (border) pixel
; 762  :   const int32_t right_border_idx = 15 - right;

	mov	QWORD PTR [rsp+56], r15
	mov	r15d, 2
	npad	10
$LL4@hor_sad_ss:

; 801  :     __m128i a = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * pic_stride));

	mov	ecx, eax

; 802  :     __m128i b = _mm_loadu_si128((__m128i *)(ref_data + (y + 0) * ref_stride + leftoff));
; 803  :     __m128i c = _mm_loadu_si128((__m128i *)(pic_data + (y + 1) * pic_stride));

	lea	r9d, DWORD PTR [r15-1]
	imul	ecx, ebx

; 804  :     __m128i d = _mm_loadu_si128((__m128i *)(ref_data + (y + 1) * ref_stride + leftoff));
; 805  :     __m128i e = _mm_loadu_si128((__m128i *)(pic_data + (y + 2) * pic_stride));
; 806  :     __m128i f = _mm_loadu_si128((__m128i *)(ref_data + (y + 2) * ref_stride + leftoff));
; 807  :     __m128i g = _mm_loadu_si128((__m128i *)(pic_data + (y + 3) * pic_stride));

	lea	r10d, DWORD PTR [r15+1]
	mov	edx, eax
	add	eax, 4
	imul	edx, ebp
	add	rdx, r11

; 808  :     __m128i h = _mm_loadu_si128((__m128i *)(ref_data + (y + 3) * ref_stride + leftoff));
; 809  : 
; 810  :     __m128i b_epol = _mm_shuffle_epi8(b, epol_mask);

	vmovdqu	xmm0, XMMWORD PTR [rdx+rdi]
	mov	edx, r9d
	imul	r9d, ebp
	imul	edx, ebx
	vpshufb	xmm2, xmm0, xmm5

; 811  :     __m128i d_epol = _mm_shuffle_epi8(d, epol_mask);
; 812  :     __m128i f_epol = _mm_shuffle_epi8(f, epol_mask);
; 813  :     __m128i h_epol = _mm_shuffle_epi8(h, epol_mask);
; 814  : 
; 815  :     __m128i curr_sads_ab = _mm_sad_epu8(a, b_epol);

	vpsadbw	xmm2, xmm2, XMMWORD PTR [rcx+rsi]

; 816  :     __m128i curr_sads_cd = _mm_sad_epu8(c, d_epol);
; 817  :     __m128i curr_sads_ef = _mm_sad_epu8(e, f_epol);
; 818  :     __m128i curr_sads_gh = _mm_sad_epu8(g, h_epol);
; 819  : 
; 820  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm3, xmm2, xmm3
	mov	ecx, r9d
	add	rcx, r11
	vmovdqu	xmm0, XMMWORD PTR [rcx+rdi]
	mov	ecx, r15d
	imul	ecx, ebx
	vpshufb	xmm2, xmm0, xmm5
	vpsadbw	xmm2, xmm2, XMMWORD PTR [rdx+rsi]

; 821  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vpaddq	xmm4, xmm3, xmm2
	mov	edx, r15d
	add	r15d, 4
	imul	edx, ebp
	add	rdx, r11
	vmovdqu	xmm0, XMMWORD PTR [rdx+rdi]
	mov	edx, r10d
	imul	r10d, ebp
	vpshufb	xmm2, xmm0, xmm5
	vpsadbw	xmm2, xmm2, XMMWORD PTR [rcx+rsi]

; 822  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	vpaddq	xmm3, xmm4, xmm2
	imul	edx, ebx
	mov	ecx, r10d
	add	rcx, r11
	vmovdqu	xmm0, XMMWORD PTR [rcx+rdi]
	vpshufb	xmm2, xmm0, xmm5
	vpsadbw	xmm2, xmm2, XMMWORD PTR [rdx+rsi]

; 823  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	vpaddq	xmm3, xmm3, xmm2
	cmp	eax, r12d
	jl	$LL4@hor_sad_ss

; 763  :   const int32_t border_idx       = left ? left : right_border_idx;
; 764  : 
; 765  :   const __m128i ns               = _mm_setr_epi8(0,  1,  2,  3,  4,  5,  6,  7,
; 766  :                                                  8,  9,  10, 11, 12, 13, 14, 15);
; 767  :   const __m128i zero             = _mm_setzero_si128();
; 768  : 
; 769  :   // Dirty hack alert! If right == block_width (ie. the entire vector is
; 770  :   // outside the frame), move the block offset one pixel to the left (so
; 771  :   // that the leftmost pixel in vector is actually the valid border pixel
; 772  :   // from which we want to extrapolate), and use an epol mask that will
; 773  :   // simply stretch the pixel all over the vector.
; 774  :   //
; 775  :   // To avoid a branch here:
; 776  :   // The mask will be -1 (0xffffffff) for border_idx -1 and 0 for >= 0
; 777  :   const int32_t border_idx_negative = border_idx >> 31;
; 778  :   const int32_t leftoff             = border_idx_negative | left;
; 779  : 
; 780  :   __m128i right_border_idxs = _mm_set1_epi8((int8_t)right_border_idx);
; 781  :   __m128i left_128          = _mm_set1_epi8((int8_t)left);
; 782  : 
; 783  :   // If we're straddling the left border, right_border_idx is 15 and the first
; 784  :   // operation does nothing. If right border, left is 0 and the second
; 785  :   // operation does nothing.
; 786  :   __m128i mask_right        = _mm_min_epi8 (ns,         right_border_idxs);
; 787  :   __m128i mask1             = _mm_sub_epi8 (mask_right, left_128);
; 788  : 
; 789  :   // If right == 16 (we're completely outside the frame), right_border_idx is
; 790  :   // -1 and so is mask1. Clamp negative values to zero and as discussed
; 791  :   // earlier, adjust the load offset instead to load the "-1'st" pixel and
; 792  :   // using an all-zero shuffle mask, broadcast it all over the vector.
; 793  :   const __m128i epol_mask = _mm_max_epi8(mask1, zero);
; 794  : 
; 795  :   const int32_t height_fourline_groups = height & ~3;
; 796  :   const int32_t height_residual_lines  = height &  3;
; 797  : 
; 798  :   __m128i sse_inc = _mm_setzero_si128();
; 799  :   int32_t y;
; 800  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r15, QWORD PTR [rsp+56]
$LN3@hor_sad_ss:
	mov	r12, QWORD PTR [rsp+40]

; 824  :   }
; 825  :   if (height_residual_lines) {

	test	r13d, r13d
	mov	r13, QWORD PTR [rsp+48]
	je	SHORT $LN6@hor_sad_ss

; 826  :     for (; y < height; y++) {

	cmp	eax, r8d
	jge	SHORT $LN6@hor_sad_ss
	npad	7
$LL7@hor_sad_ss:

; 827  :       __m128i a = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * pic_stride));
; 828  :       __m128i b = _mm_loadu_si128((__m128i *)(ref_data + (y + 0) * ref_stride + leftoff));

	mov	edx, eax
	mov	ecx, eax
	imul	edx, ebp
	inc	eax
	imul	ecx, ebx
	add	rdx, r11

; 829  :       __m128i b_epol = _mm_shuffle_epi8(b, epol_mask);

	vmovdqu	xmm0, XMMWORD PTR [rdx+rdi]
	vpshufb	xmm2, xmm0, xmm5

; 830  :       __m128i curr_sads = _mm_sad_epu8(a, b_epol);

	vpsadbw	xmm2, xmm2, XMMWORD PTR [rcx+rsi]

; 831  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm3, xmm2, xmm3
	cmp	eax, r8d
	jl	SHORT $LL7@hor_sad_ss
$LN6@hor_sad_ss:

; 832  :     }
; 833  :   }
; 834  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 835  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 836  :   return _mm_cvtsi128_si32(sad);
; 837  : }

	mov	rbx, QWORD PTR [rsp+64]
	vpshufd	xmm0, xmm3, 78				; 0000004eH
	vpaddq	xmm1, xmm0, xmm3
	vmovd	eax, xmm1
	pop	r14
	pop	rdi
	pop	rsi
	pop	rbp
	ret	0
hor_sad_sse41_w16 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
border_off$1$ = 0
a_off$1$ = 4
is_left_bm$1$ = 8
invec_lstart$1$ = 12
invec_linc$1$ = 16
x$2$ = 20
tv1895 = 20
invec_lend$1$ = 24
a_off$1$ = 28
outside_vecs$1$ = 32
y$1$ = 36
tv1877 = 40
height_fourline_groups$1$ = 44
tv1868 = 48
tv1896 = 52
tv1885 = 56
tv1866 = 56
tv1884 = 64
tv1867 = 64
tv1892 = 72
tv1873 = 72
tv1881 = 80
tv1865 = 80
old_f$1$ = 96
tv1864 = 96
old_d$1$ = 112
tv1880 = 112
old_b$1$ = 128
tv1863 = 128
tv1989 = 144
tv1889 = 152
move_old_to_b_imask$1$ = 160
shufmask1$1$ = 176
tv1872 = 192
tv1871 = 200
old_h$1$ = 208
blk_widths$1$ = 224
is_left$1$ = 240
pic_data$ = 480
ref_data$ = 488
width$ = 496
height$ = 504
pic_stride$ = 512
ref_stride$ = 520
left$ = 528
right$ = 536
hor_sad_sse41_arbitrary PROC

; 842  : {

	mov	rax, rsp
	mov	QWORD PTR [rax+8], rbx
	mov	DWORD PTR [rax+32], r9d
	mov	QWORD PTR [rax+16], rdx
	push	rbp
	push	rsi
	push	rdi
	push	r12
	push	r13
	push	r14
	push	r15
	sub	rsp, 416				; 000001a0H

; 843  :   __m128i sse_inc = _mm_setzero_si128();
; 844  : 
; 845  :   const size_t vec_width = 16;
; 846  :   const size_t vecwid_bitmask = 15;
; 847  :   const size_t vec_width_log2 = 4;
; 848  : 
; 849  :   const int32_t height_fourline_groups = height & ~3;
; 850  :   const int32_t height_residual_lines  = height &  3;
; 851  : 
; 852  :   const __m128i rights     = _mm_set1_epi8((uint8_t)right);

	mov	ebx, DWORD PTR right$[rsp]
	mov	rsi, rdx

; 853  :   const __m128i blk_widths = _mm_set1_epi8((uint8_t)width);
; 854  :   const __m128i vec_widths = _mm_set1_epi8((uint8_t)vec_width);
; 855  :   const __m128i nslo       = _mm_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
; 856  : 
; 857  :   uint32_t outside_vecs,  inside_vecs,  left_offset, is_left_bm;
; 858  :   int32_t  outside_width, inside_width, border_off,  invec_lstart,
; 859  :            invec_lend,    invec_linc;
; 860  :   if (left) {

	mov	r13d, DWORD PTR left$[rsp]
	mov	r10, rcx
	vmovdqu	xmm5, XMMWORD PTR __xmm@10101010101010101010101010101010
	vmovaps	XMMWORD PTR [rax-72], xmm6
	vmovaps	XMMWORD PTR [rax-88], xmm7
	vmovaps	XMMWORD PTR [rax-104], xmm8
	vmovaps	XMMWORD PTR [rax-120], xmm9
	vmovdqu	xmm9, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	vmovaps	XMMWORD PTR [rax-136], xmm10
	vmovaps	XMMWORD PTR [rax-168], xmm12
	vmovaps	XMMWORD PTR [rax-216], xmm15
	mov	eax, r9d
	and	eax, -4
	movsxd	rdx, r8d
	mov	DWORD PTR height_fourline_groups$1$[rsp], eax
	add	rdx, 15
	mov	eax, r9d
	shr	rdx, 4
	and	eax, 3
	mov	DWORD PTR tv1896[rsp], eax
	movsx	eax, bl
	vmovd	xmm0, eax
	movsx	eax, r8b
	vmovd	xmm8, eax
	vpbroadcastb xmm8, xmm8
	vmovdqu	XMMWORD PTR blk_widths$1$[rsp], xmm8
	vpxor	xmm7, xmm7, xmm7
	vpxor	xmm15, xmm15, xmm15
	vpbroadcastb xmm0, xmm0
	test	r13d, r13d
	je	SHORT $LN20@hor_sad_ss

; 861  :     outside_vecs  =    left                              >> vec_width_log2;

	mov	r11d, r13d

; 862  :     inside_vecs   = (( width           + vecwid_bitmask) >> vec_width_log2) - outside_vecs;
; 863  :     outside_width =    outside_vecs * vec_width;
; 864  :     inside_width  =    inside_vecs  * vec_width;
; 865  :     left_offset   =    left;
; 866  :     border_off    =    left;

	mov	DWORD PTR border_off$1$[rsp], r13d
	shr	r11d, 4
	mov	r14d, r13d
	sub	edx, r11d

; 867  :     invec_lstart  =    0;
; 868  :     invec_lend    =    inside_vecs;
; 869  :     invec_linc    =    1;

	mov	DWORD PTR invec_linc$1$[rsp], 1
	mov	ebp, edx
	xor	eax, eax
	mov	r15d, edx

; 870  :     is_left_bm    =    -1;

	mov	r12d, -1				; ffffffffH

; 871  :   } else {

	jmp	SHORT $LN54@hor_sad_ss
$LN20@hor_sad_ss:

; 872  :     inside_vecs   =  ((width - right) + vecwid_bitmask)  >> vec_width_log2;

	mov	eax, r8d

; 873  :     outside_vecs  = (( width          + vecwid_bitmask)  >> vec_width_log2) - inside_vecs;

	mov	r11d, edx
	sub	eax, ebx

; 874  :     outside_width =    outside_vecs * vec_width;
; 875  :     inside_width  =    inside_vecs  * vec_width;
; 876  :     left_offset   =    right - width;

	mov	r13d, ebx
	sub	r13d, r8d

; 877  :     border_off    =    width - 1 - right;
; 878  :     invec_lstart  =    inside_vecs - 1;
; 879  :     invec_lend    =    -1;

	mov	r15d, -1

; 880  :     invec_linc    =    -1;

	mov	DWORD PTR invec_linc$1$[rsp], r15d
	lea	rbp, QWORD PTR [rax+15]
	shr	rbp, 4
	lea	r14d, DWORD PTR [rax-1]
	sub	r11d, ebp
	mov	DWORD PTR border_off$1$[rsp], r14d

; 881  :     is_left_bm    =    0;

	xor	r12d, r12d
	lea	eax, DWORD PTR [rbp-1]
$LN54@hor_sad_ss:

; 884  : 
; 885  :   const __m128i left_offsets = _mm_set1_epi8 ((uint8_t)left_offset);
; 886  :   const __m128i is_left      = _mm_cmpeq_epi8(rights, _mm_setzero_si128());
; 887  :   const __m128i vw_for_left  = _mm_and_si128 (is_left, vec_widths);
; 888  : 
; 889  :   // -x == (x ^ 0xff) + 1 = (x ^ 0xff) - 0xff. Also x == (x ^ 0x00) - 0x00.
; 890  :   // in other words, calculate inverse of left_offsets if is_left is true.
; 891  :   const __m128i offs_neg            = _mm_xor_si128 (left_offsets, is_left);
; 892  :   const __m128i offs_for_sm1        = _mm_sub_epi8  (offs_neg,     is_left);
; 893  : 
; 894  :   const __m128i ns_for_sm1          = _mm_or_si128  (vw_for_left,  nslo);
; 895  :   const __m128i shufmask1           = _mm_add_epi8  (ns_for_sm1,   offs_for_sm1);
; 896  : 
; 897  :   const __m128i mo2bmask_l          = _mm_cmpgt_epi8(left_offsets, nslo);
; 898  :   const __m128i mo2bimask_l         = _mm_cmpeq_epi8(mo2bmask_l,   _mm_setzero_si128());
; 899  :   const __m128i mo2bimask_r         = _mm_cmpgt_epi8(vec_widths,   shufmask1);
; 900  :   const __m128i move_old_to_b_imask = _mm_blendv_epi8(mo2bimask_r, mo2bimask_l, is_left);
; 901  : 
; 902  :   const int32_t outvec_offset = (~is_left_bm) & inside_width;
; 903  :   int32_t x, y;
; 904  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	edx, DWORD PTR ref_stride$[rsp]
	and	r13d, 15
	mov	edi, DWORD PTR pic_stride$[rsp]
	xor	r9d, r9d
	mov	DWORD PTR invec_lstart$1$[rsp], eax
	mov	ebx, r11d
	vpcmpeqb xmm10, xmm7, xmm0
	movsx	eax, r13b
	vmovd	xmm4, eax
	mov	eax, ebp
	shl	ebx, 4
	shl	eax, 4
	movsxd	r8, eax
	mov	eax, r12d
	vpbroadcastb xmm4, xmm4
	vpxor	xmm0, xmm10, xmm4
	vpand	xmm1, xmm10, xmm5
	vpor	xmm2, xmm1, xmm9
	vpsubb	xmm3, xmm0, xmm10
	vpaddb	xmm6, xmm3, xmm2
	not	eax
	mov	DWORD PTR is_left_bm$1$[rsp], r12d
	vpcmpgtb xmm0, xmm4, xmm9
	cdqe
	and	r8, rax
	mov	DWORD PTR invec_lend$1$[rsp], r15d
	vpcmpeqb xmm2, xmm0, xmm7
	vpcmpgtb xmm1, xmm5, xmm6
	vpblendvb xmm12, xmm1, xmm2, xmm10
	mov	DWORD PTR outside_vecs$1$[rsp], r11d
	mov	QWORD PTR tv1989[rsp], rbp
	vmovaps	XMMWORD PTR [rsp+320], xmm11
	mov	DWORD PTR a_off$1$[rsp], ebx
	mov	DWORD PTR left$[rsp], r13d
	vmovdqu	XMMWORD PTR is_left$1$[rsp], xmm10
	vmovdqu	XMMWORD PTR shufmask1$1$[rsp], xmm6
	vmovdqu	XMMWORD PTR move_old_to_b_imask$1$[rsp], xmm12
	mov	DWORD PTR y$1$[rsp], r9d
	mov	QWORD PTR tv1889[rsp], r8
	cmp	DWORD PTR height_fourline_groups$1$[rsp], r9d
	jle	$LN3@hor_sad_ss

; 882  :   }
; 883  :   left_offset &= vecwid_bitmask;

	mov	eax, ebx
	vmovaps	XMMWORD PTR [rsp+288], xmm13
	and	eax, r12d
	vmovaps	XMMWORD PTR [rsp+272], xmm14
	mov	ebx, 2
	mov	DWORD PTR a_off$1$[rsp], eax
	mov	DWORD PTR tv1877[rsp], ebx

; 884  : 
; 885  :   const __m128i left_offsets = _mm_set1_epi8 ((uint8_t)left_offset);
; 886  :   const __m128i is_left      = _mm_cmpeq_epi8(rights, _mm_setzero_si128());
; 887  :   const __m128i vw_for_left  = _mm_and_si128 (is_left, vec_widths);
; 888  : 
; 889  :   // -x == (x ^ 0xff) + 1 = (x ^ 0xff) - 0xff. Also x == (x ^ 0x00) - 0x00.
; 890  :   // in other words, calculate inverse of left_offsets if is_left is true.
; 891  :   const __m128i offs_neg            = _mm_xor_si128 (left_offsets, is_left);
; 892  :   const __m128i offs_for_sm1        = _mm_sub_epi8  (offs_neg,     is_left);
; 893  : 
; 894  :   const __m128i ns_for_sm1          = _mm_or_si128  (vw_for_left,  nslo);
; 895  :   const __m128i shufmask1           = _mm_add_epi8  (ns_for_sm1,   offs_for_sm1);
; 896  : 
; 897  :   const __m128i mo2bmask_l          = _mm_cmpgt_epi8(left_offsets, nslo);
; 898  :   const __m128i mo2bimask_l         = _mm_cmpeq_epi8(mo2bmask_l,   _mm_setzero_si128());
; 899  :   const __m128i mo2bimask_r         = _mm_cmpgt_epi8(vec_widths,   shufmask1);
; 900  :   const __m128i move_old_to_b_imask = _mm_blendv_epi8(mo2bimask_r, mo2bimask_l, is_left);
; 901  : 
; 902  :   const int32_t outvec_offset = (~is_left_bm) & inside_width;
; 903  :   int32_t x, y;
; 904  :   for (y = 0; y < height_fourline_groups; y += 4) {

	jmp	SHORT $LN4@hor_sad_ss
	npad	5
$LL53@hor_sad_ss:
	mov	r14d, DWORD PTR border_off$1$[rsp]
$LN4@hor_sad_ss:

; 905  :     __m128i borderpx_vec_b = _mm_set1_epi8(ref_data[(int32_t)((y + 0) * ref_stride + border_off)]);

	mov	r12d, r9d

; 906  :     __m128i borderpx_vec_d = _mm_set1_epi8(ref_data[(int32_t)((y + 1) * ref_stride + border_off)]);
; 907  :     __m128i borderpx_vec_f = _mm_set1_epi8(ref_data[(int32_t)((y + 2) * ref_stride + border_off)]);

	mov	r13d, ebx
	imul	r12d, edx
	imul	r13d, edx
	mov	DWORD PTR tv1895[rsp], r12d
	lea	eax, DWORD PTR [r12+r14]
	movsxd	rcx, eax
	movsx	eax, BYTE PTR [rcx+rsi]
	vmovd	xmm3, eax
	lea	eax, DWORD PTR [rbx-1]
	imul	eax, edx
	vpbroadcastb xmm3, xmm3
	vmovdqu	XMMWORD PTR old_b$1$[rsp], xmm3
	add	eax, r14d
	cdqe
	movsx	ecx, BYTE PTR [rax+rsi]
	lea	eax, DWORD PTR [r14+r13]
	vmovd	xmm4, ecx
	movsxd	rcx, eax

; 908  :     __m128i borderpx_vec_h = _mm_set1_epi8(ref_data[(int32_t)((y + 3) * ref_stride + border_off)]);

	lea	r14d, DWORD PTR [rbx+1]
	vpbroadcastb xmm4, xmm4
	vmovdqu	XMMWORD PTR old_d$1$[rsp], xmm4
	movsx	eax, BYTE PTR [rcx+rsi]
	vmovd	xmm5, eax
	mov	eax, r14d
	imul	eax, edx

; 909  : 
; 910  :     for (x = 0; x < outside_vecs; x++) {

	xor	edx, edx
	vpbroadcastb xmm5, xmm5
	vmovdqu	XMMWORD PTR old_f$1$[rsp], xmm5
	mov	DWORD PTR tv1868[rsp], eax
	add	eax, DWORD PTR border_off$1$[rsp]
	movsxd	rcx, eax
	movsx	eax, BYTE PTR [rcx+rsi]
	vmovd	xmm14, eax
	vpbroadcastb xmm14, xmm14
	vmovdqu	XMMWORD PTR old_h$1$[rsp], xmm14
	test	r11d, r11d
	je	$LN43@hor_sad_ss

; 911  :       __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + outvec_offset));

	mov	eax, r9d
	imul	eax, edi
	vmovdqu	xmm11, xmm3
	vmovdqu	xmm12, xmm4
	vmovdqu	xmm13, xmm5
	mov	QWORD PTR tv1885[rsp], rax
	lea	eax, DWORD PTR [rbx-1]
	imul	eax, edi
	mov	QWORD PTR tv1867[rsp], rax
	mov	eax, ebx
	mov	rsi, QWORD PTR tv1867[rsp]
	imul	eax, edi
	mov	QWORD PTR tv1873[rsp], rax
	mov	eax, r14d
	mov	r15, QWORD PTR tv1873[rsp]
	imul	eax, edi
	mov	rdi, QWORD PTR tv1885[rsp]
	mov	QWORD PTR tv1865[rsp], rax
	mov	r12d, eax
	npad	6
$LL7@hor_sad_ss:
	movsxd	rcx, edx
	add	rcx, rcx
	lea	rax, QWORD PTR [rdi+rcx*8]
	add	rax, r8
	vmovdqu	xmm3, XMMWORD PTR [rax+r10]

; 912  :       __m128i c = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 1) * pic_stride + outvec_offset));

	lea	rax, QWORD PTR [rsi+rcx*8]
	add	rax, r8
	vmovdqu	xmm4, XMMWORD PTR [rax+r10]

; 913  :       __m128i e = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 2) * pic_stride + outvec_offset));

	lea	rax, QWORD PTR [r15+rcx*8]
	add	rax, r8
	vmovdqu	xmm5, XMMWORD PTR [rax+r10]

; 914  :       __m128i g = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 3) * pic_stride + outvec_offset));

	lea	rax, QWORD PTR [r12+rcx*8]
	add	rax, r8
	vmovdqu	xmm6, XMMWORD PTR [rax+r10]

; 915  : 
; 916  :       __m128i startoffs  = _mm_set1_epi8  ((x + inside_vecs) << vec_width_log2);

	lea	eax, DWORD PTR [rdx+rbp]
	inc	edx
	shl	al, 4
	movsx	eax, al
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0

; 917  :       __m128i ns         = _mm_add_epi8   (startoffs, nslo);

	vpaddb	xmm0, xmm0, xmm9

; 918  : 
; 919  :       // Unread imask is (is_left NOR unrd_imask_for_right), do the maths etc
; 920  :       __m128i unrd_imask = _mm_cmpgt_epi8 (blk_widths, ns);

	vpcmpgtb xmm1, xmm8, xmm0

; 921  :               unrd_imask = _mm_or_si128   (unrd_imask, is_left);

	vpor	xmm2, xmm1, xmm10

; 922  :       __m128i unrd_mask  = _mm_cmpeq_epi8 (unrd_imask, _mm_setzero_si128());

	vpcmpeqb xmm0, xmm2, xmm7

; 923  : 
; 924  :       __m128i b_unread   = _mm_blendv_epi8(borderpx_vec_b, a, unrd_mask);

	vpblendvb xmm1, xmm11, xmm3, xmm0

; 925  :       __m128i d_unread   = _mm_blendv_epi8(borderpx_vec_d, c, unrd_mask);
; 926  :       __m128i f_unread   = _mm_blendv_epi8(borderpx_vec_f, e, unrd_mask);
; 927  :       __m128i h_unread   = _mm_blendv_epi8(borderpx_vec_h, g, unrd_mask);
; 928  : 
; 929  :       __m128i sad_ab     = _mm_sad_epu8   (a, b_unread);

	vpsadbw	xmm2, xmm3, xmm1

; 930  :       __m128i sad_cd     = _mm_sad_epu8   (c, d_unread);
; 931  :       __m128i sad_ef     = _mm_sad_epu8   (e, f_unread);
; 932  :       __m128i sad_gh     = _mm_sad_epu8   (g, h_unread);
; 933  : 
; 934  :       sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	vpaddq	xmm3, xmm2, xmm15
	vpblendvb xmm1, xmm12, xmm4, xmm0
	vpsadbw	xmm2, xmm4, xmm1

; 935  :       sse_inc = _mm_add_epi64(sse_inc, sad_cd);

	vpaddq	xmm4, xmm3, xmm2
	vpblendvb xmm1, xmm13, xmm5, xmm0
	vpsadbw	xmm2, xmm5, xmm1

; 936  :       sse_inc = _mm_add_epi64(sse_inc, sad_ef);

	vpaddq	xmm3, xmm4, xmm2
	vpblendvb xmm1, xmm14, xmm6, xmm0
	vpsadbw	xmm2, xmm6, xmm1

; 937  :       sse_inc = _mm_add_epi64(sse_inc, sad_gh);

	vpaddq	xmm15, xmm3, xmm2
	cmp	edx, r11d
	jb	$LL7@hor_sad_ss
	vmovdqu	xmm3, XMMWORD PTR old_b$1$[rsp]
	vmovdqu	xmm4, XMMWORD PTR old_d$1$[rsp]
	vmovdqu	xmm5, XMMWORD PTR old_f$1$[rsp]
	vmovdqu	xmm6, XMMWORD PTR shufmask1$1$[rsp]
	mov	edi, DWORD PTR pic_stride$[rsp]
	mov	rsi, QWORD PTR ref_data$[rsp]
	mov	r15d, DWORD PTR invec_lend$1$[rsp]
	mov	r12d, DWORD PTR tv1895[rsp]
$LN43@hor_sad_ss:

; 941  : 
; 942  :     __m128i old_b = borderpx_vec_b;
; 943  :     __m128i old_d = borderpx_vec_d;
; 944  :     __m128i old_f = borderpx_vec_f;
; 945  :     __m128i old_h = borderpx_vec_h;
; 946  : 
; 947  :     for (x = invec_lstart; x != invec_lend; x += invec_linc) {

	mov	eax, DWORD PTR invec_lstart$1$[rsp]
	mov	DWORD PTR x$2$[rsp], eax
	cmp	eax, r15d
	je	$LN52@hor_sad_ss

; 938  :     }
; 939  :     int32_t a_off = outside_width & is_left_bm;
; 940  :     int32_t leftoff_with_sign_neg = (left_offset ^ is_left_bm) - is_left_bm;

	mov	eax, DWORD PTR is_left_bm$1$[rsp]
	xor	eax, DWORD PTR left$[rsp]
	sub	eax, DWORD PTR is_left_bm$1$[rsp]
	mov	r8d, DWORD PTR invec_linc$1$[rsp]

; 948  :       __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + a_off));
; 949  :       __m128i c = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 1) * pic_stride + a_off));
; 950  :       __m128i e = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 2) * pic_stride + a_off));
; 951  :       __m128i g = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 3) * pic_stride + a_off));
; 952  :       __m128i b = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 0) * ref_stride + a_off - leftoff_with_sign_neg));

	movsxd	rcx, eax
	mov	eax, r9d
	imul	eax, edi
	imul	r14d, edi
	mov	QWORD PTR tv1892[rsp], rcx
	lea	ecx, DWORD PTR [rbx-1]
	mov	QWORD PTR tv1884[rsp], rax
	movsxd	rax, DWORD PTR a_off$1$[rsp]
	mov	QWORD PTR tv1881[rsp], rax
	mov	eax, ecx
	imul	eax, edi
	mov	QWORD PTR tv1866[rsp], rax
	mov	eax, ebx
	mov	rdx, QWORD PTR tv1866[rsp]
	imul	eax, edi
	mov	rdi, QWORD PTR tv1884[rsp]
	mov	QWORD PTR tv1872[rsp], rax
	mov	rbp, QWORD PTR tv1872[rsp]
	mov	eax, r14d
	mov	r14, QWORD PTR tv1881[rsp]
	mov	QWORD PTR tv1864[rsp], rax
	mov	r11, QWORD PTR tv1864[rsp]
	mov	eax, r12d
	mov	r12d, DWORD PTR x$2$[rsp]
	mov	QWORD PTR tv1880[rsp], rax
	mov	eax, DWORD PTR ref_stride$[rsp]
	mov	r9, QWORD PTR tv1880[rsp]
	imul	eax, ecx
	mov	QWORD PTR tv1863[rsp], rax
	mov	rbx, QWORD PTR tv1863[rsp]
	mov	eax, r13d
	mov	r13, QWORD PTR tv1892[rsp]
	mov	QWORD PTR tv1871[rsp], rax
	npad	7
$LL10@hor_sad_ss:

; 953  :       __m128i d = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 1) * ref_stride + a_off - leftoff_with_sign_neg));
; 954  :       __m128i f = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 2) * ref_stride + a_off - leftoff_with_sign_neg));
; 955  :       __m128i h = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 3) * ref_stride + a_off - leftoff_with_sign_neg));
; 956  : 
; 957  :       __m128i b_shifted    = _mm_shuffle_epi8(b,     shufmask1);
; 958  :       __m128i d_shifted    = _mm_shuffle_epi8(d,     shufmask1);
; 959  :       __m128i f_shifted    = _mm_shuffle_epi8(f,     shufmask1);
; 960  :       __m128i h_shifted    = _mm_shuffle_epi8(h,     shufmask1);
; 961  : 
; 962  :       __m128i b_with_old   = _mm_blendv_epi8 (old_b, b_shifted, move_old_to_b_imask);
; 963  :       __m128i d_with_old   = _mm_blendv_epi8 (old_d, d_shifted, move_old_to_b_imask);
; 964  :       __m128i f_with_old   = _mm_blendv_epi8 (old_f, f_shifted, move_old_to_b_imask);
; 965  :       __m128i h_with_old   = _mm_blendv_epi8 (old_h, h_shifted, move_old_to_b_imask);
; 966  : 
; 967  :       uint8_t startoff     = (x << vec_width_log2) + a_off;
; 968  :       __m128i startoffs    = _mm_set1_epi8   (startoff);
; 969  :       __m128i curr_ns      = _mm_add_epi8    (startoffs,    nslo);
; 970  :       __m128i unrd_imask   = _mm_cmpgt_epi8  (blk_widths,   curr_ns);

	vmovdqu	xmm1, XMMWORD PTR blk_widths$1$[rsp]
	movsxd	rcx, r12d
	lea	rax, QWORD PTR [r14+rdi]
	shl	rcx, 4
	add	rax, rcx
	vmovdqu	xmm14, XMMWORD PTR [rax+r10]
	lea	rax, QWORD PTR [rdx+r14]
	add	rax, rcx
	vmovdqu	xmm13, XMMWORD PTR [rax+r10]
	lea	rax, QWORD PTR [r14+rbp]
	add	rax, rcx
	vmovdqu	xmm12, XMMWORD PTR [rax+r10]
	lea	rax, QWORD PTR [r11+r14]
	add	rax, rcx
	vmovdqu	xmm11, XMMWORD PTR [rax+r10]
	mov	rax, r9
	sub	rax, r13
	add	rax, r14
	add	rax, rcx
	vmovdqu	xmm0, XMMWORD PTR [rax+rsi]
	vpshufb	xmm10, xmm0, xmm6
	mov	rax, rbx
	sub	rax, r13
	add	rax, r14
	add	rax, rcx

; 971  :       __m128i unrd_mask    = _mm_cmpeq_epi8  (unrd_imask,   _mm_setzero_si128());
; 972  : 
; 973  :       __m128i b_unread     = _mm_blendv_epi8 (b_with_old,   a, unrd_mask);
; 974  :       __m128i d_unread     = _mm_blendv_epi8 (d_with_old,   c, unrd_mask);
; 975  :       __m128i f_unread     = _mm_blendv_epi8 (f_with_old,   e, unrd_mask);
; 976  :       __m128i h_unread     = _mm_blendv_epi8 (h_with_old,   g, unrd_mask);
; 977  : 
; 978  :       old_b = b_shifted;

	vmovdqu	XMMWORD PTR old_b$1$[rsp], xmm10
	vmovdqu	xmm0, XMMWORD PTR [rax+rsi]
	mov	rax, QWORD PTR tv1871[rsp]
	vpshufb	xmm9, xmm0, xmm6
	sub	rax, r13
	add	rax, r14
	add	rax, rcx

; 979  :       old_d = d_shifted;

	vmovdqu	XMMWORD PTR old_d$1$[rsp], xmm9
	vmovdqu	xmm0, XMMWORD PTR [rax+rsi]
	mov	eax, DWORD PTR tv1868[rsp]
	vpshufb	xmm8, xmm0, xmm6
	sub	rax, r13
	add	rax, r14
	add	rax, rcx

; 980  :       old_f = f_shifted;

	vmovdqu	XMMWORD PTR old_f$1$[rsp], xmm8
	vmovdqu	xmm0, XMMWORD PTR [rax+rsi]
	vpshufb	xmm7, xmm0, xmm6
	movzx	eax, r12b
	add	r12d, r8d
	shl	al, 4
	add	al, BYTE PTR a_off$1$[rsp]
	movsx	eax, al
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vpaddb	xmm0, xmm0, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	vpcmpgtb xmm1, xmm1, xmm0
	vpxor	xmm0, xmm0, xmm0
	vpcmpeqb xmm2, xmm1, xmm0
	vmovdqu	xmm0, XMMWORD PTR move_old_to_b_imask$1$[rsp]
	vpblendvb xmm1, xmm3, xmm10, xmm0
	vpblendvb xmm6, xmm1, xmm14, xmm2
	vpblendvb xmm1, xmm4, xmm9, xmm0
	vpblendvb xmm3, xmm1, xmm13, xmm2
	vpblendvb xmm1, xmm5, xmm8, xmm0
	vpblendvb xmm4, xmm1, xmm12, xmm2
	vmovdqu	xmm1, XMMWORD PTR old_h$1$[rsp]
	vpblendvb xmm1, xmm1, xmm7, xmm0
	vpblendvb xmm5, xmm1, xmm11, xmm2

; 981  :       old_h = h_shifted;
; 982  : 
; 983  :       __m128i sad_ab     = _mm_sad_epu8(a, b_unread);

	vpsadbw	xmm0, xmm6, xmm14
	vmovdqu	xmm6, XMMWORD PTR shufmask1$1$[rsp]

; 984  :       __m128i sad_cd     = _mm_sad_epu8(c, d_unread);
; 985  :       __m128i sad_ef     = _mm_sad_epu8(e, f_unread);
; 986  :       __m128i sad_gh     = _mm_sad_epu8(g, h_unread);
; 987  : 
; 988  :       sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	vpaddq	xmm2, xmm0, xmm15
	vpsadbw	xmm1, xmm3, xmm13

; 989  :       sse_inc = _mm_add_epi64(sse_inc, sad_cd);

	vpaddq	xmm3, xmm2, xmm1
	vpsadbw	xmm0, xmm4, xmm12

; 990  :       sse_inc = _mm_add_epi64(sse_inc, sad_ef);

	vpaddq	xmm4, xmm3, xmm0
	vpsadbw	xmm1, xmm5, xmm11

; 991  :       sse_inc = _mm_add_epi64(sse_inc, sad_gh);

	vpaddq	xmm15, xmm4, xmm1
	vmovdqu	xmm4, xmm9
	vmovdqu	XMMWORD PTR old_h$1$[rsp], xmm7
	vmovdqu	xmm3, xmm10
	vmovdqu	xmm5, xmm8
	cmp	r12d, r15d
	jne	$LL10@hor_sad_ss
	vmovdqu	xmm8, XMMWORD PTR blk_widths$1$[rsp]
	vmovdqu	xmm9, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	vmovdqu	xmm10, XMMWORD PTR is_left$1$[rsp]
	mov	r8, QWORD PTR tv1889[rsp]
	mov	r9d, DWORD PTR y$1$[rsp]
	mov	r11d, DWORD PTR outside_vecs$1$[rsp]
	mov	ebx, DWORD PTR tv1877[rsp]
	mov	edi, DWORD PTR pic_stride$[rsp]
	mov	rbp, QWORD PTR tv1989[rsp]
	vpxor	xmm7, xmm7, xmm7
$LN52@hor_sad_ss:

; 884  : 
; 885  :   const __m128i left_offsets = _mm_set1_epi8 ((uint8_t)left_offset);
; 886  :   const __m128i is_left      = _mm_cmpeq_epi8(rights, _mm_setzero_si128());
; 887  :   const __m128i vw_for_left  = _mm_and_si128 (is_left, vec_widths);
; 888  : 
; 889  :   // -x == (x ^ 0xff) + 1 = (x ^ 0xff) - 0xff. Also x == (x ^ 0x00) - 0x00.
; 890  :   // in other words, calculate inverse of left_offsets if is_left is true.
; 891  :   const __m128i offs_neg            = _mm_xor_si128 (left_offsets, is_left);
; 892  :   const __m128i offs_for_sm1        = _mm_sub_epi8  (offs_neg,     is_left);
; 893  : 
; 894  :   const __m128i ns_for_sm1          = _mm_or_si128  (vw_for_left,  nslo);
; 895  :   const __m128i shufmask1           = _mm_add_epi8  (ns_for_sm1,   offs_for_sm1);
; 896  : 
; 897  :   const __m128i mo2bmask_l          = _mm_cmpgt_epi8(left_offsets, nslo);
; 898  :   const __m128i mo2bimask_l         = _mm_cmpeq_epi8(mo2bmask_l,   _mm_setzero_si128());
; 899  :   const __m128i mo2bimask_r         = _mm_cmpgt_epi8(vec_widths,   shufmask1);
; 900  :   const __m128i move_old_to_b_imask = _mm_blendv_epi8(mo2bimask_r, mo2bimask_l, is_left);
; 901  : 
; 902  :   const int32_t outvec_offset = (~is_left_bm) & inside_width;
; 903  :   int32_t x, y;
; 904  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	edx, DWORD PTR ref_stride$[rsp]
	add	r9d, 4
	vmovdqu	xmm6, XMMWORD PTR shufmask1$1$[rsp]
	add	ebx, 4
	mov	DWORD PTR y$1$[rsp], r9d
	mov	DWORD PTR tv1877[rsp], ebx
	cmp	r9d, DWORD PTR height_fourline_groups$1$[rsp]
	jl	$LL53@hor_sad_ss
	vmovdqu	xmm12, XMMWORD PTR move_old_to_b_imask$1$[rsp]
	mov	ebx, DWORD PTR a_off$1$[rsp]
	mov	r12d, DWORD PTR is_left_bm$1$[rsp]
	mov	r13d, DWORD PTR left$[rsp]
	vmovaps	xmm14, XMMWORD PTR [rsp+272]
	vmovaps	xmm13, XMMWORD PTR [rsp+288]
$LN3@hor_sad_ss:

; 992  :     }
; 993  :   }
; 994  :   if (height_residual_lines) {

	cmp	DWORD PTR tv1896[rsp], 0
	je	$LN12@hor_sad_ss

; 995  :     for (; y < height; y++) {

	cmp	r9d, DWORD PTR height$[rsp]
	jge	$LN12@hor_sad_ss
	vmovdqu	xmm11, XMMWORD PTR shufmask1$1$[rsp]
	and	ebx, r12d
	mov	DWORD PTR a_off$1$[rsp], ebx
	npad	7
$LL13@hor_sad_ss:

; 996  :       __m128i borderpx_vec = _mm_set1_epi8(ref_data[(int32_t)((y + 0) * ref_stride + border_off)]);

	mov	eax, DWORD PTR border_off$1$[rsp]
	mov	r14d, r9d
	imul	r14d, edx
	add	eax, r14d
	movsxd	rcx, eax
	movsx	eax, BYTE PTR [rcx+rsi]
	vmovd	xmm6, eax

; 997  :       for (x = 0; x < outside_vecs; x++) {

	xor	eax, eax
	vpbroadcastb xmm6, xmm6
	test	r11d, r11d
	je	SHORT $LN45@hor_sad_ss

; 998  :         __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + outvec_offset));

	mov	edx, r9d
	imul	edx, edi
	npad	6
$LL16@hor_sad_ss:
	movsxd	rcx, eax
	shl	rcx, 4
	add	rcx, rdx
	add	rcx, r8
	vmovdqu	xmm3, XMMWORD PTR [rcx+r10]

; 999  : 
; 1000 :         __m128i startoffs  = _mm_set1_epi8  ((x + inside_vecs) << vec_width_log2);

	lea	ecx, DWORD PTR [rax+rbp]
	inc	eax
	shl	cl, 4
	movsx	ecx, cl
	vmovd	xmm0, ecx
	vpbroadcastb xmm0, xmm0

; 1001 :         __m128i ns         = _mm_add_epi8   (startoffs, nslo);

	vpaddb	xmm0, xmm0, xmm9

; 1002 : 
; 1003 :         // Unread imask is (is_left NOR unrd_imask_for_right), do the maths etc
; 1004 :         __m128i unrd_imask = _mm_cmpgt_epi8 (blk_widths, ns);

	vpcmpgtb xmm1, xmm8, xmm0

; 1005 :                 unrd_imask = _mm_or_si128   (unrd_imask, is_left);

	vpor	xmm2, xmm1, xmm10

; 1006 :         __m128i unrd_mask  = _mm_cmpeq_epi8 (unrd_imask, _mm_setzero_si128());

	vpcmpeqb xmm0, xmm2, xmm7

; 1007 :         __m128i b_unread   = _mm_blendv_epi8(borderpx_vec, a, unrd_mask);

	vpblendvb xmm1, xmm6, xmm3, xmm0

; 1008 : 
; 1009 :         __m128i sad_ab     = _mm_sad_epu8   (a, b_unread);

	vpsadbw	xmm2, xmm3, xmm1

; 1010 :         sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	vpaddq	xmm15, xmm2, xmm15
	cmp	eax, r11d
	jb	SHORT $LL16@hor_sad_ss
$LN45@hor_sad_ss:

; 1014 : 
; 1015 :       __m128i old_b = borderpx_vec;
; 1016 :       for (x = invec_lstart; x != invec_lend; x += invec_linc) {

	mov	edx, DWORD PTR invec_lstart$1$[rsp]
	cmp	edx, r15d
	je	$LN11@hor_sad_ss

; 1011 :       }
; 1012 :       int32_t a_off = outside_width & is_left_bm;
; 1013 :       int32_t leftoff_with_sign_neg = (left_offset ^ is_left_bm) - is_left_bm;

	mov	r8d, DWORD PTR invec_linc$1$[rsp]
	mov	eax, r12d
	xor	eax, r13d
	movsxd	rbx, ebx
	sub	eax, r12d

; 1017 :         __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + a_off));

	mov	r13d, r9d
	imul	r13d, edi
	mov	edi, DWORD PTR a_off$1$[rsp]

; 1018 :         __m128i b = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 0) * ref_stride + a_off - leftoff_with_sign_neg));

	movsxd	r12, eax
	npad	7
$LL19@hor_sad_ss:

; 1017 :         __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + a_off));

	movsxd	rcx, edx
	lea	rax, QWORD PTR [rbx+r13]
	shl	rcx, 4
	add	rax, rcx
	vmovdqu	xmm5, XMMWORD PTR [rax+r10]

; 1019 : 
; 1020 :         __m128i b_shifted    = _mm_shuffle_epi8(b,     shufmask1);

	mov	rax, rbx
	sub	rax, r12
	add	rax, r14
	add	rax, rcx
	vmovdqu	xmm0, XMMWORD PTR [rax+rsi]
	vpshufb	xmm4, xmm0, xmm11

; 1021 :         __m128i b_with_old   = _mm_blendv_epi8 (old_b, b_shifted, move_old_to_b_imask);
; 1022 : 
; 1023 :         uint8_t startoff     = (x << vec_width_log2) + a_off;

	movzx	eax, dl
	add	edx, r8d
	shl	al, 4
	add	al, dil

; 1024 :         __m128i startoffs    = _mm_set1_epi8   (startoff);

	movsx	eax, al
	vpblendvb xmm2, xmm6, xmm4, xmm12
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0

; 1025 :         __m128i curr_ns      = _mm_add_epi8    (startoffs,    nslo);

	vpaddb	xmm0, xmm0, xmm9

; 1026 :         __m128i unrd_imask   = _mm_cmpgt_epi8  (blk_widths,   curr_ns);

	vpcmpgtb xmm1, xmm8, xmm0

; 1027 :         __m128i unrd_mask    = _mm_cmpeq_epi8  (unrd_imask,   _mm_setzero_si128());

	vpcmpeqb xmm3, xmm1, xmm7

; 1028 :         __m128i b_unread     = _mm_blendv_epi8 (b_with_old,   a, unrd_mask);

	vpblendvb xmm1, xmm2, xmm5, xmm3

; 1029 : 
; 1030 :         old_b = b_shifted;
; 1031 : 
; 1032 :         __m128i sad_ab     = _mm_sad_epu8(a, b_unread);

	vpsadbw	xmm0, xmm1, xmm5

; 1033 :         sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	vpaddq	xmm15, xmm0, xmm15
	vmovdqu	xmm6, xmm4
	cmp	edx, r15d
	jne	SHORT $LL19@hor_sad_ss
	mov	r8, QWORD PTR tv1889[rsp]
	mov	edi, DWORD PTR pic_stride$[rsp]
	mov	ebx, DWORD PTR a_off$1$[rsp]
	mov	r12d, DWORD PTR is_left_bm$1$[rsp]
$LN11@hor_sad_ss:

; 995  :     for (; y < height; y++) {

	mov	edx, DWORD PTR ref_stride$[rsp]
	inc	r9d
	mov	r13d, DWORD PTR left$[rsp]
	cmp	r9d, DWORD PTR height$[rsp]
	jl	$LL13@hor_sad_ss
$LN12@hor_sad_ss:

; 1034 :       }
; 1035 :     }
; 1036 :   }
; 1037 :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 1038 :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 1039 :   return _mm_cvtsi128_si32(sad);

	vmovaps	xmm11, XMMWORD PTR [rsp+320]

; 1040 : }

	lea	r11, QWORD PTR [rsp+416]
	mov	rbx, QWORD PTR [r11+64]
	vmovaps	xmm6, XMMWORD PTR [r11-16]
	vmovaps	xmm7, XMMWORD PTR [r11-32]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm9, XMMWORD PTR [r11-64]
	vmovaps	xmm10, XMMWORD PTR [r11-80]
	vmovaps	xmm12, XMMWORD PTR [r11-112]
	vpshufd	xmm0, xmm15, 78				; 0000004eH
	vpaddq	xmm1, xmm0, xmm15
	vmovaps	xmm15, XMMWORD PTR [r11-160]
	vmovd	eax, xmm1
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r13
	pop	r12
	pop	rdi
	pop	rsi
	pop	rbp
	ret	0
hor_sad_sse41_arbitrary ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\reg_sad_pow2_widths-avx2.h
_TEXT	SEGMENT
data1$ = 24
data2$ = 32
height$ = 40
stride1$ = 48
stride2$ = 56
reg_sad_w32 PROC

; 45   : {

	mov	QWORD PTR [rsp+32], rbx
	push	rsi
	push	rdi

; 47   :   int32_t y;
; 48   : 
; 49   :   const int32_t height_fourline_groups = height & ~3;
; 50   :   const int32_t height_residual_lines  = height &  3;
; 51   : 
; 52   :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	esi, DWORD PTR stride2$[rsp]
	xor	eax, eax
	mov	QWORD PTR [rsp+24], r12
	mov	r11d, r9d
	mov	QWORD PTR [rsp+40], r15
	mov	r12d, r8d
	mov	r15d, r8d
	and	r12d, 3
	and	r15d, -4
	mov	r10d, r8d
	mov	rbx, rdx
	mov	rdi, rcx
	vpxor	xmm2, xmm2, xmm2
	test	r15d, r15d
	jle	$LN3@reg_sad_w3

; 46   :   __m256i avx_inc = _mm256_setzero_si256();

	mov	QWORD PTR [rsp+32], r14
	mov	r14d, 2
	npad	7
$LL4@reg_sad_w3:

; 53   :     __m256i a = _mm256_loadu_si256((const __m256i *)(data1 + (y + 0) * stride1));
; 54   :     __m256i b = _mm256_loadu_si256((const __m256i *)(data2 + (y + 0) * stride2));
; 55   :     __m256i c = _mm256_loadu_si256((const __m256i *)(data1 + (y + 1) * stride1));
; 56   :     __m256i d = _mm256_loadu_si256((const __m256i *)(data2 + (y + 1) * stride2));
; 57   :     __m256i e = _mm256_loadu_si256((const __m256i *)(data1 + (y + 2) * stride1));
; 58   :     __m256i f = _mm256_loadu_si256((const __m256i *)(data2 + (y + 2) * stride2));
; 59   :     __m256i g = _mm256_loadu_si256((const __m256i *)(data1 + (y + 3) * stride1));
; 60   :     __m256i h = _mm256_loadu_si256((const __m256i *)(data2 + (y + 3) * stride2));

	lea	r8d, DWORD PTR [r14+1]
	mov	edx, r14d
	mov	ecx, r8d
	imul	edx, esi
	imul	ecx, esi
	lea	r9d, DWORD PTR [r14-1]
	imul	r8d, r11d

; 61   : 
; 62   :     __m256i curr_sads_ab = _mm256_sad_epu8(a, b);
; 63   :     __m256i curr_sads_cd = _mm256_sad_epu8(c, d);
; 64   :     __m256i curr_sads_ef = _mm256_sad_epu8(e, f);
; 65   :     __m256i curr_sads_gh = _mm256_sad_epu8(g, h);

	vmovdqu	ymm0, YMMWORD PTR [r8+rdi]
	vpsadbw	ymm5, ymm0, YMMWORD PTR [rcx+rbx]
	mov	ecx, r14d
	add	r14d, 4
	imul	ecx, r11d
	vmovdqu	ymm0, YMMWORD PTR [rcx+rdi]
	vpsadbw	ymm4, ymm0, YMMWORD PTR [rdx+rbx]
	mov	ecx, r9d
	mov	edx, eax
	imul	ecx, esi
	imul	r9d, r11d
	imul	edx, esi
	vmovdqu	ymm0, YMMWORD PTR [r9+rdi]
	vpsadbw	ymm3, ymm0, YMMWORD PTR [rcx+rbx]
	mov	ecx, eax
	add	eax, 4
	imul	ecx, r11d
	vmovdqu	ymm0, YMMWORD PTR [rcx+rdi]
	vpsadbw	ymm1, ymm0, YMMWORD PTR [rdx+rbx]

; 66   : 
; 67   :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_ab);

	vpaddq	ymm2, ymm1, ymm2

; 68   :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_cd);

	vpaddq	ymm3, ymm2, ymm3

; 69   :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_ef);

	vpaddq	ymm0, ymm3, ymm4

; 70   :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_gh);

	vpaddq	ymm2, ymm0, ymm5
	cmp	eax, r15d
	jl	SHORT $LL4@reg_sad_w3

; 47   :   int32_t y;
; 48   : 
; 49   :   const int32_t height_fourline_groups = height & ~3;
; 50   :   const int32_t height_residual_lines  = height &  3;
; 51   : 
; 52   :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r14, QWORD PTR [rsp+32]
$LN3@reg_sad_w3:
	mov	r15, QWORD PTR [rsp+40]

; 71   :   }
; 72   :   if (height_residual_lines) {

	test	r12d, r12d
	mov	r12, QWORD PTR [rsp+24]
	je	SHORT $LN6@reg_sad_w3

; 73   :     for (; y < height; y++) {

	cmp	eax, r10d
	jge	SHORT $LN6@reg_sad_w3
$LL7@reg_sad_w3:

; 74   :       __m256i a = _mm256_loadu_si256((const __m256i *)(data1 + (y + 0) * stride1));

	mov	ecx, eax

; 75   :       __m256i b = _mm256_loadu_si256((const __m256i *)(data2 + (y + 0) * stride2));

	mov	edx, eax
	imul	ecx, r11d
	inc	eax
	imul	edx, esi

; 76   : 
; 77   :       __m256i curr_sads = _mm256_sad_epu8(a, b);

	vmovdqu	ymm0, YMMWORD PTR [rcx+rdi]
	vpsadbw	ymm1, ymm0, YMMWORD PTR [rdx+rbx]

; 78   :       avx_inc = _mm256_add_epi64(avx_inc, curr_sads);

	vpaddq	ymm2, ymm1, ymm2
	cmp	eax, r10d
	jl	SHORT $LL7@reg_sad_w3
$LN6@reg_sad_w3:

; 79   :     }
; 80   :   }
; 81   : 
; 82   :   __m128i inchi = _mm256_extracti128_si256(avx_inc, 1);

	vextracti128 xmm0, ymm2, 1

; 83   :   __m128i inclo = _mm256_castsi256_si128  (avx_inc);
; 84   : 
; 85   :   __m128i sum_1 = _mm_add_epi64    (inclo, inchi);

	vpaddq	xmm1, xmm0, xmm2

; 86   :   __m128i sum_2 = _mm_shuffle_epi32(sum_1, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm1, 78				; 0000004eH

; 87   :   __m128i sad   = _mm_add_epi64    (sum_1, sum_2);

	vpaddq	xmm1, xmm0, xmm1

; 88   : 
; 89   :   return _mm_cvtsi128_si32(sad);

	vmovd	eax, xmm1
	vzeroupper

; 90   : }

	mov	rbx, QWORD PTR [rsp+48]
	pop	rdi
	pop	rsi
	ret	0
reg_sad_w32 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\reg_sad_pow2_widths-avx2.h
_TEXT	SEGMENT
data1$ = 32
data2$ = 40
height$ = 48
stride1$ = 56
stride2$ = 64
reg_sad_w64 PROC

; 95   : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rsi
	mov	QWORD PTR [rsp+24], rdi
	mov	QWORD PTR [rsp+32], r14
	push	r15
	sub	rsp, 16

; 96   :   __m256i avx_inc = _mm256_setzero_si256();
; 97   :   int32_t y;
; 98   : 
; 99   :   const int32_t height_twoline_groups = height & ~1;
; 100  :   const int32_t height_residual_lines = height &  1;
; 101  : 
; 102  :   for (y = 0; y < height_twoline_groups; y += 2) {

	mov	esi, DWORD PTR stride2$[rsp]
	mov	r14d, r8d
	mov	r15d, r8d
	vmovaps	XMMWORD PTR [rsp], xmm6
	and	r14d, -2
	and	r15d, 1
	xor	eax, eax
	mov	r11d, r9d
	mov	r10d, r8d
	mov	rbx, rdx
	mov	rdi, rcx
	vpxor	xmm6, xmm6, xmm6
	test	r14d, r14d
	jle	SHORT $LN3@reg_sad_w6
	npad	8
$LL4@reg_sad_w6:

; 103  :     __m256i a = _mm256_loadu_si256((const __m256i *)(data1 + (y + 0) * stride1));
; 104  :     __m256i b = _mm256_loadu_si256((const __m256i *)(data2 + (y + 0) * stride2));
; 105  :     __m256i c = _mm256_loadu_si256((const __m256i *)(data1 + (y + 0) * stride1 + 32));
; 106  :     __m256i d = _mm256_loadu_si256((const __m256i *)(data2 + (y + 0) * stride2 + 32));
; 107  : 
; 108  :     __m256i e = _mm256_loadu_si256((const __m256i *)(data1 + (y + 1) * stride1));
; 109  :     __m256i f = _mm256_loadu_si256((const __m256i *)(data2 + (y + 1) * stride2));
; 110  :     __m256i g = _mm256_loadu_si256((const __m256i *)(data1 + (y + 1) * stride1 + 32));
; 111  :     __m256i h = _mm256_loadu_si256((const __m256i *)(data2 + (y + 1) * stride2 + 32));

	lea	edx, DWORD PTR [rax+1]
	mov	ecx, eax
	mov	r9d, edx
	imul	ecx, r11d
	imul	edx, r11d
	imul	r9d, esi
	mov	r8d, edx
	mov	edx, eax
	imul	edx, esi
	add	eax, 2

; 112  : 
; 113  :     __m256i curr_sads_ab = _mm256_sad_epu8(a, b);
; 114  :     __m256i curr_sads_cd = _mm256_sad_epu8(c, d);
; 115  :     __m256i curr_sads_ef = _mm256_sad_epu8(e, f);

	vmovdqu	ymm1, YMMWORD PTR [r9+rbx]

; 116  :     __m256i curr_sads_gh = _mm256_sad_epu8(g, h);

	vmovdqu	ymm0, YMMWORD PTR [r8+rdi+32]
	vpsadbw	ymm5, ymm0, YMMWORD PTR [r9+rbx+32]
	vpsadbw	ymm4, ymm1, YMMWORD PTR [r8+rdi]
	vmovdqu	ymm0, YMMWORD PTR [rcx+rdi+32]
	vmovdqu	ymm1, YMMWORD PTR [rdx+rbx]
	vpsadbw	ymm2, ymm1, YMMWORD PTR [rcx+rdi]
	vpsadbw	ymm3, ymm0, YMMWORD PTR [rdx+rbx+32]

; 117  : 
; 118  :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_ab);

	vpaddq	ymm0, ymm2, ymm6

; 119  :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_cd);

	vpaddq	ymm3, ymm0, ymm3

; 120  :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_ef);

	vpaddq	ymm1, ymm3, ymm4

; 121  :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_gh);

	vpaddq	ymm6, ymm1, ymm5
	cmp	eax, r14d
	jl	SHORT $LL4@reg_sad_w6
$LN3@reg_sad_w6:

; 122  :   }
; 123  :   if (height_residual_lines) {

	test	r15d, r15d
	je	SHORT $LN6@reg_sad_w6

; 124  :     for (; y < height; y++) {

	cmp	eax, r10d
	jge	SHORT $LN6@reg_sad_w6
	npad	2
$LL7@reg_sad_w6:

; 125  :       __m256i a = _mm256_loadu_si256((const __m256i *)(data1 + (y + 0) * stride1));
; 126  :       __m256i b = _mm256_loadu_si256((const __m256i *)(data2 + (y + 0) * stride2));
; 127  :       __m256i c = _mm256_loadu_si256((const __m256i *)(data1 + (y + 0) * stride1 + 32));
; 128  :       __m256i d = _mm256_loadu_si256((const __m256i *)(data2 + (y + 0) * stride2 + 32));

	mov	edx, eax
	mov	ecx, eax
	imul	edx, esi
	inc	eax
	imul	ecx, r11d

; 129  : 
; 130  :       __m256i curr_sads_ab = _mm256_sad_epu8(a, b);

	vmovdqu	ymm1, YMMWORD PTR [rdx+rbx]

; 131  :       __m256i curr_sads_cd = _mm256_sad_epu8(c, d);

	vmovdqu	ymm0, YMMWORD PTR [rcx+rdi+32]
	vpsadbw	ymm3, ymm0, YMMWORD PTR [rdx+rbx+32]
	vpsadbw	ymm2, ymm1, YMMWORD PTR [rcx+rdi]

; 132  :       avx_inc = _mm256_add_epi64(avx_inc, curr_sads_ab);

	vpaddq	ymm0, ymm2, ymm6

; 133  :       avx_inc = _mm256_add_epi64(avx_inc, curr_sads_cd);

	vpaddq	ymm6, ymm0, ymm3
	cmp	eax, r10d
	jl	SHORT $LL7@reg_sad_w6
$LN6@reg_sad_w6:

; 134  :     }
; 135  :   }
; 136  : 
; 137  :   __m128i inchi = _mm256_extracti128_si256(avx_inc, 1);

	vextracti128 xmm0, ymm6, 1

; 138  :   __m128i inclo = _mm256_castsi256_si128  (avx_inc);
; 139  : 
; 140  :   __m128i sum_1 = _mm_add_epi64    (inclo, inchi);

	vpaddq	xmm1, xmm0, xmm6

; 141  :   __m128i sum_2 = _mm_shuffle_epi32(sum_1, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm1, 78				; 0000004eH

; 142  :   __m128i sad   = _mm_add_epi64    (sum_1, sum_2);

	vpaddq	xmm1, xmm0, xmm1

; 143  : 
; 144  :   return _mm_cvtsi128_si32(sad);

	vmovd	eax, xmm1
	vzeroupper

; 145  : }

	mov	rbx, QWORD PTR [rsp+32]
	mov	rsi, QWORD PTR [rsp+40]
	mov	rdi, QWORD PTR [rsp+48]
	mov	r14, QWORD PTR [rsp+56]
	vmovaps	xmm6, XMMWORD PTR [rsp]
	add	rsp, 16
	pop	r15
	ret	0
reg_sad_w64 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\reg_sad_pow2_widths-avx2.h
_TEXT	SEGMENT
pic_data$ = 80
ref_data$ = 88
height$ = 96
pic_stride$ = 104
ref_stride$ = 112
left$ = 120
right$ = 128
hor_sad_avx2_w32 PROC

; 150  : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rsi
	mov	QWORD PTR [rsp+24], rdi
	push	r14
	sub	rsp, 64					; 00000040H

; 151  :   __m256i avx_inc = _mm256_setzero_si256();
; 152  : 
; 153  :   const size_t block_width      = 32;
; 154  :   const size_t block_width_log2 = 5;
; 155  :   const size_t lane_width       = 16;
; 156  : 
; 157  :   const int32_t left_eq_wid     = left  >> block_width_log2;
; 158  :   const int32_t left_clamped    = left  -  left_eq_wid;
; 159  :   const int32_t right_eq_wid    = right >> block_width_log2;
; 160  :   const int32_t right_clamped   = right -  right_eq_wid;
; 161  : 
; 162  :   const __m256i zero        = _mm256_setzero_si256();
; 163  :   const __m256i lane_widths = _mm256_set1_epi8((uint8_t)lane_width);
; 164  :   const __m256i lefts       = _mm256_set1_epi8((uint8_t)left_clamped);
; 165  :   const __m256i rights      = _mm256_set1_epi8((uint8_t)right_clamped);
; 166  :   const __m256i unsign_mask = _mm256_set1_epi8(0x7f);
; 167  :   const __m256i ns          = _mm256_setr_epi8(0,  1,  2,  3,  4,  5,  6,  7,  8,  9,  10, 11, 12, 13, 14, 15,

	vmovdqu	ymm5, YMMWORD PTR __ymm@1f1e1d1c1b1a191817161514131211100f0e0d0c0b0a09080706050403020100
	mov	r11d, DWORD PTR left$[rsp]
	mov	r14, rcx
	vmovaps	XMMWORD PTR [rsp+48], xmm6
	movzx	eax, r11b
	vmovaps	XMMWORD PTR [rsp+32], xmm7
	mov	r10d, r11d
	shr	r10d, 5
	mov	rsi, rdx
	mov	edx, DWORD PTR right$[rsp]
	sub	al, r10b
	movsx	eax, al
	mov	ecx, edx
	vmovd	xmm6, eax
	vpbroadcastb ymm6, xmm6

; 168  :                                                16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31);
; 169  : 
; 170  :   const __m256i rightmost_good_idx = _mm256_set1_epi8((uint8_t)(block_width - right - 1));
; 171  : 
; 172  :   const __m256i shufmask1_l    = _mm256_sub_epi8  (ns,          lefts);

	vpsubb	ymm0, ymm5, ymm6
	movzx	eax, dl
	shr	ecx, 5
	sub	al, cl
	vmovaps	XMMWORD PTR [rsp+16], xmm8
	movsx	eax, al
	mov	edi, r9d
	vmovd	xmm3, eax
	vpbroadcastb ymm3, xmm3
	vmovaps	XMMWORD PTR [rsp], xmm9
	mov	eax, 31

; 173  :   const __m256i shufmask1_r    = _mm256_add_epi8  (shufmask1_l, rights);

	vpaddb	ymm1, ymm0, ymm3

; 174  :   const __m256i shufmask1      = _mm256_and_si256 (shufmask1_r, unsign_mask);

	vpand	ymm8, ymm1, YMMWORD PTR __ymm@7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f7f
	sub	al, dl
	mov	ebx, r8d

; 175  : 
; 176  :   const __m256i epol_mask_r    = _mm256_min_epi8  (ns,    rightmost_good_idx);
; 177  :   const __m256i epol_mask      = _mm256_max_epi8  (lefts, epol_mask_r);
; 178  : 
; 179  :   const __m256i mlo2hi_mask_l  = _mm256_cmpgt_epi8(lefts, ns);
; 180  :   const __m256i mlo2hi_imask_r = _mm256_cmpgt_epi8(lane_widths, shufmask1);
; 181  :   const __m256i mlo2hi_mask_r  = _mm256_cmpeq_epi8(mlo2hi_imask_r, zero);
; 182  : 
; 183  :   // For left != 0,  use low lane of mlo2hi_mask_l as blend mask for high lane.
; 184  :   // For right != 0, use low lane of mlo2hi_mask_r as blend mask for low lane.
; 185  :   const __m256i xchg_mask1     = _mm256_permute2x128_si256(mlo2hi_mask_l, mlo2hi_mask_r, 0x02);
; 186  : 
; 187  :   // If left != 0 (ie. right == 0), the xchg should only affect high lane,
; 188  :   // if right != 0 (ie. left == 0), the low lane. Set bits on the lane that
; 189  :   // the xchg should affect. left == right == 0 should never happen, this'll
; 190  :   // break if it does.
; 191  :   const __m256i lanes_llo_rhi  = _mm256_blend_epi32(lefts, rights, 0xf0);

	vpblendd ymm1, ymm6, ymm3, 240			; 000000f0H
	movsx	eax, al
	vmovd	xmm0, eax
	vpbroadcastb ymm0, xmm0
	vpminsb	ymm0, ymm0, ymm5
	vpmaxsb	ymm9, ymm0, ymm6
	vmovdqu	ymm0, YMMWORD PTR __ymm@1010101010101010101010101010101010101010101010101010101010101010
	vpxor	xmm7, xmm7, xmm7
	vpcmpgtb ymm2, ymm0, ymm8

; 192  :   const __m256i xchg_lane_mask = _mm256_cmpeq_epi32(lanes_llo_rhi, zero);

	vpcmpeqd ymm4, ymm1, ymm7
	vpcmpgtb ymm1, ymm6, ymm5
	vpcmpeqb ymm3, ymm2, ymm7
	xor	eax, eax
	vperm2i128 ymm0, ymm1, ymm3, 2

; 193  : 
; 194  :   const __m256i xchg_data_mask = _mm256_and_si256(xchg_mask1, xchg_lane_mask);

	vpand	ymm5, ymm0, ymm4

; 199  : 
; 200  :   int32_t y;
; 201  :   for (y = 0; y < height; y++) {

	test	r8d, r8d
	jle	SHORT $LN3@hor_sad_av

; 195  : 
; 196  :   // If we're straddling the left border, start from the left border instead,
; 197  :   // and if right border, end on the border
; 198  :   const int32_t ld_offset = left - right;

	mov	r9d, DWORD PTR ref_stride$[rsp]
	sub	r11d, edx
	movsxd	r8, r11d
	npad	8
$LL4@hor_sad_av:

; 202  :     __m256i a = _mm256_loadu_si256((__m256i *)(pic_data + (y + 0) * pic_stride + 0));
; 203  :     __m256i b = _mm256_loadu_si256((__m256i *)(ref_data + (y + 0) * ref_stride + 0  + ld_offset));

	mov	edx, eax
	mov	ecx, eax
	imul	edx, r9d
	inc	eax
	imul	ecx, edi
	add	rdx, r8

; 204  : 
; 205  :     __m256i b_shifted            = _mm256_shuffle_epi8     (b, shufmask1);

	vmovdqu	ymm0, YMMWORD PTR [rdx+rsi]
	vpshufb	ymm1, ymm0, ymm8

; 206  :     __m256i b_lanes_reversed     = _mm256_permute4x64_epi64(b_shifted,   _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm0, ymm1, 78				; 0000004eH

; 207  :     __m256i b_data_transfered    = _mm256_blendv_epi8      (b_shifted, b_lanes_reversed, xchg_data_mask);

	vpblendvb ymm1, ymm1, ymm0, ymm5

; 208  :     __m256i b_epoled             = _mm256_shuffle_epi8     (b_data_transfered, epol_mask);

	vpshufb	ymm2, ymm1, ymm9

; 209  : 
; 210  :     __m256i curr_sads_ab         = _mm256_sad_epu8(a, b_epoled);

	vpsadbw	ymm3, ymm2, YMMWORD PTR [rcx+r14]

; 211  : 
; 212  :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_ab);

	vpaddq	ymm7, ymm3, ymm7
	cmp	eax, ebx
	jl	SHORT $LL4@hor_sad_av
$LN3@hor_sad_av:

; 213  :   }
; 214  :   __m128i inchi = _mm256_extracti128_si256(avx_inc, 1);

	vextracti128 xmm0, ymm7, 1

; 215  :   __m128i inclo = _mm256_castsi256_si128  (avx_inc);
; 216  : 
; 217  :   __m128i sum_1 = _mm_add_epi64    (inclo, inchi);

	vpaddq	xmm1, xmm0, xmm7

; 218  :   __m128i sum_2 = _mm_shuffle_epi32(sum_1, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm1, 78				; 0000004eH

; 219  :   __m128i sad   = _mm_add_epi64    (sum_1, sum_2);

	vpaddq	xmm1, xmm0, xmm1

; 220  : 
; 221  :   return _mm_cvtsi128_si32(sad);

	vmovd	eax, xmm1
	vzeroupper

; 222  : }

	mov	rbx, QWORD PTR [rsp+80]
	mov	rsi, QWORD PTR [rsp+88]
	mov	rdi, QWORD PTR [rsp+96]
	vmovaps	xmm6, XMMWORD PTR [rsp+48]
	vmovaps	xmm7, XMMWORD PTR [rsp+32]
	vmovaps	xmm8, XMMWORD PTR [rsp+16]
	vmovaps	xmm9, XMMWORD PTR [rsp]
	add	rsp, 64					; 00000040H
	pop	r14
	ret	0
hor_sad_avx2_w32 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\reg_sad_pow2_widths-avx2.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\reg_sad_pow2_widths-avx2.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
tv3281 = 0
tv3246 = 4
tv3274 = 8
tv3273 = 16
tv3348 = 24
data1$ = 112
data2$ = 120
height_fourline_groups$1$ = 128
tv3285 = 128
width$ = 128
height$ = 136
stride1$ = 144
stride2$ = 152
kvz_reg_sad_avx2 PROC

; 70   : {

$LN165:
	push	rbx
	push	rsi
	push	rdi
	sub	rsp, 80					; 00000050H
	mov	esi, r9d
	mov	rbx, rdx
	mov	rdi, rcx

; 71   :   if (width == 0)

	test	r8d, r8d
	jne	SHORT $LN2@kvz_reg_sa

; 72   :     return 0;

	xor	eax, eax
	vzeroupper

; 89   : }

	add	rsp, 80					; 00000050H
	pop	rdi
	pop	rsi
	pop	rbx
	ret	0
$LN2@kvz_reg_sa:
	mov	QWORD PTR [rsp+112], r12
	mov	QWORD PTR [rsp+120], r13
	mov	QWORD PTR [rsp+72], r14
	mov	QWORD PTR [rsp+64], r15

; 73   :   if (width == 4)

	cmp	r8d, 4
	jne	$LN3@kvz_reg_sa
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 60   :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r14d, DWORD PTR stride2$[rsp]
	mov	eax, esi
	mov	r12d, DWORD PTR stride1$[rsp]
	and	eax, 3
	mov	r13d, esi
	mov	DWORD PTR tv3285[rsp], eax
	xor	eax, eax
	and	r13d, -4
	vpxor	xmm5, xmm5, xmm5
	test	r13d, r13d
	jle	$LN14@kvz_reg_sa

; 54   :   __m128i sse_inc = _mm_setzero_si128();

	mov	r15d, 2
	npad	7
$LL15@kvz_reg_sa:

; 61   :     __m128i a = _mm_cvtsi32_si128(*(uint32_t *)(data1 + y * stride1));
; 62   :     __m128i b = _mm_cvtsi32_si128(*(uint32_t *)(data2 + y * stride2));
; 63   : 
; 64   :     a = _mm_insert_epi32(a, *(const uint32_t *)(data1 + (y + 1) * stride1), 1);

	lea	r11d, DWORD PTR [r15-1]
	mov	ecx, eax
	imul	ecx, r12d

; 65   :     b = _mm_insert_epi32(b, *(const uint32_t *)(data2 + (y + 1) * stride2), 1);
; 66   :     a = _mm_insert_epi32(a, *(const uint32_t *)(data1 + (y + 2) * stride1), 2);
; 67   :     b = _mm_insert_epi32(b, *(const uint32_t *)(data2 + (y + 2) * stride2), 2);
; 68   :     a = _mm_insert_epi32(a, *(const uint32_t *)(data1 + (y + 3) * stride1), 3);

	lea	r10d, DWORD PTR [r15+1]
	mov	r8d, r15d
	mov	edx, r11d
	imul	r8d, r12d
	mov	r9d, r10d
	imul	edx, r12d
	imul	r11d, r14d
	imul	r9d, r12d

; 69   :     b = _mm_insert_epi32(b, *(const uint32_t *)(data2 + (y + 3) * stride2), 3);

	imul	r10d, r14d
	vmovd	xmm0, DWORD PTR [rcx+rdi]
	mov	ecx, eax
	add	eax, 4
	imul	ecx, r14d
	vpinsrd	xmm1, xmm0, DWORD PTR [rdx+rdi], 1
	vpinsrd	xmm2, xmm1, DWORD PTR [r8+rdi], 2
	vpinsrd	xmm4, xmm2, DWORD PTR [r9+rdi], 3
	mov	r8d, r15d
	add	r15d, 4
	imul	r8d, r14d
	vmovd	xmm0, DWORD PTR [rcx+rbx]
	vpinsrd	xmm1, xmm0, DWORD PTR [r11+rbx], 1
	vpinsrd	xmm2, xmm1, DWORD PTR [r8+rbx], 2
	vpinsrd	xmm3, xmm2, DWORD PTR [r10+rbx], 3

; 70   : 
; 71   :     __m128i curr_sads = _mm_sad_epu8(a, b);

	vpsadbw	xmm0, xmm4, xmm3

; 72   :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm5, xmm0, xmm5
	cmp	eax, r13d
	jl	SHORT $LL15@kvz_reg_sa
$LN14@kvz_reg_sa:

; 73   :   }
; 74   :   if (height_residual_lines) {

	cmp	DWORD PTR tv3285[rsp], 0
	je	SHORT $LN17@kvz_reg_sa

; 75   :     for (; y < height; y++) {

	cmp	eax, esi
	jge	SHORT $LN17@kvz_reg_sa
	npad	2
$LL18@kvz_reg_sa:

; 76   :       __m128i a = _mm_cvtsi32_si128(*(const uint32_t *)(data1 + y * stride1));

	mov	ecx, eax
	imul	ecx, r12d
	vmovd	xmm1, DWORD PTR [rcx+rdi]

; 77   :       __m128i b = _mm_cvtsi32_si128(*(const uint32_t *)(data2 + y * stride2));

	mov	ecx, eax
	inc	eax
	imul	ecx, r14d
	vmovd	xmm0, DWORD PTR [rcx+rbx]

; 78   : 
; 79   :       __m128i curr_sads = _mm_sad_epu8(a, b);

	vpsadbw	xmm1, xmm1, xmm0

; 80   :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm5, xmm1, xmm5
	cmp	eax, esi
	jl	SHORT $LL18@kvz_reg_sa
$LN17@kvz_reg_sa:

; 81   :     }
; 82   :   }
; 83   :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm5, 78				; 0000004eH

; 84   :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vpaddq	xmm1, xmm0, xmm5
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 74   :     return reg_sad_w4(data1, data2, height, stride1, stride2);

	jmp	$LN163@kvz_reg_sa
$LN3@kvz_reg_sa:

; 75   :   if (width == 8)

	cmp	r8d, 8
	jne	$LN4@kvz_reg_sa
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 99   :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r9d, DWORD PTR stride2$[rsp]
	mov	r11d, esi
	mov	r10d, DWORD PTR stride1$[rsp]
	mov	r14d, esi
	and	r11d, -4
	and	r14d, 3
	xor	eax, eax
	vpxor	xmm4, xmm4, xmm4
	test	r11d, r11d
	jle	$LN23@kvz_reg_sa

; 85   : 
; 86   :   return _mm_cvtsi128_si32(sad);
; 87   : }
; 88   : 
; 89   : static INLINE uint32_t reg_sad_w8(const uint8_t * const data1, const uint8_t * const data2,
; 90   :                            const int32_t height, const uint32_t stride1,
; 91   :                            const uint32_t stride2)
; 92   : {
; 93   :   __m128i sse_inc = _mm_setzero_si128();

	mov	r15d, 2
	npad	1
$LL24@kvz_reg_sa:

; 100  :     __m128d a_d = _mm_setzero_pd();
; 101  :     __m128d b_d = _mm_setzero_pd();
; 102  :     __m128d c_d = _mm_setzero_pd();
; 103  :     __m128d d_d = _mm_setzero_pd();
; 104  : 
; 105  :     a_d = _mm_loadl_pd(a_d, (const double *)(data1 + (y + 0) * stride1));

	mov	ecx, eax

; 106  :     b_d = _mm_loadl_pd(b_d, (const double *)(data2 + (y + 0) * stride2));
; 107  :     a_d = _mm_loadh_pd(a_d, (const double *)(data1 + (y + 1) * stride1));

	lea	r8d, DWORD PTR [r15-1]
	imul	ecx, r10d
	vmovddup xmm2, QWORD PTR [rcx+rdi]
	mov	ecx, r8d

; 108  :     b_d = _mm_loadh_pd(b_d, (const double *)(data2 + (y + 1) * stride2));

	imul	r8d, r9d
	imul	ecx, r10d
	vmovhpd	xmm2, xmm2, QWORD PTR [rcx+rdi]
	mov	ecx, eax
	add	eax, 4
	imul	ecx, r9d
	vmovddup xmm0, QWORD PTR [rcx+rdx]
	vmovhpd	xmm0, xmm0, QWORD PTR [r8+rdx]

; 109  : 
; 110  :     c_d = _mm_loadl_pd(c_d, (const double *)(data1 + (y + 2) * stride1));

	mov	ecx, r15d

; 111  :     d_d = _mm_loadl_pd(d_d, (const double *)(data2 + (y + 2) * stride2));
; 112  :     c_d = _mm_loadh_pd(c_d, (const double *)(data1 + (y + 3) * stride1));

	lea	r8d, DWORD PTR [r15+1]
	imul	ecx, r10d

; 113  :     d_d = _mm_loadh_pd(d_d, (const double *)(data2 + (y + 3) * stride2));
; 114  : 
; 115  :     __m128i a = _mm_castpd_si128(a_d);
; 116  :     __m128i b = _mm_castpd_si128(b_d);
; 117  :     __m128i c = _mm_castpd_si128(c_d);
; 118  :     __m128i d = _mm_castpd_si128(d_d);
; 119  : 
; 120  :     __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	vpsadbw	xmm0, xmm2, xmm0

; 121  :     __m128i curr_sads_cd = _mm_sad_epu8(c, d);
; 122  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm2, xmm0, xmm4
	vmovddup xmm3, QWORD PTR [rcx+rdi]
	mov	ecx, r8d
	imul	r8d, r9d
	imul	ecx, r10d
	vmovhpd	xmm3, xmm3, QWORD PTR [rcx+rdi]
	mov	ecx, r15d
	add	r15d, 4
	imul	ecx, r9d
	vmovddup xmm1, QWORD PTR [rcx+rdx]
	vmovhpd	xmm1, xmm1, QWORD PTR [r8+rdx]
	vpsadbw	xmm1, xmm3, xmm1

; 123  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vpaddq	xmm4, xmm2, xmm1
	cmp	eax, r11d
	jl	SHORT $LL24@kvz_reg_sa
$LN23@kvz_reg_sa:

; 124  :   }
; 125  :   if (height_residual_lines) {

	test	r14d, r14d
	je	SHORT $LN26@kvz_reg_sa

; 126  :     for (; y < height; y++) {

	cmp	eax, esi
	jge	SHORT $LN26@kvz_reg_sa
	npad	9
$LL27@kvz_reg_sa:

; 127  :       __m128i a = _mm_loadl_epi64((__m128i *)(data1 + y * stride1));

	mov	ecx, eax
	imul	ecx, r10d
	vmovq	xmm1, QWORD PTR [rcx+rdi]

; 128  :       __m128i b = _mm_loadl_epi64((__m128i *)(data2 + y * stride2));

	mov	ecx, eax
	inc	eax
	imul	ecx, r9d
	vmovq	xmm0, QWORD PTR [rcx+rdx]

; 129  : 
; 130  :       __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	vpsadbw	xmm1, xmm1, xmm0

; 131  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm4, xmm1, xmm4
	cmp	eax, esi
	jl	SHORT $LL27@kvz_reg_sa
$LN26@kvz_reg_sa:

; 132  :     }
; 133  :   }
; 134  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm4, 78				; 0000004eH

; 135  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vpaddq	xmm1, xmm0, xmm4
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 76   :     return reg_sad_w8(data1, data2, height, stride1, stride2);

	jmp	$LN163@kvz_reg_sa
$LN4@kvz_reg_sa:

; 77   :   if (width == 12)

	cmp	r8d, 12
	jne	SHORT $LN5@kvz_reg_sa
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 144  :   __m128i sse_inc = _mm_setzero_si128();

	xor	eax, eax
	vpxor	xmm2, xmm2, xmm2

; 145  :   int32_t y;
; 146  :   for (y = 0; y < height; y++) {

	test	esi, esi
	jle	$LN40@kvz_reg_sa
	mov	edx, DWORD PTR stride2$[rsp]
	mov	r8d, DWORD PTR stride1$[rsp]
	npad	11
$LL33@kvz_reg_sa:

; 147  :     __m128i a = _mm_loadu_si128((const __m128i *)(data1 + y * stride1));

	mov	ecx, eax
	imul	ecx, r8d
	vmovdqu	xmm1, XMMWORD PTR [rcx+rdi]

; 148  :     __m128i b = _mm_loadu_si128((const __m128i *)(data2 + y * stride2));

	mov	ecx, eax
	inc	eax
	imul	ecx, edx

; 149  : 
; 150  :     __m128i b_masked  = _mm_blend_epi16(a, b, 0x3f);

	vpblendw xmm0, xmm1, XMMWORD PTR [rcx+rbx], 63	; 0000003fH

; 151  :     __m128i curr_sads = _mm_sad_epu8   (a, b_masked);

	vpsadbw	xmm1, xmm0, xmm1

; 152  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm2, xmm1, xmm2
	cmp	eax, esi
	jl	SHORT $LL33@kvz_reg_sa

; 153  :   }
; 154  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm2, 78				; 0000004eH

; 155  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vpaddq	xmm1, xmm0, xmm2
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 78   :     return reg_sad_w12(data1, data2, height, stride1, stride2);

	jmp	$LN163@kvz_reg_sa
$LN5@kvz_reg_sa:

; 79   :   if (width == 16)

	cmp	r8d, 16
	jne	$LN6@kvz_reg_sa
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 169  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r11d, DWORD PTR stride2$[rsp]
	mov	r12d, esi
	mov	r14d, DWORD PTR stride1$[rsp]
	mov	r13d, esi
	and	r12d, -4
	and	r13d, 3
	xor	eax, eax
	vpxor	xmm2, xmm2, xmm2
	test	r12d, r12d
	jle	$LN37@kvz_reg_sa

; 156  :   return _mm_cvtsi128_si32(sad);
; 157  : }
; 158  : 
; 159  : static INLINE uint32_t reg_sad_w16(const uint8_t * const data1, const uint8_t * const data2,
; 160  :                             const int32_t height, const uint32_t stride1,
; 161  :                             const uint32_t stride2)
; 162  : {
; 163  :   __m128i sse_inc = _mm_setzero_si128();

	mov	r15d, 2
$LL38@kvz_reg_sa:

; 170  :     __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1));
; 171  :     __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2));

	mov	ecx, eax

; 172  :     __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1));

	lea	r9d, DWORD PTR [r15-1]
	imul	ecx, r11d

; 173  :     __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2));
; 174  :     __m128i e = _mm_loadu_si128((const __m128i *)(data1 + (y + 2) * stride1));
; 175  :     __m128i f = _mm_loadu_si128((const __m128i *)(data2 + (y + 2) * stride2));
; 176  :     __m128i g = _mm_loadu_si128((const __m128i *)(data1 + (y + 3) * stride1));

	lea	r10d, DWORD PTR [r15+1]
	mov	r8d, eax
	add	eax, 4
	imul	r8d, r14d

; 177  :     __m128i h = _mm_loadu_si128((const __m128i *)(data2 + (y + 3) * stride2));
; 178  : 
; 179  :     __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	vmovdqu	xmm0, XMMWORD PTR [r8+rdi]
	vpsadbw	xmm1, xmm0, XMMWORD PTR [rcx+rdx]

; 180  :     __m128i curr_sads_cd = _mm_sad_epu8(c, d);
; 181  :     __m128i curr_sads_ef = _mm_sad_epu8(e, f);
; 182  :     __m128i curr_sads_gh = _mm_sad_epu8(g, h);
; 183  : 
; 184  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm2, xmm1, xmm2
	mov	ecx, r9d
	mov	r8d, r15d
	imul	ecx, r14d
	imul	r9d, r11d
	imul	r8d, r14d
	vmovdqu	xmm0, XMMWORD PTR [rcx+rdi]
	vpsadbw	xmm1, xmm0, XMMWORD PTR [r9+rdx]
	vmovdqu	xmm0, XMMWORD PTR [r8+rdi]
	mov	ecx, r15d
	add	r15d, 4
	imul	ecx, r11d

; 185  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vpaddq	xmm3, xmm2, xmm1
	vpsadbw	xmm1, xmm0, XMMWORD PTR [rcx+rdx]
	mov	ecx, r10d
	imul	r10d, r11d
	imul	ecx, r14d

; 186  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	vpaddq	xmm2, xmm3, xmm1
	vmovdqu	xmm0, XMMWORD PTR [rcx+rdi]
	vpsadbw	xmm1, xmm0, XMMWORD PTR [r10+rdx]

; 187  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	vpaddq	xmm2, xmm2, xmm1
	cmp	eax, r12d
	jl	$LL38@kvz_reg_sa
$LN37@kvz_reg_sa:

; 188  :   }
; 189  :   if (height_residual_lines) {

	test	r13d, r13d
	je	SHORT $LN40@kvz_reg_sa

; 190  :     for (; y < height; y++) {

	cmp	eax, esi
	jge	SHORT $LN40@kvz_reg_sa
	npad	2
$LL41@kvz_reg_sa:

; 191  :       __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1));

	mov	r8d, eax

; 192  :       __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2));

	mov	ecx, eax
	imul	r8d, r14d
	inc	eax
	imul	ecx, r11d

; 193  : 
; 194  :       __m128i curr_sads = _mm_sad_epu8(a, b);

	vmovdqu	xmm0, XMMWORD PTR [r8+rdi]
	vpsadbw	xmm1, xmm0, XMMWORD PTR [rcx+rdx]

; 195  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm2, xmm1, xmm2
	cmp	eax, esi
	jl	SHORT $LL41@kvz_reg_sa
$LN40@kvz_reg_sa:

; 196  :     }
; 197  :   }
; 198  : 
; 199  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm2, 78				; 0000004eH

; 200  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vpaddq	xmm1, xmm0, xmm2
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 80   :     return reg_sad_w16(data1, data2, height, stride1, stride2);

	jmp	$LN163@kvz_reg_sa
$LN6@kvz_reg_sa:
	vmovaps	XMMWORD PTR [rsp+48], xmm6
	vmovaps	XMMWORD PTR [rsp+32], xmm7

; 81   :   if (width == 24)

	cmp	r8d, 24
	jne	$LN7@kvz_reg_sa
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 214  :   for (y = 0; y < height_doublelines; y += 2) {

	mov	r14d, DWORD PTR stride2$[rsp]
	mov	r11d, esi
	mov	r15d, DWORD PTR stride1$[rsp]
	and	r11d, -2
	and	esi, 1
	xor	eax, eax
	vpxor	xmm6, xmm6, xmm6
	test	r11d, r11d
	jle	SHORT $LN46@kvz_reg_sa
	npad	5
$LL47@kvz_reg_sa:

; 215  :     __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1));
; 216  :     __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2));
; 217  :     __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1));
; 218  :     __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2));
; 219  : 
; 220  :     __m128d e_d = _mm_setzero_pd();
; 221  :     __m128d f_d = _mm_setzero_pd();
; 222  : 
; 223  :     e_d = _mm_loadl_pd(e_d, (const double *)(data1 + (y + 0) * stride1 + 16));
; 224  :     f_d = _mm_loadl_pd(f_d, (const double *)(data2 + (y + 0) * stride2 + 16));
; 225  :     e_d = _mm_loadh_pd(e_d, (const double *)(data1 + (y + 1) * stride1 + 16));

	lea	r8d, DWORD PTR [rax+1]
	mov	r10d, eax
	mov	ecx, r8d
	imul	r10d, r15d
	imul	ecx, r15d

; 226  :     f_d = _mm_loadh_pd(f_d, (const double *)(data2 + (y + 1) * stride2 + 16));

	imul	r8d, r14d
	mov	r9d, ecx

; 227  : 
; 228  :     __m128i e = _mm_castpd_si128(e_d);
; 229  :     __m128i f = _mm_castpd_si128(f_d);
; 230  : 
; 231  :     __m128i curr_sads_1 = _mm_sad_epu8(a, b);

	vmovdqu	xmm0, XMMWORD PTR [r10+rdi]
	vmovddup xmm5, QWORD PTR [r10+rdi+16]
	vmovhpd	xmm5, xmm5, QWORD PTR [rcx+rdi+16]

; 232  :     __m128i curr_sads_2 = _mm_sad_epu8(c, d);

	vmovdqu	xmm2, XMMWORD PTR [r9+rdi]
	mov	ecx, eax
	add	eax, 2
	imul	ecx, r14d
	vpsadbw	xmm1, xmm0, XMMWORD PTR [rcx+rdx]
	vmovddup xmm4, QWORD PTR [rcx+rdx+16]
	vmovhpd	xmm4, xmm4, QWORD PTR [r8+rdx+16]
	vpsadbw	xmm0, xmm2, XMMWORD PTR [r8+rdx]

; 233  :     __m128i curr_sads_3 = _mm_sad_epu8(e, f);
; 234  : 
; 235  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_1);

	vpaddq	xmm3, xmm1, xmm6

; 236  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_2);

	vpaddq	xmm3, xmm3, xmm0
	vpsadbw	xmm1, xmm5, xmm4

; 237  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_3);

	vpaddq	xmm6, xmm3, xmm1
	cmp	eax, r11d
	jl	SHORT $LL47@kvz_reg_sa
$LN46@kvz_reg_sa:

; 238  :   }
; 239  :   if (height_parity) {

	test	esi, esi
	je	SHORT $LN48@kvz_reg_sa

; 240  :     __m128i a = _mm_loadu_si128   ((const __m128i *)(data1 + y * stride1));
; 241  :     __m128i b = _mm_loadu_si128   ((const __m128i *)(data2 + y * stride2));
; 242  :     __m128i c = _mm_loadl_epi64   ((const __m128i *)(data1 + y * stride1 + 16));

	mov	ecx, eax
	imul	ecx, r15d

; 243  :     __m128i d = _mm_loadl_epi64   ((const __m128i *)(data2 + y * stride2 + 16));

	imul	eax, r14d
	vmovq	xmm1, QWORD PTR [rcx+rdi+16]
	vmovq	xmm0, QWORD PTR [rax+rdx+16]

; 244  : 
; 245  :     __m128i curr_sads_1 = _mm_sad_epu8(a, b);
; 246  :     __m128i curr_sads_2 = _mm_sad_epu8(c, d);

	vpsadbw	xmm3, xmm1, xmm0
	vmovdqu	xmm1, XMMWORD PTR [rcx+rdi]
	vpsadbw	xmm2, xmm1, XMMWORD PTR [rax+rdx]

; 247  : 
; 248  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_1);

	vpaddq	xmm0, xmm2, xmm6

; 249  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_2);

	vpaddq	xmm6, xmm3, xmm0
$LN48@kvz_reg_sa:

; 250  :   }
; 251  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm6, 78				; 0000004eH

; 252  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vpaddq	xmm1, xmm0, xmm6
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 82   :     return reg_sad_w24(data1, data2, height, stride1, stride2);

	jmp	$LN162@kvz_reg_sa
$LN7@kvz_reg_sa:

; 83   :   if (width == 32)

	mov	r14d, esi
	cmp	r8d, 32					; 00000020H
	jne	$LN8@kvz_reg_sa
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\reg_sad_pow2_widths-avx2.h

; 52   :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r10d, DWORD PTR stride2$[rsp]
	mov	r12d, esi
	mov	r11d, DWORD PTR stride1$[rsp]
	and	r12d, 3
	and	r14d, -4
	xor	eax, eax
	vpxor	xmm2, xmm2, xmm2
	test	r14d, r14d
	jle	$LN52@kvz_reg_sa

; 46   :   __m256i avx_inc = _mm256_setzero_si256();

	mov	r15d, 2
	npad	13
$LL53@kvz_reg_sa:

; 53   :     __m256i a = _mm256_loadu_si256((const __m256i *)(data1 + (y + 0) * stride1));
; 54   :     __m256i b = _mm256_loadu_si256((const __m256i *)(data2 + (y + 0) * stride2));
; 55   :     __m256i c = _mm256_loadu_si256((const __m256i *)(data1 + (y + 1) * stride1));
; 56   :     __m256i d = _mm256_loadu_si256((const __m256i *)(data2 + (y + 1) * stride2));
; 57   :     __m256i e = _mm256_loadu_si256((const __m256i *)(data1 + (y + 2) * stride1));
; 58   :     __m256i f = _mm256_loadu_si256((const __m256i *)(data2 + (y + 2) * stride2));
; 59   :     __m256i g = _mm256_loadu_si256((const __m256i *)(data1 + (y + 3) * stride1));
; 60   :     __m256i h = _mm256_loadu_si256((const __m256i *)(data2 + (y + 3) * stride2));

	lea	r8d, DWORD PTR [r15+1]
	mov	edx, r15d
	mov	ecx, r8d
	imul	edx, r10d
	imul	ecx, r10d
	lea	r9d, DWORD PTR [r15-1]
	imul	r8d, r11d

; 61   : 
; 62   :     __m256i curr_sads_ab = _mm256_sad_epu8(a, b);
; 63   :     __m256i curr_sads_cd = _mm256_sad_epu8(c, d);
; 64   :     __m256i curr_sads_ef = _mm256_sad_epu8(e, f);
; 65   :     __m256i curr_sads_gh = _mm256_sad_epu8(g, h);

	vmovdqu	ymm0, YMMWORD PTR [r8+rdi]
	vpsadbw	ymm5, ymm0, YMMWORD PTR [rcx+rbx]
	mov	ecx, r15d
	add	r15d, 4
	imul	ecx, r11d
	vmovdqu	ymm0, YMMWORD PTR [rcx+rdi]
	vpsadbw	ymm4, ymm0, YMMWORD PTR [rdx+rbx]
	mov	ecx, r9d
	mov	edx, eax
	imul	ecx, r10d
	imul	r9d, r11d
	imul	edx, r10d
	vmovdqu	ymm0, YMMWORD PTR [r9+rdi]
	vpsadbw	ymm3, ymm0, YMMWORD PTR [rcx+rbx]
	mov	ecx, eax
	add	eax, 4
	imul	ecx, r11d
	vmovdqu	ymm0, YMMWORD PTR [rcx+rdi]
	vpsadbw	ymm1, ymm0, YMMWORD PTR [rdx+rbx]

; 66   : 
; 67   :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_ab);

	vpaddq	ymm2, ymm1, ymm2

; 68   :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_cd);

	vpaddq	ymm3, ymm2, ymm3

; 69   :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_ef);

	vpaddq	ymm0, ymm3, ymm4

; 70   :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_gh);

	vpaddq	ymm2, ymm0, ymm5
	cmp	eax, r14d
	jl	SHORT $LL53@kvz_reg_sa
$LN52@kvz_reg_sa:

; 71   :   }
; 72   :   if (height_residual_lines) {

	test	r12d, r12d
	je	SHORT $LN55@kvz_reg_sa

; 73   :     for (; y < height; y++) {

	cmp	eax, esi
	jge	SHORT $LN55@kvz_reg_sa
	npad	9
$LL56@kvz_reg_sa:

; 74   :       __m256i a = _mm256_loadu_si256((const __m256i *)(data1 + (y + 0) * stride1));

	mov	ecx, eax

; 75   :       __m256i b = _mm256_loadu_si256((const __m256i *)(data2 + (y + 0) * stride2));

	mov	edx, eax
	imul	ecx, r11d
	inc	eax
	imul	edx, r10d

; 76   : 
; 77   :       __m256i curr_sads = _mm256_sad_epu8(a, b);

	vmovdqu	ymm0, YMMWORD PTR [rcx+rdi]
	vpsadbw	ymm1, ymm0, YMMWORD PTR [rdx+rbx]

; 78   :       avx_inc = _mm256_add_epi64(avx_inc, curr_sads);

	vpaddq	ymm2, ymm1, ymm2
	cmp	eax, esi
	jl	SHORT $LL56@kvz_reg_sa
$LN55@kvz_reg_sa:

; 79   :     }
; 80   :   }
; 81   : 
; 82   :   __m128i inchi = _mm256_extracti128_si256(avx_inc, 1);

	vextracti128 xmm0, ymm2, 1

; 83   :   __m128i inclo = _mm256_castsi256_si128  (avx_inc);
; 84   : 
; 85   :   __m128i sum_1 = _mm_add_epi64    (inclo, inchi);

	vpaddq	xmm1, xmm0, xmm2

; 86   :   __m128i sum_2 = _mm_shuffle_epi32(sum_1, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm1, 78				; 0000004eH

; 87   :   __m128i sad   = _mm_add_epi64    (sum_1, sum_2);

	vpaddq	xmm1, xmm0, xmm1
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 84   :     return reg_sad_w32(data1, data2, height, stride1, stride2);

	jmp	$LN162@kvz_reg_sa
$LN8@kvz_reg_sa:

; 85   :   if (width == 64)

	cmp	r8d, 64					; 00000040H
	jne	$LN9@kvz_reg_sa
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\reg_sad_pow2_widths-avx2.h

; 102  :   for (y = 0; y < height_twoline_groups; y += 2) {

	mov	r10d, DWORD PTR stride2$[rsp]
	mov	r15d, esi
	mov	r11d, DWORD PTR stride1$[rsp]
	and	r15d, 1
	and	r14d, -2
	xor	eax, eax
	vpxor	xmm6, xmm6, xmm6
	test	r14d, r14d
	jle	SHORT $LN61@kvz_reg_sa
	npad	8
$LL62@kvz_reg_sa:

; 103  :     __m256i a = _mm256_loadu_si256((const __m256i *)(data1 + (y + 0) * stride1));
; 104  :     __m256i b = _mm256_loadu_si256((const __m256i *)(data2 + (y + 0) * stride2));
; 105  :     __m256i c = _mm256_loadu_si256((const __m256i *)(data1 + (y + 0) * stride1 + 32));
; 106  :     __m256i d = _mm256_loadu_si256((const __m256i *)(data2 + (y + 0) * stride2 + 32));
; 107  : 
; 108  :     __m256i e = _mm256_loadu_si256((const __m256i *)(data1 + (y + 1) * stride1));
; 109  :     __m256i f = _mm256_loadu_si256((const __m256i *)(data2 + (y + 1) * stride2));
; 110  :     __m256i g = _mm256_loadu_si256((const __m256i *)(data1 + (y + 1) * stride1 + 32));
; 111  :     __m256i h = _mm256_loadu_si256((const __m256i *)(data2 + (y + 1) * stride2 + 32));

	lea	edx, DWORD PTR [rax+1]
	mov	ecx, eax
	mov	r9d, edx
	imul	ecx, r11d
	imul	edx, r11d
	imul	r9d, r10d
	mov	r8d, edx
	mov	edx, eax
	imul	edx, r10d
	add	eax, 2

; 112  : 
; 113  :     __m256i curr_sads_ab = _mm256_sad_epu8(a, b);
; 114  :     __m256i curr_sads_cd = _mm256_sad_epu8(c, d);
; 115  :     __m256i curr_sads_ef = _mm256_sad_epu8(e, f);

	vmovdqu	ymm1, YMMWORD PTR [r9+rbx]

; 116  :     __m256i curr_sads_gh = _mm256_sad_epu8(g, h);

	vmovdqu	ymm0, YMMWORD PTR [r8+rdi+32]
	vpsadbw	ymm5, ymm0, YMMWORD PTR [r9+rbx+32]
	vpsadbw	ymm4, ymm1, YMMWORD PTR [r8+rdi]
	vmovdqu	ymm0, YMMWORD PTR [rcx+rdi+32]
	vmovdqu	ymm1, YMMWORD PTR [rdx+rbx]
	vpsadbw	ymm2, ymm1, YMMWORD PTR [rcx+rdi]
	vpsadbw	ymm3, ymm0, YMMWORD PTR [rdx+rbx+32]

; 117  : 
; 118  :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_ab);

	vpaddq	ymm0, ymm2, ymm6

; 119  :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_cd);

	vpaddq	ymm3, ymm0, ymm3

; 120  :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_ef);

	vpaddq	ymm1, ymm3, ymm4

; 121  :     avx_inc = _mm256_add_epi64(avx_inc, curr_sads_gh);

	vpaddq	ymm6, ymm1, ymm5
	cmp	eax, r14d
	jl	SHORT $LL62@kvz_reg_sa
$LN61@kvz_reg_sa:

; 122  :   }
; 123  :   if (height_residual_lines) {

	test	r15d, r15d
	je	SHORT $LN64@kvz_reg_sa

; 124  :     for (; y < height; y++) {

	cmp	eax, esi
	jge	SHORT $LN64@kvz_reg_sa
	npad	2
$LL65@kvz_reg_sa:

; 125  :       __m256i a = _mm256_loadu_si256((const __m256i *)(data1 + (y + 0) * stride1));
; 126  :       __m256i b = _mm256_loadu_si256((const __m256i *)(data2 + (y + 0) * stride2));
; 127  :       __m256i c = _mm256_loadu_si256((const __m256i *)(data1 + (y + 0) * stride1 + 32));
; 128  :       __m256i d = _mm256_loadu_si256((const __m256i *)(data2 + (y + 0) * stride2 + 32));

	mov	edx, eax
	mov	ecx, eax
	imul	edx, r10d
	inc	eax
	imul	ecx, r11d

; 129  : 
; 130  :       __m256i curr_sads_ab = _mm256_sad_epu8(a, b);

	vmovdqu	ymm1, YMMWORD PTR [rdx+rbx]

; 131  :       __m256i curr_sads_cd = _mm256_sad_epu8(c, d);

	vmovdqu	ymm0, YMMWORD PTR [rcx+rdi+32]
	vpsadbw	ymm3, ymm0, YMMWORD PTR [rdx+rbx+32]
	vpsadbw	ymm2, ymm1, YMMWORD PTR [rcx+rdi]

; 132  :       avx_inc = _mm256_add_epi64(avx_inc, curr_sads_ab);

	vpaddq	ymm0, ymm2, ymm6

; 133  :       avx_inc = _mm256_add_epi64(avx_inc, curr_sads_cd);

	vpaddq	ymm6, ymm0, ymm3
	cmp	eax, esi
	jl	SHORT $LL65@kvz_reg_sa
$LN64@kvz_reg_sa:

; 134  :     }
; 135  :   }
; 136  : 
; 137  :   __m128i inchi = _mm256_extracti128_si256(avx_inc, 1);

	vextracti128 xmm0, ymm6, 1

; 138  :   __m128i inclo = _mm256_castsi256_si128  (avx_inc);
; 139  : 
; 140  :   __m128i sum_1 = _mm_add_epi64    (inclo, inchi);

	vpaddq	xmm1, xmm0, xmm6

; 141  :   __m128i sum_2 = _mm_shuffle_epi32(sum_1, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm1, 78				; 0000004eH

; 142  :   __m128i sad   = _mm_add_epi64    (sum_1, sum_2);

	vpaddq	xmm1, xmm0, xmm1
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 86   :     return reg_sad_w64(data1, data2, height, stride1, stride2);

	jmp	$LN162@kvz_reg_sa
$LN9@kvz_reg_sa:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 264  :   const int32_t width_xmms             = width  & ~15;

	mov	r12d, DWORD PTR stride2$[rsp]

; 265  :   const int32_t width_residual_pixels  = width  &  15;

	mov	eax, r8d
	mov	r13d, DWORD PTR stride1$[rsp]
	and	eax, 15
	mov	DWORD PTR tv3274[rsp], eax

; 266  : 
; 267  :   const int32_t height_fourline_groups = height & ~3;
; 268  :   const int32_t height_residual_lines  = height &  3;

	and	r14d, 3

; 269  : 
; 270  :   const __m128i rds    = _mm_set1_epi8 (width_residual_pixels);

	movsx	eax, al
	mov	ecx, esi
	vmovd	xmm1, eax

; 271  :   const __m128i ns     = _mm_setr_epi8 (0,  1,  2,  3,  4,  5,  6,  7,
; 272  :                                         8,  9,  10, 11, 12, 13, 14, 15);
; 273  :   const __m128i rdmask = _mm_cmpgt_epi8(rds, ns);
; 274  : 
; 275  :   for (x = 0; x < width_xmms; x += 16) {

	xor	eax, eax
	movsxd	r9, r8d
	and	ecx, -4
	mov	DWORD PTR tv3281[rsp], r14d
	vpbroadcastb xmm1, xmm1
	vpcmpgtb xmm7, xmm1, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	and	r9, -16
	mov	DWORD PTR height_fourline_groups$1$[rsp], ecx
	lea	r15d, QWORD PTR [rax+2]
	mov	QWORD PTR tv3273[rsp], r9
	vpxor	xmm3, xmm3, xmm3
	mov	edx, eax
	jle	$LN70@kvz_reg_sa

; 295  :     }
; 296  :     if (height_residual_lines) {

	lea	rdx, QWORD PTR [r9-1]
	mov	r14d, eax
	shr	rdx, 4
	inc	edx
	shl	edx, 4
	mov	QWORD PTR tv3348[rsp], rdx
	npad	3
$LL71@kvz_reg_sa:

; 276  :     for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r11d, eax
	test	ecx, ecx
	jle	$LN73@kvz_reg_sa
	mov	eax, DWORD PTR height_fourline_groups$1$[rsp]
	npad	14
$LL74@kvz_reg_sa:

; 277  :       __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1 + x));

	mov	r8d, r11d

; 278  :       __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2 + x));
; 279  :       __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1 + x));

	lea	r9d, DWORD PTR [r15-1]
	imul	r8d, r13d

; 280  :       __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2 + x));
; 281  :       __m128i e = _mm_loadu_si128((const __m128i *)(data1 + (y + 2) * stride1 + x));
; 282  :       __m128i f = _mm_loadu_si128((const __m128i *)(data2 + (y + 2) * stride2 + x));
; 283  :       __m128i g = _mm_loadu_si128((const __m128i *)(data1 + (y + 3) * stride1 + x));

	lea	r10d, DWORD PTR [r15+1]
	mov	edx, r11d
	add	r11d, 4
	imul	edx, r12d
	add	r8, r14
	add	rdx, r14

; 284  :       __m128i h = _mm_loadu_si128((const __m128i *)(data2 + (y + 3) * stride2 + x));
; 285  : 
; 286  :       __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	vmovdqu	xmm0, XMMWORD PTR [r8+rdi]
	mov	r8d, r15d
	vpsadbw	xmm1, xmm0, XMMWORD PTR [rdx+rbx]

; 287  :       __m128i curr_sads_cd = _mm_sad_epu8(c, d);
; 288  :       __m128i curr_sads_ef = _mm_sad_epu8(e, f);
; 289  :       __m128i curr_sads_gh = _mm_sad_epu8(g, h);
; 290  : 
; 291  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm2, xmm1, xmm3
	mov	edx, r9d
	imul	r8d, r13d
	imul	edx, r13d
	imul	r9d, r12d
	add	r8, r14
	add	rdx, r14
	mov	ecx, r9d
	add	rcx, r14
	vmovdqu	xmm0, XMMWORD PTR [rdx+rdi]
	mov	edx, r15d
	add	r15d, 4
	vpsadbw	xmm1, xmm0, XMMWORD PTR [rcx+rbx]
	vmovdqu	xmm0, XMMWORD PTR [r8+rdi]
	imul	edx, r12d

; 292  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vpaddq	xmm3, xmm2, xmm1
	add	rdx, r14
	vpsadbw	xmm1, xmm0, XMMWORD PTR [rdx+rbx]
	mov	edx, r10d
	imul	r10d, r12d
	imul	edx, r13d

; 293  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	vpaddq	xmm2, xmm3, xmm1
	mov	ecx, r10d
	add	rdx, r14
	add	rcx, r14
	vmovdqu	xmm0, XMMWORD PTR [rdx+rdi]
	vpsadbw	xmm1, xmm0, XMMWORD PTR [rcx+rbx]

; 294  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	vpaddq	xmm3, xmm2, xmm1
	cmp	r11d, eax
	jl	$LL74@kvz_reg_sa
	mov	r9, QWORD PTR tv3273[rsp]
	xor	eax, eax
	lea	r15d, QWORD PTR [rax+2]
$LN73@kvz_reg_sa:

; 295  :     }
; 296  :     if (height_residual_lines) {

	cmp	DWORD PTR tv3281[rsp], 0
	je	SHORT $LN69@kvz_reg_sa

; 297  :       for (; y < height; y++) {

	cmp	r11d, esi
	jge	SHORT $LN69@kvz_reg_sa
	npad	7
$LL77@kvz_reg_sa:

; 298  :         __m128i a = _mm_loadu_si128((const __m128i *)(data1 + y * stride1 + x));

	mov	r8d, r11d

; 299  :         __m128i b = _mm_loadu_si128((const __m128i *)(data2 + y * stride2 + x));

	mov	edx, r11d
	imul	r8d, r13d
	inc	r11d
	imul	edx, r12d
	add	r8, r14
	add	rdx, r14

; 300  : 
; 301  :         __m128i curr_sads = _mm_sad_epu8(a, b);

	vmovdqu	xmm0, XMMWORD PTR [r8+rdi]
	vpsadbw	xmm1, xmm0, XMMWORD PTR [rdx+rbx]

; 302  : 
; 303  :         sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm3, xmm1, xmm3
	cmp	r11d, esi
	jl	SHORT $LL77@kvz_reg_sa
$LN69@kvz_reg_sa:

; 271  :   const __m128i ns     = _mm_setr_epi8 (0,  1,  2,  3,  4,  5,  6,  7,
; 272  :                                         8,  9,  10, 11, 12, 13, 14, 15);
; 273  :   const __m128i rdmask = _mm_cmpgt_epi8(rds, ns);
; 274  : 
; 275  :   for (x = 0; x < width_xmms; x += 16) {

	mov	ecx, DWORD PTR height_fourline_groups$1$[rsp]
	add	r14, 16
	cmp	r14, r9
	jl	$LL71@kvz_reg_sa
	mov	rdx, QWORD PTR tv3348[rsp]
	mov	r14d, DWORD PTR tv3281[rsp]
$LN70@kvz_reg_sa:

; 304  :       }
; 305  :     }
; 306  :   }
; 307  : 
; 308  :   if (width_residual_pixels) {

	cmp	DWORD PTR tv3274[rsp], 0
	je	$LN82@kvz_reg_sa

; 309  :     for (y = 0; y < height_fourline_groups; y += 4) {

	movsxd	r10, edx
	test	ecx, ecx
	jle	$LN79@kvz_reg_sa
	mov	r11d, DWORD PTR height_fourline_groups$1$[rsp]
	npad	10
$LL80@kvz_reg_sa:

; 310  :       __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1 + x));

	mov	edx, eax

; 311  :       __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2 + x));
; 312  :       __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1 + x));

	lea	r8d, DWORD PTR [r15-1]
	imul	edx, r13d

; 313  :       __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2 + x));
; 314  :       __m128i e = _mm_loadu_si128((const __m128i *)(data1 + (y + 2) * stride1 + x));
; 315  :       __m128i f = _mm_loadu_si128((const __m128i *)(data2 + (y + 2) * stride2 + x));
; 316  :       __m128i g = _mm_loadu_si128((const __m128i *)(data1 + (y + 3) * stride1 + x));

	lea	r9d, DWORD PTR [r15+1]
	add	rdx, r10
	vmovdqu	xmm2, XMMWORD PTR [rdx+rdi]
	mov	edx, r8d
	imul	r8d, r12d
	imul	edx, r13d
	mov	ecx, r8d
	add	rdx, r10
	add	rcx, r10
	vmovdqu	xmm4, XMMWORD PTR [rdx+rdi]
	mov	edx, r15d
	imul	edx, r13d
	add	rdx, r10
	vmovdqu	xmm5, XMMWORD PTR [rdx+rdi]
	mov	edx, r9d

; 317  :       __m128i h = _mm_loadu_si128((const __m128i *)(data2 + (y + 3) * stride2 + x));

	imul	r9d, r12d
	imul	edx, r13d
	add	rdx, r10
	vmovdqu	xmm6, XMMWORD PTR [rdx+rdi]
	mov	edx, eax
	add	eax, 4
	imul	edx, r12d
	add	rdx, r10

; 318  : 
; 319  :       __m128i b_masked     = _mm_blendv_epi8(a, b, rdmask);

	vpblendvb xmm1, xmm2, XMMWORD PTR [rdx+rbx], xmm7

; 320  :       __m128i d_masked     = _mm_blendv_epi8(c, d, rdmask);
; 321  :       __m128i f_masked     = _mm_blendv_epi8(e, f, rdmask);
; 322  :       __m128i h_masked     = _mm_blendv_epi8(g, h, rdmask);
; 323  : 
; 324  :       __m128i curr_sads_ab = _mm_sad_epu8   (a, b_masked);

	vpsadbw	xmm2, xmm1, xmm2
	vpblendvb xmm1, xmm4, XMMWORD PTR [rcx+rbx], xmm7

; 325  :       __m128i curr_sads_cd = _mm_sad_epu8   (c, d_masked);
; 326  :       __m128i curr_sads_ef = _mm_sad_epu8   (e, f_masked);
; 327  :       __m128i curr_sads_gh = _mm_sad_epu8   (g, h_masked);
; 328  : 
; 329  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm3, xmm2, xmm3
	vpsadbw	xmm2, xmm1, xmm4

; 330  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vpaddq	xmm4, xmm3, xmm2
	mov	edx, r15d
	mov	ecx, r9d
	imul	edx, r12d
	add	rcx, r10
	add	r15d, 4
	add	rdx, r10
	vpblendvb xmm1, xmm5, XMMWORD PTR [rdx+rbx], xmm7
	vpsadbw	xmm2, xmm1, xmm5
	vpblendvb xmm1, xmm6, XMMWORD PTR [rcx+rbx], xmm7

; 331  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	vpaddq	xmm3, xmm4, xmm2
	vpsadbw	xmm2, xmm1, xmm6

; 332  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	vpaddq	xmm3, xmm3, xmm2
	cmp	eax, r11d
	jl	$LL80@kvz_reg_sa
$LN79@kvz_reg_sa:

; 333  :     }
; 334  :     if (height_residual_lines) {

	test	r14d, r14d
	je	SHORT $LN82@kvz_reg_sa

; 335  :       for (; y < height; y++) {

	cmp	eax, esi
	jge	SHORT $LN82@kvz_reg_sa
	npad	1
$LL83@kvz_reg_sa:

; 336  :         __m128i a = _mm_loadu_si128((const __m128i *)(data1 + y * stride1 + x));

	mov	edx, eax
	imul	edx, r13d
	add	rdx, r10
	vmovdqu	xmm2, XMMWORD PTR [rdx+rdi]

; 337  :         __m128i b = _mm_loadu_si128((const __m128i *)(data2 + y * stride2 + x));

	mov	edx, eax
	inc	eax
	imul	edx, r12d
	add	rdx, r10

; 338  : 
; 339  :         __m128i b_masked  = _mm_blendv_epi8(a, b, rdmask);

	vpblendvb xmm1, xmm2, XMMWORD PTR [rdx+rbx], xmm7

; 340  :         __m128i curr_sads = _mm_sad_epu8   (a, b_masked);

	vpsadbw	xmm2, xmm1, xmm2

; 341  : 
; 342  :         sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm3, xmm2, xmm3
	cmp	eax, esi
	jl	SHORT $LL83@kvz_reg_sa
$LN82@kvz_reg_sa:

; 343  :       }
; 344  :     }
; 345  :   }
; 346  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm3, 78				; 0000004eH

; 347  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vpaddq	xmm1, xmm0, xmm3
$LN162@kvz_reg_sa:
	vmovaps	xmm6, XMMWORD PTR [rsp+48]
	vmovaps	xmm7, XMMWORD PTR [rsp+32]
$LN163@kvz_reg_sa:
	mov	r14, QWORD PTR [rsp+72]
	mov	r13, QWORD PTR [rsp+120]
	mov	r12, QWORD PTR [rsp+112]
	mov	r15, QWORD PTR [rsp+64]

; 348  : 
; 349  :   return _mm_cvtsi128_si32(sad);

	vmovd	eax, xmm1
	vzeroupper
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 89   : }

	add	rsp, 80					; 00000050H
	pop	rdi
	pop	rsi
	pop	rbx
	ret	0
kvz_reg_sad_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
a$ = 8
b$ = 16
inline_8bit_sad_8x8_avx2 PROC

; 96   :   __m256i sum0, sum1;
; 97   :   sum0 = _mm256_sad_epu8(_mm256_load_si256(a + 0), _mm256_load_si256(b + 0));

	vmovdqu	ymm0, YMMWORD PTR [rcx]

; 98   :   sum1 = _mm256_sad_epu8(_mm256_load_si256(a + 1), _mm256_load_si256(b + 1));

	vmovdqu	ymm1, YMMWORD PTR [rcx+32]
	vpsadbw	ymm3, ymm0, YMMWORD PTR [rdx]
	vpsadbw	ymm2, ymm1, YMMWORD PTR [rdx+32]

; 99   : 
; 100  :   return _mm256_add_epi32(sum0, sum1);

	vpaddd	ymm0, ymm2, ymm3

; 101  : }

	ret	0
inline_8bit_sad_8x8_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
a$ = 32
b$ = 40
inline_8bit_sad_16x16_avx2 PROC

; 108  : {

	sub	rsp, 24

; 97   :   sum0 = _mm256_sad_epu8(_mm256_load_si256(a + 0), _mm256_load_si256(b + 0));

	vmovdqu	ymm0, YMMWORD PTR [rcx]
	vpsadbw	ymm3, ymm0, YMMWORD PTR [rdx]

; 98   :   sum1 = _mm256_sad_epu8(_mm256_load_si256(a + 1), _mm256_load_si256(b + 1));

	vmovdqu	ymm1, YMMWORD PTR [rcx+32]
	vpsadbw	ymm2, ymm1, YMMWORD PTR [rdx+32]

; 97   :   sum0 = _mm256_sad_epu8(_mm256_load_si256(a + 0), _mm256_load_si256(b + 0));

	vmovdqu	ymm0, YMMWORD PTR [rcx+64]
	vpsadbw	ymm4, ymm0, YMMWORD PTR [rdx+64]

; 98   :   sum1 = _mm256_sad_epu8(_mm256_load_si256(a + 1), _mm256_load_si256(b + 1));

	vmovdqu	ymm1, YMMWORD PTR [rcx+96]

; 100  :   return _mm256_add_epi32(sum0, sum1);

	vpaddd	ymm5, ymm2, ymm3

; 98   :   sum1 = _mm256_sad_epu8(_mm256_load_si256(a + 1), _mm256_load_si256(b + 1));

	vpsadbw	ymm2, ymm1, YMMWORD PTR [rdx+96]

; 97   :   sum0 = _mm256_sad_epu8(_mm256_load_si256(a + 0), _mm256_load_si256(b + 0));

	vmovdqu	ymm1, YMMWORD PTR [rcx+128]
	vpsadbw	ymm3, ymm1, YMMWORD PTR [rdx+128]
	vmovdqu	ymm1, YMMWORD PTR [rcx+192]

; 100  :   return _mm256_add_epi32(sum0, sum1);

	vpaddd	ymm0, ymm2, ymm4

; 97   :   sum0 = _mm256_sad_epu8(_mm256_load_si256(a + 0), _mm256_load_si256(b + 0));

	vpsadbw	ymm4, ymm1, YMMWORD PTR [rdx+192]
	vmovaps	XMMWORD PTR [rsp], xmm6

; 109  :   const unsigned size_of_8x8 = 8 * 8 / sizeof(__m256i);
; 110  : 
; 111  :   // Calculate in 4 chunks of 16x4.
; 112  :   __m256i sum0, sum1, sum2, sum3;
; 113  :   sum0 = inline_8bit_sad_8x8_avx2(a + 0 * size_of_8x8, b + 0 * size_of_8x8);
; 114  :   sum1 = inline_8bit_sad_8x8_avx2(a + 1 * size_of_8x8, b + 1 * size_of_8x8);
; 115  :   sum2 = inline_8bit_sad_8x8_avx2(a + 2 * size_of_8x8, b + 2 * size_of_8x8);
; 116  :   sum3 = inline_8bit_sad_8x8_avx2(a + 3 * size_of_8x8, b + 3 * size_of_8x8);
; 117  : 
; 118  :   sum0 = _mm256_add_epi32(sum0, sum1);

	vpaddd	ymm6, ymm0, ymm5

; 98   :   sum1 = _mm256_sad_epu8(_mm256_load_si256(a + 1), _mm256_load_si256(b + 1));

	vmovdqu	ymm0, YMMWORD PTR [rcx+160]
	vpsadbw	ymm2, ymm0, YMMWORD PTR [rdx+160]
	vmovdqu	ymm0, YMMWORD PTR [rcx+224]

; 100  :   return _mm256_add_epi32(sum0, sum1);

	vpaddd	ymm5, ymm2, ymm3

; 98   :   sum1 = _mm256_sad_epu8(_mm256_load_si256(a + 1), _mm256_load_si256(b + 1));

	vpsadbw	ymm2, ymm0, YMMWORD PTR [rdx+224]

; 99   : 
; 100  :   return _mm256_add_epi32(sum0, sum1);

	vpaddd	ymm1, ymm2, ymm4

; 119  :   sum2 = _mm256_add_epi32(sum2, sum3);

	vpaddd	ymm3, ymm1, ymm5

; 120  : 
; 121  :   return _mm256_add_epi32(sum0, sum2);

	vpaddd	ymm0, ymm3, ymm6

; 122  : }

	vmovaps	xmm6, XMMWORD PTR [rsp]
	add	rsp, 24
	ret	0
inline_8bit_sad_16x16_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
sum$ = 8
m256i_horizontal_sum PROC

; 130  :   // Add the high 128 bits to low 128 bits.
; 131  :   __m128i mm128_result = _mm_add_epi32(_mm256_castsi256_si128(sum), _mm256_extractf128_si256(sum, 1));

	vmovdqu	ymm1, YMMWORD PTR [rcx]
	vextractf128 xmm0, ymm1, 1
	vpaddd	xmm1, xmm0, xmm1

; 132  :   // Add the high 64 bits  to low 64 bits.
; 133  :   uint32_t result[4];
; 134  :   _mm_storeu_si128((__m128i*)result, mm128_result);
; 135  :   return result[0] + result[2];

	vpextrd	ecx, xmm1, 2
	vmovd	eax, xmm1
	add	eax, ecx
	vzeroupper

; 136  : }

	ret	0
m256i_horizontal_sum ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
buf1$ = 8
buf2$ = 16
sad_8bit_8x8_avx2 PROC

; 97   :   sum0 = _mm256_sad_epu8(_mm256_load_si256(a + 0), _mm256_load_si256(b + 0));

	vmovdqu	ymm0, YMMWORD PTR [rcx]

; 98   :   sum1 = _mm256_sad_epu8(_mm256_load_si256(a + 1), _mm256_load_si256(b + 1));

	vmovdqu	ymm1, YMMWORD PTR [rcx+32]
	vpsadbw	ymm3, ymm0, YMMWORD PTR [rdx]
	vpsadbw	ymm2, ymm1, YMMWORD PTR [rdx+32]

; 99   : 
; 100  :   return _mm256_add_epi32(sum0, sum1);

	vpaddd	ymm4, ymm2, ymm3

; 131  :   __m128i mm128_result = _mm_add_epi32(_mm256_castsi256_si128(sum), _mm256_extractf128_si256(sum, 1));

	vextractf128 xmm0, ymm4, 1
	vpaddd	xmm1, xmm0, xmm4

; 132  :   // Add the high 64 bits  to low 64 bits.
; 133  :   uint32_t result[4];
; 134  :   _mm_storeu_si128((__m128i*)result, mm128_result);
; 135  :   return result[0] + result[2];

	vpextrd	ecx, xmm1, 2
	vmovd	eax, xmm1
	add	eax, ecx
	vzeroupper

; 141  :   const __m256i *const a = (const __m256i *)buf1;
; 142  :   const __m256i *const b = (const __m256i *)buf2;
; 143  :   __m256i sum = inline_8bit_sad_8x8_avx2(a, b);
; 144  : 
; 145  :   return m256i_horizontal_sum(sum);
; 146  : }

	ret	0
sad_8bit_8x8_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
buf1$ = 32
buf2$ = 40
sad_8bit_16x16_avx2 PROC

; 150  : {

	sub	rsp, 24

; 97   :   sum0 = _mm256_sad_epu8(_mm256_load_si256(a + 0), _mm256_load_si256(b + 0));

	vmovdqu	ymm0, YMMWORD PTR [rcx+128]
	vpsadbw	ymm3, ymm0, YMMWORD PTR [rdx+128]

; 98   :   sum1 = _mm256_sad_epu8(_mm256_load_si256(a + 1), _mm256_load_si256(b + 1));

	vmovdqu	ymm1, YMMWORD PTR [rcx+160]
	vpsadbw	ymm2, ymm1, YMMWORD PTR [rdx+160]

; 97   :   sum0 = _mm256_sad_epu8(_mm256_load_si256(a + 0), _mm256_load_si256(b + 0));

	vmovdqu	ymm0, YMMWORD PTR [rcx+192]
	vpsadbw	ymm4, ymm0, YMMWORD PTR [rdx+192]

; 98   :   sum1 = _mm256_sad_epu8(_mm256_load_si256(a + 1), _mm256_load_si256(b + 1));

	vmovdqu	ymm1, YMMWORD PTR [rcx+224]

; 100  :   return _mm256_add_epi32(sum0, sum1);

	vpaddd	ymm5, ymm2, ymm3

; 98   :   sum1 = _mm256_sad_epu8(_mm256_load_si256(a + 1), _mm256_load_si256(b + 1));

	vpsadbw	ymm2, ymm1, YMMWORD PTR [rdx+224]

; 97   :   sum0 = _mm256_sad_epu8(_mm256_load_si256(a + 0), _mm256_load_si256(b + 0));

	vmovdqu	ymm1, YMMWORD PTR [rcx]
	vpsadbw	ymm3, ymm1, YMMWORD PTR [rdx]
	vmovdqu	ymm1, YMMWORD PTR [rcx+64]

; 100  :   return _mm256_add_epi32(sum0, sum1);

	vpaddd	ymm0, ymm2, ymm4

; 97   :   sum0 = _mm256_sad_epu8(_mm256_load_si256(a + 0), _mm256_load_si256(b + 0));

	vpsadbw	ymm4, ymm1, YMMWORD PTR [rdx+64]
	vmovaps	XMMWORD PTR [rsp], xmm6

; 119  :   sum2 = _mm256_add_epi32(sum2, sum3);

	vpaddd	ymm6, ymm0, ymm5

; 98   :   sum1 = _mm256_sad_epu8(_mm256_load_si256(a + 1), _mm256_load_si256(b + 1));

	vmovdqu	ymm0, YMMWORD PTR [rcx+32]
	vpsadbw	ymm2, ymm0, YMMWORD PTR [rdx+32]
	vmovdqu	ymm0, YMMWORD PTR [rcx+96]

; 100  :   return _mm256_add_epi32(sum0, sum1);

	vpaddd	ymm5, ymm2, ymm3

; 98   :   sum1 = _mm256_sad_epu8(_mm256_load_si256(a + 1), _mm256_load_si256(b + 1));

	vpsadbw	ymm2, ymm0, YMMWORD PTR [rdx+96]

; 99   : 
; 100  :   return _mm256_add_epi32(sum0, sum1);

	vpaddd	ymm1, ymm2, ymm4

; 118  :   sum0 = _mm256_add_epi32(sum0, sum1);

	vpaddd	ymm3, ymm1, ymm5

; 120  : 
; 121  :   return _mm256_add_epi32(sum0, sum2);

	vpaddd	ymm2, ymm3, ymm6

; 131  :   __m128i mm128_result = _mm_add_epi32(_mm256_castsi256_si128(sum), _mm256_extractf128_si256(sum, 1));

	vextractf128 xmm0, ymm2, 1
	vpaddd	xmm1, xmm0, xmm2

; 132  :   // Add the high 64 bits  to low 64 bits.
; 133  :   uint32_t result[4];
; 134  :   _mm_storeu_si128((__m128i*)result, mm128_result);
; 135  :   return result[0] + result[2];

	vpextrd	ecx, xmm1, 2
	vmovd	eax, xmm1
	add	eax, ecx
	vzeroupper

; 151  :   const __m256i *const a = (const __m256i *)buf1;
; 152  :   const __m256i *const b = (const __m256i *)buf2;
; 153  :   __m256i sum = inline_8bit_sad_16x16_avx2(a, b);
; 154  : 
; 155  :   return m256i_horizontal_sum(sum);
; 156  : }

	vmovaps	xmm6, XMMWORD PTR [rsp]
	add	rsp, 24
	ret	0
sad_8bit_16x16_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
buf1$ = 8
buf2$ = 16
sad_8bit_32x32_avx2 PROC

; 97   :   sum0 = _mm256_sad_epu8(_mm256_load_si256(a + 0), _mm256_load_si256(b + 0));

	vmovdqu	ymm0, YMMWORD PTR [rcx]

; 98   :   sum1 = _mm256_sad_epu8(_mm256_load_si256(a + 1), _mm256_load_si256(b + 1));

	vmovdqu	ymm1, YMMWORD PTR [rcx+32]
	vpsadbw	ymm3, ymm0, YMMWORD PTR [rdx]
	vpsadbw	ymm2, ymm1, YMMWORD PTR [rdx+32]
	sub	rdx, rcx
	lea	rax, QWORD PTR [rcx+96]

; 99   : 
; 100  :   return _mm256_add_epi32(sum0, sum1);

	vpaddd	ymm4, ymm2, ymm3
	mov	ecx, 15
	npad	14
$LL4@sad_8bit_3:

; 98   :   sum1 = _mm256_sad_epu8(_mm256_load_si256(a + 1), _mm256_load_si256(b + 1));

	vmovdqu	ymm0, YMMWORD PTR [rdx+rax]
	vmovdqu	ymm1, YMMWORD PTR [rdx+rax-32]
	vpsadbw	ymm3, ymm0, YMMWORD PTR [rax]
	vpsadbw	ymm2, ymm1, YMMWORD PTR [rax-32]

; 161  :   const __m256i *const a = (const __m256i *)buf1;
; 162  :   const __m256i *const b = (const __m256i *)buf2;
; 163  : 
; 164  :   const unsigned size_of_8x8 = 8 * 8 / sizeof(__m256i);
; 165  :   const unsigned size_of_32x32 = 32 * 32 / sizeof(__m256i);
; 166  : 
; 167  :   // Looping 512 bytes at a time seems faster than letting VC figure it out
; 168  :   // through inlining, like inline_8bit_sad_16x16_avx2 does.
; 169  :   __m256i sum0 = inline_8bit_sad_8x8_avx2(a, b);
; 170  :   for (unsigned i = size_of_8x8; i < size_of_32x32; i += size_of_8x8) {

	lea	rax, QWORD PTR [rax+64]

; 100  :   return _mm256_add_epi32(sum0, sum1);

	vpaddd	ymm0, ymm2, ymm3

; 171  :     __m256i sum1 = inline_8bit_sad_8x8_avx2(a + i, b + i);
; 172  :     sum0 = _mm256_add_epi32(sum0, sum1);

	vpaddd	ymm4, ymm0, ymm4
	sub	rcx, 1
	jne	SHORT $LL4@sad_8bit_3

; 131  :   __m128i mm128_result = _mm_add_epi32(_mm256_castsi256_si128(sum), _mm256_extractf128_si256(sum, 1));

	vextractf128 xmm0, ymm4, 1
	vpaddd	xmm1, xmm0, xmm4

; 132  :   // Add the high 64 bits  to low 64 bits.
; 133  :   uint32_t result[4];
; 134  :   _mm_storeu_si128((__m128i*)result, mm128_result);
; 135  :   return result[0] + result[2];

	vmovd	ecx, xmm1
	vpextrd	eax, xmm1, 2
	add	eax, ecx
	vzeroupper

; 173  :   }
; 174  : 
; 175  :   return m256i_horizontal_sum(sum0);
; 176  : }

	ret	0
sad_8bit_32x32_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
buf1$ = 8
buf2$ = 16
sad_8bit_64x64_avx2 PROC

; 97   :   sum0 = _mm256_sad_epu8(_mm256_load_si256(a + 0), _mm256_load_si256(b + 0));

	vmovdqu	ymm0, YMMWORD PTR [rcx]

; 98   :   sum1 = _mm256_sad_epu8(_mm256_load_si256(a + 1), _mm256_load_si256(b + 1));

	vmovdqu	ymm1, YMMWORD PTR [rcx+32]
	vpsadbw	ymm3, ymm0, YMMWORD PTR [rdx]
	vpsadbw	ymm2, ymm1, YMMWORD PTR [rdx+32]
	sub	rdx, rcx
	lea	rax, QWORD PTR [rcx+96]

; 99   : 
; 100  :   return _mm256_add_epi32(sum0, sum1);

	vpaddd	ymm4, ymm2, ymm3
	mov	ecx, 63					; 0000003fH
	npad	14
$LL4@sad_8bit_6:

; 98   :   sum1 = _mm256_sad_epu8(_mm256_load_si256(a + 1), _mm256_load_si256(b + 1));

	vmovdqu	ymm0, YMMWORD PTR [rdx+rax]
	vmovdqu	ymm1, YMMWORD PTR [rdx+rax-32]
	vpsadbw	ymm3, ymm0, YMMWORD PTR [rax]
	vpsadbw	ymm2, ymm1, YMMWORD PTR [rax-32]

; 181  :   const __m256i *const a = (const __m256i *)buf1;
; 182  :   const __m256i *const b = (const __m256i *)buf2;
; 183  : 
; 184  :   const unsigned size_of_8x8 = 8 * 8 / sizeof(__m256i);
; 185  :   const unsigned size_of_64x64 = 64 * 64 / sizeof(__m256i);
; 186  : 
; 187  :   // Looping 512 bytes at a time seems faster than letting VC figure it out
; 188  :   // through inlining, like inline_8bit_sad_16x16_avx2 does.
; 189  :   __m256i sum0 = inline_8bit_sad_8x8_avx2(a, b);
; 190  :   for (unsigned i = size_of_8x8; i < size_of_64x64; i += size_of_8x8) {

	lea	rax, QWORD PTR [rax+64]

; 100  :   return _mm256_add_epi32(sum0, sum1);

	vpaddd	ymm0, ymm2, ymm3

; 191  :     __m256i sum1 = inline_8bit_sad_8x8_avx2(a + i, b + i);
; 192  :     sum0 = _mm256_add_epi32(sum0, sum1);

	vpaddd	ymm4, ymm0, ymm4
	sub	rcx, 1
	jne	SHORT $LL4@sad_8bit_6

; 131  :   __m128i mm128_result = _mm_add_epi32(_mm256_castsi256_si128(sum), _mm256_extractf128_si256(sum, 1));

	vextractf128 xmm0, ymm4, 1
	vpaddd	xmm1, xmm0, xmm4

; 132  :   // Add the high 64 bits  to low 64 bits.
; 133  :   uint32_t result[4];
; 134  :   _mm_storeu_si128((__m128i*)result, mm128_result);
; 135  :   return result[0] + result[2];

	vmovd	ecx, xmm1
	vpextrd	eax, xmm1, 2
	add	eax, ecx
	vzeroupper

; 193  :   }
; 194  : 
; 195  :   return m256i_horizontal_sum(sum0);
; 196  : }

	ret	0
sad_8bit_64x64_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
org$ = 8
cur$ = 16
satd_4x4_8bit_avx2 PROC

; 200  : 
; 201  :   __m128i original = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)org));
; 202  :   __m128i current = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)cur));

	vmovq	xmm0, QWORD PTR [rdx]
	vmovq	xmm1, QWORD PTR [rcx]
	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1

; 203  : 
; 204  :   __m128i diff_lo = _mm_sub_epi16(current, original);
; 205  : 
; 206  :   original = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)(org + 8)));

	vmovq	xmm1, QWORD PTR [rcx+8]
	vpsubw	xmm4, xmm2, xmm0

; 207  :   current = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)(cur + 8)));

	vmovq	xmm0, QWORD PTR [rdx+8]
	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1

; 208  : 
; 209  :   __m128i diff_hi = _mm_sub_epi16(current, original);

	vpsubw	xmm3, xmm2, xmm0

; 210  : 
; 211  : 
; 212  :   //Hor
; 213  :   __m128i row0 = _mm_hadd_epi16(diff_lo, diff_hi);
; 214  :   __m128i row1 = _mm_hsub_epi16(diff_lo, diff_hi);

	vphsubw	xmm0, xmm4, xmm3
	vphaddw	xmm1, xmm4, xmm3

; 215  : 
; 216  :   __m128i row2 = _mm_hadd_epi16(row0, row1);

	vphaddw	xmm2, xmm1, xmm0

; 217  :   __m128i row3 = _mm_hsub_epi16(row0, row1);

	vphsubw	xmm1, xmm1, xmm0

; 218  : 
; 219  :   //Ver
; 220  :   row0 = _mm_hadd_epi16(row2, row3);

	vphaddw	xmm4, xmm2, xmm1

; 221  :   row1 = _mm_hsub_epi16(row2, row3);

	vphsubw	xmm2, xmm2, xmm1

; 222  : 
; 223  :   row2 = _mm_hadd_epi16(row0, row1);
; 224  :   row3 = _mm_hsub_epi16(row0, row1);

	vphsubw	xmm0, xmm4, xmm2
	vphaddw	xmm1, xmm4, xmm2

; 225  : 
; 226  :   //Abs and sum
; 227  :   row2 = _mm_abs_epi16(row2);
; 228  :   row3 = _mm_abs_epi16(row3);

	vpabsw	xmm3, xmm0
	vpabsw	xmm2, xmm1

; 229  : 
; 230  :   row3 = _mm_add_epi16(row2, row3);

	vpaddw	xmm4, xmm3, xmm2

; 231  : 
; 232  :   row3 = _mm_add_epi16(row3, _mm_shuffle_epi32(row3, _MM_SHUFFLE(1, 0, 3, 2) ));

	vpshufd	xmm0, xmm4, 78				; 0000004eH
	vpaddw	xmm2, xmm0, xmm4

; 233  :   row3 = _mm_add_epi16(row3, _mm_shuffle_epi32(row3, _MM_SHUFFLE(0, 1, 0, 1) ));

	vpshufd	xmm1, xmm2, 17
	vpaddw	xmm3, xmm1, xmm2

; 234  :   row3 = _mm_add_epi16(row3, _mm_shufflelo_epi16(row3, _MM_SHUFFLE(0, 1, 0, 1) ));

	vpshuflw xmm0, xmm3, 17
	vpaddw	xmm1, xmm0, xmm3

; 235  : 
; 236  :   unsigned sum = _mm_extract_epi16(row3, 0);

	vpextrw	eax, xmm1, 0

; 237  :   unsigned satd = (sum + 1) >> 1;

	inc	eax
	shr	eax, 1

; 238  : 
; 239  :   return satd;
; 240  : }

	ret	0
satd_4x4_8bit_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
$T1 = 0
$T2 = 0
preds$ = 32
orig$ = 40
num_modes$ = 48
satds_out$ = 56
satd_8bit_4x4_dual_avx2 PROC

; 245  : {

	sub	rsp, 24

; 246  : 
; 247  :   __m256i original = _mm256_broadcastsi128_si256(_mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)orig)));
; 248  :   __m256i pred = _mm256_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)preds[0]));

	vmovq	xmm1, QWORD PTR [rcx]

; 249  :   pred = _mm256_inserti128_si256(pred, _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)preds[1])), 1);

	vmovq	xmm0, QWORD PTR [rcx+1024]
	vpmovzxbw xmm2, xmm0
	vpmovzxbw ymm0, xmm1
	vmovq	xmm1, QWORD PTR [rdx]
	vinserti128 ymm3, ymm0, xmm2, 1
	vpmovzxbw xmm0, xmm1

; 250  : 
; 251  :   __m256i diff_lo = _mm256_sub_epi16(pred, original);
; 252  : 
; 253  :   original = _mm256_broadcastsi128_si256(_mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)(orig + 8))));
; 254  :   pred = _mm256_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)(preds[0] + 8)));

	vmovq	xmm1, QWORD PTR [rcx+8]
	vmovdqu	XMMWORD PTR $T1[rsp], xmm0
	vbroadcasti128 ymm2, XMMWORD PTR $T1[rsp]

; 255  :   pred = _mm256_inserti128_si256(pred, _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)(preds[1] + 8))), 1);

	vmovq	xmm0, QWORD PTR [rcx+1032]
	vpsubw	ymm5, ymm3, ymm2
	vpmovzxbw xmm2, xmm0
	vpmovzxbw ymm0, xmm1
	vmovq	xmm1, QWORD PTR [rdx+8]
	vinserti128 ymm3, ymm0, xmm2, 1
	vpmovzxbw xmm0, xmm1
	vmovdqu	XMMWORD PTR $T2[rsp], xmm0
	vbroadcasti128 ymm2, XMMWORD PTR $T2[rsp]

; 256  : 
; 257  :   __m256i diff_hi = _mm256_sub_epi16(pred, original);

	vpsubw	ymm1, ymm3, ymm2

; 258  : 
; 259  :   //Hor
; 260  :   __m256i row0 = _mm256_hadd_epi16(diff_lo, diff_hi);

	vphaddw	ymm4, ymm5, ymm1

; 261  :   __m256i row1 = _mm256_hsub_epi16(diff_lo, diff_hi);

	vphsubw	ymm0, ymm5, ymm1

; 262  : 
; 263  :   __m256i row2 = _mm256_hadd_epi16(row0, row1);
; 264  :   __m256i row3 = _mm256_hsub_epi16(row0, row1);

	vphsubw	ymm1, ymm4, ymm0
	vphaddw	ymm2, ymm4, ymm0

; 265  : 
; 266  :   //Ver
; 267  :   row0 = _mm256_hadd_epi16(row2, row3);

	vphaddw	ymm5, ymm2, ymm1

; 268  :   row1 = _mm256_hsub_epi16(row2, row3);

	vphsubw	ymm2, ymm2, ymm1

; 269  : 
; 270  :   row2 = _mm256_hadd_epi16(row0, row1);

	vphaddw	ymm0, ymm5, ymm2

; 271  :   row3 = _mm256_hsub_epi16(row0, row1);
; 272  : 
; 273  :   //Abs and sum
; 274  :   row2 = _mm256_abs_epi16(row2);

	vpabsw	ymm3, ymm0
	vphsubw	ymm1, ymm5, ymm2

; 275  :   row3 = _mm256_abs_epi16(row3);

	vpabsw	ymm2, ymm1

; 276  : 
; 277  :   row3 = _mm256_add_epi16(row2, row3);

	vpaddw	ymm4, ymm2, ymm3

; 278  : 
; 279  :   row3 = _mm256_add_epi16(row3, _mm256_shuffle_epi32(row3, _MM_SHUFFLE(1, 0, 3, 2) ));

	vpshufd	ymm0, ymm4, 78				; 0000004eH
	vpaddw	ymm2, ymm0, ymm4

; 280  :   row3 = _mm256_add_epi16(row3, _mm256_shuffle_epi32(row3, _MM_SHUFFLE(0, 1, 0, 1) ));

	vpshufd	ymm1, ymm2, 17
	vpaddw	ymm3, ymm1, ymm2

; 281  :   row3 = _mm256_add_epi16(row3, _mm256_shufflelo_epi16(row3, _MM_SHUFFLE(0, 1, 0, 1) ));

	vpshuflw ymm0, ymm3, 17
	vpaddw	ymm1, ymm0, ymm3

; 282  : 
; 283  :   unsigned sum1 = _mm_extract_epi16(_mm256_castsi256_si128(row3), 0);
; 284  :   sum1 = (sum1 + 1) >> 1;
; 285  : 
; 286  :   unsigned sum2 = _mm_extract_epi16(_mm256_extracti128_si256(row3, 1), 0);

	vextracti128 xmm0, ymm1, 1
	vpextrw	eax, xmm1, 0
	inc	eax
	shr	eax, 1

; 287  :   sum2 = (sum2 + 1) >> 1;
; 288  : 
; 289  :   satds_out[0] = sum1;

	mov	DWORD PTR [r9], eax
	vpextrw	eax, xmm0, 0
	inc	eax
	shr	eax, 1

; 290  :   satds_out[1] = sum2;

	mov	DWORD PTR [r9+4], eax
	vzeroupper

; 291  : }

	add	rsp, 24
	ret	0
satd_8bit_4x4_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
row$ = 32
hor_transform_row_avx2 PROC

; 293  : static INLINE void hor_transform_row_avx2(__m128i* row){

	sub	rsp, 24

; 294  :   
; 295  :   __m128i mask_pos = _mm_set1_epi16(1);
; 296  :   __m128i mask_neg = _mm_set1_epi16(-1);

	vmovdqu	xmm5, XMMWORD PTR __xmm@ffffffffffffffffffffffffffffffff

; 297  :   __m128i sign_mask = _mm_unpacklo_epi64(mask_pos, mask_neg);
; 298  :   __m128i temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vmovdqu	xmm1, XMMWORD PTR [rcx]
	vmovaps	XMMWORD PTR [rsp], xmm6
	vmovdqu	xmm6, XMMWORD PTR __xmm@00010001000100010001000100010001
	vpunpcklqdq xmm0, xmm6, xmm5

; 299  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm2, xmm1, xmm0
	vpshufd	xmm1, xmm1, 78				; 0000004eH

; 300  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm3, xmm2, xmm1

; 301  : 
; 302  :   sign_mask = _mm_unpacklo_epi32(mask_pos, mask_neg);

	vpunpckldq xmm0, xmm6, xmm5

; 303  :   temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));
; 304  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm2, xmm3, xmm0
	vpshufd	xmm1, xmm3, 177				; 000000b1H

; 305  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm4, xmm2, xmm1

; 306  : 
; 307  :   sign_mask = _mm_unpacklo_epi16(mask_pos, mask_neg);

	vpunpcklwd xmm0, xmm6, xmm5

; 308  :   temp = _mm_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));
; 309  :   temp = _mm_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));
; 310  :   *row = _mm_sign_epi16(*row, sign_mask);
; 311  :   *row = _mm_add_epi16(*row, temp);
; 312  : }

	vmovaps	xmm6, XMMWORD PTR [rsp]
	vpshuflw xmm1, xmm4, 177			; 000000b1H
	vpshufhw xmm2, xmm1, 177			; 000000b1H
	vpsignw	xmm3, xmm4, xmm0
	vpaddw	xmm0, xmm3, xmm2
	vmovdqu	XMMWORD PTR [rcx], xmm0
	add	rsp, 24
	ret	0
hor_transform_row_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
row$ = 32
hor_transform_row_dual_avx2 PROC

; 314  : static INLINE void hor_transform_row_dual_avx2(__m256i* row){

	sub	rsp, 24

; 315  :   
; 316  :   __m256i mask_pos = _mm256_set1_epi16(1);
; 317  :   __m256i mask_neg = _mm256_set1_epi16(-1);

	vmovdqu	ymm5, YMMWORD PTR __ymm@ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff

; 318  :   __m256i sign_mask = _mm256_unpacklo_epi64(mask_pos, mask_neg);
; 319  :   __m256i temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vmovdqu	ymm1, YMMWORD PTR [rcx]
	vpshufd	ymm2, ymm1, 78				; 0000004eH
	vmovaps	XMMWORD PTR [rsp], xmm6
	vmovdqu	ymm6, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vpunpcklqdq ymm0, ymm6, ymm5

; 320  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm1, ymm1, ymm0

; 321  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm4, ymm1, ymm2

; 322  : 
; 323  :   sign_mask = _mm256_unpacklo_epi32(mask_pos, mask_neg);

	vpunpckldq ymm0, ymm6, ymm5

; 324  :   temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));
; 325  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm1, ymm4, ymm0
	vpshufd	ymm3, ymm4, 177				; 000000b1H

; 326  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm2, ymm1, ymm3

; 327  : 
; 328  :   sign_mask = _mm256_unpacklo_epi16(mask_pos, mask_neg);
; 329  :   temp = _mm256_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw ymm0, ymm2, 177			; 000000b1H
	vpunpcklwd ymm1, ymm6, ymm5

; 330  :   temp = _mm256_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw ymm3, ymm0, 177			; 000000b1H

; 331  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm2, ymm2, ymm1

; 332  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm0, ymm2, ymm3
	vmovdqu	YMMWORD PTR [rcx], ymm0
	vzeroupper

; 333  : }

	vmovaps	xmm6, XMMWORD PTR [rsp]
	add	rsp, 24
	ret	0
hor_transform_row_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
out$ = 8
in$ = 16
out_idx0$ = 24
out_idx1$ = 32
in_idx0$ = 40
in_idx1$ = 48
add_sub_avx2 PROC

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	mov	r10d, DWORD PTR in_idx0$[rsp]
	mov	eax, DWORD PTR in_idx1$[rsp]
	shl	rax, 4
	shl	r10, 4
	add	r10, rdx
	add	rdx, rax
	mov	eax, r8d
	add	rax, rax
	vmovdqu	xmm0, XMMWORD PTR [r10]
	vpaddw	xmm1, xmm0, XMMWORD PTR [rdx]
	vmovdqu	XMMWORD PTR [rcx+rax*8], xmm1

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vmovdqu	xmm0, XMMWORD PTR [r10]
	vpsubw	xmm2, xmm0, XMMWORD PTR [rdx]
	mov	eax, r9d
	add	rax, rax
	vmovdqu	XMMWORD PTR [rcx+rax*8], xmm2

; 339  : }

	ret	0
add_sub_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
rows$ = 112
ver_transform_block_avx2 PROC

; 341  : static INLINE void ver_transform_block_avx2(__m128i (*rows)[8]){

	mov	rax, rsp
	sub	rsp, 104				; 00000068H

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vmovdqu	xmm0, XMMWORD PTR [rcx]
	vmovaps	XMMWORD PTR [rax-24], xmm6
	vmovaps	XMMWORD PTR [rax-40], xmm7
	vpaddw	xmm7, xmm0, XMMWORD PTR [rcx+16]
	vmovaps	XMMWORD PTR [rax-56], xmm8
	vmovaps	XMMWORD PTR [rax-72], xmm9
	vmovaps	XMMWORD PTR [rax-88], xmm10
	vmovaps	XMMWORD PTR [rsp], xmm11

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	xmm11, xmm0, XMMWORD PTR [rcx+16]

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vmovdqu	xmm0, XMMWORD PTR [rcx+32]
	vpaddw	xmm3, xmm0, XMMWORD PTR [rcx+48]

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	xmm9, xmm0, XMMWORD PTR [rcx+48]

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vmovdqu	xmm0, XMMWORD PTR [rcx+64]
	vpaddw	xmm5, xmm0, XMMWORD PTR [rcx+80]

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	xmm10, xmm0, XMMWORD PTR [rcx+80]

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vmovdqu	xmm0, XMMWORD PTR [rcx+96]
	vpaddw	xmm2, xmm0, XMMWORD PTR [rcx+112]

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	xmm4, xmm0, XMMWORD PTR [rcx+112]

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	xmm6, xmm3, xmm7
	vpaddw	xmm8, xmm9, xmm11
	vpaddw	xmm1, xmm2, xmm5
	vpaddw	xmm0, xmm1, xmm6

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	xmm1, xmm6, xmm1

; 342  : 
; 343  :   __m128i temp0[8];
; 344  :   add_sub_avx2(temp0, (*rows), 0, 1, 0, 1);
; 345  :   add_sub_avx2(temp0, (*rows), 2, 3, 2, 3);
; 346  :   add_sub_avx2(temp0, (*rows), 4, 5, 4, 5);
; 347  :   add_sub_avx2(temp0, (*rows), 6, 7, 6, 7);
; 348  : 
; 349  :   __m128i temp1[8];
; 350  :   add_sub_avx2(temp1, temp0, 0, 1, 0, 2);
; 351  :   add_sub_avx2(temp1, temp0, 2, 3, 1, 3);
; 352  :   add_sub_avx2(temp1, temp0, 4, 5, 4, 6);
; 353  :   add_sub_avx2(temp1, temp0, 6, 7, 5, 7);
; 354  : 
; 355  :   add_sub_avx2((*rows), temp1, 0, 1, 0, 4);
; 356  :   add_sub_avx2((*rows), temp1, 2, 3, 1, 5);
; 357  :   add_sub_avx2((*rows), temp1, 4, 5, 2, 6);
; 358  :   add_sub_avx2((*rows), temp1, 6, 7, 3, 7);
; 359  :   
; 360  : }

	vmovaps	xmm6, XMMWORD PTR [rax-24]

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	xmm7, xmm7, xmm3
	vpsubw	xmm9, xmm11, xmm9

; 342  : 
; 343  :   __m128i temp0[8];
; 344  :   add_sub_avx2(temp0, (*rows), 0, 1, 0, 1);
; 345  :   add_sub_avx2(temp0, (*rows), 2, 3, 2, 3);
; 346  :   add_sub_avx2(temp0, (*rows), 4, 5, 4, 5);
; 347  :   add_sub_avx2(temp0, (*rows), 6, 7, 6, 7);
; 348  : 
; 349  :   __m128i temp1[8];
; 350  :   add_sub_avx2(temp1, temp0, 0, 1, 0, 2);
; 351  :   add_sub_avx2(temp1, temp0, 2, 3, 1, 3);
; 352  :   add_sub_avx2(temp1, temp0, 4, 5, 4, 6);
; 353  :   add_sub_avx2(temp1, temp0, 6, 7, 5, 7);
; 354  : 
; 355  :   add_sub_avx2((*rows), temp1, 0, 1, 0, 4);
; 356  :   add_sub_avx2((*rows), temp1, 2, 3, 1, 5);
; 357  :   add_sub_avx2((*rows), temp1, 4, 5, 2, 6);
; 358  :   add_sub_avx2((*rows), temp1, 6, 7, 3, 7);
; 359  :   
; 360  : }

	vmovaps	xmm11, XMMWORD PTR [rsp]

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	xmm2, xmm5, xmm2
	vmovdqu	XMMWORD PTR [rcx+16], xmm1
	vmovdqu	XMMWORD PTR [rcx], xmm0
	vpsubw	xmm1, xmm7, xmm2
	vpaddw	xmm0, xmm2, xmm7

; 342  : 
; 343  :   __m128i temp0[8];
; 344  :   add_sub_avx2(temp0, (*rows), 0, 1, 0, 1);
; 345  :   add_sub_avx2(temp0, (*rows), 2, 3, 2, 3);
; 346  :   add_sub_avx2(temp0, (*rows), 4, 5, 4, 5);
; 347  :   add_sub_avx2(temp0, (*rows), 6, 7, 6, 7);
; 348  : 
; 349  :   __m128i temp1[8];
; 350  :   add_sub_avx2(temp1, temp0, 0, 1, 0, 2);
; 351  :   add_sub_avx2(temp1, temp0, 2, 3, 1, 3);
; 352  :   add_sub_avx2(temp1, temp0, 4, 5, 4, 6);
; 353  :   add_sub_avx2(temp1, temp0, 6, 7, 5, 7);
; 354  : 
; 355  :   add_sub_avx2((*rows), temp1, 0, 1, 0, 4);
; 356  :   add_sub_avx2((*rows), temp1, 2, 3, 1, 5);
; 357  :   add_sub_avx2((*rows), temp1, 4, 5, 2, 6);
; 358  :   add_sub_avx2((*rows), temp1, 6, 7, 3, 7);
; 359  :   
; 360  : }

	vmovaps	xmm7, XMMWORD PTR [rax-40]

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	xmm3, xmm4, xmm10

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	xmm5, xmm10, xmm4

; 342  : 
; 343  :   __m128i temp0[8];
; 344  :   add_sub_avx2(temp0, (*rows), 0, 1, 0, 1);
; 345  :   add_sub_avx2(temp0, (*rows), 2, 3, 2, 3);
; 346  :   add_sub_avx2(temp0, (*rows), 4, 5, 4, 5);
; 347  :   add_sub_avx2(temp0, (*rows), 6, 7, 6, 7);
; 348  : 
; 349  :   __m128i temp1[8];
; 350  :   add_sub_avx2(temp1, temp0, 0, 1, 0, 2);
; 351  :   add_sub_avx2(temp1, temp0, 2, 3, 1, 3);
; 352  :   add_sub_avx2(temp1, temp0, 4, 5, 4, 6);
; 353  :   add_sub_avx2(temp1, temp0, 6, 7, 5, 7);
; 354  : 
; 355  :   add_sub_avx2((*rows), temp1, 0, 1, 0, 4);
; 356  :   add_sub_avx2((*rows), temp1, 2, 3, 1, 5);
; 357  :   add_sub_avx2((*rows), temp1, 4, 5, 2, 6);
; 358  :   add_sub_avx2((*rows), temp1, 6, 7, 3, 7);
; 359  :   
; 360  : }

	vmovaps	xmm10, XMMWORD PTR [rax-88]

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vmovdqu	XMMWORD PTR [rcx+48], xmm1
	vpsubw	xmm1, xmm8, xmm3

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vmovdqu	XMMWORD PTR [rcx+32], xmm0
	vpaddw	xmm0, xmm3, xmm8

; 342  : 
; 343  :   __m128i temp0[8];
; 344  :   add_sub_avx2(temp0, (*rows), 0, 1, 0, 1);
; 345  :   add_sub_avx2(temp0, (*rows), 2, 3, 2, 3);
; 346  :   add_sub_avx2(temp0, (*rows), 4, 5, 4, 5);
; 347  :   add_sub_avx2(temp0, (*rows), 6, 7, 6, 7);
; 348  : 
; 349  :   __m128i temp1[8];
; 350  :   add_sub_avx2(temp1, temp0, 0, 1, 0, 2);
; 351  :   add_sub_avx2(temp1, temp0, 2, 3, 1, 3);
; 352  :   add_sub_avx2(temp1, temp0, 4, 5, 4, 6);
; 353  :   add_sub_avx2(temp1, temp0, 6, 7, 5, 7);
; 354  : 
; 355  :   add_sub_avx2((*rows), temp1, 0, 1, 0, 4);
; 356  :   add_sub_avx2((*rows), temp1, 2, 3, 1, 5);
; 357  :   add_sub_avx2((*rows), temp1, 4, 5, 2, 6);
; 358  :   add_sub_avx2((*rows), temp1, 6, 7, 3, 7);
; 359  :   
; 360  : }

	vmovaps	xmm8, XMMWORD PTR [rax-56]

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vmovdqu	XMMWORD PTR [rcx+80], xmm1
	vpsubw	xmm1, xmm9, xmm5

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vmovdqu	XMMWORD PTR [rcx+64], xmm0
	vpaddw	xmm0, xmm5, xmm9

; 342  : 
; 343  :   __m128i temp0[8];
; 344  :   add_sub_avx2(temp0, (*rows), 0, 1, 0, 1);
; 345  :   add_sub_avx2(temp0, (*rows), 2, 3, 2, 3);
; 346  :   add_sub_avx2(temp0, (*rows), 4, 5, 4, 5);
; 347  :   add_sub_avx2(temp0, (*rows), 6, 7, 6, 7);
; 348  : 
; 349  :   __m128i temp1[8];
; 350  :   add_sub_avx2(temp1, temp0, 0, 1, 0, 2);
; 351  :   add_sub_avx2(temp1, temp0, 2, 3, 1, 3);
; 352  :   add_sub_avx2(temp1, temp0, 4, 5, 4, 6);
; 353  :   add_sub_avx2(temp1, temp0, 6, 7, 5, 7);
; 354  : 
; 355  :   add_sub_avx2((*rows), temp1, 0, 1, 0, 4);
; 356  :   add_sub_avx2((*rows), temp1, 2, 3, 1, 5);
; 357  :   add_sub_avx2((*rows), temp1, 4, 5, 2, 6);
; 358  :   add_sub_avx2((*rows), temp1, 6, 7, 3, 7);
; 359  :   
; 360  : }

	vmovaps	xmm9, XMMWORD PTR [rax-72]

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vmovdqu	XMMWORD PTR [rcx+112], xmm1
	vmovdqu	XMMWORD PTR [rcx+96], xmm0

; 342  : 
; 343  :   __m128i temp0[8];
; 344  :   add_sub_avx2(temp0, (*rows), 0, 1, 0, 1);
; 345  :   add_sub_avx2(temp0, (*rows), 2, 3, 2, 3);
; 346  :   add_sub_avx2(temp0, (*rows), 4, 5, 4, 5);
; 347  :   add_sub_avx2(temp0, (*rows), 6, 7, 6, 7);
; 348  : 
; 349  :   __m128i temp1[8];
; 350  :   add_sub_avx2(temp1, temp0, 0, 1, 0, 2);
; 351  :   add_sub_avx2(temp1, temp0, 2, 3, 1, 3);
; 352  :   add_sub_avx2(temp1, temp0, 4, 5, 4, 6);
; 353  :   add_sub_avx2(temp1, temp0, 6, 7, 5, 7);
; 354  : 
; 355  :   add_sub_avx2((*rows), temp1, 0, 1, 0, 4);
; 356  :   add_sub_avx2((*rows), temp1, 2, 3, 1, 5);
; 357  :   add_sub_avx2((*rows), temp1, 4, 5, 2, 6);
; 358  :   add_sub_avx2((*rows), temp1, 6, 7, 3, 7);
; 359  :   
; 360  : }

	add	rsp, 104				; 00000068H
	ret	0
ver_transform_block_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
out$ = 8
in$ = 16
out_idx0$ = 24
out_idx1$ = 32
in_idx0$ = 40
in_idx1$ = 48
add_sub_dual_avx2 PROC

; 364  :   out[out_idx0] = _mm256_add_epi16(in[in_idx0], in[in_idx1]);

	mov	r10d, DWORD PTR in_idx1$[rsp]
	mov	eax, DWORD PTR in_idx0$[rsp]
	shl	rax, 5
	shl	r10, 5
	add	r10, rdx
	add	rdx, rax
	mov	eax, r8d
	shl	rax, 5
	vmovdqu	ymm0, YMMWORD PTR [r10]
	vpaddw	ymm1, ymm0, YMMWORD PTR [rdx]
	vmovdqu	YMMWORD PTR [rax+rcx], ymm1

; 365  :   out[out_idx1] = _mm256_sub_epi16(in[in_idx0], in[in_idx1]);

	vmovdqu	ymm0, YMMWORD PTR [rdx]
	vpsubw	ymm2, ymm0, YMMWORD PTR [r10]
	mov	eax, r9d
	shl	rax, 5
	vmovdqu	YMMWORD PTR [rax+rcx], ymm2
	vzeroupper

; 366  : }

	ret	0
add_sub_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
rows$ = 112
ver_transform_block_dual_avx2 PROC

; 369  : static INLINE void ver_transform_block_dual_avx2(__m256i (*rows)[8]){

	mov	rax, rsp
	sub	rsp, 104				; 00000068H

; 364  :   out[out_idx0] = _mm256_add_epi16(in[in_idx0], in[in_idx1]);

	vmovdqu	ymm1, YMMWORD PTR [rcx]
	vmovdqu	ymm0, YMMWORD PTR [rcx+160]
	vmovaps	XMMWORD PTR [rax-24], xmm6
	vmovaps	XMMWORD PTR [rax-40], xmm7
	vpaddw	ymm7, ymm1, YMMWORD PTR [rcx+32]
	vmovaps	XMMWORD PTR [rax-56], xmm8
	vmovaps	XMMWORD PTR [rax-72], xmm9
	vmovaps	XMMWORD PTR [rax-88], xmm10
	vmovaps	XMMWORD PTR [rsp], xmm11

; 365  :   out[out_idx1] = _mm256_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	ymm11, ymm1, YMMWORD PTR [rcx+32]

; 364  :   out[out_idx0] = _mm256_add_epi16(in[in_idx0], in[in_idx1]);

	vmovdqu	ymm1, YMMWORD PTR [rcx+64]
	vpaddw	ymm3, ymm1, YMMWORD PTR [rcx+96]

; 365  :   out[out_idx1] = _mm256_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	ymm9, ymm1, YMMWORD PTR [rcx+96]

; 364  :   out[out_idx0] = _mm256_add_epi16(in[in_idx0], in[in_idx1]);

	vmovdqu	ymm1, YMMWORD PTR [rcx+128]
	vpaddw	ymm5, ymm0, ymm1

; 365  :   out[out_idx1] = _mm256_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	ymm10, ymm1, ymm0

; 364  :   out[out_idx0] = _mm256_add_epi16(in[in_idx0], in[in_idx1]);

	vmovdqu	ymm1, YMMWORD PTR [rcx+192]
	vmovdqu	ymm0, YMMWORD PTR [rcx+224]

; 365  :   out[out_idx1] = _mm256_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	ymm4, ymm1, ymm0
	vpaddw	ymm2, ymm0, ymm1

; 364  :   out[out_idx0] = _mm256_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	ymm1, ymm2, ymm5
	vpaddw	ymm6, ymm3, ymm7
	vpaddw	ymm0, ymm1, ymm6

; 365  :   out[out_idx1] = _mm256_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	ymm1, ymm6, ymm1
	vpsubw	ymm7, ymm7, ymm3
	vpsubw	ymm2, ymm5, ymm2
	vmovdqu	YMMWORD PTR [rcx+32], ymm1
	vmovdqu	YMMWORD PTR [rcx], ymm0

; 364  :   out[out_idx0] = _mm256_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	ymm8, ymm9, ymm11

; 365  :   out[out_idx1] = _mm256_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	ymm1, ymm7, ymm2
	vpaddw	ymm0, ymm2, ymm7

; 364  :   out[out_idx0] = _mm256_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	ymm3, ymm4, ymm10

; 365  :   out[out_idx1] = _mm256_sub_epi16(in[in_idx0], in[in_idx1]);

	vmovdqu	YMMWORD PTR [rcx+96], ymm1
	vmovdqu	YMMWORD PTR [rcx+64], ymm0
	vpsubw	ymm1, ymm8, ymm3
	vpaddw	ymm0, ymm3, ymm8
	vpsubw	ymm9, ymm11, ymm9
	vpsubw	ymm5, ymm10, ymm4
	vmovdqu	YMMWORD PTR [rcx+160], ymm1
	vmovdqu	YMMWORD PTR [rcx+128], ymm0
	vpsubw	ymm1, ymm9, ymm5
	vpaddw	ymm0, ymm5, ymm9
	vmovdqu	YMMWORD PTR [rcx+224], ymm1
	vmovdqu	YMMWORD PTR [rcx+192], ymm0
	vzeroupper

; 370  : 
; 371  :   __m256i temp0[8];
; 372  :   add_sub_dual_avx2(temp0, (*rows), 0, 1, 0, 1);
; 373  :   add_sub_dual_avx2(temp0, (*rows), 2, 3, 2, 3);
; 374  :   add_sub_dual_avx2(temp0, (*rows), 4, 5, 4, 5);
; 375  :   add_sub_dual_avx2(temp0, (*rows), 6, 7, 6, 7);
; 376  : 
; 377  :   __m256i temp1[8];
; 378  :   add_sub_dual_avx2(temp1, temp0, 0, 1, 0, 2);
; 379  :   add_sub_dual_avx2(temp1, temp0, 2, 3, 1, 3);
; 380  :   add_sub_dual_avx2(temp1, temp0, 4, 5, 4, 6);
; 381  :   add_sub_dual_avx2(temp1, temp0, 6, 7, 5, 7);
; 382  : 
; 383  :   add_sub_dual_avx2((*rows), temp1, 0, 1, 0, 4);
; 384  :   add_sub_dual_avx2((*rows), temp1, 2, 3, 1, 5);
; 385  :   add_sub_dual_avx2((*rows), temp1, 4, 5, 2, 6);
; 386  :   add_sub_dual_avx2((*rows), temp1, 6, 7, 3, 7);
; 387  :   
; 388  : }

	vmovaps	xmm6, XMMWORD PTR [rax-24]
	vmovaps	xmm7, XMMWORD PTR [rax-40]
	vmovaps	xmm8, XMMWORD PTR [rax-56]
	vmovaps	xmm9, XMMWORD PTR [rax-72]
	vmovaps	xmm10, XMMWORD PTR [rax-88]
	vmovaps	xmm11, XMMWORD PTR [rsp]
	add	rsp, 104				; 00000068H
	ret	0
ver_transform_block_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
accumulate$ = 8
ver_row$ = 16
haddwd_accumulate_avx2 PROC

; 392  :   __m128i abs_value = _mm_abs_epi16(*ver_row);

	vpabsw	xmm1, XMMWORD PTR [rdx]

; 393  :   *accumulate = _mm_add_epi32(*accumulate, _mm_madd_epi16(abs_value, _mm_set1_epi16(1)));

	vpmaddwd xmm1, xmm1, XMMWORD PTR __xmm@00010001000100010001000100010001
	vpaddd	xmm2, xmm1, XMMWORD PTR [rcx]
	vmovdqu	XMMWORD PTR [rcx], xmm2

; 394  : }

	ret	0
haddwd_accumulate_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
accumulate$ = 8
ver_row$ = 16
haddwd_accumulate_dual_avx2 PROC

; 398  :   __m256i abs_value = _mm256_abs_epi16(*ver_row);

	vpabsw	ymm0, YMMWORD PTR [rdx]

; 399  :   *accumulate = _mm256_add_epi32(*accumulate, _mm256_madd_epi16(abs_value, _mm256_set1_epi16(1)));

	vpmaddwd ymm1, ymm0, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vpaddd	ymm2, ymm1, YMMWORD PTR [rcx]
	vmovdqu	YMMWORD PTR [rcx], ymm2
	vzeroupper

; 400  : }

	ret	0
haddwd_accumulate_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
ver_row$ = 8
sum_block_avx2 PROC

; 393  :   *accumulate = _mm_add_epi32(*accumulate, _mm_madd_epi16(abs_value, _mm_set1_epi16(1)));

	vmovdqu	xmm5, XMMWORD PTR __xmm@00010001000100010001000100010001

; 392  :   __m128i abs_value = _mm_abs_epi16(*ver_row);

	vpabsw	xmm0, XMMWORD PTR [rcx+16]
	vpabsw	xmm1, XMMWORD PTR [rcx]

; 393  :   *accumulate = _mm_add_epi32(*accumulate, _mm_madd_epi16(abs_value, _mm_set1_epi16(1)));

	vpmaddwd xmm3, xmm0, xmm5

; 392  :   __m128i abs_value = _mm_abs_epi16(*ver_row);

	vpabsw	xmm0, XMMWORD PTR [rcx+32]

; 393  :   *accumulate = _mm_add_epi32(*accumulate, _mm_madd_epi16(abs_value, _mm_set1_epi16(1)));

	vpmaddwd xmm2, xmm1, xmm5
	vpaddd	xmm4, xmm3, xmm2

; 392  :   __m128i abs_value = _mm_abs_epi16(*ver_row);

	vpabsw	xmm2, XMMWORD PTR [rcx+48]

; 393  :   *accumulate = _mm_add_epi32(*accumulate, _mm_madd_epi16(abs_value, _mm_set1_epi16(1)));

	vpmaddwd xmm1, xmm0, xmm5
	vpaddd	xmm3, xmm4, xmm1

; 392  :   __m128i abs_value = _mm_abs_epi16(*ver_row);

	vpabsw	xmm1, XMMWORD PTR [rcx+64]

; 393  :   *accumulate = _mm_add_epi32(*accumulate, _mm_madd_epi16(abs_value, _mm_set1_epi16(1)));

	vpmaddwd xmm0, xmm2, xmm5
	vpaddd	xmm4, xmm3, xmm0

; 392  :   __m128i abs_value = _mm_abs_epi16(*ver_row);

	vpabsw	xmm0, XMMWORD PTR [rcx+80]

; 393  :   *accumulate = _mm_add_epi32(*accumulate, _mm_madd_epi16(abs_value, _mm_set1_epi16(1)));

	vpmaddwd xmm2, xmm1, xmm5
	vpaddd	xmm3, xmm4, xmm2

; 392  :   __m128i abs_value = _mm_abs_epi16(*ver_row);

	vpabsw	xmm2, XMMWORD PTR [rcx+96]

; 393  :   *accumulate = _mm_add_epi32(*accumulate, _mm_madd_epi16(abs_value, _mm_set1_epi16(1)));

	vpmaddwd xmm1, xmm0, xmm5
	vpaddd	xmm4, xmm3, xmm1

; 392  :   __m128i abs_value = _mm_abs_epi16(*ver_row);

	vpabsw	xmm1, XMMWORD PTR [rcx+112]

; 393  :   *accumulate = _mm_add_epi32(*accumulate, _mm_madd_epi16(abs_value, _mm_set1_epi16(1)));

	vpmaddwd xmm0, xmm2, xmm5
	vpaddd	xmm3, xmm4, xmm0
	vpmaddwd xmm2, xmm1, xmm5
	vpaddd	xmm4, xmm3, xmm2

; 404  :   __m128i sad = _mm_setzero_si128();
; 405  :   haddwd_accumulate_avx2(&sad, ver_row + 0);
; 406  :   haddwd_accumulate_avx2(&sad, ver_row + 1);
; 407  :   haddwd_accumulate_avx2(&sad, ver_row + 2);
; 408  :   haddwd_accumulate_avx2(&sad, ver_row + 3); 
; 409  :   haddwd_accumulate_avx2(&sad, ver_row + 4);
; 410  :   haddwd_accumulate_avx2(&sad, ver_row + 5);
; 411  :   haddwd_accumulate_avx2(&sad, ver_row + 6);
; 412  :   haddwd_accumulate_avx2(&sad, ver_row + 7);
; 413  : 
; 414  :   sad = _mm_add_epi32(sad, _mm_shuffle_epi32(sad, _MM_SHUFFLE(1, 0, 3, 2)));

	vpshufd	xmm0, xmm4, 78				; 0000004eH
	vpaddd	xmm2, xmm0, xmm4

; 415  :   sad = _mm_add_epi32(sad, _mm_shuffle_epi32(sad, _MM_SHUFFLE(0, 1, 0, 1)));

	vpshufd	xmm1, xmm2, 17
	vpaddd	xmm0, xmm1, xmm2

; 416  : 
; 417  :   return _mm_cvtsi128_si32(sad);

	vmovd	eax, xmm0

; 418  : }

	ret	0
sum_block_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
ver_row$ = 96
sum0$ = 104
sum1$ = 112
sum_block_dual_avx2 PROC

; 421  : {

	mov	rax, rsp
	sub	rsp, 88					; 00000058H

; 398  :   __m256i abs_value = _mm256_abs_epi16(*ver_row);

	vpabsw	ymm0, YMMWORD PTR [rcx+224]
	vpabsw	ymm1, YMMWORD PTR [rcx+192]
	vmovaps	XMMWORD PTR [rax-24], xmm6
	vmovaps	XMMWORD PTR [rax-40], xmm7
	vmovaps	XMMWORD PTR [rax-56], xmm8

; 399  :   *accumulate = _mm256_add_epi32(*accumulate, _mm256_madd_epi16(abs_value, _mm256_set1_epi16(1)));

	vmovdqu	ymm8, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vmovaps	XMMWORD PTR [rax-72], xmm9
	vpmaddwd ymm9, ymm1, ymm8

; 398  :   __m256i abs_value = _mm256_abs_epi16(*ver_row);

	vpabsw	ymm1, YMMWORD PTR [rcx+128]

; 399  :   *accumulate = _mm256_add_epi32(*accumulate, _mm256_madd_epi16(abs_value, _mm256_set1_epi16(1)));

	vpmaddwd ymm6, ymm1, ymm8

; 398  :   __m256i abs_value = _mm256_abs_epi16(*ver_row);

	vpabsw	ymm1, YMMWORD PTR [rcx+64]

; 399  :   *accumulate = _mm256_add_epi32(*accumulate, _mm256_madd_epi16(abs_value, _mm256_set1_epi16(1)));

	vpmaddwd ymm4, ymm1, ymm8

; 398  :   __m256i abs_value = _mm256_abs_epi16(*ver_row);

	vpabsw	ymm1, YMMWORD PTR [rcx]
	vmovaps	XMMWORD PTR [rsp], xmm10

; 399  :   *accumulate = _mm256_add_epi32(*accumulate, _mm256_madd_epi16(abs_value, _mm256_set1_epi16(1)));

	vpmaddwd ymm10, ymm0, ymm8

; 398  :   __m256i abs_value = _mm256_abs_epi16(*ver_row);

	vpabsw	ymm0, YMMWORD PTR [rcx+160]

; 399  :   *accumulate = _mm256_add_epi32(*accumulate, _mm256_madd_epi16(abs_value, _mm256_set1_epi16(1)));

	vpmaddwd ymm7, ymm0, ymm8

; 398  :   __m256i abs_value = _mm256_abs_epi16(*ver_row);

	vpabsw	ymm0, YMMWORD PTR [rcx+96]

; 399  :   *accumulate = _mm256_add_epi32(*accumulate, _mm256_madd_epi16(abs_value, _mm256_set1_epi16(1)));

	vpmaddwd ymm5, ymm0, ymm8

; 398  :   __m256i abs_value = _mm256_abs_epi16(*ver_row);

	vpabsw	ymm0, YMMWORD PTR [rcx+32]

; 399  :   *accumulate = _mm256_add_epi32(*accumulate, _mm256_madd_epi16(abs_value, _mm256_set1_epi16(1)));

	vpmaddwd ymm3, ymm0, ymm8
	vpmaddwd ymm0, ymm1, ymm8
	vpaddd	ymm3, ymm0, ymm3
	vpaddd	ymm1, ymm3, ymm4
	vpaddd	ymm0, ymm1, ymm5
	vpaddd	ymm2, ymm0, ymm6
	vpaddd	ymm3, ymm2, ymm7
	vpaddd	ymm1, ymm3, ymm9
	vpaddd	ymm4, ymm1, ymm10

; 422  :   __m256i sad = _mm256_setzero_si256();
; 423  :   haddwd_accumulate_dual_avx2(&sad, ver_row + 0);
; 424  :   haddwd_accumulate_dual_avx2(&sad, ver_row + 1);
; 425  :   haddwd_accumulate_dual_avx2(&sad, ver_row + 2);
; 426  :   haddwd_accumulate_dual_avx2(&sad, ver_row + 3); 
; 427  :   haddwd_accumulate_dual_avx2(&sad, ver_row + 4);
; 428  :   haddwd_accumulate_dual_avx2(&sad, ver_row + 5);
; 429  :   haddwd_accumulate_dual_avx2(&sad, ver_row + 6);
; 430  :   haddwd_accumulate_dual_avx2(&sad, ver_row + 7);
; 431  : 
; 432  :   sad = _mm256_add_epi32(sad, _mm256_shuffle_epi32(sad, _MM_SHUFFLE(1, 0, 3, 2)));

	vpshufd	ymm0, ymm4, 78				; 0000004eH
	vpaddd	ymm2, ymm0, ymm4

; 433  :   sad = _mm256_add_epi32(sad, _mm256_shuffle_epi32(sad, _MM_SHUFFLE(0, 1, 0, 1)));

	vpshufd	ymm1, ymm2, 17
	vpaddd	ymm0, ymm1, ymm2

; 434  : 
; 435  :   *sum0 = _mm_cvtsi128_si32(_mm256_extracti128_si256(sad, 0));

	vmovd	DWORD PTR [rdx], xmm0

; 436  :   *sum1 = _mm_cvtsi128_si32(_mm256_extracti128_si256(sad, 1));

	vextracti128 xmm0, ymm0, 1
	vmovd	DWORD PTR [r8], xmm0
	vzeroupper

; 437  : }

	vmovaps	xmm6, XMMWORD PTR [rax-24]
	vmovaps	xmm7, XMMWORD PTR [rax-40]
	vmovaps	xmm8, XMMWORD PTR [rax-56]
	vmovaps	xmm9, XMMWORD PTR [rax-72]
	vmovaps	xmm10, XMMWORD PTR [rsp]
	add	rsp, 88					; 00000058H
	ret	0
sum_block_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
buf1$ = 8
buf2$ = 16
diff_row_avx2 PROC

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vmovq	xmm0, QWORD PTR [rcx]

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	vmovq	xmm1, QWORD PTR [rdx]
	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1

; 443  :   return _mm_sub_epi16(buf1_row, buf2_row);

	vpsubw	xmm0, xmm2, xmm0

; 444  : }

	ret	0
diff_row_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
buf1$ = 8
buf2$ = 16
orig$ = 24
diff_row_dual_avx2 PROC

; 448  :   __m128i temp1 = _mm_loadl_epi64((__m128i*)buf1);
; 449  :   __m128i temp2 = _mm_loadl_epi64((__m128i*)buf2);

	vmovq	xmm0, QWORD PTR [rdx]
	vmovq	xmm1, QWORD PTR [rcx]

; 450  :   __m128i temp3 = _mm_loadl_epi64((__m128i*)orig);
; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpunpcklqdq xmm1, xmm1, xmm0
	vpmovzxbw ymm3, xmm1

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpbroadcastq xmm1, QWORD PTR [r8]
	vpmovzxbw ymm2, xmm1

; 453  : 
; 454  :   return _mm256_sub_epi16(buf1_row, buf2_row);

	vpsubw	ymm0, ymm3, ymm2

; 455  : }

	ret	0
diff_row_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
row_diff$ = 8
buf1$ = 16
stride1$ = 24
orig$ = 32
stride_orig$ = 40
diff_blocks_avx2 PROC

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vmovq	xmm0, QWORD PTR [rdx]

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	vmovq	xmm1, QWORD PTR [r9]
	vpmovzxbw xmm2, xmm0

; 462  :   (*row_diff)[1] = diff_row_avx2(buf1 + 1 * stride1, orig + 1 * stride_orig);

	mov	eax, r8d
	mov	r10, rdx

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	vpmovzxbw xmm0, xmm1

; 443  :   return _mm_sub_epi16(buf1_row, buf2_row);

	vpsubw	xmm2, xmm2, xmm0

; 461  :   (*row_diff)[0] = diff_row_avx2(buf1 + 0 * stride1, orig + 0 * stride_orig);

	vmovdqu	XMMWORD PTR [rcx], xmm2

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vmovq	xmm0, QWORD PTR [rax+rdx]

; 462  :   (*row_diff)[1] = diff_row_avx2(buf1 + 1 * stride1, orig + 1 * stride_orig);

	mov	edx, DWORD PTR stride_orig$[rsp]

; 463  :   (*row_diff)[2] = diff_row_avx2(buf1 + 2 * stride1, orig + 2 * stride_orig);

	lea	eax, DWORD PTR [r8+r8]

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vpmovzxbw xmm2, xmm0

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	vmovq	xmm0, QWORD PTR [rdx+r9]
	vpmovzxbw xmm1, xmm0

; 443  :   return _mm_sub_epi16(buf1_row, buf2_row);

	vpsubw	xmm2, xmm2, xmm1

; 462  :   (*row_diff)[1] = diff_row_avx2(buf1 + 1 * stride1, orig + 1 * stride_orig);

	vmovdqu	XMMWORD PTR [rcx+16], xmm2

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vmovq	xmm0, QWORD PTR [rax+r10]

; 463  :   (*row_diff)[2] = diff_row_avx2(buf1 + 2 * stride1, orig + 2 * stride_orig);

	lea	eax, DWORD PTR [rdx+rdx]

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vpmovzxbw xmm2, xmm0

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	vmovq	xmm0, QWORD PTR [rax+r9]
	vpmovzxbw xmm1, xmm0

; 443  :   return _mm_sub_epi16(buf1_row, buf2_row);

	vpsubw	xmm2, xmm2, xmm1

; 463  :   (*row_diff)[2] = diff_row_avx2(buf1 + 2 * stride1, orig + 2 * stride_orig);

	vmovdqu	XMMWORD PTR [rcx+32], xmm2

; 464  :   (*row_diff)[3] = diff_row_avx2(buf1 + 3 * stride1, orig + 3 * stride_orig);

	lea	eax, DWORD PTR [r8+r8*2]

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vmovq	xmm0, QWORD PTR [rax+r10]

; 464  :   (*row_diff)[3] = diff_row_avx2(buf1 + 3 * stride1, orig + 3 * stride_orig);

	lea	eax, DWORD PTR [rdx+rdx*2]

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vpmovzxbw xmm2, xmm0

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	vmovq	xmm0, QWORD PTR [rax+r9]
	vpmovzxbw xmm1, xmm0

; 443  :   return _mm_sub_epi16(buf1_row, buf2_row);

	vpsubw	xmm2, xmm2, xmm1

; 464  :   (*row_diff)[3] = diff_row_avx2(buf1 + 3 * stride1, orig + 3 * stride_orig);

	vmovdqu	XMMWORD PTR [rcx+48], xmm2

; 465  :   (*row_diff)[4] = diff_row_avx2(buf1 + 4 * stride1, orig + 4 * stride_orig);

	lea	eax, DWORD PTR [r8*4]

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vmovq	xmm0, QWORD PTR [rax+r10]

; 465  :   (*row_diff)[4] = diff_row_avx2(buf1 + 4 * stride1, orig + 4 * stride_orig);

	lea	eax, DWORD PTR [rdx*4]

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vpmovzxbw xmm2, xmm0

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	vmovq	xmm0, QWORD PTR [rax+r9]
	vpmovzxbw xmm1, xmm0

; 443  :   return _mm_sub_epi16(buf1_row, buf2_row);

	vpsubw	xmm2, xmm2, xmm1

; 465  :   (*row_diff)[4] = diff_row_avx2(buf1 + 4 * stride1, orig + 4 * stride_orig);

	vmovdqu	XMMWORD PTR [rcx+64], xmm2

; 466  :   (*row_diff)[5] = diff_row_avx2(buf1 + 5 * stride1, orig + 5 * stride_orig);

	lea	eax, DWORD PTR [r8+r8*4]

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vmovq	xmm0, QWORD PTR [rax+r10]

; 466  :   (*row_diff)[5] = diff_row_avx2(buf1 + 5 * stride1, orig + 5 * stride_orig);

	lea	eax, DWORD PTR [rdx+rdx*4]

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vpmovzxbw xmm2, xmm0

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	vmovq	xmm0, QWORD PTR [rax+r9]
	vpmovzxbw xmm1, xmm0

; 443  :   return _mm_sub_epi16(buf1_row, buf2_row);

	vpsubw	xmm2, xmm2, xmm1

; 466  :   (*row_diff)[5] = diff_row_avx2(buf1 + 5 * stride1, orig + 5 * stride_orig);

	vmovdqu	XMMWORD PTR [rcx+80], xmm2

; 467  :   (*row_diff)[6] = diff_row_avx2(buf1 + 6 * stride1, orig + 6 * stride_orig);

	lea	eax, DWORD PTR [r8+r8*2]
	add	eax, eax

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vmovq	xmm0, QWORD PTR [rax+r10]

; 467  :   (*row_diff)[6] = diff_row_avx2(buf1 + 6 * stride1, orig + 6 * stride_orig);

	lea	eax, DWORD PTR [rdx+rdx*2]
	add	eax, eax

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vpmovzxbw xmm2, xmm0

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	vmovq	xmm0, QWORD PTR [rax+r9]
	vpmovzxbw xmm1, xmm0

; 443  :   return _mm_sub_epi16(buf1_row, buf2_row);

	vpsubw	xmm2, xmm2, xmm1

; 467  :   (*row_diff)[6] = diff_row_avx2(buf1 + 6 * stride1, orig + 6 * stride_orig);

	vmovdqu	XMMWORD PTR [rcx+96], xmm2

; 468  :   (*row_diff)[7] = diff_row_avx2(buf1 + 7 * stride1, orig + 7 * stride_orig);

	imul	eax, r8d, 7

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vmovq	xmm0, QWORD PTR [rax+r10]
	vpmovzxbw xmm2, xmm0

; 468  :   (*row_diff)[7] = diff_row_avx2(buf1 + 7 * stride1, orig + 7 * stride_orig);

	imul	eax, edx, 7

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	vmovq	xmm0, QWORD PTR [rax+r9]
	vpmovzxbw xmm1, xmm0

; 443  :   return _mm_sub_epi16(buf1_row, buf2_row);

	vpsubw	xmm2, xmm2, xmm1

; 468  :   (*row_diff)[7] = diff_row_avx2(buf1 + 7 * stride1, orig + 7 * stride_orig);

	vmovdqu	XMMWORD PTR [rcx+112], xmm2

; 469  : 
; 470  : }

	ret	0
diff_blocks_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
row_diff$ = 8
buf1$ = 16
stride1$ = 24
buf2$ = 32
stride2$ = 40
orig$ = 48
stride_orig$ = 56
diff_blocks_dual_avx2 PROC

; 476  : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rdi

; 448  :   __m128i temp1 = _mm_loadl_epi64((__m128i*)buf1);

	vmovq	xmm1, QWORD PTR [rdx]

; 449  :   __m128i temp2 = _mm_loadl_epi64((__m128i*)buf2);

	vmovq	xmm0, QWORD PTR [r9]

; 450  :   __m128i temp3 = _mm_loadl_epi64((__m128i*)orig);

	mov	rbx, QWORD PTR orig$[rsp]

; 476  : {

	mov	r11, rdx

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpunpcklqdq xmm1, xmm1, xmm0
	vpmovzxbw ymm3, xmm1

; 476  : {

	mov	r10d, r8d

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpbroadcastq xmm1, QWORD PTR [rbx]

; 478  :   (*row_diff)[1] = diff_row_dual_avx2(buf1 + 1 * stride1, buf2 + 1 * stride2, orig + 1 * stride_orig);

	mov	r8d, DWORD PTR stride_orig$[rsp]

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpmovzxbw ymm2, xmm1

; 453  : 
; 454  :   return _mm256_sub_epi16(buf1_row, buf2_row);

	vpsubw	ymm3, ymm3, ymm2

; 477  :   (*row_diff)[0] = diff_row_dual_avx2(buf1 + 0 * stride1, buf2 + 0 * stride2, orig + 0 * stride_orig);

	vmovdqu	YMMWORD PTR [rcx], ymm3

; 448  :   __m128i temp1 = _mm_loadl_epi64((__m128i*)buf1);

	vmovq	xmm1, QWORD PTR [r10+rdx]

; 478  :   (*row_diff)[1] = diff_row_dual_avx2(buf1 + 1 * stride1, buf2 + 1 * stride2, orig + 1 * stride_orig);

	mov	edx, DWORD PTR stride2$[rsp]

; 479  :   (*row_diff)[2] = diff_row_dual_avx2(buf1 + 2 * stride1, buf2 + 2 * stride2, orig + 2 * stride_orig);

	lea	eax, DWORD PTR [r10+r10]

; 449  :   __m128i temp2 = _mm_loadl_epi64((__m128i*)buf2);

	vmovq	xmm0, QWORD PTR [rdx+r9]

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpunpcklqdq xmm1, xmm1, xmm0
	vpmovzxbw ymm3, xmm1

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpbroadcastq xmm1, QWORD PTR [r8+rbx]
	vpmovzxbw ymm2, xmm1

; 453  : 
; 454  :   return _mm256_sub_epi16(buf1_row, buf2_row);

	vpsubw	ymm3, ymm3, ymm2

; 478  :   (*row_diff)[1] = diff_row_dual_avx2(buf1 + 1 * stride1, buf2 + 1 * stride2, orig + 1 * stride_orig);

	vmovdqu	YMMWORD PTR [rcx+32], ymm3

; 448  :   __m128i temp1 = _mm_loadl_epi64((__m128i*)buf1);

	vmovq	xmm1, QWORD PTR [rax+r11]

; 479  :   (*row_diff)[2] = diff_row_dual_avx2(buf1 + 2 * stride1, buf2 + 2 * stride2, orig + 2 * stride_orig);

	lea	eax, DWORD PTR [rdx+rdx]

; 449  :   __m128i temp2 = _mm_loadl_epi64((__m128i*)buf2);

	vmovq	xmm0, QWORD PTR [rax+r9]

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpunpcklqdq xmm1, xmm1, xmm0

; 479  :   (*row_diff)[2] = diff_row_dual_avx2(buf1 + 2 * stride1, buf2 + 2 * stride2, orig + 2 * stride_orig);

	lea	eax, DWORD PTR [r8+r8]

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpmovzxbw ymm3, xmm1

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpbroadcastq xmm1, QWORD PTR [rax+rbx]
	vpmovzxbw ymm2, xmm1

; 480  :   (*row_diff)[3] = diff_row_dual_avx2(buf1 + 3 * stride1, buf2 + 3 * stride2, orig + 3 * stride_orig);

	lea	eax, DWORD PTR [r10+r10*2]

; 454  :   return _mm256_sub_epi16(buf1_row, buf2_row);

	vpsubw	ymm3, ymm3, ymm2

; 479  :   (*row_diff)[2] = diff_row_dual_avx2(buf1 + 2 * stride1, buf2 + 2 * stride2, orig + 2 * stride_orig);

	vmovdqu	YMMWORD PTR [rcx+64], ymm3

; 448  :   __m128i temp1 = _mm_loadl_epi64((__m128i*)buf1);

	vmovq	xmm1, QWORD PTR [rax+r11]

; 480  :   (*row_diff)[3] = diff_row_dual_avx2(buf1 + 3 * stride1, buf2 + 3 * stride2, orig + 3 * stride_orig);

	lea	eax, DWORD PTR [rdx+rdx*2]

; 449  :   __m128i temp2 = _mm_loadl_epi64((__m128i*)buf2);

	vmovq	xmm0, QWORD PTR [rax+r9]

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpunpcklqdq xmm1, xmm1, xmm0

; 480  :   (*row_diff)[3] = diff_row_dual_avx2(buf1 + 3 * stride1, buf2 + 3 * stride2, orig + 3 * stride_orig);

	lea	eax, DWORD PTR [r8+r8*2]

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpmovzxbw ymm3, xmm1

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpbroadcastq xmm1, QWORD PTR [rax+rbx]
	vpmovzxbw ymm2, xmm1

; 481  :   (*row_diff)[4] = diff_row_dual_avx2(buf1 + 4 * stride1, buf2 + 4 * stride2, orig + 4 * stride_orig);

	lea	eax, DWORD PTR [r10*4]

; 454  :   return _mm256_sub_epi16(buf1_row, buf2_row);

	vpsubw	ymm3, ymm3, ymm2

; 480  :   (*row_diff)[3] = diff_row_dual_avx2(buf1 + 3 * stride1, buf2 + 3 * stride2, orig + 3 * stride_orig);

	vmovdqu	YMMWORD PTR [rcx+96], ymm3

; 448  :   __m128i temp1 = _mm_loadl_epi64((__m128i*)buf1);

	vmovq	xmm1, QWORD PTR [rax+r11]

; 481  :   (*row_diff)[4] = diff_row_dual_avx2(buf1 + 4 * stride1, buf2 + 4 * stride2, orig + 4 * stride_orig);

	lea	eax, DWORD PTR [rdx*4]

; 449  :   __m128i temp2 = _mm_loadl_epi64((__m128i*)buf2);

	vmovq	xmm0, QWORD PTR [rax+r9]

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpunpcklqdq xmm1, xmm1, xmm0

; 481  :   (*row_diff)[4] = diff_row_dual_avx2(buf1 + 4 * stride1, buf2 + 4 * stride2, orig + 4 * stride_orig);

	lea	eax, DWORD PTR [r8*4]

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpmovzxbw ymm3, xmm1

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpbroadcastq xmm1, QWORD PTR [rax+rbx]
	vpmovzxbw ymm2, xmm1

; 482  :   (*row_diff)[5] = diff_row_dual_avx2(buf1 + 5 * stride1, buf2 + 5 * stride2, orig + 5 * stride_orig);

	lea	eax, DWORD PTR [r10+r10*4]

; 454  :   return _mm256_sub_epi16(buf1_row, buf2_row);

	vpsubw	ymm3, ymm3, ymm2

; 481  :   (*row_diff)[4] = diff_row_dual_avx2(buf1 + 4 * stride1, buf2 + 4 * stride2, orig + 4 * stride_orig);

	vmovdqu	YMMWORD PTR [rcx+128], ymm3

; 448  :   __m128i temp1 = _mm_loadl_epi64((__m128i*)buf1);

	vmovq	xmm1, QWORD PTR [rax+r11]

; 482  :   (*row_diff)[5] = diff_row_dual_avx2(buf1 + 5 * stride1, buf2 + 5 * stride2, orig + 5 * stride_orig);

	lea	eax, DWORD PTR [rdx+rdx*4]

; 449  :   __m128i temp2 = _mm_loadl_epi64((__m128i*)buf2);

	vmovq	xmm0, QWORD PTR [rax+r9]

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpunpcklqdq xmm1, xmm1, xmm0

; 482  :   (*row_diff)[5] = diff_row_dual_avx2(buf1 + 5 * stride1, buf2 + 5 * stride2, orig + 5 * stride_orig);

	lea	eax, DWORD PTR [r8+r8*4]

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpmovzxbw ymm3, xmm1

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpbroadcastq xmm1, QWORD PTR [rax+rbx]
	vpmovzxbw ymm2, xmm1

; 483  :   (*row_diff)[6] = diff_row_dual_avx2(buf1 + 6 * stride1, buf2 + 6 * stride2, orig + 6 * stride_orig);

	lea	eax, DWORD PTR [r10+r10*2]
	add	eax, eax

; 454  :   return _mm256_sub_epi16(buf1_row, buf2_row);

	vpsubw	ymm3, ymm3, ymm2

; 482  :   (*row_diff)[5] = diff_row_dual_avx2(buf1 + 5 * stride1, buf2 + 5 * stride2, orig + 5 * stride_orig);

	vmovdqu	YMMWORD PTR [rcx+160], ymm3

; 448  :   __m128i temp1 = _mm_loadl_epi64((__m128i*)buf1);

	vmovq	xmm1, QWORD PTR [rax+r11]

; 483  :   (*row_diff)[6] = diff_row_dual_avx2(buf1 + 6 * stride1, buf2 + 6 * stride2, orig + 6 * stride_orig);

	lea	eax, DWORD PTR [rdx+rdx*2]
	add	eax, eax

; 449  :   __m128i temp2 = _mm_loadl_epi64((__m128i*)buf2);

	vmovq	xmm0, QWORD PTR [rax+r9]

; 483  :   (*row_diff)[6] = diff_row_dual_avx2(buf1 + 6 * stride1, buf2 + 6 * stride2, orig + 6 * stride_orig);

	lea	eax, DWORD PTR [r8+r8*2]
	add	eax, eax

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpunpcklqdq xmm1, xmm1, xmm0
	vpmovzxbw ymm3, xmm1

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpbroadcastq xmm1, QWORD PTR [rax+rbx]
	vpmovzxbw ymm2, xmm1

; 484  :   (*row_diff)[7] = diff_row_dual_avx2(buf1 + 7 * stride1, buf2 + 7 * stride2, orig + 7 * stride_orig);

	imul	eax, r10d, 7

; 454  :   return _mm256_sub_epi16(buf1_row, buf2_row);

	vpsubw	ymm3, ymm3, ymm2

; 483  :   (*row_diff)[6] = diff_row_dual_avx2(buf1 + 6 * stride1, buf2 + 6 * stride2, orig + 6 * stride_orig);

	vmovdqu	YMMWORD PTR [rcx+192], ymm3

; 448  :   __m128i temp1 = _mm_loadl_epi64((__m128i*)buf1);

	vmovq	xmm1, QWORD PTR [rax+r11]

; 484  :   (*row_diff)[7] = diff_row_dual_avx2(buf1 + 7 * stride1, buf2 + 7 * stride2, orig + 7 * stride_orig);

	imul	eax, edx, 7

; 449  :   __m128i temp2 = _mm_loadl_epi64((__m128i*)buf2);

	vmovq	xmm0, QWORD PTR [rax+r9]

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpunpcklqdq xmm1, xmm1, xmm0
	vpmovzxbw ymm3, xmm1

; 484  :   (*row_diff)[7] = diff_row_dual_avx2(buf1 + 7 * stride1, buf2 + 7 * stride2, orig + 7 * stride_orig);

	imul	eax, r8d, 7

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpbroadcastq xmm1, QWORD PTR [rax+rbx]
	vpmovzxbw ymm2, xmm1

; 453  : 
; 454  :   return _mm256_sub_epi16(buf1_row, buf2_row);

	vpsubw	ymm3, ymm3, ymm2

; 484  :   (*row_diff)[7] = diff_row_dual_avx2(buf1 + 7 * stride1, buf2 + 7 * stride2, orig + 7 * stride_orig);

	vmovdqu	YMMWORD PTR [rcx+224], ymm3
	vzeroupper

; 485  : 
; 486  : }

	mov	rbx, QWORD PTR [rsp+8]
	mov	rdi, QWORD PTR [rsp+16]
	ret	0
diff_blocks_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
row_diff$ = 48
hor_transform_block_avx2 PROC

; 489  : {

	sub	rsp, 40					; 00000028H

; 298  :   __m128i temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vmovdqu	xmm0, XMMWORD PTR [rcx]
	vmovdqu	xmm4, XMMWORD PTR __xmm@ffffffffffffffffffffffffffffffff
	vmovdqu	xmm5, XMMWORD PTR __xmm@00010001000100010001000100010001
	vmovaps	XMMWORD PTR [rsp+16], xmm6

; 301  : 
; 302  :   sign_mask = _mm_unpacklo_epi32(mask_pos, mask_neg);

	vpunpckldq xmm6, xmm5, xmm4
	vmovaps	XMMWORD PTR [rsp], xmm7

; 297  :   __m128i sign_mask = _mm_unpacklo_epi64(mask_pos, mask_neg);

	vpunpcklqdq xmm7, xmm5, xmm4

; 299  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm1, xmm0, xmm7
	vpshufd	xmm0, xmm0, 78				; 0000004eH

; 300  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm2, xmm1, xmm0

; 304  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm1, xmm2, xmm6
	vpshufd	xmm0, xmm2, 177				; 000000b1H

; 305  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm3, xmm1, xmm0

; 306  : 
; 307  :   sign_mask = _mm_unpacklo_epi16(mask_pos, mask_neg);

	vpunpcklwd xmm5, xmm5, xmm4

; 308  :   temp = _mm_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw xmm0, xmm3, 177			; 000000b1H

; 309  :   temp = _mm_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw xmm2, xmm0, 177			; 000000b1H

; 310  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm1, xmm3, xmm5

; 311  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm2, xmm2, xmm1
	vmovdqu	XMMWORD PTR [rcx], xmm2

; 298  :   __m128i temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vmovdqu	xmm0, XMMWORD PTR [rcx+16]

; 299  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm1, xmm0, xmm7
	vpshufd	xmm0, xmm0, 78				; 0000004eH

; 300  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm3, xmm1, xmm0

; 304  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm2, xmm3, xmm6
	vpshufd	xmm0, xmm3, 177				; 000000b1H

; 305  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm4, xmm2, xmm0

; 308  :   temp = _mm_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw xmm1, xmm4, 177			; 000000b1H

; 309  :   temp = _mm_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw xmm2, xmm1, 177			; 000000b1H

; 310  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm0, xmm4, xmm5

; 311  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm2, xmm2, xmm0
	vmovdqu	XMMWORD PTR [rcx+16], xmm2

; 298  :   __m128i temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vmovdqu	xmm0, XMMWORD PTR [rcx+32]

; 299  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm1, xmm0, xmm7
	vpshufd	xmm0, xmm0, 78				; 0000004eH

; 300  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm3, xmm1, xmm0

; 304  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm2, xmm3, xmm6
	vpshufd	xmm0, xmm3, 177				; 000000b1H

; 305  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm4, xmm2, xmm0

; 308  :   temp = _mm_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw xmm1, xmm4, 177			; 000000b1H

; 309  :   temp = _mm_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw xmm2, xmm1, 177			; 000000b1H

; 310  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm0, xmm4, xmm5

; 311  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm2, xmm2, xmm0
	vmovdqu	XMMWORD PTR [rcx+32], xmm2

; 298  :   __m128i temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vmovdqu	xmm0, XMMWORD PTR [rcx+48]

; 299  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm1, xmm0, xmm7
	vpshufd	xmm0, xmm0, 78				; 0000004eH

; 300  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm3, xmm1, xmm0

; 304  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm2, xmm3, xmm6
	vpshufd	xmm0, xmm3, 177				; 000000b1H

; 305  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm4, xmm2, xmm0

; 308  :   temp = _mm_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw xmm1, xmm4, 177			; 000000b1H

; 309  :   temp = _mm_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw xmm2, xmm1, 177			; 000000b1H

; 310  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm0, xmm4, xmm5

; 311  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm2, xmm2, xmm0
	vmovdqu	XMMWORD PTR [rcx+48], xmm2

; 298  :   __m128i temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vmovdqu	xmm0, XMMWORD PTR [rcx+64]

; 299  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm1, xmm0, xmm7
	vpshufd	xmm0, xmm0, 78				; 0000004eH

; 300  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm3, xmm1, xmm0

; 304  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm2, xmm3, xmm6
	vpshufd	xmm0, xmm3, 177				; 000000b1H

; 305  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm4, xmm2, xmm0

; 308  :   temp = _mm_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw xmm1, xmm4, 177			; 000000b1H

; 309  :   temp = _mm_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw xmm2, xmm1, 177			; 000000b1H

; 310  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm0, xmm4, xmm5

; 311  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm2, xmm2, xmm0
	vmovdqu	XMMWORD PTR [rcx+64], xmm2

; 298  :   __m128i temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vmovdqu	xmm0, XMMWORD PTR [rcx+80]

; 299  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm1, xmm0, xmm7
	vpshufd	xmm0, xmm0, 78				; 0000004eH

; 300  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm3, xmm1, xmm0

; 304  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm2, xmm3, xmm6
	vpshufd	xmm0, xmm3, 177				; 000000b1H

; 305  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm4, xmm2, xmm0

; 308  :   temp = _mm_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw xmm1, xmm4, 177			; 000000b1H

; 309  :   temp = _mm_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw xmm2, xmm1, 177			; 000000b1H

; 310  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm0, xmm4, xmm5

; 311  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm2, xmm2, xmm0
	vmovdqu	XMMWORD PTR [rcx+80], xmm2

; 298  :   __m128i temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vmovdqu	xmm0, XMMWORD PTR [rcx+96]

; 299  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm1, xmm0, xmm7

; 300  :   *row = _mm_add_epi16(*row, temp);

	vpshufd	xmm0, xmm0, 78				; 0000004eH
	vpaddw	xmm3, xmm1, xmm0

; 304  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm2, xmm3, xmm6
	vpshufd	xmm0, xmm3, 177				; 000000b1H

; 305  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm4, xmm2, xmm0

; 308  :   temp = _mm_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw xmm1, xmm4, 177			; 000000b1H

; 309  :   temp = _mm_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw xmm2, xmm1, 177			; 000000b1H

; 310  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm0, xmm4, xmm5

; 311  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm2, xmm2, xmm0
	vmovdqu	XMMWORD PTR [rcx+96], xmm2

; 298  :   __m128i temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vmovdqu	xmm0, XMMWORD PTR [rcx+112]

; 299  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm1, xmm0, xmm7

; 490  :   hor_transform_row_avx2((*row_diff) + 0);
; 491  :   hor_transform_row_avx2((*row_diff) + 1);
; 492  :   hor_transform_row_avx2((*row_diff) + 2);
; 493  :   hor_transform_row_avx2((*row_diff) + 3);
; 494  :   hor_transform_row_avx2((*row_diff) + 4);
; 495  :   hor_transform_row_avx2((*row_diff) + 5);
; 496  :   hor_transform_row_avx2((*row_diff) + 6);
; 497  :   hor_transform_row_avx2((*row_diff) + 7);
; 498  : }

	vmovaps	xmm7, XMMWORD PTR [rsp]

; 298  :   __m128i temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm0, 78				; 0000004eH

; 300  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm3, xmm1, xmm0

; 304  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm2, xmm3, xmm6

; 490  :   hor_transform_row_avx2((*row_diff) + 0);
; 491  :   hor_transform_row_avx2((*row_diff) + 1);
; 492  :   hor_transform_row_avx2((*row_diff) + 2);
; 493  :   hor_transform_row_avx2((*row_diff) + 3);
; 494  :   hor_transform_row_avx2((*row_diff) + 4);
; 495  :   hor_transform_row_avx2((*row_diff) + 5);
; 496  :   hor_transform_row_avx2((*row_diff) + 6);
; 497  :   hor_transform_row_avx2((*row_diff) + 7);
; 498  : }

	vmovaps	xmm6, XMMWORD PTR [rsp+16]

; 303  :   temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	xmm0, xmm3, 177				; 000000b1H

; 305  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm4, xmm2, xmm0

; 308  :   temp = _mm_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw xmm1, xmm4, 177			; 000000b1H

; 309  :   temp = _mm_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw xmm2, xmm1, 177			; 000000b1H

; 310  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm0, xmm4, xmm5

; 311  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm2, xmm2, xmm0
	vmovdqu	XMMWORD PTR [rcx+112], xmm2

; 490  :   hor_transform_row_avx2((*row_diff) + 0);
; 491  :   hor_transform_row_avx2((*row_diff) + 1);
; 492  :   hor_transform_row_avx2((*row_diff) + 2);
; 493  :   hor_transform_row_avx2((*row_diff) + 3);
; 494  :   hor_transform_row_avx2((*row_diff) + 4);
; 495  :   hor_transform_row_avx2((*row_diff) + 5);
; 496  :   hor_transform_row_avx2((*row_diff) + 6);
; 497  :   hor_transform_row_avx2((*row_diff) + 7);
; 498  : }

	add	rsp, 40					; 00000028H
	ret	0
hor_transform_block_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
row_diff$ = 48
hor_transform_block_dual_avx2 PROC

; 501  : {

	sub	rsp, 40					; 00000028H

; 319  :   __m256i temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vmovdqu	ymm0, YMMWORD PTR [rcx]
	vmovdqu	ymm5, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vmovdqu	ymm4, YMMWORD PTR __ymm@ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
	vpshufd	ymm1, ymm0, 78				; 0000004eH
	vmovaps	XMMWORD PTR [rsp+16], xmm6

; 322  : 
; 323  :   sign_mask = _mm256_unpacklo_epi32(mask_pos, mask_neg);

	vpunpckldq ymm6, ymm5, ymm4
	vmovaps	XMMWORD PTR [rsp], xmm7

; 318  :   __m256i sign_mask = _mm256_unpacklo_epi64(mask_pos, mask_neg);

	vpunpcklqdq ymm7, ymm5, ymm4

; 320  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm0, ymm7

; 321  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm2, ymm0, ymm1

; 324  :   temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm1, ymm2, 177				; 000000b1H

; 325  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm2, ymm6

; 326  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm3, ymm0, ymm1

; 327  : 
; 328  :   sign_mask = _mm256_unpacklo_epi16(mask_pos, mask_neg);
; 329  :   temp = _mm256_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw ymm0, ymm3, 177			; 000000b1H

; 330  :   temp = _mm256_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw ymm1, ymm0, 177			; 000000b1H
	vpunpcklwd ymm4, ymm5, ymm4

; 331  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm2, ymm3, ymm4

; 332  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm2, ymm1, ymm2
	vmovdqu	YMMWORD PTR [rcx], ymm2

; 319  :   __m256i temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vmovdqu	ymm0, YMMWORD PTR [rcx+32]
	vpshufd	ymm1, ymm0, 78				; 0000004eH

; 320  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm0, ymm7

; 321  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm3, ymm0, ymm1

; 324  :   temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm2, ymm3, 177				; 000000b1H

; 325  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm3, ymm6

; 326  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm1, ymm0, ymm2

; 331  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm3, ymm1, ymm4
	vpshuflw ymm0, ymm1, 177			; 000000b1H
	vpshufhw ymm1, ymm0, 177			; 000000b1H

; 332  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm2, ymm1, ymm3
	vmovdqu	YMMWORD PTR [rcx+32], ymm2

; 319  :   __m256i temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vmovdqu	ymm0, YMMWORD PTR [rcx+64]
	vpshufd	ymm1, ymm0, 78				; 0000004eH

; 320  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm0, ymm7

; 321  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm3, ymm0, ymm1

; 324  :   temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm2, ymm3, 177				; 000000b1H

; 325  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm3, ymm6

; 326  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm1, ymm0, ymm2

; 331  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm3, ymm1, ymm4
	vpshuflw ymm0, ymm1, 177			; 000000b1H
	vpshufhw ymm1, ymm0, 177			; 000000b1H

; 332  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm2, ymm1, ymm3
	vmovdqu	YMMWORD PTR [rcx+64], ymm2

; 319  :   __m256i temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vmovdqu	ymm0, YMMWORD PTR [rcx+96]
	vpshufd	ymm1, ymm0, 78				; 0000004eH

; 320  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm0, ymm7

; 321  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm3, ymm0, ymm1

; 324  :   temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm2, ymm3, 177				; 000000b1H

; 325  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm3, ymm6

; 326  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm1, ymm0, ymm2

; 331  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm3, ymm1, ymm4
	vpshuflw ymm0, ymm1, 177			; 000000b1H
	vpshufhw ymm1, ymm0, 177			; 000000b1H

; 332  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm2, ymm1, ymm3
	vmovdqu	YMMWORD PTR [rcx+96], ymm2

; 319  :   __m256i temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vmovdqu	ymm0, YMMWORD PTR [rcx+128]
	vpshufd	ymm1, ymm0, 78				; 0000004eH

; 320  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm0, ymm7

; 321  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm3, ymm0, ymm1

; 324  :   temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm2, ymm3, 177				; 000000b1H

; 325  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm3, ymm6

; 326  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm1, ymm0, ymm2

; 331  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm3, ymm1, ymm4
	vpshuflw ymm0, ymm1, 177			; 000000b1H
	vpshufhw ymm1, ymm0, 177			; 000000b1H

; 332  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm2, ymm1, ymm3
	vmovdqu	YMMWORD PTR [rcx+128], ymm2

; 319  :   __m256i temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vmovdqu	ymm0, YMMWORD PTR [rcx+160]
	vpshufd	ymm1, ymm0, 78				; 0000004eH

; 320  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm0, ymm7

; 321  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm3, ymm0, ymm1

; 324  :   temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm2, ymm3, 177				; 000000b1H

; 325  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm3, ymm6

; 326  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm1, ymm0, ymm2

; 327  : 
; 328  :   sign_mask = _mm256_unpacklo_epi16(mask_pos, mask_neg);
; 329  :   temp = _mm256_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw ymm0, ymm1, 177			; 000000b1H

; 331  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm3, ymm1, ymm4
	vpshufhw ymm1, ymm0, 177			; 000000b1H

; 332  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm2, ymm1, ymm3
	vmovdqu	YMMWORD PTR [rcx+160], ymm2

; 319  :   __m256i temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vmovdqu	ymm0, YMMWORD PTR [rcx+192]
	vpshufd	ymm1, ymm0, 78				; 0000004eH

; 320  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm0, ymm7

; 321  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm3, ymm0, ymm1

; 324  :   temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm2, ymm3, 177				; 000000b1H

; 325  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm3, ymm6

; 326  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm1, ymm0, ymm2

; 331  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm3, ymm1, ymm4
	vpshuflw ymm0, ymm1, 177			; 000000b1H
	vpshufhw ymm1, ymm0, 177			; 000000b1H

; 332  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm2, ymm1, ymm3
	vmovdqu	YMMWORD PTR [rcx+192], ymm2

; 319  :   __m256i temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vmovdqu	ymm0, YMMWORD PTR [rcx+224]
	vpshufd	ymm1, ymm0, 78				; 0000004eH

; 320  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm0, ymm7

; 321  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm3, ymm0, ymm1

; 324  :   temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm2, ymm3, 177				; 000000b1H

; 325  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm3, ymm6

; 326  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm1, ymm0, ymm2

; 331  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm3, ymm1, ymm4
	vpshuflw ymm0, ymm1, 177			; 000000b1H
	vpshufhw ymm1, ymm0, 177			; 000000b1H

; 332  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm2, ymm1, ymm3
	vmovdqu	YMMWORD PTR [rcx+224], ymm2
	vzeroupper

; 502  :   hor_transform_row_dual_avx2((*row_diff) + 0);
; 503  :   hor_transform_row_dual_avx2((*row_diff) + 1);
; 504  :   hor_transform_row_dual_avx2((*row_diff) + 2);
; 505  :   hor_transform_row_dual_avx2((*row_diff) + 3);
; 506  :   hor_transform_row_dual_avx2((*row_diff) + 4);
; 507  :   hor_transform_row_dual_avx2((*row_diff) + 5);
; 508  :   hor_transform_row_dual_avx2((*row_diff) + 6);
; 509  :   hor_transform_row_dual_avx2((*row_diff) + 7);
; 510  : }

	vmovaps	xmm6, XMMWORD PTR [rsp+16]
	vmovaps	xmm7, XMMWORD PTR [rsp]
	add	rsp, 40					; 00000028H
	ret	0
hor_transform_block_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
buf1$ = 176
stride1$ = 184
buf2$ = 192
stride2$ = 200
orig$ = 208
stride_orig$ = 216
sum0$ = 224
sum1$ = 232
kvz_satd_8bit_8x8_general_dual_avx2 PROC

; 516  : {

	mov	r11, rsp
	push	rbx
	sub	rsp, 160				; 000000a0H

; 450  :   __m128i temp3 = _mm_loadl_epi64((__m128i*)orig);

	mov	rbx, QWORD PTR orig$[rsp]
	vmovq	xmm0, QWORD PTR [r8]
	vmovq	xmm1, QWORD PTR [rcx]

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpunpcklqdq xmm1, xmm1, xmm0
	vpmovzxbw ymm3, xmm1

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpbroadcastq xmm1, QWORD PTR [rbx]
	vpmovzxbw ymm2, xmm1

; 478  :   (*row_diff)[1] = diff_row_dual_avx2(buf1 + 1 * stride1, buf2 + 1 * stride2, orig + 1 * stride_orig);

	mov	eax, r9d
	vmovaps	XMMWORD PTR [r11-24], xmm6
	vmovaps	XMMWORD PTR [r11-40], xmm7

; 454  :   return _mm256_sub_epi16(buf1_row, buf2_row);

	vpsubw	ymm7, ymm3, ymm2

; 449  :   __m128i temp2 = _mm_loadl_epi64((__m128i*)buf2);

	vmovq	xmm0, QWORD PTR [rax+r8]
	vmovaps	XMMWORD PTR [r11-56], xmm8

; 516  : {

	mov	r10d, edx

; 478  :   (*row_diff)[1] = diff_row_dual_avx2(buf1 + 1 * stride1, buf2 + 1 * stride2, orig + 1 * stride_orig);

	mov	edx, DWORD PTR stride_orig$[rsp]
	vmovaps	XMMWORD PTR [r11-72], xmm9
	vmovaps	XMMWORD PTR [r11-88], xmm10

; 448  :   __m128i temp1 = _mm_loadl_epi64((__m128i*)buf1);

	vmovq	xmm1, QWORD PTR [r10+rcx]

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpunpcklqdq xmm1, xmm1, xmm0
	vpmovzxbw ymm3, xmm1
	vmovaps	XMMWORD PTR [r11-104], xmm11
	vmovaps	XMMWORD PTR [r11-120], xmm12
	vmovaps	XMMWORD PTR [rsp+32], xmm13
	vmovaps	XMMWORD PTR [rsp+16], xmm14
	vmovaps	XMMWORD PTR [rsp], xmm15

; 479  :   (*row_diff)[2] = diff_row_dual_avx2(buf1 + 2 * stride1, buf2 + 2 * stride2, orig + 2 * stride_orig);

	lea	eax, DWORD PTR [r10+r10]

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpbroadcastq xmm1, QWORD PTR [rdx+rbx]
	vpmovzxbw ymm2, xmm1

; 448  :   __m128i temp1 = _mm_loadl_epi64((__m128i*)buf1);

	vmovq	xmm1, QWORD PTR [rax+rcx]

; 479  :   (*row_diff)[2] = diff_row_dual_avx2(buf1 + 2 * stride1, buf2 + 2 * stride2, orig + 2 * stride_orig);

	lea	eax, DWORD PTR [r9+r9]

; 449  :   __m128i temp2 = _mm_loadl_epi64((__m128i*)buf2);

	vmovq	xmm0, QWORD PTR [rax+r8]

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpunpcklqdq xmm1, xmm1, xmm0

; 479  :   (*row_diff)[2] = diff_row_dual_avx2(buf1 + 2 * stride1, buf2 + 2 * stride2, orig + 2 * stride_orig);

	lea	eax, DWORD PTR [rdx+rdx]

; 454  :   return _mm256_sub_epi16(buf1_row, buf2_row);

	vpsubw	ymm15, ymm3, ymm2

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpmovzxbw ymm3, xmm1

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpbroadcastq xmm1, QWORD PTR [rax+rbx]
	vpmovzxbw ymm2, xmm1

; 480  :   (*row_diff)[3] = diff_row_dual_avx2(buf1 + 3 * stride1, buf2 + 3 * stride2, orig + 3 * stride_orig);

	lea	eax, DWORD PTR [r10+r10*2]

; 448  :   __m128i temp1 = _mm_loadl_epi64((__m128i*)buf1);

	vmovq	xmm1, QWORD PTR [rax+rcx]

; 480  :   (*row_diff)[3] = diff_row_dual_avx2(buf1 + 3 * stride1, buf2 + 3 * stride2, orig + 3 * stride_orig);

	lea	eax, DWORD PTR [r9+r9*2]

; 449  :   __m128i temp2 = _mm_loadl_epi64((__m128i*)buf2);

	vmovq	xmm0, QWORD PTR [rax+r8]

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpunpcklqdq xmm1, xmm1, xmm0

; 480  :   (*row_diff)[3] = diff_row_dual_avx2(buf1 + 3 * stride1, buf2 + 3 * stride2, orig + 3 * stride_orig);

	lea	eax, DWORD PTR [rdx+rdx*2]

; 454  :   return _mm256_sub_epi16(buf1_row, buf2_row);

	vpsubw	ymm10, ymm3, ymm2

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpmovzxbw ymm3, xmm1

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpbroadcastq xmm1, QWORD PTR [rax+rbx]
	vpmovzxbw ymm2, xmm1

; 481  :   (*row_diff)[4] = diff_row_dual_avx2(buf1 + 4 * stride1, buf2 + 4 * stride2, orig + 4 * stride_orig);

	lea	eax, DWORD PTR [r10*4]

; 448  :   __m128i temp1 = _mm_loadl_epi64((__m128i*)buf1);

	vmovq	xmm1, QWORD PTR [rax+rcx]

; 481  :   (*row_diff)[4] = diff_row_dual_avx2(buf1 + 4 * stride1, buf2 + 4 * stride2, orig + 4 * stride_orig);

	lea	eax, DWORD PTR [r9*4]

; 449  :   __m128i temp2 = _mm_loadl_epi64((__m128i*)buf2);

	vmovq	xmm0, QWORD PTR [rax+r8]

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpunpcklqdq xmm1, xmm1, xmm0

; 481  :   (*row_diff)[4] = diff_row_dual_avx2(buf1 + 4 * stride1, buf2 + 4 * stride2, orig + 4 * stride_orig);

	lea	eax, DWORD PTR [rdx*4]

; 454  :   return _mm256_sub_epi16(buf1_row, buf2_row);

	vpsubw	ymm6, ymm3, ymm2

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpmovzxbw ymm3, xmm1

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpbroadcastq xmm1, QWORD PTR [rax+rbx]
	vpmovzxbw ymm2, xmm1

; 482  :   (*row_diff)[5] = diff_row_dual_avx2(buf1 + 5 * stride1, buf2 + 5 * stride2, orig + 5 * stride_orig);

	lea	eax, DWORD PTR [r10+r10*4]

; 448  :   __m128i temp1 = _mm_loadl_epi64((__m128i*)buf1);

	vmovq	xmm1, QWORD PTR [rax+rcx]

; 482  :   (*row_diff)[5] = diff_row_dual_avx2(buf1 + 5 * stride1, buf2 + 5 * stride2, orig + 5 * stride_orig);

	lea	eax, DWORD PTR [r9+r9*4]

; 449  :   __m128i temp2 = _mm_loadl_epi64((__m128i*)buf2);

	vmovq	xmm0, QWORD PTR [rax+r8]

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpunpcklqdq xmm1, xmm1, xmm0

; 482  :   (*row_diff)[5] = diff_row_dual_avx2(buf1 + 5 * stride1, buf2 + 5 * stride2, orig + 5 * stride_orig);

	lea	eax, DWORD PTR [rdx+rdx*4]

; 454  :   return _mm256_sub_epi16(buf1_row, buf2_row);

	vpsubw	ymm9, ymm3, ymm2

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpmovzxbw ymm3, xmm1

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpbroadcastq xmm1, QWORD PTR [rax+rbx]
	vpmovzxbw ymm2, xmm1

; 483  :   (*row_diff)[6] = diff_row_dual_avx2(buf1 + 6 * stride1, buf2 + 6 * stride2, orig + 6 * stride_orig);

	lea	eax, DWORD PTR [r10+r10*2]
	add	eax, eax

; 454  :   return _mm256_sub_epi16(buf1_row, buf2_row);

	vpsubw	ymm8, ymm3, ymm2

; 448  :   __m128i temp1 = _mm_loadl_epi64((__m128i*)buf1);

	vmovq	xmm1, QWORD PTR [rax+rcx]

; 483  :   (*row_diff)[6] = diff_row_dual_avx2(buf1 + 6 * stride1, buf2 + 6 * stride2, orig + 6 * stride_orig);

	lea	eax, DWORD PTR [r9+r9*2]
	add	eax, eax

; 449  :   __m128i temp2 = _mm_loadl_epi64((__m128i*)buf2);

	vmovq	xmm0, QWORD PTR [rax+r8]

; 483  :   (*row_diff)[6] = diff_row_dual_avx2(buf1 + 6 * stride1, buf2 + 6 * stride2, orig + 6 * stride_orig);

	lea	eax, DWORD PTR [rdx+rdx*2]

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpunpcklqdq xmm1, xmm1, xmm0
	vpmovzxbw ymm3, xmm1

; 483  :   (*row_diff)[6] = diff_row_dual_avx2(buf1 + 6 * stride1, buf2 + 6 * stride2, orig + 6 * stride_orig);

	add	eax, eax

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpbroadcastq xmm1, QWORD PTR [rax+rbx]
	vpmovzxbw ymm2, xmm1

; 453  : 
; 454  :   return _mm256_sub_epi16(buf1_row, buf2_row);

	vpsubw	ymm5, ymm3, ymm2

; 316  :   __m256i mask_pos = _mm256_set1_epi16(1);

	vmovdqu	ymm11, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 317  :   __m256i mask_neg = _mm256_set1_epi16(-1);

	vmovdqu	ymm4, YMMWORD PTR __ymm@ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff

; 318  :   __m256i sign_mask = _mm256_unpacklo_epi64(mask_pos, mask_neg);

	vpunpcklqdq ymm13, ymm11, ymm4

; 322  : 
; 323  :   sign_mask = _mm256_unpacklo_epi32(mask_pos, mask_neg);

	vpunpckldq ymm12, ymm11, ymm4

; 327  : 
; 328  :   sign_mask = _mm256_unpacklo_epi16(mask_pos, mask_neg);

	vpunpcklwd ymm4, ymm11, ymm4

; 484  :   (*row_diff)[7] = diff_row_dual_avx2(buf1 + 7 * stride1, buf2 + 7 * stride2, orig + 7 * stride_orig);

	imul	eax, r10d, 7

; 448  :   __m128i temp1 = _mm_loadl_epi64((__m128i*)buf1);

	vmovq	xmm1, QWORD PTR [rax+rcx]

; 484  :   (*row_diff)[7] = diff_row_dual_avx2(buf1 + 7 * stride1, buf2 + 7 * stride2, orig + 7 * stride_orig);

	imul	eax, r9d, 7

; 449  :   __m128i temp2 = _mm_loadl_epi64((__m128i*)buf2);

	vmovq	xmm0, QWORD PTR [rax+r8]

; 451  :   __m256i buf1_row = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(temp1, temp2));

	vpunpcklqdq xmm1, xmm1, xmm0
	vpmovzxbw ymm3, xmm1

; 320  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm7, ymm13

; 484  :   (*row_diff)[7] = diff_row_dual_avx2(buf1 + 7 * stride1, buf2 + 7 * stride2, orig + 7 * stride_orig);

	imul	eax, edx, 7

; 452  :   __m256i buf2_row = _mm256_cvtepu8_epi16(_mm_broadcastq_epi64(temp3));

	vpbroadcastq xmm1, QWORD PTR [rax+rbx]
	vpmovzxbw ymm2, xmm1

; 453  : 
; 454  :   return _mm256_sub_epi16(buf1_row, buf2_row);

	vpsubw	ymm14, ymm3, ymm2

; 319  :   __m256i temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	ymm1, ymm7, 78				; 0000004eH

; 321  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm2, ymm0, ymm1

; 324  :   temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm1, ymm2, 177				; 000000b1H

; 325  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm2, ymm12

; 326  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm3, ymm0, ymm1

; 329  :   temp = _mm256_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw ymm0, ymm3, 177			; 000000b1H

; 330  :   temp = _mm256_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw ymm1, ymm0, 177			; 000000b1H

; 331  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm2, ymm3, ymm4

; 332  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm11, ymm1, ymm2

; 319  :   __m256i temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	ymm2, ymm15, 78				; 0000004eH

; 320  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm15, ymm13

; 321  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm3, ymm0, ymm2

; 324  :   temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm1, ymm3, 177				; 000000b1H

; 325  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm3, ymm12

; 326  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm2, ymm0, ymm1

; 329  :   temp = _mm256_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw ymm0, ymm2, 177			; 000000b1H

; 330  :   temp = _mm256_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw ymm1, ymm0, 177			; 000000b1H

; 331  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm3, ymm2, ymm4

; 332  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm7, ymm1, ymm3

; 320  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm10, ymm13
	vpshufd	ymm2, ymm10, 78				; 0000004eH

; 321  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm3, ymm0, ymm2

; 324  :   temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm1, ymm3, 177				; 000000b1H

; 325  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm3, ymm12

; 326  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm2, ymm0, ymm1

; 329  :   temp = _mm256_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw ymm0, ymm2, 177			; 000000b1H

; 330  :   temp = _mm256_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw ymm1, ymm0, 177			; 000000b1H

; 331  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm3, ymm2, ymm4

; 332  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm10, ymm1, ymm3

; 320  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm6, ymm13
	vpshufd	ymm2, ymm6, 78				; 0000004eH

; 321  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm3, ymm0, ymm2

; 324  :   temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm1, ymm3, 177				; 000000b1H

; 325  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm3, ymm12

; 326  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm2, ymm0, ymm1

; 329  :   temp = _mm256_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw ymm0, ymm2, 177			; 000000b1H

; 330  :   temp = _mm256_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw ymm1, ymm0, 177			; 000000b1H

; 331  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm3, ymm2, ymm4

; 332  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm6, ymm1, ymm3

; 320  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm9, ymm13
	vpshufd	ymm2, ymm9, 78				; 0000004eH

; 321  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm3, ymm0, ymm2

; 324  :   temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm1, ymm3, 177				; 000000b1H

; 325  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm3, ymm12

; 326  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm2, ymm0, ymm1

; 329  :   temp = _mm256_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw ymm0, ymm2, 177			; 000000b1H

; 330  :   temp = _mm256_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw ymm1, ymm0, 177			; 000000b1H

; 331  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm3, ymm2, ymm4

; 332  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm9, ymm1, ymm3

; 320  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm8, ymm13
	vpshufd	ymm2, ymm8, 78				; 0000004eH

; 321  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm3, ymm0, ymm2

; 324  :   temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm1, ymm3, 177				; 000000b1H

; 325  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm3, ymm12

; 326  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm2, ymm0, ymm1

; 329  :   temp = _mm256_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw ymm0, ymm2, 177			; 000000b1H

; 330  :   temp = _mm256_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw ymm1, ymm0, 177			; 000000b1H

; 331  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm3, ymm2, ymm4

; 320  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm5, ymm13
	vpshufd	ymm2, ymm5, 78				; 0000004eH

; 332  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm8, ymm1, ymm3

; 321  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm3, ymm0, ymm2

; 324  :   temp = _mm256_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm1, ymm3, 177				; 000000b1H

; 325  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm3, ymm12

; 326  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm2, ymm0, ymm1

; 435  :   *sum0 = _mm_cvtsi128_si32(_mm256_extracti128_si256(sad, 0));

	mov	rcx, QWORD PTR sum0$[rsp]

; 436  :   *sum1 = _mm_cvtsi128_si32(_mm256_extracti128_si256(sad, 1));

	mov	rdx, QWORD PTR sum1$[rsp]

; 331  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm3, ymm2, ymm4
	vpshuflw ymm0, ymm2, 177			; 000000b1H
	vpshufhw ymm1, ymm0, 177			; 000000b1H

; 332  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm5, ymm1, ymm3

; 320  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm14, ymm13
	vpshufd	ymm2, ymm14, 78				; 0000004eH

; 321  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm3, ymm0, ymm2

; 325  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm0, ymm3, ymm12
	vpshufd	ymm1, ymm3, 177				; 000000b1H

; 326  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm2, ymm0, ymm1

; 331  :   *row = _mm256_sign_epi16(*row, sign_mask);

	vpsignw	ymm3, ymm2, ymm4
	vpshuflw ymm0, ymm2, 177			; 000000b1H
	vpshufhw ymm1, ymm0, 177			; 000000b1H

; 332  :   *row = _mm256_add_epi16(*row, temp);

	vpaddw	ymm4, ymm1, ymm3

; 364  :   out[out_idx0] = _mm256_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	ymm1, ymm4, ymm5
	vpaddw	ymm3, ymm7, ymm11
	vpaddw	ymm0, ymm6, ymm10

; 365  :   out[out_idx1] = _mm256_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	ymm12, ymm3, ymm0
	vpaddw	ymm15, ymm0, ymm3
	vpsubw	ymm2, ymm10, ymm6

; 364  :   out[out_idx0] = _mm256_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	ymm6, ymm8, ymm9
	vpaddw	ymm13, ymm1, ymm6

; 365  :   out[out_idx1] = _mm256_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	ymm10, ymm6, ymm1
	vpsubw	ymm7, ymm11, ymm7
	vpsubw	ymm3, ymm7, ymm2
	vpsubw	ymm5, ymm5, ymm4
	vpsubw	ymm8, ymm9, ymm8

; 364  :   out[out_idx0] = _mm256_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	ymm9, ymm2, ymm7
	vpaddw	ymm4, ymm5, ymm8

; 365  :   out[out_idx1] = _mm256_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	ymm2, ymm8, ymm5
	vpsubw	ymm0, ymm3, ymm2

; 398  :   __m256i abs_value = _mm256_abs_epi16(*ver_row);

	vpabsw	ymm1, ymm0

; 399  :   *accumulate = _mm256_add_epi32(*accumulate, _mm256_madd_epi16(abs_value, _mm256_set1_epi16(1)));

	vpmaddwd ymm14, ymm1, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 364  :   out[out_idx0] = _mm256_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	ymm2, ymm2, ymm3

; 399  :   *accumulate = _mm256_add_epi32(*accumulate, _mm256_madd_epi16(abs_value, _mm256_set1_epi16(1)));

	vmovdqu	ymm3, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vpabsw	ymm0, ymm2
	vpmaddwd ymm11, ymm0, ymm3

; 365  :   out[out_idx1] = _mm256_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	ymm1, ymm9, ymm4

; 398  :   __m256i abs_value = _mm256_abs_epi16(*ver_row);

	vpabsw	ymm2, ymm1

; 399  :   *accumulate = _mm256_add_epi32(*accumulate, _mm256_madd_epi16(abs_value, _mm256_set1_epi16(1)));

	vpmaddwd ymm8, ymm2, ymm3

; 364  :   out[out_idx0] = _mm256_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	ymm0, ymm4, ymm9

; 398  :   __m256i abs_value = _mm256_abs_epi16(*ver_row);

	vpabsw	ymm1, ymm0

; 399  :   *accumulate = _mm256_add_epi32(*accumulate, _mm256_madd_epi16(abs_value, _mm256_set1_epi16(1)));

	vpmaddwd ymm7, ymm1, ymm3

; 365  :   out[out_idx1] = _mm256_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	ymm2, ymm12, ymm10

; 398  :   __m256i abs_value = _mm256_abs_epi16(*ver_row);

	vpabsw	ymm0, ymm2

; 399  :   *accumulate = _mm256_add_epi32(*accumulate, _mm256_madd_epi16(abs_value, _mm256_set1_epi16(1)));

	vpmaddwd ymm6, ymm0, ymm3

; 364  :   out[out_idx0] = _mm256_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	ymm1, ymm10, ymm12

; 398  :   __m256i abs_value = _mm256_abs_epi16(*ver_row);

	vpabsw	ymm2, ymm1

; 399  :   *accumulate = _mm256_add_epi32(*accumulate, _mm256_madd_epi16(abs_value, _mm256_set1_epi16(1)));

	vpmaddwd ymm5, ymm2, ymm3

; 365  :   out[out_idx1] = _mm256_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	ymm0, ymm15, ymm13

; 398  :   __m256i abs_value = _mm256_abs_epi16(*ver_row);

	vpabsw	ymm1, ymm0

; 399  :   *accumulate = _mm256_add_epi32(*accumulate, _mm256_madd_epi16(abs_value, _mm256_set1_epi16(1)));

	vpmaddwd ymm4, ymm1, ymm3

; 364  :   out[out_idx0] = _mm256_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	ymm0, ymm13, ymm15

; 398  :   __m256i abs_value = _mm256_abs_epi16(*ver_row);

	vpabsw	ymm1, ymm0

; 399  :   *accumulate = _mm256_add_epi32(*accumulate, _mm256_madd_epi16(abs_value, _mm256_set1_epi16(1)));

	vpmaddwd ymm2, ymm1, ymm3
	vpaddd	ymm0, ymm2, ymm4
	vpaddd	ymm1, ymm0, ymm5
	vpaddd	ymm2, ymm1, ymm6
	vpaddd	ymm3, ymm2, ymm7
	vpaddd	ymm0, ymm3, ymm8
	vpaddd	ymm1, ymm0, ymm11
	vpaddd	ymm2, ymm1, ymm14

; 432  :   sad = _mm256_add_epi32(sad, _mm256_shuffle_epi32(sad, _MM_SHUFFLE(1, 0, 3, 2)));

	vpshufd	ymm0, ymm2, 78				; 0000004eH
	vpaddd	ymm3, ymm0, ymm2

; 433  :   sad = _mm256_add_epi32(sad, _mm256_shuffle_epi32(sad, _MM_SHUFFLE(0, 1, 0, 1)));

	vpshufd	ymm1, ymm3, 17
	vpaddd	ymm0, ymm1, ymm3

; 434  : 
; 435  :   *sum0 = _mm_cvtsi128_si32(_mm256_extracti128_si256(sad, 0));

	vmovd	DWORD PTR [rcx], xmm0

; 436  :   *sum1 = _mm_cvtsi128_si32(_mm256_extracti128_si256(sad, 1));

	vextracti128 xmm0, ymm0, 1
	vmovd	DWORD PTR [rdx], xmm0

; 517  :   __m256i temp[8];
; 518  : 
; 519  :   diff_blocks_dual_avx2(&temp, buf1, stride1, buf2, stride2, orig, stride_orig);
; 520  :   hor_transform_block_dual_avx2(&temp);
; 521  :   ver_transform_block_dual_avx2(&temp);
; 522  :   
; 523  :   sum_block_dual_avx2(temp, sum0, sum1);
; 524  : 
; 525  :   *sum0 = (*sum0 + 2) >> 2;

	mov	eax, DWORD PTR [rcx]
	add	eax, 2
	shr	eax, 2
	mov	DWORD PTR [rcx], eax

; 526  :   *sum1 = (*sum1 + 2) >> 2;

	mov	eax, DWORD PTR [rdx]
	add	eax, 2
	shr	eax, 2
	mov	DWORD PTR [rdx], eax
	vzeroupper

; 527  : }

	lea	r11, QWORD PTR [r11-8]
	vmovaps	xmm6, XMMWORD PTR [r11-16]
	vmovaps	xmm7, XMMWORD PTR [r11-32]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm9, XMMWORD PTR [r11-64]
	vmovaps	xmm10, XMMWORD PTR [r11-80]
	vmovaps	xmm11, XMMWORD PTR [r11-96]
	vmovaps	xmm12, XMMWORD PTR [r11-112]
	vmovaps	xmm13, XMMWORD PTR [r11-128]
	vmovaps	xmm14, XMMWORD PTR [rsp+16]
	vmovaps	xmm15, XMMWORD PTR [rsp]
	mov	rsp, r11
	pop	rbx
	ret	0
kvz_satd_8bit_8x8_general_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
buf1$ = 8
stride1$ = 16
buf2$ = 24
stride2$ = 32
kvz_satd_4x4_subblock_8bit_avx2 PROC

; 537  :   // TODO: AVX2 implementation
; 538  :   return kvz_satd_4x4_subblock_generic(buf1, stride1, buf2, stride2);

	jmp	kvz_satd_4x4_subblock_generic
kvz_satd_4x4_subblock_8bit_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
preds$ = 8
stride$ = 16
orig$ = 24
orig_stride$ = 32
costs$ = 40
kvz_satd_4x4_subblock_quad_avx2 PROC

; 547  :   // TODO: AVX2 implementation
; 548  :   kvz_satd_4x4_subblock_quad_generic(preds, stride, orig, orig_stride, costs);

	jmp	kvz_satd_4x4_subblock_quad_generic
kvz_satd_4x4_subblock_quad_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
tv2046 = 0
tv2045 = 16
buf1$ = 208
stride1$ = 216
buf2$ = 224
stride2$ = 232
satd_8x8_subblock_8bit_avx2 PROC

; 552  : {

	mov	r11, rsp
	sub	rsp, 200				; 000000c8H

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vmovq	xmm0, QWORD PTR [rcx]

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	vmovq	xmm1, QWORD PTR [r8]
	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1

; 443  :   return _mm_sub_epi16(buf1_row, buf2_row);

	vpsubw	xmm3, xmm2, xmm0
	vmovaps	XMMWORD PTR [r11-24], xmm6
	vmovaps	XMMWORD PTR [r11-40], xmm7
	vmovaps	XMMWORD PTR [r11-56], xmm8

; 295  :   __m128i mask_pos = _mm_set1_epi16(1);

	vmovdqu	xmm8, XMMWORD PTR __xmm@00010001000100010001000100010001
	vmovaps	XMMWORD PTR [r11-72], xmm9
	vmovaps	XMMWORD PTR [r11-88], xmm10

; 462  :   (*row_diff)[1] = diff_row_avx2(buf1 + 1 * stride1, orig + 1 * stride_orig);

	mov	eax, edx
	vmovaps	XMMWORD PTR [r11-104], xmm11
	vmovaps	XMMWORD PTR [r11-120], xmm12
	vmovaps	XMMWORD PTR [rsp+64], xmm13

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vmovq	xmm0, QWORD PTR [rax+rcx]
	vpmovzxbw xmm2, xmm0

; 462  :   (*row_diff)[1] = diff_row_avx2(buf1 + 1 * stride1, orig + 1 * stride_orig);

	mov	eax, r9d
	vmovaps	XMMWORD PTR [rsp+48], xmm14
	vmovaps	XMMWORD PTR [rsp+32], xmm15

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	vmovq	xmm0, QWORD PTR [rax+r8]
	vpmovzxbw xmm1, xmm0

; 443  :   return _mm_sub_epi16(buf1_row, buf2_row);

	vpsubw	xmm6, xmm2, xmm1

; 463  :   (*row_diff)[2] = diff_row_avx2(buf1 + 2 * stride1, orig + 2 * stride_orig);

	lea	eax, DWORD PTR [rdx+rdx]

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vmovq	xmm0, QWORD PTR [rax+rcx]
	vpmovzxbw xmm2, xmm0

; 463  :   (*row_diff)[2] = diff_row_avx2(buf1 + 2 * stride1, orig + 2 * stride_orig);

	lea	eax, DWORD PTR [r9+r9]

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	vmovq	xmm0, QWORD PTR [rax+r8]
	vpmovzxbw xmm1, xmm0

; 443  :   return _mm_sub_epi16(buf1_row, buf2_row);

	vpsubw	xmm10, xmm2, xmm1

; 464  :   (*row_diff)[3] = diff_row_avx2(buf1 + 3 * stride1, orig + 3 * stride_orig);

	lea	eax, DWORD PTR [rdx+rdx*2]

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vmovq	xmm0, QWORD PTR [rax+rcx]
	vpmovzxbw xmm2, xmm0

; 464  :   (*row_diff)[3] = diff_row_avx2(buf1 + 3 * stride1, orig + 3 * stride_orig);

	lea	eax, DWORD PTR [r9+r9*2]

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	vmovq	xmm0, QWORD PTR [rax+r8]
	vpmovzxbw xmm1, xmm0

; 443  :   return _mm_sub_epi16(buf1_row, buf2_row);

	vpsubw	xmm9, xmm2, xmm1

; 465  :   (*row_diff)[4] = diff_row_avx2(buf1 + 4 * stride1, orig + 4 * stride_orig);

	lea	eax, DWORD PTR [rdx*4]

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vmovq	xmm0, QWORD PTR [rax+rcx]
	vpmovzxbw xmm2, xmm0

; 465  :   (*row_diff)[4] = diff_row_avx2(buf1 + 4 * stride1, orig + 4 * stride_orig);

	lea	eax, DWORD PTR [r9*4]

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	vmovq	xmm0, QWORD PTR [rax+r8]
	vpmovzxbw xmm1, xmm0

; 443  :   return _mm_sub_epi16(buf1_row, buf2_row);

	vpsubw	xmm4, xmm2, xmm1

; 466  :   (*row_diff)[5] = diff_row_avx2(buf1 + 5 * stride1, orig + 5 * stride_orig);

	lea	eax, DWORD PTR [rdx+rdx*4]

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vmovq	xmm0, QWORD PTR [rax+rcx]
	vpmovzxbw xmm2, xmm0

; 466  :   (*row_diff)[5] = diff_row_avx2(buf1 + 5 * stride1, orig + 5 * stride_orig);

	lea	eax, DWORD PTR [r9+r9*4]

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	vmovq	xmm0, QWORD PTR [rax+r8]
	vpmovzxbw xmm1, xmm0

; 467  :   (*row_diff)[6] = diff_row_avx2(buf1 + 6 * stride1, orig + 6 * stride_orig);

	lea	eax, DWORD PTR [rdx+rdx*2]
	add	eax, eax

; 443  :   return _mm_sub_epi16(buf1_row, buf2_row);

	vmovdqu	XMMWORD PTR tv2046[rsp], xmm4
	vpsubw	xmm4, xmm2, xmm1
	vmovdqu	XMMWORD PTR tv2045[rsp], xmm4

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vmovq	xmm0, QWORD PTR [rax+rcx]

; 296  :   __m128i mask_neg = _mm_set1_epi16(-1);

	vmovdqu	xmm4, XMMWORD PTR __xmm@ffffffffffffffffffffffffffffffff

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vpmovzxbw xmm2, xmm0

; 467  :   (*row_diff)[6] = diff_row_avx2(buf1 + 6 * stride1, orig + 6 * stride_orig);

	lea	eax, DWORD PTR [r9+r9*2]
	add	eax, eax

; 297  :   __m128i sign_mask = _mm_unpacklo_epi64(mask_pos, mask_neg);

	vpunpcklqdq xmm14, xmm8, xmm4

; 301  : 
; 302  :   sign_mask = _mm_unpacklo_epi32(mask_pos, mask_neg);

	vpunpckldq xmm5, xmm8, xmm4

; 306  : 
; 307  :   sign_mask = _mm_unpacklo_epi16(mask_pos, mask_neg);

	vpunpcklwd xmm12, xmm8, xmm4

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	vmovq	xmm0, QWORD PTR [rax+r8]
	vpmovzxbw xmm1, xmm0

; 443  :   return _mm_sub_epi16(buf1_row, buf2_row);

	vpsubw	xmm7, xmm2, xmm1

; 468  :   (*row_diff)[7] = diff_row_avx2(buf1 + 7 * stride1, orig + 7 * stride_orig);

	imul	eax, edx, 7

; 441  :   __m128i buf1_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf1));

	vmovq	xmm0, QWORD PTR [rax+rcx]
	vpmovzxbw xmm2, xmm0

; 468  :   (*row_diff)[7] = diff_row_avx2(buf1 + 7 * stride1, orig + 7 * stride_orig);

	imul	eax, r9d, 7

; 442  :   __m128i buf2_row = _mm_cvtepu8_epi16(_mm_loadl_epi64((__m128i*)buf2));

	vmovq	xmm0, QWORD PTR [rax+r8]
	vpmovzxbw xmm1, xmm0

; 443  :   return _mm_sub_epi16(buf1_row, buf2_row);

	vpsubw	xmm15, xmm2, xmm1

; 299  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm1, xmm3, xmm14
	vpshufd	xmm0, xmm3, 78				; 0000004eH

; 300  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm2, xmm1, xmm0

; 303  :   temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	xmm0, xmm2, 177				; 000000b1H

; 304  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm1, xmm2, xmm5

; 305  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm3, xmm1, xmm0

; 308  :   temp = _mm_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw xmm0, xmm3, 177			; 000000b1H

; 309  :   temp = _mm_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw xmm2, xmm0, 177			; 000000b1H

; 310  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm1, xmm3, xmm12

; 311  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm13, xmm2, xmm1

; 299  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm3, xmm6, xmm14
	vpshufd	xmm0, xmm6, 78				; 0000004eH

; 300  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm2, xmm3, xmm0

; 304  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm1, xmm2, xmm5
	vpshufd	xmm0, xmm2, 177				; 000000b1H

; 305  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm3, xmm1, xmm0

; 308  :   temp = _mm_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw xmm1, xmm3, 177			; 000000b1H

; 309  :   temp = _mm_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw xmm2, xmm1, 177			; 000000b1H

; 310  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm0, xmm3, xmm12

; 311  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm8, xmm2, xmm0

; 298  :   __m128i temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm10, 78				; 0000004eH

; 299  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm1, xmm10, xmm14

; 300  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm3, xmm1, xmm0

; 304  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm2, xmm3, xmm5
	vpshufd	xmm0, xmm3, 177				; 000000b1H

; 305  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm4, xmm2, xmm0

; 308  :   temp = _mm_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw xmm1, xmm4, 177			; 000000b1H

; 309  :   temp = _mm_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw xmm2, xmm1, 177			; 000000b1H

; 310  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm0, xmm4, xmm12

; 311  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm11, xmm2, xmm0

; 298  :   __m128i temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm9, 78				; 0000004eH

; 299  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm1, xmm9, xmm14

; 300  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm3, xmm1, xmm0

; 304  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm2, xmm3, xmm5
	vpshufd	xmm0, xmm3, 177				; 000000b1H

; 305  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm4, xmm2, xmm0

; 308  :   temp = _mm_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw xmm1, xmm4, 177			; 000000b1H

; 309  :   temp = _mm_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw xmm2, xmm1, 177			; 000000b1H

; 310  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm0, xmm4, xmm12

; 311  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm6, xmm2, xmm0

; 299  :   *row = _mm_sign_epi16(*row, sign_mask);

	vmovdqu	xmm0, XMMWORD PTR tv2046[rsp]
	vpsignw	xmm1, xmm0, xmm14
	vpshufd	xmm0, xmm0, 78				; 0000004eH

; 300  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm3, xmm1, xmm0

; 304  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm2, xmm3, xmm5
	vpshufd	xmm0, xmm3, 177				; 000000b1H

; 305  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm4, xmm2, xmm0

; 308  :   temp = _mm_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw xmm1, xmm4, 177			; 000000b1H

; 309  :   temp = _mm_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw xmm2, xmm1, 177			; 000000b1H

; 310  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm0, xmm4, xmm12

; 311  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm10, xmm2, xmm0

; 299  :   *row = _mm_sign_epi16(*row, sign_mask);

	vmovdqu	xmm0, XMMWORD PTR tv2045[rsp]
	vpsignw	xmm1, xmm0, xmm14
	vpshufd	xmm0, xmm0, 78				; 0000004eH

; 300  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm3, xmm1, xmm0

; 304  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm2, xmm3, xmm5
	vpshufd	xmm0, xmm3, 177				; 000000b1H

; 305  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm4, xmm2, xmm0

; 308  :   temp = _mm_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw xmm1, xmm4, 177			; 000000b1H

; 309  :   temp = _mm_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw xmm2, xmm1, 177			; 000000b1H

; 310  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm0, xmm4, xmm12

; 311  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm9, xmm2, xmm0

; 298  :   __m128i temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm7, 78				; 0000004eH

; 299  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm1, xmm7, xmm14

; 300  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm3, xmm1, xmm0

; 304  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm2, xmm3, xmm5
	vpshufd	xmm0, xmm3, 177				; 000000b1H

; 305  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm4, xmm2, xmm0

; 308  :   temp = _mm_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw xmm1, xmm4, 177			; 000000b1H

; 309  :   temp = _mm_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw xmm2, xmm1, 177			; 000000b1H

; 310  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm0, xmm4, xmm12

; 311  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm7, xmm2, xmm0

; 298  :   __m128i temp = _mm_shuffle_epi32(*row, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm15, 78				; 0000004eH

; 299  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm1, xmm15, xmm14

; 300  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm3, xmm1, xmm0

; 304  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm2, xmm3, xmm5
	vpshufd	xmm0, xmm3, 177				; 000000b1H

; 305  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm4, xmm2, xmm0

; 308  :   temp = _mm_shufflelo_epi16(*row, _MM_SHUFFLE(2,3,0,1));

	vpshuflw xmm1, xmm4, 177			; 000000b1H

; 309  :   temp = _mm_shufflehi_epi16(temp, _MM_SHUFFLE(2,3,0,1));

	vpshufhw xmm2, xmm1, 177			; 000000b1H

; 310  :   *row = _mm_sign_epi16(*row, sign_mask);

	vpsignw	xmm0, xmm4, xmm12

; 311  :   *row = _mm_add_epi16(*row, temp);

	vpaddw	xmm3, xmm2, xmm0

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	xmm0, xmm6, xmm11

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	xmm2, xmm11, xmm6

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	xmm6, xmm9, xmm10
	vpaddw	xmm5, xmm8, xmm13

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	xmm9, xmm10, xmm9
	vpsubw	xmm8, xmm13, xmm8

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	xmm1, xmm3, xmm7

; 562  :   return result;
; 563  : }

	vmovaps	xmm13, XMMWORD PTR [rsp+64]
	lea	r11, QWORD PTR [r11]
	vmovaps	xmm14, XMMWORD PTR [rsp+48]
	vmovaps	xmm15, XMMWORD PTR [rsp+32]

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	xmm10, xmm5, xmm0

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	xmm11, xmm2, xmm8

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	xmm12, xmm8, xmm2

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	xmm2, xmm1, xmm6

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	xmm4, xmm7, xmm3

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	xmm7, xmm0, xmm5

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	xmm5, xmm6, xmm1
	vpsubw	xmm0, xmm7, xmm2

; 392  :   __m128i abs_value = _mm_abs_epi16(*ver_row);

	vpabsw	xmm1, xmm0

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	xmm2, xmm2, xmm7

; 562  :   return result;
; 563  : }

	vmovaps	xmm7, XMMWORD PTR [r11-40]

; 392  :   __m128i abs_value = _mm_abs_epi16(*ver_row);

	vpabsw	xmm0, xmm2

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	xmm6, xmm4, xmm9

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	xmm8, xmm9, xmm4

; 393  :   *accumulate = _mm_add_epi32(*accumulate, _mm_madd_epi16(abs_value, _mm_set1_epi16(1)));

	vmovdqu	xmm9, XMMWORD PTR __xmm@00010001000100010001000100010001
	vpmaddwd xmm3, xmm1, xmm9
	vpmaddwd xmm1, xmm0, xmm9
	vpaddd	xmm4, xmm3, xmm1

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	xmm2, xmm5, xmm10

; 392  :   __m128i abs_value = _mm_abs_epi16(*ver_row);

	vpabsw	xmm0, xmm2

; 393  :   *accumulate = _mm_add_epi32(*accumulate, _mm_madd_epi16(abs_value, _mm_set1_epi16(1)));

	vpmaddwd xmm1, xmm0, xmm9
	vpaddd	xmm3, xmm4, xmm1

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	xmm2, xmm10, xmm5

; 562  :   return result;
; 563  : }

	vmovaps	xmm10, XMMWORD PTR [r11-88]

; 392  :   __m128i abs_value = _mm_abs_epi16(*ver_row);

	vpabsw	xmm0, xmm2

; 393  :   *accumulate = _mm_add_epi32(*accumulate, _mm_madd_epi16(abs_value, _mm_set1_epi16(1)));

	vpmaddwd xmm1, xmm0, xmm9
	vpaddd	xmm4, xmm3, xmm1

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	xmm2, xmm6, xmm11

; 392  :   __m128i abs_value = _mm_abs_epi16(*ver_row);

	vpabsw	xmm0, xmm2

; 393  :   *accumulate = _mm_add_epi32(*accumulate, _mm_madd_epi16(abs_value, _mm_set1_epi16(1)));

	vpmaddwd xmm1, xmm0, xmm9
	vpaddd	xmm3, xmm4, xmm1

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	xmm2, xmm11, xmm6

; 562  :   return result;
; 563  : }

	vmovaps	xmm6, XMMWORD PTR [r11-24]
	vmovaps	xmm11, XMMWORD PTR [r11-104]

; 392  :   __m128i abs_value = _mm_abs_epi16(*ver_row);

	vpabsw	xmm0, xmm2

; 393  :   *accumulate = _mm_add_epi32(*accumulate, _mm_madd_epi16(abs_value, _mm_set1_epi16(1)));

	vpmaddwd xmm1, xmm0, xmm9
	vpaddd	xmm4, xmm3, xmm1

; 337  :   out[out_idx0] = _mm_add_epi16(in[in_idx0], in[in_idx1]);

	vpaddw	xmm2, xmm8, xmm12

; 392  :   __m128i abs_value = _mm_abs_epi16(*ver_row);

	vpabsw	xmm0, xmm2

; 393  :   *accumulate = _mm_add_epi32(*accumulate, _mm_madd_epi16(abs_value, _mm_set1_epi16(1)));

	vpmaddwd xmm1, xmm0, xmm9

; 338  :   out[out_idx1] = _mm_sub_epi16(in[in_idx0], in[in_idx1]);

	vpsubw	xmm2, xmm12, xmm8

; 562  :   return result;
; 563  : }

	vmovaps	xmm8, XMMWORD PTR [r11-56]
	vmovaps	xmm12, XMMWORD PTR [r11-120]

; 392  :   __m128i abs_value = _mm_abs_epi16(*ver_row);

	vpabsw	xmm0, xmm2

; 393  :   *accumulate = _mm_add_epi32(*accumulate, _mm_madd_epi16(abs_value, _mm_set1_epi16(1)));

	vpaddd	xmm3, xmm4, xmm1
	vpmaddwd xmm1, xmm0, xmm9

; 562  :   return result;
; 563  : }

	vmovaps	xmm9, XMMWORD PTR [r11-72]

; 393  :   *accumulate = _mm_add_epi32(*accumulate, _mm_madd_epi16(abs_value, _mm_set1_epi16(1)));

	vpaddd	xmm3, xmm3, xmm1

; 414  :   sad = _mm_add_epi32(sad, _mm_shuffle_epi32(sad, _MM_SHUFFLE(1, 0, 3, 2)));

	vpshufd	xmm0, xmm3, 78				; 0000004eH
	vpaddd	xmm2, xmm0, xmm3

; 415  :   sad = _mm_add_epi32(sad, _mm_shuffle_epi32(sad, _MM_SHUFFLE(0, 1, 0, 1)));

	vpshufd	xmm1, xmm2, 17
	vpaddd	xmm0, xmm1, xmm2

; 416  : 
; 417  :   return _mm_cvtsi128_si32(sad);

	vmovd	eax, xmm0

; 553  :   __m128i temp[8];
; 554  : 
; 555  :   diff_blocks_avx2(&temp, buf1, stride1, buf2, stride2);
; 556  :   hor_transform_block_avx2(&temp);
; 557  :   ver_transform_block_avx2(&temp);
; 558  :   
; 559  :   unsigned sad = sum_block_avx2(temp);
; 560  : 
; 561  :   unsigned result = (sad + 2) >> 2;

	add	eax, 2
	shr	eax, 2

; 562  :   return result;
; 563  : }

	mov	rsp, r11
	ret	0
satd_8x8_subblock_8bit_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
preds$ = 80
stride$ = 88
orig$ = 96
orig_stride$ = 104
costs$ = 112
satd_8x8_subblock_quad_avx2 PROC

; 570  : {

	mov	r11, rsp
	mov	QWORD PTR [r11+8], rbx
	mov	QWORD PTR [r11+16], rbp
	mov	QWORD PTR [r11+24], rsi
	mov	QWORD PTR [r11+32], rdi
	push	r14
	sub	rsp, 64					; 00000040H

; 571  :   kvz_satd_8bit_8x8_general_dual_avx2(preds[0], stride, preds[1], stride, orig, orig_stride, &costs[0], &costs[1]);

	mov	rbx, QWORD PTR costs$[rsp]
	mov	rsi, r8
	mov	edi, r9d
	mov	r14, rcx
	mov	ebp, edx
	lea	rax, QWORD PTR [rbx+4]
	mov	QWORD PTR [r11-16], rax
	mov	QWORD PTR [r11-24], rbx
	mov	DWORD PTR [rsp+40], r9d
	mov	r9d, edx
	mov	QWORD PTR [r11-40], r8
	mov	r8, QWORD PTR [rcx+8]
	mov	rcx, QWORD PTR [rcx]
	call	kvz_satd_8bit_8x8_general_dual_avx2

; 572  :   kvz_satd_8bit_8x8_general_dual_avx2(preds[2], stride, preds[3], stride, orig, orig_stride, &costs[2], &costs[3]);

	mov	r8, QWORD PTR [r14+24]
	lea	rdx, QWORD PTR [rbx+8]
	mov	rcx, QWORD PTR [r14+16]
	lea	rax, QWORD PTR [rbx+12]
	mov	QWORD PTR [rsp+56], rax
	mov	r9d, ebp
	mov	QWORD PTR [rsp+48], rdx
	mov	edx, ebp
	mov	DWORD PTR [rsp+40], edi
	mov	QWORD PTR [rsp+32], rsi
	call	kvz_satd_8bit_8x8_general_dual_avx2

; 573  : }

	mov	rbx, QWORD PTR [rsp+80]
	mov	rbp, QWORD PTR [rsp+88]
	mov	rsi, QWORD PTR [rsp+96]
	mov	rdi, QWORD PTR [rsp+104]
	add	rsp, 64					; 00000040H
	pop	r14
	ret	0
satd_8x8_subblock_quad_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
block1$ = 8
block2$ = 16
satd_8x8_8bit_avx2 PROC

; 575  : SATD_NxN(8bit_avx2,  8)

	mov	r9d, 8
	mov	r8, rdx
	mov	edx, r9d
	jmp	satd_8x8_subblock_8bit_avx2
satd_8x8_8bit_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
block1$ = 64
block2$ = 72
satd_16x16_8bit_avx2 PROC

; 576  : SATD_NxN(8bit_avx2, 16)

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rbp
	mov	QWORD PTR [rsp+24], rsi
	push	rdi
	push	r14
	push	r15
	sub	rsp, 32					; 00000020H
	xor	ebp, ebp
	mov	r14, rdx
	xor	esi, esi
	mov	r15, rcx
$LL4@satd_16x16:
	mov	edi, esi
	add	edi, edi
	xor	ebx, ebx
	npad	8
$LL7@satd_16x16:
	lea	eax, DWORD PTR [rbx+rdi*8]
	mov	edx, 16
	mov	ecx, eax
	lea	r8, QWORD PTR [rax+r14]
	add	rcx, r15
	mov	r9d, edx
	call	satd_8x8_subblock_8bit_avx2
	add	ebp, eax
	add	ebx, 8
	cmp	ebx, 16
	jb	SHORT $LL7@satd_16x16
	add	esi, 8
	cmp	esi, 16
	jb	SHORT $LL4@satd_16x16
	mov	rbx, QWORD PTR [rsp+64]
	mov	eax, ebp
	mov	rbp, QWORD PTR [rsp+72]
	mov	rsi, QWORD PTR [rsp+80]
	add	rsp, 32					; 00000020H
	pop	r15
	pop	r14
	pop	rdi
	ret	0
satd_16x16_8bit_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
block1$ = 64
block2$ = 72
satd_32x32_8bit_avx2 PROC

; 577  : SATD_NxN(8bit_avx2, 32)

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rbp
	mov	QWORD PTR [rsp+24], rsi
	push	rdi
	push	r14
	push	r15
	sub	rsp, 32					; 00000020H
	xor	ebp, ebp
	mov	r14, rdx
	xor	esi, esi
	mov	r15, rcx
	npad	14
$LL4@satd_32x32:
	mov	edi, esi
	shl	edi, 5
	xor	ebx, ebx
	npad	9
$LL7@satd_32x32:
	lea	eax, DWORD PTR [rbx+rdi]
	mov	edx, 32					; 00000020H
	mov	ecx, eax
	lea	r8, QWORD PTR [rax+r14]
	add	rcx, r15
	mov	r9d, edx
	call	satd_8x8_subblock_8bit_avx2
	add	ebp, eax
	add	ebx, 8
	cmp	ebx, 32					; 00000020H
	jb	SHORT $LL7@satd_32x32
	add	esi, 8
	cmp	esi, 32					; 00000020H
	jb	SHORT $LL4@satd_32x32
	mov	rbx, QWORD PTR [rsp+64]
	mov	eax, ebp
	mov	rbp, QWORD PTR [rsp+72]
	mov	rsi, QWORD PTR [rsp+80]
	add	rsp, 32					; 00000020H
	pop	r15
	pop	r14
	pop	rdi
	ret	0
satd_32x32_8bit_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
block1$ = 64
block2$ = 72
satd_64x64_8bit_avx2 PROC

; 578  : SATD_NxN(8bit_avx2, 64)

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rbp
	mov	QWORD PTR [rsp+24], rsi
	push	rdi
	push	r14
	push	r15
	sub	rsp, 32					; 00000020H
	xor	ebp, ebp
	mov	r14, rdx
	xor	esi, esi
	mov	r15, rcx
	npad	14
$LL4@satd_64x64:
	mov	edi, esi
	shl	edi, 6
	xor	ebx, ebx
	npad	9
$LL7@satd_64x64:
	lea	eax, DWORD PTR [rbx+rdi]
	mov	edx, 64					; 00000040H
	mov	ecx, eax
	lea	r8, QWORD PTR [rax+r14]
	add	rcx, r15
	mov	r9d, edx
	call	satd_8x8_subblock_8bit_avx2
	add	ebp, eax
	add	ebx, 8
	cmp	ebx, 64					; 00000040H
	jb	SHORT $LL7@satd_64x64
	add	esi, 8
	cmp	esi, 64					; 00000040H
	jb	SHORT $LL4@satd_64x64
	mov	rbx, QWORD PTR [rsp+64]
	mov	eax, ebp
	mov	rbp, QWORD PTR [rsp+72]
	mov	rsi, QWORD PTR [rsp+80]
	add	rsp, 32					; 00000020H
	pop	r15
	pop	r14
	pop	rdi
	ret	0
satd_64x64_8bit_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\picture-generic.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\picture-generic.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
tv2975 = 32
tv2947 = 32
tv2969 = 40
tv2946 = 40
tv2970 = 48
tv2948 = 48
diff$1 = 64
diff$2 = 64
width$ = 192
height$ = 200
block1$ = 208
stride1$ = 216
block2$ = 224
stride2$ = 232
satd_any_size_8bit_avx2 PROC

; 579  : SATD_ANY_SIZE(8bit_avx2)

	mov	DWORD PTR [rsp+32], r9d
	mov	DWORD PTR [rsp+16], edx
	mov	DWORD PTR [rsp+8], ecx
	push	rbp
	push	rdi
	push	r13
	push	r14
	sub	rsp, 152				; 00000098H
	xor	edi, edi
	movsxd	r14, r9d
	mov	eax, ecx
	mov	rbp, r8
	mov	r8d, edx
	mov	edx, ecx
	and	eax, -2147483641			; ffffffff80000007H
	jge	SHORT $LN158@satd_any_s
	dec	eax
	or	eax, -8
	inc	eax
$LN158@satd_any_s:
	movsxd	r13, DWORD PTR stride2$[rsp]
	mov	QWORD PTR [rsp+208], rbx
	mov	QWORD PTR [rsp+144], rsi
	mov	QWORD PTR [rsp+136], r12
	mov	QWORD PTR [rsp+128], r15
	mov	r15, QWORD PTR block2$[rsp]
	test	eax, eax
	je	$LN14@satd_any_s
	test	r8d, r8d
	jle	$LN3@satd_any_s
	lea	ecx, DWORD PTR [r8-1]
	mov	r11, r14
	shr	ecx, 2
	lea	eax, DWORD PTR [r14*4]
	cdqe
	lea	rbx, QWORD PTR [rbp+2]
	mov	QWORD PTR tv2970[rsp], rax
	lea	rsi, QWORD PTR [r15+2]
	lea	eax, DWORD PTR [r13*4]
	inc	ecx
	cdqe
	mov	QWORD PTR tv2969[rsp], rax
	mov	QWORD PTR tv2975[rsp], rcx
	npad	1
$LL4@satd_any_s:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\picture-generic.c

; 219  :   for (int y = 0; y < 4; y++) {

	lea	rdx, QWORD PTR diff$2[rsp+8]
	mov	r8, rsi
	mov	r9, rbx
	mov	r10d, 4
	npad	15
$LL22@satd_any_s:

; 220  :     for (int x = 0; x < 4; x++) {
; 221  :       diff[x + y * 4] = buf1[x + y * stride1] - buf2[x + y * stride2];

	movzx	eax, BYTE PTR [r8-2]
	lea	rdx, QWORD PTR [rdx+16]
	movzx	ecx, BYTE PTR [r9-2]
	sub	ecx, eax
	movzx	eax, BYTE PTR [r8-1]
	mov	DWORD PTR [rdx-24], ecx
	movzx	ecx, BYTE PTR [r9-1]
	sub	ecx, eax
	movzx	eax, BYTE PTR [r8]
	mov	DWORD PTR [rdx-20], ecx
	movzx	ecx, BYTE PTR [r9]
	sub	ecx, eax
	movzx	eax, BYTE PTR [r8+1]
	mov	DWORD PTR [rdx-16], ecx
	add	r8, r13
	movzx	ecx, BYTE PTR [r9+1]
	add	r9, r11
	sub	ecx, eax
	mov	DWORD PTR [rdx-12], ecx
	sub	r10, 1
	jne	SHORT $LL22@satd_any_s

; 222  :     }
; 223  :   }
; 224  :   return hadamard_4x4_generic(diff);

	lea	rcx, QWORD PTR diff$2[rsp]
	call	hadamard_4x4_generic
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 579  : SATD_ANY_SIZE(8bit_avx2)

	add	rbx, QWORD PTR tv2970[rsp]
	add	edi, eax
	add	rsi, QWORD PTR tv2969[rsp]
	mov	r11, r14
	sub	QWORD PTR tv2975[rsp], 1
	jne	$LL4@satd_any_s
	mov	edx, DWORD PTR width$[rsp]
	mov	r8d, DWORD PTR height$[rsp]
$LN3@satd_any_s:
	add	rbp, 4
	add	r15, 4
	sub	edx, 4
	mov	DWORD PTR width$[rsp], edx
$LN14@satd_any_s:
	mov	eax, r8d
	and	eax, -2147483641			; ffffffff80000007H
	jge	SHORT $LN157@satd_any_s
	dec	eax
	or	eax, -8
	inc	eax
$LN157@satd_any_s:
	test	eax, eax
	je	$LN15@satd_any_s
	movsxd	rsi, edx
	test	edx, edx
	jle	$LN6@satd_any_s
	mov	r12, r14
	lea	rbx, QWORD PTR [r15+2]
	mov	r14, rbp
	mov	r11, r13
	sub	r14, r15
	dec	rsi
	shr	rsi, 2
	inc	rsi
	npad	2
$LL7@satd_any_s:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\picture-generic.c

; 219  :   for (int y = 0; y < 4; y++) {

	lea	rdx, QWORD PTR diff$1[rsp+8]
	mov	r9, rbx
	lea	r8, QWORD PTR [r14+rbx]
	mov	r10d, 4
	npad	14
$LL32@satd_any_s:

; 220  :     for (int x = 0; x < 4; x++) {
; 221  :       diff[x + y * 4] = buf1[x + y * stride1] - buf2[x + y * stride2];

	movzx	eax, BYTE PTR [r9-2]
	lea	rdx, QWORD PTR [rdx+16]
	movzx	ecx, BYTE PTR [r8-2]
	sub	ecx, eax
	movzx	eax, BYTE PTR [r9-1]
	mov	DWORD PTR [rdx-24], ecx
	movzx	ecx, BYTE PTR [r8-1]
	sub	ecx, eax
	movzx	eax, BYTE PTR [r9]
	mov	DWORD PTR [rdx-20], ecx
	movzx	ecx, BYTE PTR [r8]
	sub	ecx, eax
	movzx	eax, BYTE PTR [r9+1]
	mov	DWORD PTR [rdx-16], ecx
	add	r9, r11
	movzx	ecx, BYTE PTR [r8+1]
	add	r8, r12
	sub	ecx, eax
	mov	DWORD PTR [rdx-12], ecx
	sub	r10, 1
	jne	SHORT $LL32@satd_any_s

; 222  :     }
; 223  :   }
; 224  :   return hadamard_4x4_generic(diff);

	lea	rcx, QWORD PTR diff$1[rsp]
	call	hadamard_4x4_generic
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 579  : SATD_ANY_SIZE(8bit_avx2)

	add	edi, eax
	add	rbx, 4
	mov	r11, r13
	sub	rsi, 1
	jne	$LL7@satd_any_s
	mov	r14d, DWORD PTR stride1$[rsp]
	mov	edx, DWORD PTR width$[rsp]
	mov	r8d, DWORD PTR height$[rsp]
$LN6@satd_any_s:
	lea	eax, DWORD PTR [r14*4]
	movsxd	rcx, eax
	lea	eax, DWORD PTR [r13*4]
	add	rbp, rcx
	movsxd	rcx, eax
	add	r15, rcx
	sub	r8d, 4
$LN15@satd_any_s:
	test	r8d, r8d
	jle	$LN159@satd_any_s
	lea	r12d, DWORD PTR [r8-1]
	shr	r12d, 3
	lea	eax, DWORD PTR [r13*8]
	cdqe
	inc	r12d
	mov	QWORD PTR tv2947[rsp], rax
	lea	eax, DWORD PTR [r14*8]
	cdqe
	mov	QWORD PTR tv2946[rsp], rax
	mov	rcx, QWORD PTR tv2946[rsp]
	mov	rax, QWORD PTR tv2947[rsp]
	mov	QWORD PTR tv2948[rsp], r12
	npad	1
$LL10@satd_any_s:
	test	edx, edx
	jle	SHORT $LN8@satd_any_s
	mov	r12d, DWORD PTR stride1$[rsp]
	lea	esi, DWORD PTR [rdx-1]
	mov	r14, r15
	shr	esi, 3
	sub	r14, rbp
	mov	rbx, rbp
	inc	esi
	npad	3
$LL13@satd_any_s:
	lea	r8, QWORD PTR [r14+rbx]
	mov	r9d, r13d
	mov	edx, r12d
	mov	rcx, rbx
	call	satd_8x8_subblock_8bit_avx2
	add	edi, eax
	add	rbx, 8
	sub	rsi, 1
	jne	SHORT $LL13@satd_any_s
	mov	r12, QWORD PTR tv2948[rsp]
	mov	rax, QWORD PTR tv2947[rsp]
	mov	rcx, QWORD PTR tv2946[rsp]
	mov	edx, DWORD PTR width$[rsp]
$LN8@satd_any_s:
	add	rbp, rcx
	add	r15, rax
	sub	r12, 1
	mov	QWORD PTR tv2948[rsp], r12
	jne	SHORT $LL10@satd_any_s
$LN159@satd_any_s:
	mov	r15, QWORD PTR [rsp+128]
	mov	eax, edi
	mov	r12, QWORD PTR [rsp+136]
	mov	rsi, QWORD PTR [rsp+144]
	mov	rbx, QWORD PTR [rsp+208]
	add	rsp, 152				; 00000098H
	pop	r14
	pop	r13
	pop	rdi
	pop	rbp
	ret	0
satd_any_size_8bit_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
sum2$ = 64
preds$ = 96
orig$ = 104
num_modes$ = 112
sum1$ = 120
satds_out$ = 120
satd_8bit_8x8_dual_avx2 PROC

; 607  : { 

	push	rbx
	sub	rsp, 80					; 00000050H

; 608  :   unsigned x, y; 
; 609  :   satds_out[0] = 0;

	xor	eax, eax

; 610  :   satds_out[1] = 0;
; 611  :   unsigned sum1 = 0;
; 612  :   unsigned sum2 = 0;
; 613  :   for (y = 0; y < (8); y += 8) { 
; 614  :   unsigned row = y * (8); 
; 615  :   for (x = 0; x < (8); x += 8) { 
; 616  :   kvz_satd_8bit_8x8_general_dual_avx2(&preds[0][row + x], (8), &preds[1][row + x], (8), &orig[row + x], (8), &sum1, &sum2); 

	lea	r8, QWORD PTR [rcx+1024]
	mov	QWORD PTR [r9], rax
	mov	rbx, r9
	mov	DWORD PTR sum1$[rsp], eax
	mov	DWORD PTR sum2$[rsp], eax
	lea	rax, QWORD PTR sum2$[rsp]
	mov	QWORD PTR [rsp+56], rax
	lea	rax, QWORD PTR sum1$[rsp]
	mov	QWORD PTR [rsp+48], rax
	mov	DWORD PTR [rsp+40], 8
	mov	QWORD PTR [rsp+32], rdx
	mov	edx, 8
	mov	r9d, edx
	call	kvz_satd_8bit_8x8_general_dual_avx2

; 617  :   satds_out[0] += sum1;

	mov	eax, DWORD PTR sum1$[rsp]
	add	DWORD PTR [rbx], eax

; 618  :   satds_out[1] += sum2;
; 619  :       } 
; 620  :       } 
; 621  :   satds_out[0] >>= (KVZ_BIT_DEPTH-8);
; 622  :   satds_out[1] >>= (KVZ_BIT_DEPTH-8);

	mov	eax, DWORD PTR sum2$[rsp]
	add	DWORD PTR [rbx+4], eax

; 623  : }

	add	rsp, 80					; 00000050H
	pop	rbx
	ret	0
satd_8bit_8x8_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
sum2$ = 64
preds$ = 128
orig$ = 136
num_modes$ = 144
sum1$ = 152
satds_out$ = 152
satd_8bit_16x16_dual_avx2 PROC

; 626  : SATD_NXN_DUAL_AVX2(16)

	mov	rax, rsp
	mov	QWORD PTR [rax+8], rbx
	mov	QWORD PTR [rax+16], rbp
	push	rsi
	push	rdi
	push	r12
	push	r14
	push	r15
	sub	rsp, 80					; 00000050H
	xor	r12d, r12d
	mov	rdi, r9
	mov	QWORD PTR [r9], r12
	mov	ebp, r12d
	mov	DWORD PTR [rax+32], r12d
	mov	r14, rdx
	mov	DWORD PTR [rax-56], r12d
	mov	r15, rcx
$LL4@satd_8bit_:
	mov	esi, ebp
	mov	ebx, r12d
	add	esi, esi
	npad	8
$LL7@satd_8bit_:
	lea	rdx, QWORD PTR sum2$[rsp]
	mov	QWORD PTR [rsp+56], rdx
	lea	eax, DWORD PTR [rbx+rsi*8]
	lea	rdx, QWORD PTR sum1$[rsp]
	mov	QWORD PTR [rsp+48], rdx
	lea	rcx, QWORD PTR [rax+r15]
	mov	edx, 16
	mov	DWORD PTR [rsp+40], 16
	add	rax, r14
	lea	r8, QWORD PTR [rcx+1024]
	mov	r9d, edx
	mov	QWORD PTR [rsp+32], rax
	call	kvz_satd_8bit_8x8_general_dual_avx2
	mov	eax, DWORD PTR sum1$[rsp]
	add	ebx, 8
	add	DWORD PTR [rdi], eax
	mov	eax, DWORD PTR sum2$[rsp]
	add	DWORD PTR [rdi+4], eax
	cmp	ebx, 16
	jb	SHORT $LL7@satd_8bit_
	add	ebp, 8
	cmp	ebp, 16
	jb	SHORT $LL4@satd_8bit_
	lea	r11, QWORD PTR [rsp+80]
	mov	rbx, QWORD PTR [r11+48]
	mov	rbp, QWORD PTR [r11+56]
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r12
	pop	rdi
	pop	rsi
	ret	0
satd_8bit_16x16_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
sum2$ = 64
preds$ = 128
orig$ = 136
num_modes$ = 144
sum1$ = 152
satds_out$ = 152
satd_8bit_32x32_dual_avx2 PROC

; 627  : SATD_NXN_DUAL_AVX2(32)

	mov	rax, rsp
	mov	QWORD PTR [rax+8], rbx
	mov	QWORD PTR [rax+16], rbp
	push	rsi
	push	rdi
	push	r12
	push	r14
	push	r15
	sub	rsp, 80					; 00000050H
	xor	r12d, r12d
	mov	rdi, r9
	mov	QWORD PTR [r9], r12
	mov	ebp, r12d
	mov	DWORD PTR [rax+32], r12d
	mov	r14, rdx
	mov	DWORD PTR [rax-56], r12d
	mov	r15, rcx
$LL4@satd_8bit_:
	mov	esi, ebp
	mov	ebx, r12d
	shl	esi, 5
	npad	7
$LL7@satd_8bit_:
	lea	rdx, QWORD PTR sum2$[rsp]
	mov	QWORD PTR [rsp+56], rdx
	lea	eax, DWORD PTR [rbx+rsi]
	lea	rdx, QWORD PTR sum1$[rsp]
	mov	QWORD PTR [rsp+48], rdx
	lea	rcx, QWORD PTR [rax+r15]
	mov	edx, 32					; 00000020H
	mov	DWORD PTR [rsp+40], 32			; 00000020H
	add	rax, r14
	lea	r8, QWORD PTR [rcx+1024]
	mov	r9d, edx
	mov	QWORD PTR [rsp+32], rax
	call	kvz_satd_8bit_8x8_general_dual_avx2
	mov	eax, DWORD PTR sum1$[rsp]
	add	ebx, 8
	add	DWORD PTR [rdi], eax
	mov	eax, DWORD PTR sum2$[rsp]
	add	DWORD PTR [rdi+4], eax
	cmp	ebx, 32					; 00000020H
	jb	SHORT $LL7@satd_8bit_
	add	ebp, 8
	cmp	ebp, 32					; 00000020H
	jb	SHORT $LL4@satd_8bit_
	lea	r11, QWORD PTR [rsp+80]
	mov	rbx, QWORD PTR [r11+48]
	mov	rbp, QWORD PTR [r11+56]
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r12
	pop	rdi
	pop	rsi
	ret	0
satd_8bit_32x32_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
sum2$ = 64
preds$ = 128
orig$ = 136
num_modes$ = 144
sum1$ = 152
satds_out$ = 152
satd_8bit_64x64_dual_avx2 PROC

; 628  : SATD_NXN_DUAL_AVX2(64)

	mov	rax, rsp
	mov	QWORD PTR [rax+8], rbx
	mov	QWORD PTR [rax+16], rbp
	push	rsi
	push	rdi
	push	r12
	push	r14
	push	r15
	sub	rsp, 80					; 00000050H
	xor	r12d, r12d
	mov	rdi, r9
	mov	QWORD PTR [r9], r12
	mov	ebp, r12d
	mov	DWORD PTR [rax+32], r12d
	mov	r14, rdx
	mov	DWORD PTR [rax-56], r12d
	mov	r15, rcx
$LL4@satd_8bit_:
	mov	esi, ebp
	mov	ebx, r12d
	shl	esi, 6
	npad	7
$LL7@satd_8bit_:
	lea	rdx, QWORD PTR sum2$[rsp]
	mov	QWORD PTR [rsp+56], rdx
	lea	eax, DWORD PTR [rbx+rsi]
	lea	rdx, QWORD PTR sum1$[rsp]
	mov	QWORD PTR [rsp+48], rdx
	lea	rcx, QWORD PTR [rax+r15]
	mov	edx, 64					; 00000040H
	mov	DWORD PTR [rsp+40], 64			; 00000040H
	add	rax, r14
	lea	r8, QWORD PTR [rcx+1024]
	mov	r9d, edx
	mov	QWORD PTR [rsp+32], rax
	call	kvz_satd_8bit_8x8_general_dual_avx2
	mov	eax, DWORD PTR sum1$[rsp]
	add	ebx, 8
	add	DWORD PTR [rdi], eax
	mov	eax, DWORD PTR sum2$[rsp]
	add	DWORD PTR [rdi+4], eax
	cmp	ebx, 64					; 00000040H
	jb	SHORT $LL7@satd_8bit_
	add	ebp, 8
	cmp	ebp, 64					; 00000040H
	jb	SHORT $LL4@satd_8bit_
	lea	r11, QWORD PTR [rsp+80]
	mov	rbx, QWORD PTR [r11+48]
	mov	rbp, QWORD PTR [r11+56]
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r12
	pop	rdi
	pop	rsi
	ret	0
satd_8bit_64x64_dual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
tv1828 = 64
sums$ = 72
pred_ptrs$5$ = 88
pred_ptrs$ = 88
width$ = 176
height$ = 184
preds$ = 192
stride$ = 200
tv1823 = 208
orig$ = 208
orig_stride$ = 216
num_modes$ = 224
costs_out$ = 232
valid$ = 240
satd_any_size_quad_avx2 PROC

; 694  : SATD_ANY_SIZE_MULTI_AVX2(quad_avx2, 4)

	mov	QWORD PTR [rsp+32], rbx
	mov	QWORD PTR [rsp+24], r8
	mov	DWORD PTR [rsp+8], ecx
	push	rsi
	push	rdi
	push	r12
	push	r13
	push	r15
	sub	rsp, 128				; 00000080H
	mov	rax, QWORD PTR [r8]
	mov	ebx, r9d
	mov	rdi, QWORD PTR orig$[rsp]
	mov	r12d, edx
	mov	QWORD PTR pred_ptrs$[rsp], rax
	mov	r13d, ecx
	mov	rax, QWORD PTR [r8+8]
	mov	r15, rdi
	mov	QWORD PTR pred_ptrs$[rsp+8], rax
	mov	rax, QWORD PTR [r8+16]
	mov	QWORD PTR pred_ptrs$[rsp+16], rax
	mov	rax, QWORD PTR [r8+24]
	mov	QWORD PTR pred_ptrs$[rsp+24], rax
	mov	rax, QWORD PTR costs_out$[rsp]
	vpxor	xmm0, xmm0, xmm0
	vmovups	XMMWORD PTR sums$[rsp], xmm0
	vmovups	XMMWORD PTR [rax], xmm0
	mov	eax, ecx
	and	eax, -2147483641			; ffffffff80000007H
	jge	SHORT $LN280@satd_any_s
	dec	eax
	or	eax, -8
	inc	eax
$LN280@satd_any_s:
	mov	esi, DWORD PTR orig_stride$[rsp]
	mov	QWORD PTR [rsp+184], r14
	test	eax, eax
	je	SHORT $LN23@satd_any_s
	test	r12d, r12d
	jle	SHORT $LN3@satd_any_s
	mov	r13, QWORD PTR preds$[rsp]
	lea	r14d, DWORD PTR [rdx-1]
	shr	r14d, 2
	inc	r14d
	npad	11
$LL4@satd_any_s:

; 548  :   kvz_satd_4x4_subblock_quad_generic(preds, stride, orig, orig_stride, costs);

	lea	rax, QWORD PTR sums$[rsp]
	mov	r9d, esi
	mov	r8, rdi
	mov	QWORD PTR [rsp+32], rax
	mov	edx, ebx
	mov	rcx, r13
	call	kvz_satd_4x4_subblock_quad_generic

; 694  : SATD_ANY_SIZE_MULTI_AVX2(quad_avx2, 4)

	sub	r14, 1
	jne	SHORT $LL4@satd_any_s
	mov	r13d, DWORD PTR width$[rsp]
$LN3@satd_any_s:
	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000000000004000000000000000400000000000000040000000000000004
	vpaddq	ymm1, ymm0, YMMWORD PTR pred_ptrs$[rsp]
	sub	r13d, 4
	lea	r15, QWORD PTR [rdi+4]
	vmovdqu	YMMWORD PTR pred_ptrs$[rsp], ymm1
	mov	DWORD PTR width$[rsp], r13d
$LN23@satd_any_s:
	mov	eax, r12d
	and	eax, -2147483641			; ffffffff80000007H
	jge	SHORT $LN279@satd_any_s
	dec	eax
	or	eax, -8
	inc	eax
$LN279@satd_any_s:
	test	eax, eax
	je	SHORT $LN24@satd_any_s
	test	r13d, r13d
	jle	SHORT $LL9@satd_any_s
	lea	r14d, DWORD PTR [r13-1]
	shr	r14d, 2
	inc	r14d
$LL10@satd_any_s:

; 548  :   kvz_satd_4x4_subblock_quad_generic(preds, stride, orig, orig_stride, costs);

	lea	rax, QWORD PTR sums$[rsp]
	mov	r9d, esi
	mov	QWORD PTR [rsp+32], rax
	lea	rcx, QWORD PTR pred_ptrs$[rsp]
	mov	r8, r15
	mov	edx, ebx
	vzeroupper
	call	kvz_satd_4x4_subblock_quad_generic

; 694  : SATD_ANY_SIZE_MULTI_AVX2(quad_avx2, 4)

	sub	r14, 1
	jne	SHORT $LL10@satd_any_s
	npad	10
$LL9@satd_any_s:
	sub	r12d, 4
$LN24@satd_any_s:
	test	r12d, r12d
	jle	$LN15@satd_any_s
	lea	ecx, DWORD PTR [r12-1]
	xor	r15d, r15d
	shr	ecx, 3
	lea	eax, DWORD PTR [rsi*8]
	cdqe
	lea	edx, DWORD PTR [rbx*8]
	mov	QWORD PTR tv1823[rsp], rax
	inc	ecx
	mov	r8, QWORD PTR tv1823[rsp]
	mov	rax, QWORD PTR preds$[rsp]
	mov	QWORD PTR tv1828[rsp], rcx
	npad	9
$LL16@satd_any_s:
	mov	r14, rdi
	vmovd	xmm0, r15d
	vpbroadcastd xmm0, xmm0
	vpmovsxdq ymm0, xmm0
	vpaddq	ymm2, ymm0, YMMWORD PTR [rax]
	vmovdqu	YMMWORD PTR pred_ptrs$5$[rsp], ymm2
	test	r13d, r13d
	jle	$LN14@satd_any_s
	lea	r12d, DWORD PTR [r13-1]
	mov	r13, QWORD PTR costs_out$[rsp]
	shr	r12d, 3
	inc	r12d
	npad	8
$LL19@satd_any_s:

; 571  :   kvz_satd_8bit_8x8_general_dual_avx2(preds[0], stride, preds[1], stride, orig, orig_stride, &costs[0], &costs[1]);

	lea	rax, QWORD PTR sums$[rsp+4]
	mov	r9d, ebx
	mov	QWORD PTR [rsp+56], rax
	mov	edx, ebx
	lea	rax, QWORD PTR sums$[rsp]
	mov	QWORD PTR [rsp+48], rax
	mov	DWORD PTR [rsp+40], esi
	mov	QWORD PTR [rsp+32], r14
	vpextrq	r8, xmm2, 1
	vmovq	rcx, xmm2
	vzeroupper
	call	kvz_satd_8bit_8x8_general_dual_avx2

; 572  :   kvz_satd_8bit_8x8_general_dual_avx2(preds[2], stride, preds[3], stride, orig, orig_stride, &costs[2], &costs[3]);

	vmovdqu	ymm1, YMMWORD PTR pred_ptrs$5$[rsp]
	vextractf128 xmm0, ymm1, 1
	lea	rax, QWORD PTR sums$[rsp+12]
	mov	r9d, ebx
	mov	QWORD PTR [rsp+56], rax
	mov	edx, ebx
	lea	rax, QWORD PTR sums$[rsp+8]
	mov	QWORD PTR [rsp+48], rax
	vpextrq	r8, xmm0, 1
	vextractf128 xmm0, ymm1, 1
	mov	DWORD PTR [rsp+40], esi
	mov	QWORD PTR [rsp+32], r14
	vmovq	rcx, xmm0
	vzeroupper
	call	kvz_satd_8bit_8x8_general_dual_avx2

; 694  : SATD_ANY_SIZE_MULTI_AVX2(quad_avx2, 4)

	vmovdqu	ymm2, YMMWORD PTR pred_ptrs$5$[rsp]
	vmovdqu	xmm0, XMMWORD PTR [r13]
	vpaddq	ymm2, ymm2, YMMWORD PTR __ymm@0000000000000008000000000000000800000000000000080000000000000008
	vpaddd	xmm1, xmm0, XMMWORD PTR sums$[rsp]
	add	r14, 8
	vmovdqu	YMMWORD PTR pred_ptrs$5$[rsp], ymm2
	vmovdqu	XMMWORD PTR [r13], xmm1
	sub	r12, 1
	jne	$LL19@satd_any_s
	mov	r13d, DWORD PTR width$[rsp]
	lea	edx, DWORD PTR [rbx*8]
	mov	rax, QWORD PTR preds$[rsp]
	mov	rcx, QWORD PTR tv1828[rsp]
	mov	r8, QWORD PTR tv1823[rsp]
$LN14@satd_any_s:
	add	rdi, r8
	add	r15d, edx
	sub	rcx, 1
	mov	QWORD PTR tv1828[rsp], rcx
	jne	$LL16@satd_any_s
$LN15@satd_any_s:
	vzeroupper
	mov	r14, QWORD PTR [rsp+184]
	mov	rbx, QWORD PTR [rsp+200]
	add	rsp, 128				; 00000080H
	pop	r15
	pop	r13
	pop	r12
	pop	rdi
	pop	rsi
	ret	0
satd_any_size_quad_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
tv1381 = 0
tv1379 = 8
tv1380 = 16
tv1417 = 24
tv1424 = 32
ref$ = 96
rec$ = 104
ref_stride$ = 112
rec_stride$ = 120
width$ = 128
pixels_calc_ssd_avx2 PROC

; 700  : {

	mov	rax, rsp
	mov	QWORD PTR [rax+16], rdx
	mov	QWORD PTR [rax+8], rcx
	push	rsi
	push	rdi
	sub	rsp, 72					; 00000048H

; 701  :   __m256i ssd_part;
; 702  :   __m256i diff = _mm256_setzero_si256();
; 703  :   __m128i sum;
; 704  : 
; 705  :   __m256i ref_epi16;
; 706  :   __m256i rec_epi16;
; 707  : 
; 708  :   __m128i ref_row0, ref_row1, ref_row2, ref_row3;
; 709  :   __m128i rec_row0, rec_row1, rec_row2, rec_row3;
; 710  : 
; 711  :   int ssd;
; 712  : 
; 713  :   switch (width) {

	mov	edi, DWORD PTR width$[rsp]
	mov	rsi, rdx
	mov	rdx, rcx
	cmp	edi, 4
	je	$LN13@pixels_cal

; 750  :     for (int y = 0; y < width; y += 8) {

	vpxor	xmm4, xmm4, xmm4
	test	edi, edi
	jle	$LN5@pixels_cal

; 739  :     sum = _mm_add_epi32(sum, _mm_shuffle_epi32(sum, _MM_SHUFFLE(1, 0, 3, 2)));
; 740  :     sum = _mm_add_epi32(sum, _mm_shuffle_epi32(sum, _MM_SHUFFLE(0, 1, 0, 1)));
; 741  : 
; 742  :     ssd = _mm_cvtsi128_si32(sum);
; 743  : 
; 744  :     return ssd >> (2*(KVZ_BIT_DEPTH-8));
; 745  :     break;
; 746  : 
; 747  :   default:
; 748  : 
; 749  :     ssd_part = _mm256_setzero_si256();

	mov	QWORD PTR [rax+24], rbx
	mov	QWORD PTR [rax+32], r12
	mov	QWORD PTR [rax-24], r13
	mov	QWORD PTR [rax-32], r14
	mov	QWORD PTR [rax-40], r15
	lea	eax, DWORD PTR [r9+r9]
	movsxd	r10, eax
	xor	r15d, r15d
	lea	eax, DWORD PTR [r8+r8]
	movsxd	rcx, r9d
	movsxd	r11, eax
	mov	r13d, r15d
	lea	eax, DWORD PTR [r8*8]
	movsxd	rbx, r8d
	cdqe
	mov	QWORD PTR tv1380[rsp], rax
	lea	eax, DWORD PTR [r9*8]
	cdqe
	mov	QWORD PTR tv1379[rsp], rax
	lea	eax, DWORD PTR [rdi-1]
	shr	eax, 3
	inc	eax
	mov	QWORD PTR tv1424[rsp], rcx
	mov	QWORD PTR tv1417[rsp], rax
	mov	QWORD PTR tv1381[rsp], rax
	npad	11
$LL6@pixels_cal:

; 751  :       for (int x = 0; x < width; x += 8) {

	mov	r14, r13
	lea	r9, QWORD PTR [r15+rdx]
	sub	r14, r15
	mov	rdi, rcx
	sub	r14, rdx
	sub	rdi, r15
	sub	rdi, rdx
	add	rdi, r13
	lea	r12, QWORD PTR [r14+rsi]
	mov	rsi, rax
$LL9@pixels_cal:

; 752  :         for (int i = 0; i < 8; i += 2) {

	mov	r8, rdi
	lea	rax, QWORD PTR [r12+r9]
	sub	r8, r14
	mov	rcx, r9
	mov	edx, 4
$LL12@pixels_cal:

; 753  :           ref_epi16 = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(_mm_loadl_epi64((__m128i*)&(ref[x + (y + i) * ref_stride])), _mm_loadl_epi64((__m128i*)&(ref[x + (y + i + 1) * ref_stride]))));

	vmovq	xmm0, QWORD PTR [rcx+rbx]

; 754  :           rec_epi16 = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(_mm_loadl_epi64((__m128i*)&(rec[x + (y + i) * rec_stride])), _mm_loadl_epi64((__m128i*)&(rec[x + (y + i + 1) * rec_stride]))));

	vmovq	xmm2, QWORD PTR [rax]
	vmovq	xmm1, QWORD PTR [rcx]
	vpunpcklqdq xmm1, xmm1, xmm0
	vmovq	xmm0, QWORD PTR [rax+r8]
	vpmovzxbw ymm3, xmm1
	vpunpcklqdq xmm1, xmm2, xmm0
	vpmovzxbw ymm2, xmm1

; 755  :           diff = _mm256_sub_epi16(ref_epi16, rec_epi16);

	vpsubw	ymm3, ymm3, ymm2
	add	rcx, r11
	add	rax, r10

; 756  :           ssd_part = _mm256_add_epi32(ssd_part, _mm256_madd_epi16(diff, diff));

	vpmaddwd ymm0, ymm3, ymm3
	vpaddd	ymm4, ymm0, ymm4
	sub	rdx, 1
	jne	SHORT $LL12@pixels_cal

; 751  :       for (int x = 0; x < width; x += 8) {

	add	r9, 8
	sub	rsi, 1
	jne	SHORT $LL9@pixels_cal

; 750  :     for (int y = 0; y < width; y += 8) {

	add	r13, QWORD PTR tv1379[rsp]
	add	r15, QWORD PTR tv1380[rsp]
	sub	QWORD PTR tv1381[rsp], 1
	mov	rdx, QWORD PTR ref$[rsp]
	mov	rax, QWORD PTR tv1417[rsp]
	mov	rsi, QWORD PTR rec$[rsp]
	mov	rcx, QWORD PTR tv1424[rsp]
	jne	$LL6@pixels_cal
	mov	r15, QWORD PTR [rsp+48]
	mov	r14, QWORD PTR [rsp+56]
	mov	r13, QWORD PTR [rsp+64]
	mov	r12, QWORD PTR [rsp+120]
	mov	rbx, QWORD PTR [rsp+112]
$LN5@pixels_cal:

; 757  :         }
; 758  :       }
; 759  :     }
; 760  : 
; 761  :     sum = _mm_add_epi32(_mm256_castsi256_si128(ssd_part), _mm256_extracti128_si256(ssd_part, 1));

	vextracti128 xmm0, ymm4, 1
	vpaddd	xmm1, xmm0, xmm4

; 762  :     sum = _mm_add_epi32(sum, _mm_shuffle_epi32(sum, _MM_SHUFFLE(1, 0, 3, 2)));
; 763  :     sum = _mm_add_epi32(sum, _mm_shuffle_epi32(sum, _MM_SHUFFLE(0, 1, 0, 1)));
; 764  : 
; 765  :     ssd = _mm_cvtsi128_si32(sum);
; 766  : 
; 767  :     return ssd >> (2*(KVZ_BIT_DEPTH-8));

	jmp	SHORT $LN28@pixels_cal
$LN13@pixels_cal:

; 714  : 
; 715  :   case 4:
; 716  : 
; 717  :     ref_row0 = _mm_cvtsi32_si128(*(int32_t*)&(ref[0 * ref_stride]));

	vmovd	xmm1, DWORD PTR [rcx]

; 718  :     ref_row1 = _mm_cvtsi32_si128(*(int32_t*)&(ref[1 * ref_stride]));

	movsxd	rax, r8d
	vmovd	xmm0, DWORD PTR [rax+rcx]

; 719  :     ref_row2 = _mm_cvtsi32_si128(*(int32_t*)&(ref[2 * ref_stride]));
; 720  :     ref_row3 = _mm_cvtsi32_si128(*(int32_t*)&(ref[3 * ref_stride]));
; 721  : 
; 722  :     ref_row0 = _mm_unpacklo_epi32(ref_row0, ref_row1);

	vpunpckldq xmm2, xmm1, xmm0
	lea	eax, DWORD PTR [r8+r8]
	movsxd	rcx, eax
	lea	eax, DWORD PTR [r8+r8*2]
	vmovd	xmm1, DWORD PTR [rcx+rdx]
	movsxd	rcx, eax

; 723  :     ref_row1 = _mm_unpacklo_epi32(ref_row2, ref_row3);
; 724  :     ref_epi16 = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(ref_row0, ref_row1) );
; 725  : 
; 726  :     rec_row0 = _mm_cvtsi32_si128(*(int32_t*)&(rec[0 * rec_stride]));
; 727  :     rec_row1 = _mm_cvtsi32_si128(*(int32_t*)&(rec[1 * rec_stride]));

	movsxd	rax, r9d
	vmovd	xmm0, DWORD PTR [rcx+rdx]
	vpunpckldq xmm1, xmm1, xmm0
	vmovd	xmm0, DWORD PTR [rax+rsi]
	vpunpcklqdq xmm2, xmm2, xmm1
	vmovd	xmm1, DWORD PTR [rsi]
	vpmovzxbw ymm4, xmm2

; 728  :     rec_row2 = _mm_cvtsi32_si128(*(int32_t*)&(rec[2 * rec_stride]));
; 729  :     rec_row3 = _mm_cvtsi32_si128(*(int32_t*)&(rec[3 * rec_stride]));
; 730  : 
; 731  :     rec_row0 = _mm_unpacklo_epi32(rec_row0, rec_row1);

	vpunpckldq xmm2, xmm1, xmm0
	lea	eax, DWORD PTR [r9+r9]
	movsxd	rcx, eax
	lea	eax, DWORD PTR [r9+r9*2]
	vmovd	xmm1, DWORD PTR [rcx+rsi]
	movsxd	rcx, eax
	vmovd	xmm0, DWORD PTR [rcx+rsi]

; 732  :     rec_row1 = _mm_unpacklo_epi32(rec_row2, rec_row3);

	vpunpckldq xmm1, xmm1, xmm0

; 733  :     rec_epi16 = _mm256_cvtepu8_epi16(_mm_unpacklo_epi64(rec_row0, rec_row1) );

	vpunpcklqdq xmm2, xmm2, xmm1
	vpmovzxbw ymm3, xmm2

; 734  : 
; 735  :     diff = _mm256_sub_epi16(ref_epi16, rec_epi16);

	vpsubw	ymm0, ymm4, ymm3

; 736  :     ssd_part =  _mm256_madd_epi16(diff, diff);

	vpmaddwd ymm2, ymm0, ymm0

; 737  : 
; 738  :     sum = _mm_add_epi32(_mm256_castsi256_si128(ssd_part), _mm256_extracti128_si256(ssd_part, 1));

	vextracti128 xmm1, ymm2, 1
	vpaddd	xmm1, xmm1, xmm2
$LN28@pixels_cal:

; 768  :     break;
; 769  :   }
; 770  : }

	vpshufd	xmm0, xmm1, 78				; 0000004eH
	vpaddd	xmm2, xmm0, xmm1
	vpshufd	xmm1, xmm2, 17
	vpaddd	xmm0, xmm1, xmm2
	vmovd	eax, xmm0
	vzeroupper
	add	rsp, 72					; 00000048H
	pop	rdi
	pop	rsi
	ret	0
pixels_calc_ssd_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
dst$ = 8
ymm$ = 16
dst_stride$ = 24
scatter_ymm_4x8_8bit PROC

; 774  :   __m128i ymm_lo = _mm256_castsi256_si128(ymm);

	vmovdqu	ymm0, YMMWORD PTR [rdx]

; 775  :   __m128i ymm_hi = _mm256_extracti128_si256(ymm, 1);
; 776  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_lo); dst += dst_stride;

	vmovd	DWORD PTR [rcx], xmm0
	mov	edx, r8d
	add	rcx, rdx
	vextracti128 xmm1, ymm0, 1

; 777  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 1); dst += dst_stride;

	vpextrd	DWORD PTR [rcx], xmm0, 1
	add	rcx, rdx

; 778  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 2); dst += dst_stride;

	vpextrd	DWORD PTR [rcx], xmm0, 2
	add	rcx, rdx

; 779  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 3); dst += dst_stride;

	vpextrd	DWORD PTR [rcx], xmm0, 3
	add	rcx, rdx

; 780  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_hi); dst += dst_stride;

	vmovd	DWORD PTR [rcx], xmm1
	add	rcx, rdx

; 781  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 1); dst += dst_stride;

	vpextrd	DWORD PTR [rcx], xmm1, 1
	add	rcx, rdx

; 782  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 2); dst += dst_stride;

	vpextrd	DWORD PTR [rcx], xmm1, 2

; 783  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 3);

	vpextrd	DWORD PTR [rdx+rcx], xmm1, 3
	vzeroupper

; 784  : }

	ret	0
scatter_ymm_4x8_8bit ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
dst$ = 8
ymm$ = 16
dst_stride$ = 24
scatter_ymm_8x4_8bit PROC

; 788  :   __m256d ymm_as_m256d = _mm256_castsi256_pd(ymm);

	vmovupd	ymm0, YMMWORD PTR [rdx]

; 789  :   __m128d ymm_lo = _mm256_castpd256_pd128(ymm_as_m256d);
; 790  :   __m128d ymm_hi = _mm256_extractf128_pd(ymm_as_m256d, 1);
; 791  :   _mm_storel_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovlpd	QWORD PTR [rcx], xmm0
	mov	eax, r8d
	vextractf128 xmm1, ymm0, 1
	add	rcx, rax

; 792  :   _mm_storeh_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovhpd	QWORD PTR [rcx], xmm0
	add	rcx, rax

; 793  :   _mm_storel_pd((double*)dst, ymm_hi); dst += dst_stride;

	vmovlpd	QWORD PTR [rcx], xmm1

; 794  :   _mm_storeh_pd((double*)dst, ymm_hi);

	vmovhpd	QWORD PTR [rax+rcx], xmm1
	vzeroupper

; 795  : }

	ret	0
scatter_ymm_8x4_8bit ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
dst$ = 8
ymm$ = 16
dst_stride$ = 24
scatter_ymm_16x2_8bit PROC

; 799  :   __m128i ymm_lo = _mm256_castsi256_si128(ymm);

	vmovdqu	ymm0, YMMWORD PTR [rdx]

; 800  :   __m128i ymm_hi = _mm256_extracti128_si256(ymm, 1);
; 801  :   _mm_storeu_si128((__m128i *)dst, ymm_lo); dst += dst_stride;

	vmovdqu	XMMWORD PTR [rcx], xmm0
	vextracti128 xmm0, ymm0, 1
	mov	eax, r8d

; 802  :   _mm_storeu_si128((__m128i *)dst, ymm_hi);

	vmovdqu	XMMWORD PTR [rax+rcx], xmm0
	vzeroupper

; 803  : }

	ret	0
scatter_ymm_16x2_8bit ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
dst$ = 8
ymm$ = 16
dst_stride$ = 24
scatter_ymm_12x2_8bit PROC

; 807  :   __m256i mask_a = _mm256_setr_epi32(-1, -1, -1, 0, 0, 0, 0, 0);
; 808  :   __m256i mask_b = _mm256_setr_epi32(0, 0, 0, -1, -1, -1, 0, 0);
; 809  :   _mm256_maskstore_epi32((int32_t*)dst, mask_a, ymm); dst += dst_stride - 3 * 4;

	vmovdqu	ymm2, YMMWORD PTR [rdx]
	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000000000000000000000000000000000000ffffffffffffffffffffffff
	vmovdqu	ymm1, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffff000000000000000000000000
	vpmaskmovd YMMWORD PTR [rcx], ymm0, ymm2
	lea	eax, DWORD PTR [r8-12]

; 810  :   _mm256_maskstore_epi32((int32_t*)dst, mask_b, ymm);

	vpmaskmovd YMMWORD PTR [rax+rcx], ymm1, ymm2
	vzeroupper

; 811  : }

	ret	0
scatter_ymm_12x2_8bit ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
dst$ = 64
px_L0$ = 72
px_L1$ = 80
pu_w$ = 88
pu_h$ = 96
dst_stride$ = 104
bipred_average_px_px_template_avx2 PROC

; 819  : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rsi
	mov	QWORD PTR [rsp+24], rdi
	mov	QWORD PTR [rsp+32], r12
	push	r13
	push	r14
	push	r15
	sub	rsp, 32					; 00000020H

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	popcnt	eax, r9d
	cmp	eax, 1

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	mov	r12d, r9d
	mov	eax, DWORD PTR pu_h$[rsp]
	mov	edi, r9d
	sete	r13b
	mov	r15, r8
	imul	r12d, eax
	mov	r14, rdx
	mov	rsi, rcx
	mov	ebx, r12d
	and	ebx, 31

; 822  :   assert(!(pu_w == 4 && pu_h == 4) && "Branch for 4x4 not yet implemented.");

	cmp	r9d, 4
	jne	SHORT $LN38@bipred_ave
	cmp	eax, r9d
	jne	SHORT $LN38@bipred_ave
	mov	r8d, 822				; 00000336H
	lea	rdx, OFFSET FLAT:$SG4294951321
	lea	rcx, OFFSET FLAT:$SG4294951320
	call	QWORD PTR __imp__wassert
$LN38@bipred_ave:

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	test	ebx, ebx
	jne	$LN27@bipred_ave
	test	r13b, r13b
	je	$LN20@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r12d, r12d
	je	$LN18@bipred_ave
	mov	r13d, DWORD PTR dst_stride$[rsp]
	sub	r15, r14
	npad	2
$LL4@bipred_ave:

; 827  : 
; 828  :       int y = i / pu_w;
; 829  :       int x = i % pu_w;
; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [r15+r14]
	vpavgb	ymm1, ymm0, YMMWORD PTR [r14]
	xor	edx, edx
	mov	eax, ebx
	div	edi
	mov	ecx, eax

; 834  : 
; 835  :       switch (pu_w) {

	cmp	edi, 4
	je	$LN22@bipred_ave
	cmp	edi, 8
	je	SHORT $LN23@bipred_ave
	cmp	edi, 16
	je	SHORT $LN24@bipred_ave
	lea	eax, DWORD PTR [rdi-32]
	test	eax, -33				; ffffffdfH
	je	SHORT $LN25@bipred_ave

; 841  :         default:
; 842  :           assert(0 && "Unexpected block width.");

	mov	r8d, 842				; 0000034aH
	lea	rdx, OFFSET FLAT:$SG4294951316
	lea	rcx, OFFSET FLAT:$SG4294951315
	vzeroupper
	call	QWORD PTR __imp__wassert
	jmp	$LN2@bipred_ave
$LN25@bipred_ave:

; 839  :         case 32: // Same as case 64
; 840  :         case 64: _mm256_storeu_si256((__m256i *)&dst[y * dst_stride + x], avg); break;

	imul	ecx, r13d
	add	ecx, edx
	vmovdqu	YMMWORD PTR [rcx+rsi], ymm1
	jmp	$LN2@bipred_ave
$LN24@bipred_ave:

; 838  :         case 16: scatter_ymm_16x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	imul	ecx, r13d
	add	ecx, edx
	add	rcx, rsi

; 801  :   _mm_storeu_si128((__m128i *)dst, ymm_lo); dst += dst_stride;

	vmovdqu	XMMWORD PTR [rcx], xmm1
	vextracti128 XMMWORD PTR [r13+rcx], ymm1, 1

; 838  :         case 16: scatter_ymm_16x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	jmp	SHORT $LN2@bipred_ave
$LN23@bipred_ave:

; 837  :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	imul	ecx, r13d

; 790  :   __m128d ymm_hi = _mm256_extractf128_pd(ymm_as_m256d, 1);

	vextractf128 xmm0, ymm1, 1

; 837  :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	add	ecx, edx
	add	rcx, rsi

; 791  :   _mm_storel_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovlpd	QWORD PTR [rcx], xmm1
	add	rcx, r13

; 792  :   _mm_storeh_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovhpd	QWORD PTR [rcx], xmm1
	add	rcx, r13

; 793  :   _mm_storel_pd((double*)dst, ymm_hi); dst += dst_stride;

	vmovlpd	QWORD PTR [rcx], xmm0

; 794  :   _mm_storeh_pd((double*)dst, ymm_hi);

	vmovhpd	QWORD PTR [r13+rcx], xmm0

; 837  :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	jmp	SHORT $LN2@bipred_ave
$LN22@bipred_ave:

; 775  :   __m128i ymm_hi = _mm256_extracti128_si256(ymm, 1);

	vextracti128 xmm0, ymm1, 1

; 836  :         case  4: scatter_ymm_4x8_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	imul	ecx, r13d
	add	edx, ecx
	add	rdx, rsi

; 776  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_lo); dst += dst_stride;

	vmovd	DWORD PTR [rdx], xmm1
	add	rdx, r13

; 777  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 1); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm1, 1
	add	rdx, r13

; 778  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 2); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm1, 2
	add	rdx, r13

; 779  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 3); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm1, 3
	add	rdx, r13

; 780  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_hi); dst += dst_stride;

	vmovd	DWORD PTR [rdx], xmm0
	add	rdx, r13

; 781  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 1); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm0, 1
	add	rdx, r13

; 782  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 2); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm0, 2

; 783  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 3);

	vpextrd	DWORD PTR [r13+rdx], xmm0, 3
$LN2@bipred_ave:

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	add	ebx, 32					; 00000020H
	add	r14, 32					; 00000020H
	cmp	ebx, r12d
	jb	$LL4@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LN18@bipred_ave
$LN20@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r12d, r12d
	je	$LN18@bipred_ave

; 848  : 
; 849  :       int y = i / pu_w;
; 850  :       int x = i % pu_w;
; 851  : 
; 852  :       // Last 64 bits of the 256 are not used to simplify the loop
; 853  :       __m256i mask      = _mm256_setr_epi64x(-1, -1, -1, 0);

	vmovdqu	ymm3, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	mov	r13d, DWORD PTR dst_stride$[rsp]
	npad	3
$LL9@bipred_ave:

; 854  :       __m256i sample_L0 = _mm256_maskload_epi64((const long long*)&px_L0[i], mask);
; 855  :       __m256i sample_L1 = _mm256_maskload_epi64((const long long*)&px_L1[i], mask);

	movsxd	rcx, ebx
	xor	edx, edx
	mov	eax, ebx
	div	edi
	vpmaskmovq ymm1, ymm3, YMMWORD PTR [rcx+r15]
	vpmaskmovq ymm0, ymm3, YMMWORD PTR [rcx+r14]

; 856  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vpavgb	ymm2, ymm0, ymm1

; 857  : 
; 858  :       switch (pu_w) {

	cmp	edi, 12
	je	SHORT $LN29@bipred_ave
	cmp	edi, 24
	je	SHORT $LN30@bipred_ave
	cmp	edi, 48					; 00000030H
	je	SHORT $LN30@bipred_ave

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	vzeroupper
	call	QWORD PTR __imp__wassert
	vmovdqu	ymm3, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	jmp	SHORT $LN7@bipred_ave
$LN30@bipred_ave:

; 860  :         case 24: // Same as case 48
; 861  :         case 48: _mm256_maskstore_epi64((long long*)&dst[y * dst_stride + x], mask, avg); break;

	imul	eax, r13d
	add	eax, edx
	vpmaskmovq YMMWORD PTR [rax+rsi], ymm3, ymm2
	jmp	SHORT $LN7@bipred_ave
$LN29@bipred_ave:

; 807  :   __m256i mask_a = _mm256_setr_epi32(-1, -1, -1, 0, 0, 0, 0, 0);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000000000000000000000000000000000000ffffffffffffffffffffffff

; 808  :   __m256i mask_b = _mm256_setr_epi32(0, 0, 0, -1, -1, -1, 0, 0);

	vmovdqu	ymm1, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffff000000000000000000000000

; 859  :         case 12: scatter_ymm_12x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	imul	eax, r13d
	add	edx, eax

; 809  :   _mm256_maskstore_epi32((int32_t*)dst, mask_a, ymm); dst += dst_stride - 3 * 4;

	lea	eax, DWORD PTR [r13-12]

; 859  :         case 12: scatter_ymm_12x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	add	rdx, rsi

; 809  :   _mm256_maskstore_epi32((int32_t*)dst, mask_a, ymm); dst += dst_stride - 3 * 4;

	vpmaskmovd YMMWORD PTR [rdx], ymm0, ymm2

; 810  :   _mm256_maskstore_epi32((int32_t*)dst, mask_b, ymm);

	vpmaskmovd YMMWORD PTR [rax+rdx], ymm1, ymm2
$LN7@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	add	ebx, 24
	cmp	ebx, r12d
	jb	$LL9@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LN18@bipred_ave
$LN27@bipred_ave:

; 868  :     // 8x2, 8x6, 6x8 blocks (and maybe 2x8 in the future)
; 869  :     switch (pu_w) {

	cmp	edi, 6
	je	SHORT $LN33@bipred_ave
	cmp	edi, 8
	je	SHORT $LN32@bipred_ave

; 901  :         }
; 902  :         break;
; 903  :       default:
; 904  :         assert(0 && "Unexpected block width.");

	mov	r8d, 904				; 00000388H
	lea	rdx, OFFSET FLAT:$SG4294951312
	lea	rcx, OFFSET FLAT:$SG4294951311
	call	QWORD PTR __imp__wassert
	jmp	$LN18@bipred_ave
$LN32@bipred_ave:

; 870  :       __m128i sample_L0, sample_L1, avg;
; 871  :       case 8: // 8x2, 8x6
; 872  :         for (int i = 0; i < pu_w * pu_h; i += 16) {

	xor	ebx, ebx
	test	r12d, r12d
	je	$LN18@bipred_ave
	mov	r8d, DWORD PTR dst_stride$[rsp]
	sub	r14, r15
$LL16@bipred_ave:

; 873  : 
; 874  :           int y = i / pu_w;
; 875  : 
; 876  :           sample_L0 = _mm_loadu_si128((__m128i*)&px_L0[i]);
; 877  :           sample_L1 = _mm_loadu_si128((__m128i*)&px_L1[i]);
; 878  :           avg       = _mm_avg_epu8(sample_L0, sample_L1);

	vmovdqu	xmm0, XMMWORD PTR [r14+r15]
	vpavgb	xmm1, xmm0, XMMWORD PTR [r15]
	mov	edx, ebx
	lea	r15, QWORD PTR [r15+16]
	shr	edx, 3
	add	ebx, 16

; 879  :           _mm_storel_epi64((__m128i*)&dst[y * dst_stride], avg);

	mov	eax, edx
	imul	eax, r8d
	vmovq	QWORD PTR [rax+rsi], xmm1

; 880  :           _mm_storeh_pd((double*)&dst[(y + 1) * dst_stride], _mm_castsi128_pd(avg));

	lea	eax, DWORD PTR [rdx+1]
	imul	eax, r8d
	vmovhpd	QWORD PTR [rax+rsi], xmm1
	cmp	ebx, r12d
	jb	SHORT $LL16@bipred_ave

; 881  :         }
; 882  :         break;

	jmp	SHORT $LN18@bipred_ave
$LN33@bipred_ave:

; 883  :       case 6: // 6x8
; 884  :         for (int i = 0; i < pu_w * pu_h; i += 12) {

	xor	ebx, ebx
	test	r12d, r12d
	je	SHORT $LN18@bipred_ave

; 887  : 
; 888  :           __m128i mask      = _mm_setr_epi32(-1, -1, -1, 0);

	vmovdqu	xmm3, XMMWORD PTR __xmm@00000000ffffffffffffffffffffffff
	mov	r9d, DWORD PTR dst_stride$[rsp]
	npad	5
$LL19@bipred_ave:

; 885  : 
; 886  :           int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH
	mul	ebx

; 889  :           __m128i sample_L0 = _mm_maskload_epi32((const int*)(&px_L0[i]), mask);

	movsxd	rax, ebx
	add	ebx, 12
	mov	r8d, edx
	shr	r8d, 2
	vpmaskmovd xmm1, xmm3, XMMWORD PTR [rax+r14]

; 890  :           __m128i sample_L1 = _mm_maskload_epi32((const int*)(&px_L1[i]), mask);

	vpmaskmovd xmm0, xmm3, XMMWORD PTR [rax+r15]

; 891  :           __m128i avg       = _mm_avg_epu8(sample_L0, sample_L1);
; 892  : 
; 893  :           uint32_t elements_0123 = _mm_cvtsi128_si32(avg);
; 894  :           uint16_t elements_45   = _mm_extract_epi16(avg, 2);
; 895  :           uint16_t elements_67   = _mm_extract_epi16(avg, 3);
; 896  :           uint32_t elements_89ab = _mm_extract_epi32(avg, 2);
; 897  :           *(uint32_t*)&dst[(y + 0) * dst_stride + 0] = elements_0123;

	mov	eax, r8d

; 898  :           *(uint16_t*)&dst[(y + 0) * dst_stride + 4] = elements_45;
; 899  :           *(uint16_t*)&dst[(y + 1) * dst_stride + 0] = elements_67;

	lea	ecx, DWORD PTR [r8+1]
	imul	eax, r9d
	vpavgb	xmm2, xmm1, xmm0
	imul	ecx, r9d
	vmovd	DWORD PTR [rax+rsi], xmm2
	add	eax, 4
	vpextrw	WORD PTR [rax+rsi], xmm2, 2

; 900  :           *(uint32_t*)&dst[(y + 1) * dst_stride + 2] = elements_89ab;

	lea	eax, DWORD PTR [rcx+2]
	vpextrw	WORD PTR [rcx+rsi], xmm2, 3
	vpextrd	DWORD PTR [rax+rsi], xmm2, 2
	cmp	ebx, r12d
	jb	SHORT $LL19@bipred_ave
$LN18@bipred_ave:
	vzeroupper

; 905  :         break;
; 906  :     }
; 907  :   }
; 908  : }

	mov	rbx, QWORD PTR [rsp+64]
	mov	rsi, QWORD PTR [rsp+72]
	mov	rdi, QWORD PTR [rsp+80]
	mov	r12, QWORD PTR [rsp+88]
	add	rsp, 32					; 00000020H
	pop	r15
	pop	r14
	pop	r13
	ret	0
bipred_average_px_px_template_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\picture-generic.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
dst$ = 64
px_L0$ = 72
px_L1$ = 80
pu_w$ = 88
pu_h$ = 96
dst_stride$ = 104
bipred_average_px_px_avx2 PROC

; 916  : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rsi
	mov	QWORD PTR [rsp+24], rdi
	push	r12
	push	r14
	push	r15
	sub	rsp, 32					; 00000020H
	mov	rdi, r8
	mov	r8d, DWORD PTR pu_h$[rsp]
	mov	r11d, r9d
	mov	rbx, rdx
	mov	rsi, rcx

; 917  :   // Use scalar code for yet unoptimized block sizes (4x4, 2x8)
; 918  :   if (!(pu_w == 4 && pu_h == 4) && pu_w > 2) {

	cmp	r9d, 4
	jne	SHORT $LN9@bipred_ave
	cmp	r8d, r9d
	je	$LN7@bipred_ave
	add	r11d, -4				; fffffffcH
	jmp	SHORT $LN687@bipred_ave
$LN9@bipred_ave:
	cmp	r11d, 2
	jbe	$LN7@bipred_ave

; 919  :     switch (pu_w) {

	add	r11d, -4				; fffffffcH
	cmp	r11d, 60				; 0000003cH
	ja	$LN19@bipred_ave
$LN687@bipred_ave:
	lea	rdx, OFFSET FLAT:__ImageBase
	movzx	eax, BYTE PTR $LN688@bipred_ave[rdx+r11]
	mov	ecx, DWORD PTR $LN689@bipred_ave[rdx+rax*4]
	add	rcx, rdx
	jmp	rcx
$LN10@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 4

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	r14d, DWORD PTR [r8*4]
	popcnt	eax, eax
	cmp	eax, 1
	mov	r15d, r14d
	sete	r12b
	and	r15d, 31

; 822  :   assert(!(pu_w == 4 && pu_h == 4) && "Branch for 4x4 not yet implemented.");

	cmp	r8d, 4
	jne	SHORT $LN62@bipred_ave
	mov	r8d, 822				; 00000336H
	lea	rdx, OFFSET FLAT:$SG4294951321
	lea	rcx, OFFSET FLAT:$SG4294951320
	call	QWORD PTR __imp__wassert
$LN62@bipred_ave:

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	test	r15d, r15d
	jne	$LN48@bipred_ave
	test	r12b, r12b
	je	$LN41@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	xor	ecx, ecx
	test	r14d, r14d
	je	$LL5@bipred_ave
	mov	r10d, DWORD PTR dst_stride$[rsp]
	sub	rdi, rbx
	npad	9
$LL25@bipred_ave:

; 827  : 
; 828  :       int y = i / pu_w;
; 829  :       int x = i % pu_w;
; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rdi+rbx]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rbx]
	mov	r8d, ecx
	lea	rbx, QWORD PTR [rbx+32]
	shr	r8d, 2
	mov	eax, ecx

; 834  : 
; 835  :       switch (pu_w) {
; 836  :         case  4: scatter_ymm_4x8_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	imul	r8d, r10d
	and	eax, 3

; 775  :   __m128i ymm_hi = _mm256_extracti128_si256(ymm, 1);

	vextracti128 xmm0, ymm1, 1

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	add	ecx, 32					; 00000020H

; 834  : 
; 835  :       switch (pu_w) {
; 836  :         case  4: scatter_ymm_4x8_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	add	r8d, eax
	add	r8, rsi

; 776  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_lo); dst += dst_stride;

	vmovd	DWORD PTR [r8], xmm1
	add	r8, r10

; 777  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 1); dst += dst_stride;

	vpextrd	DWORD PTR [r8], xmm1, 1
	add	r8, r10

; 778  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 2); dst += dst_stride;

	vpextrd	DWORD PTR [r8], xmm1, 2
	add	r8, r10

; 779  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 3); dst += dst_stride;

	vpextrd	DWORD PTR [r8], xmm1, 3
	add	r8, r10

; 780  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_hi); dst += dst_stride;

	vmovd	DWORD PTR [r8], xmm0
	add	r8, r10

; 781  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 1); dst += dst_stride;

	vpextrd	DWORD PTR [r8], xmm0, 1
	add	r8, r10

; 782  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 2); dst += dst_stride;

	vpextrd	DWORD PTR [r8], xmm0, 2

; 783  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 3);

	vpextrd	DWORD PTR [r8+r10], xmm0, 3

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	cmp	ecx, r14d
	jb	SHORT $LL25@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN41@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r14d, r14d
	je	$LL5@bipred_ave
	lea	ecx, DWORD PTR [r14-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
$LL30@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL30@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LL5@bipred_ave
$LN48@bipred_ave:

; 946  :     }
; 947  :   }
; 948  : }

	mov	r8d, 904				; 00000388H
	lea	rdx, OFFSET FLAT:$SG4294951312
	lea	rcx, OFFSET FLAT:$SG4294951311
	call	QWORD PTR __imp__wassert
	jmp	$LL5@bipred_ave
$LN11@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 8

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	r9d, DWORD PTR [r8*8]
	popcnt	eax, eax
	cmp	eax, 1
	sete	al
	test	r9b, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN100@bipred_ave
	test	al, al
	je	SHORT $LN93@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	xor	ecx, ecx
	test	r9d, r9d
	je	$LL5@bipred_ave
	mov	r10d, DWORD PTR dst_stride$[rsp]
	sub	rdi, rbx
	npad	3
$LL77@bipred_ave:

; 827  : 
; 828  :       int y = i / pu_w;
; 829  :       int x = i % pu_w;
; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rbx+rdi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rbx]
	mov	eax, ecx
	lea	rbx, QWORD PTR [rbx+32]
	and	eax, 7
	mov	edx, ecx
	shr	edx, 3
	add	ecx, 32					; 00000020H

; 837  :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	imul	edx, r10d

; 790  :   __m128d ymm_hi = _mm256_extractf128_pd(ymm_as_m256d, 1);

	vextractf128 xmm0, ymm1, 1

; 837  :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	add	eax, edx
	add	rax, rsi

; 791  :   _mm_storel_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovlpd	QWORD PTR [rax], xmm1
	add	rax, r10

; 792  :   _mm_storeh_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovhpd	QWORD PTR [rax], xmm1
	add	rax, r10

; 793  :   _mm_storel_pd((double*)dst, ymm_hi); dst += dst_stride;

	vmovlpd	QWORD PTR [rax], xmm0

; 794  :   _mm_storeh_pd((double*)dst, ymm_hi);

	vmovhpd	QWORD PTR [rax+r10], xmm0

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	cmp	ecx, r9d
	jb	SHORT $LL77@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN93@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r9d, r9d
	je	$LL5@bipred_ave
	lea	ecx, DWORD PTR [r9-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
	npad	11
$LL82@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL82@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LL5@bipred_ave
$LN100@bipred_ave:

; 868  :     // 8x2, 8x6, 6x8 blocks (and maybe 2x8 in the future)
; 869  :     switch (pu_w) {
; 870  :       __m128i sample_L0, sample_L1, avg;
; 871  :       case 8: // 8x2, 8x6
; 872  :         for (int i = 0; i < pu_w * pu_h; i += 16) {

	xor	ecx, ecx
	test	r9d, r9d
	je	$LL5@bipred_ave
	mov	r10d, DWORD PTR dst_stride$[rsp]
	sub	rbx, rdi
	npad	8
$LL89@bipred_ave:

; 873  : 
; 874  :           int y = i / pu_w;
; 875  : 
; 876  :           sample_L0 = _mm_loadu_si128((__m128i*)&px_L0[i]);
; 877  :           sample_L1 = _mm_loadu_si128((__m128i*)&px_L1[i]);
; 878  :           avg       = _mm_avg_epu8(sample_L0, sample_L1);

	vmovdqu	xmm0, XMMWORD PTR [rbx+rdi]
	vpavgb	xmm1, xmm0, XMMWORD PTR [rdi]
	mov	r8d, ecx
	lea	rdi, QWORD PTR [rdi+16]
	shr	r8d, 3
	add	ecx, 16

; 879  :           _mm_storel_epi64((__m128i*)&dst[y * dst_stride], avg);

	mov	eax, r8d
	imul	eax, r10d
	vmovq	QWORD PTR [rax+rsi], xmm1

; 880  :           _mm_storeh_pd((double*)&dst[(y + 1) * dst_stride], _mm_castsi128_pd(avg));

	lea	eax, DWORD PTR [r8+1]
	imul	eax, r10d
	vmovhpd	QWORD PTR [rax+rsi], xmm1
	cmp	ecx, r9d
	jb	SHORT $LL89@bipred_ave

; 920  :       case  4: bipred_average_px_px_template_avx2(dst, px_L0, px_L1,  4, pu_h, dst_stride); break;
; 921  :       case  8: bipred_average_px_px_template_avx2(dst, px_L0, px_L1,  8, pu_h, dst_stride); break;

	jmp	$LL5@bipred_ave
$LN12@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 16
	popcnt	eax, eax
	cmp	eax, 1
	sete	cl

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	imul	eax, r8d, -16				; fffffff0H
	test	al, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN48@bipred_ave
	shl	r8d, 4
	test	cl, cl
	je	SHORT $LN145@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	xor	ecx, ecx
	test	r8d, r8d
	je	$LL5@bipred_ave
	mov	r9d, DWORD PTR dst_stride$[rsp]
	sub	rdi, rbx
$LL129@bipred_ave:

; 827  : 
; 828  :       int y = i / pu_w;
; 829  :       int x = i % pu_w;
; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rbx+rdi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rbx]
	mov	eax, ecx
	lea	rbx, QWORD PTR [rbx+32]
	and	eax, 15
	mov	edx, ecx
	shr	edx, 4
	add	ecx, 32					; 00000020H

; 838  :         case 16: scatter_ymm_16x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	imul	edx, r9d
	add	eax, edx
	add	rax, rsi

; 801  :   _mm_storeu_si128((__m128i *)dst, ymm_lo); dst += dst_stride;

	vmovdqu	XMMWORD PTR [rax], xmm1
	vextracti128 XMMWORD PTR [rax+r9], ymm1, 1

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	cmp	ecx, r8d
	jb	SHORT $LL129@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN145@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r8d, r8d
	je	$LL5@bipred_ave
	lea	ecx, DWORD PTR [r8-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
$LL134@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL134@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LL5@bipred_ave
$LN13@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 32					; 00000020H
	popcnt	eax, eax
	cmp	eax, 1
	sete	al
	shl	r8d, 5

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	test	al, al
	je	SHORT $LN652@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	xor	ecx, ecx
	test	r8d, r8d
	je	$LL5@bipred_ave
	mov	r9d, DWORD PTR dst_stride$[rsp]
	sub	rdi, rbx
$LL181@bipred_ave:

; 827  : 
; 828  :       int y = i / pu_w;
; 829  :       int x = i % pu_w;
; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rbx+rdi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rbx]
	mov	edx, ecx
	lea	rbx, QWORD PTR [rbx+32]
	shr	edx, 5
	mov	eax, ecx

; 839  :         case 32: // Same as case 64
; 840  :         case 64: _mm256_storeu_si256((__m256i *)&dst[y * dst_stride + x], avg); break;

	imul	edx, r9d
	and	eax, 31
	add	ecx, 32					; 00000020H
	add	edx, eax
	vmovdqu	YMMWORD PTR [rdx+rsi], ymm1
	cmp	ecx, r8d
	jb	SHORT $LL181@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN652@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r8d, r8d
	je	$LL5@bipred_ave
	lea	ecx, DWORD PTR [r8-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
	npad	6
$LL186@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL186@bipred_ave

; 922  :       case 16: bipred_average_px_px_template_avx2(dst, px_L0, px_L1, 16, pu_h, dst_stride); break;
; 923  :       case 32: bipred_average_px_px_template_avx2(dst, px_L0, px_L1, 32, pu_h, dst_stride); break;

	jmp	$LL5@bipred_ave
$LN14@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 64					; 00000040H
	popcnt	eax, eax
	cmp	eax, 1
	sete	al
	shl	r8d, 6

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	test	al, al
	je	SHORT $LN658@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	xor	ecx, ecx
	test	r8d, r8d
	je	$LL5@bipred_ave
	mov	r9d, DWORD PTR dst_stride$[rsp]
	sub	rdi, rbx
	npad	1
$LL233@bipred_ave:

; 827  : 
; 828  :       int y = i / pu_w;
; 829  :       int x = i % pu_w;
; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rbx+rdi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rbx]
	mov	edx, ecx
	lea	rbx, QWORD PTR [rbx+32]
	shr	edx, 6
	mov	eax, ecx

; 839  :         case 32: // Same as case 64
; 840  :         case 64: _mm256_storeu_si256((__m256i *)&dst[y * dst_stride + x], avg); break;

	imul	edx, r9d
	and	eax, 63					; 0000003fH
	add	ecx, 32					; 00000020H
	add	edx, eax
	vmovdqu	YMMWORD PTR [rdx+rsi], ymm1
	cmp	ecx, r8d
	jb	SHORT $LL233@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN658@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r8d, r8d
	je	$LL5@bipred_ave
	lea	ecx, DWORD PTR [r8-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
	npad	7
$LL238@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL238@bipred_ave

; 924  :       case 64: bipred_average_px_px_template_avx2(dst, px_L0, px_L1, 64, pu_h, dst_stride); break;

	jmp	$LL5@bipred_ave
$LN15@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 6
	popcnt	eax, eax
	cmp	eax, 1

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	eax, DWORD PTR [r8+r8*2]
	lea	r11d, DWORD PTR [rax+rax]
	sete	cl
	test	r11b, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN308@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r11d, r11d
	je	$LL5@bipred_ave

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	test	cl, cl
	je	SHORT $LN301@bipred_ave
	lea	ebx, DWORD PTR [r11-1]
	shr	ebx, 5
	inc	ebx
	npad	4
$LL285@bipred_ave:

; 841  :         default:
; 842  :           assert(0 && "Unexpected block width.");

	mov	r8d, 842				; 0000034aH
	lea	rdx, OFFSET FLAT:$SG4294951316
	lea	rcx, OFFSET FLAT:$SG4294951315
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL285@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN301@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	lea	ecx, DWORD PTR [r11-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
	npad	10
$LL290@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL290@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LL5@bipred_ave
$LN308@bipred_ave:

; 881  :         }
; 882  :         break;
; 883  :       case 6: // 6x8
; 884  :         for (int i = 0; i < pu_w * pu_h; i += 12) {

	xor	ecx, ecx
	test	r11d, r11d
	je	$LL5@bipred_ave
	vmovdqu	xmm3, XMMWORD PTR __xmm@00000000ffffffffffffffffffffffff

; 887  : 
; 888  :           __m128i mask      = _mm_setr_epi32(-1, -1, -1, 0);

	mov	r14d, DWORD PTR dst_stride$[rsp]
	npad	3
$LL300@bipred_ave:

; 885  : 
; 886  :           int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx

; 889  :           __m128i sample_L0 = _mm_maskload_epi32((const int*)(&px_L0[i]), mask);

	movsxd	rax, ecx
	add	ecx, 12
	mov	r10d, edx
	shr	r10d, 2
	vpmaskmovd xmm1, xmm3, XMMWORD PTR [rax+rbx]

; 890  :           __m128i sample_L1 = _mm_maskload_epi32((const int*)(&px_L1[i]), mask);

	vpmaskmovd xmm0, xmm3, XMMWORD PTR [rax+rdi]

; 891  :           __m128i avg       = _mm_avg_epu8(sample_L0, sample_L1);
; 892  : 
; 893  :           uint32_t elements_0123 = _mm_cvtsi128_si32(avg);
; 894  :           uint16_t elements_45   = _mm_extract_epi16(avg, 2);
; 895  :           uint16_t elements_67   = _mm_extract_epi16(avg, 3);
; 896  :           uint32_t elements_89ab = _mm_extract_epi32(avg, 2);
; 897  :           *(uint32_t*)&dst[(y + 0) * dst_stride + 0] = elements_0123;

	mov	eax, r10d

; 898  :           *(uint16_t*)&dst[(y + 0) * dst_stride + 4] = elements_45;
; 899  :           *(uint16_t*)&dst[(y + 1) * dst_stride + 0] = elements_67;

	lea	edx, DWORD PTR [r10+1]
	imul	eax, r14d
	vpavgb	xmm2, xmm1, xmm0
	imul	edx, r14d
	vmovd	DWORD PTR [rax+rsi], xmm2
	add	eax, 4
	vpextrw	WORD PTR [rax+rsi], xmm2, 2

; 900  :           *(uint32_t*)&dst[(y + 1) * dst_stride + 2] = elements_89ab;

	lea	eax, DWORD PTR [rdx+2]
	vpextrw	WORD PTR [rdx+rsi], xmm2, 3
	vpextrd	DWORD PTR [rax+rsi], xmm2, 2
	cmp	ecx, r11d
	jb	SHORT $LL300@bipred_ave

; 925  : 
; 926  :       case  6: bipred_average_px_px_template_avx2(dst, px_L0, px_L1,  6, pu_h, dst_stride); break;

	jmp	$LL5@bipred_ave
$LN16@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 12
	popcnt	eax, eax
	cmp	eax, 1

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	eax, DWORD PTR [r8+r8*2]
	lea	r9d, DWORD PTR [rax*4]
	sete	cl
	test	r9b, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN48@bipred_ave
	test	cl, cl
	je	SHORT $LN353@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r9d, r9d
	je	$LL5@bipred_ave
	lea	ebx, DWORD PTR [r9-1]
	shr	ebx, 5
	inc	ebx
	npad	8
$LL337@bipred_ave:

; 841  :         default:
; 842  :           assert(0 && "Unexpected block width.");

	mov	r8d, 842				; 0000034aH
	lea	rdx, OFFSET FLAT:$SG4294951316
	lea	rcx, OFFSET FLAT:$SG4294951315
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL337@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN353@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	xor	ecx, ecx
	test	r9d, r9d
	je	$LL5@bipred_ave

; 851  : 
; 852  :       // Last 64 bits of the 256 are not used to simplify the loop
; 853  :       __m256i mask      = _mm256_setr_epi64x(-1, -1, -1, 0);

	mov	r10d, DWORD PTR dst_stride$[rsp]
	vmovdqu	ymm3, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	ymm4, YMMWORD PTR __ymm@0000000000000000000000000000000000000000ffffffffffffffffffffffff
	vmovdqu	ymm5, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffff000000000000000000000000
	lea	r11d, DWORD PTR [r10-12]
	npad	15
$LL342@bipred_ave:

; 850  :       int x = i % pu_w;

	mov	r8d, ecx
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 3
	lea	eax, DWORD PTR [rdx+rdx*2]

; 854  :       __m256i sample_L0 = _mm256_maskload_epi64((const long long*)&px_L0[i], mask);
; 855  :       __m256i sample_L1 = _mm256_maskload_epi64((const long long*)&px_L1[i], mask);
; 856  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);
; 857  : 
; 858  :       switch (pu_w) {
; 859  :         case 12: scatter_ymm_12x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	imul	edx, r10d
	shl	eax, 2
	sub	r8d, eax
	movsxd	rax, ecx
	add	ecx, 24
	vpmaskmovq ymm1, ymm3, YMMWORD PTR [rax+rdi]
	vpmaskmovq ymm0, ymm3, YMMWORD PTR [rax+rbx]
	vpavgb	ymm2, ymm0, ymm1
	lea	eax, DWORD PTR [r8+rdx]
	add	rax, rsi

; 809  :   _mm256_maskstore_epi32((int32_t*)dst, mask_a, ymm); dst += dst_stride - 3 * 4;

	vpmaskmovd YMMWORD PTR [rax], ymm4, ymm2

; 810  :   _mm256_maskstore_epi32((int32_t*)dst, mask_b, ymm);

	vpmaskmovd YMMWORD PTR [rax+r11], ymm5, ymm2

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	cmp	ecx, r9d
	jb	SHORT $LL342@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LL5@bipred_ave
$LN17@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 24
	popcnt	eax, eax
	cmp	eax, 1
	sete	cl

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	imul	eax, r8d, -8				; fffffff8H
	test	al, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN48@bipred_ave
	lea	r9d, DWORD PTR [r8+r8*2]
	shl	r9d, 3
	test	cl, cl
	je	SHORT $LN405@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r9d, r9d
	je	$LL5@bipred_ave
	lea	ebx, DWORD PTR [r9-1]
	shr	ebx, 5
	inc	ebx
	npad	11
$LL389@bipred_ave:

; 841  :         default:
; 842  :           assert(0 && "Unexpected block width.");

	mov	r8d, 842				; 0000034aH
	lea	rdx, OFFSET FLAT:$SG4294951316
	lea	rcx, OFFSET FLAT:$SG4294951315
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL389@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN405@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	xor	ecx, ecx
	test	r9d, r9d
	je	$LL5@bipred_ave
	vmovdqu	ymm3, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff

; 851  : 
; 852  :       // Last 64 bits of the 256 are not used to simplify the loop
; 853  :       __m256i mask      = _mm256_setr_epi64x(-1, -1, -1, 0);

	mov	r10d, DWORD PTR dst_stride$[rsp]
	npad	3
$LL394@bipred_ave:

; 848  : 
; 849  :       int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH

; 850  :       int x = i % pu_w;

	mov	r8d, ecx
	mul	ecx
	shr	edx, 4
	lea	eax, DWORD PTR [rdx+rdx*2]

; 860  :         case 24: // Same as case 48
; 861  :         case 48: _mm256_maskstore_epi64((long long*)&dst[y * dst_stride + x], mask, avg); break;

	imul	edx, r10d
	shl	eax, 3
	sub	r8d, eax
	movsxd	rax, ecx
	add	ecx, 24
	add	edx, r8d
	vpmaskmovq ymm1, ymm3, YMMWORD PTR [rdi+rax]
	vpmaskmovq ymm0, ymm3, YMMWORD PTR [rbx+rax]
	vpavgb	ymm2, ymm0, ymm1
	vpmaskmovq YMMWORD PTR [rdx+rsi], ymm3, ymm2
	cmp	ecx, r9d
	jb	SHORT $LL394@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LL5@bipred_ave
$LN18@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 48					; 00000030H
	popcnt	eax, eax
	cmp	eax, 1
	sete	cl

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	imul	eax, r8d, -16				; fffffff0H
	test	al, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN48@bipred_ave
	lea	r9d, DWORD PTR [r8+r8*2]
	shl	r9d, 4
	test	cl, cl
	je	SHORT $LN457@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r9d, r9d
	je	$LL5@bipred_ave
	lea	ebx, DWORD PTR [r9-1]
	shr	ebx, 5
	inc	ebx
	npad	4
$LL441@bipred_ave:

; 841  :         default:
; 842  :           assert(0 && "Unexpected block width.");

	mov	r8d, 842				; 0000034aH
	lea	rdx, OFFSET FLAT:$SG4294951316
	lea	rcx, OFFSET FLAT:$SG4294951315
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL441@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN457@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	xor	ecx, ecx
	test	r9d, r9d
	je	$LL5@bipred_ave
	vmovdqu	ymm3, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff

; 851  : 
; 852  :       // Last 64 bits of the 256 are not used to simplify the loop
; 853  :       __m256i mask      = _mm256_setr_epi64x(-1, -1, -1, 0);

	mov	r10d, DWORD PTR dst_stride$[rsp]
	npad	3
$LL446@bipred_ave:

; 848  : 
; 849  :       int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH

; 850  :       int x = i % pu_w;

	mov	r8d, ecx
	mul	ecx
	shr	edx, 5
	lea	eax, DWORD PTR [rdx+rdx*2]

; 860  :         case 24: // Same as case 48
; 861  :         case 48: _mm256_maskstore_epi64((long long*)&dst[y * dst_stride + x], mask, avg); break;

	imul	edx, r10d
	shl	eax, 4
	sub	r8d, eax
	movsxd	rax, ecx
	add	ecx, 24
	add	edx, r8d
	vpmaskmovq ymm1, ymm3, YMMWORD PTR [rax+rdi]
	vpmaskmovq ymm0, ymm3, YMMWORD PTR [rax+rbx]
	vpavgb	ymm2, ymm0, ymm1
	vpmaskmovq YMMWORD PTR [rdx+rsi], ymm3, ymm2
	cmp	ecx, r9d
	jb	SHORT $LL446@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LL5@bipred_ave
$LN19@bipred_ave:

; 927  :       case 12: bipred_average_px_px_template_avx2(dst, px_L0, px_L1, 12, pu_h, dst_stride); break;
; 928  :       case 24: bipred_average_px_px_template_avx2(dst, px_L0, px_L1, 24, pu_h, dst_stride); break;
; 929  :       case 48: bipred_average_px_px_template_avx2(dst, px_L0, px_L1, 48, pu_h, dst_stride); break;
; 930  :       default:
; 931  :         assert(0 && "Unexpected block width.");

	mov	r8d, 931				; 000003a3H
	lea	rdx, OFFSET FLAT:$SG4294951310
	lea	rcx, OFFSET FLAT:$SG4294951309
	call	QWORD PTR __imp__wassert

; 932  :         break;
; 933  :     }
; 934  :   } else {

	jmp	SHORT $LL5@bipred_ave
$LN7@bipred_ave:

; 935  :     int32_t shift = 15 - KVZ_BIT_DEPTH; // TODO: defines
; 936  :     int32_t offset = 1 << (shift - 1);
; 937  : 
; 938  :     for (int i = 0; i < pu_w * pu_h; ++i)

	mov	r14d, r11d
	xor	ecx, ecx
	imul	r14d, r8d
	test	r14d, r14d
	je	SHORT $LL5@bipred_ave
	mov	r15d, DWORD PTR dst_stride$[rsp]
	sub	rbx, rdi
	npad	11
$LL6@bipred_ave:

; 939  :     {
; 940  :       int y = i / pu_w;
; 941  :       int x = i % pu_w;
; 942  :       int16_t sample_L0 = px_L0[i] << (14 - KVZ_BIT_DEPTH);
; 943  :       int16_t sample_L1 = px_L1[i] << (14 - KVZ_BIT_DEPTH);
; 944  :       int32_t rounded = (sample_L0 + sample_L1 + offset) >> shift;

	movzx	r10d, BYTE PTR [rdi]
	xor	edx, edx
	movzx	r9d, BYTE PTR [rdi+rbx]
	mov	eax, ecx
	shl	r10d, 6
	add	r10d, 64				; 00000040H
	shl	r9d, 6
	add	r10d, r9d
	shr	r10d, 7
	div	r11d
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\picture-generic.c

; 69   :   if (value & ~PIXEL_MAX) {

	test	r10d, -256				; ffffff00H
	je	SHORT $LN491@bipred_ave

; 70   :     int32_t temp = (-value) >> 31;

	neg	r10d
	sar	r10d, 31
$LN491@bipred_ave:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 945  :       dst[y * dst_stride + x] = kvz_fast_clip_32bit_to_pixel(rounded);

	imul	eax, r15d
	inc	ecx
	inc	rdi
	add	eax, edx
	mov	BYTE PTR [rax+rsi], r10b
	cmp	ecx, r14d
	jb	SHORT $LL6@bipred_ave
	npad	9
$LL5@bipred_ave:
	vzeroupper

; 946  :     }
; 947  :   }
; 948  : }

	mov	rbx, QWORD PTR [rsp+64]
	mov	rsi, QWORD PTR [rsp+72]
	mov	rdi, QWORD PTR [rsp+80]
	add	rsp, 32					; 00000020H
	pop	r15
	pop	r14
	pop	r12
	ret	0
	npad	3
$LN689@bipred_ave:
	DD	$LN10@bipred_ave
	DD	$LN15@bipred_ave
	DD	$LN11@bipred_ave
	DD	$LN16@bipred_ave
	DD	$LN12@bipred_ave
	DD	$LN17@bipred_ave
	DD	$LN13@bipred_ave
	DD	$LN18@bipred_ave
	DD	$LN14@bipred_ave
	DD	$LN19@bipred_ave
$LN688@bipred_ave:
	DB	0
	DB	9
	DB	1
	DB	9
	DB	2
	DB	9
	DB	9
	DB	9
	DB	3
	DB	9
	DB	9
	DB	9
	DB	4
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	5
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	6
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	7
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	8
bipred_average_px_px_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
dst$ = 144
im_L0$ = 152
im_L1$ = 160
pu_w$ = 168
pu_h$ = 176
dst_stride$ = 184
bipred_average_im_im_template_avx2 PROC

; 956  : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rsi
	mov	QWORD PTR [rsp+24], rdi
	mov	QWORD PTR [rsp+32], r12
	push	r13
	push	r14
	push	r15
	sub	rsp, 112				; 00000070H

; 957  :   int32_t shift = 15 - KVZ_BIT_DEPTH; // TODO: defines
; 958  :   int32_t scalar_offset = 1 << (shift - 1);
; 959  :   __m256i offset = _mm256_set1_epi32(scalar_offset);
; 960  : 
; 961  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	popcnt	eax, r9d
	cmp	eax, 1
	vmovaps	XMMWORD PTR [rsp+64], xmm8

; 962  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	mov	eax, DWORD PTR pu_h$[rsp]
	mov	r15d, r9d
	vmovdqu	ymm8, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040
	sete	r12b
	mov	edi, r9d
	imul	r15d, eax
	mov	r14, r8
	mov	r13, rdx
	mov	rsi, rcx
	mov	ebx, r15d
	and	ebx, 31

; 963  :   assert(!(pu_w == 4 && pu_h == 4) && "Branch for 4x4 not yet implemented.");

	cmp	r9d, 4
	jne	SHORT $LN38@bipred_ave
	cmp	eax, r9d
	jne	SHORT $LN38@bipred_ave
	mov	r8d, 963				; 000003c3H
	lea	rdx, OFFSET FLAT:$SG4294951307
	lea	rcx, OFFSET FLAT:$SG4294951306
	vzeroupper
	call	QWORD PTR __imp__wassert
	vmovdqu	ymm8, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040
$LN38@bipred_ave:

; 964  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 965  : 
; 966  :   if (has_pow2_width && area_mod_32 == 0) {

	vmovaps	XMMWORD PTR [rsp+96], xmm6
	vmovaps	XMMWORD PTR [rsp+80], xmm7
	vmovaps	XMMWORD PTR [rsp+48], xmm9
	test	ebx, ebx
	jne	$LN27@bipred_ave
	test	r12b, r12b
	je	$LN20@bipred_ave

; 967  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r15d, r15d
	je	$LN18@bipred_ave

; 980  : 
; 981  :       __m256i all_ones = _mm256_set1_epi16(1);

	vmovdqu	ymm9, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	lea	r12, QWORD PTR [r14+32]
	sub	r13, r14
	mov	r14d, DWORD PTR dst_stride$[rsp]
	npad	5
$LL4@bipred_ave:

; 968  :       int y = i / pu_w;
; 969  :       int x = i % pu_w;
; 970  : 
; 971  :       __m256i sample_L0_a_16bit = _mm256_loadu_si256((__m256i*)&im_L0[i]);
; 972  :       __m256i sample_L1_a_16bit = _mm256_loadu_si256((__m256i*)&im_L1[i]);
; 973  :       __m256i sample_L0_b_16bit = _mm256_loadu_si256((__m256i*)&im_L0[i + 16]);
; 974  :       __m256i sample_L1_b_16bit = _mm256_loadu_si256((__m256i*)&im_L1[i + 16]);
; 975  : 
; 976  :       __m256i sample_L0_L1_a_lo = _mm256_unpacklo_epi16(sample_L0_a_16bit, sample_L1_a_16bit);
; 977  :       __m256i sample_L0_L1_a_hi = _mm256_unpackhi_epi16(sample_L0_a_16bit, sample_L1_a_16bit);
; 978  :       __m256i sample_L0_L1_b_lo = _mm256_unpacklo_epi16(sample_L0_b_16bit, sample_L1_b_16bit);
; 979  :       __m256i sample_L0_L1_b_hi = _mm256_unpackhi_epi16(sample_L0_b_16bit, sample_L1_b_16bit);

	vmovdqu	ymm5, YMMWORD PTR [r12+r13]
	vpunpckhwd ymm0, ymm5, YMMWORD PTR [r12]
	vmovdqu	ymm7, YMMWORD PTR [r12+r13-32]

; 982  :       __m256i avg_a_lo = _mm256_madd_epi16(sample_L0_L1_a_lo, all_ones);
; 983  :       __m256i avg_a_hi = _mm256_madd_epi16(sample_L0_L1_a_hi, all_ones);
; 984  :       __m256i avg_b_lo = _mm256_madd_epi16(sample_L0_L1_b_lo, all_ones);
; 985  :       __m256i avg_b_hi = _mm256_madd_epi16(sample_L0_L1_b_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm9
	vpunpcklwd ymm0, ymm5, YMMWORD PTR [r12]

; 986  : 
; 987  :       avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);
; 988  :       avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);
; 989  :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);
; 990  :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vpaddd	ymm2, ymm1, ymm8

; 991  : 
; 992  :       avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);
; 993  :       avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);
; 994  :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);
; 995  :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vpsrad	ymm4, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm9
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [r12-32]
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm9
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [r12-32]
	vpaddd	ymm2, ymm1, ymm8

; 996  : 
; 997  :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);
; 998  :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vpackusdw ymm5, ymm3, ymm4
	vpmaddwd ymm1, ymm0, ymm9
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	xor	edx, edx
	mov	eax, ebx
	vpackusdw ymm0, ymm3, ymm4
	div	edi

; 999  :       __m256i avg0213 = _mm256_packus_epi16(avg_01, avg_23);

	vpackuswb ymm1, ymm0, ymm5

; 1000 :       __m256i avg     = _mm256_permute4x64_epi64(avg0213, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm2, ymm1, 216				; 000000d8H

; 1001 : 
; 1002 :       switch (pu_w) {

	cmp	edi, 4
	je	$LN22@bipred_ave
	cmp	edi, 8
	je	SHORT $LN23@bipred_ave
	cmp	edi, 16
	je	SHORT $LN24@bipred_ave
	lea	ecx, DWORD PTR [rdi-32]
	test	ecx, -33				; ffffffdfH
	je	SHORT $LN25@bipred_ave

; 1008 :         default:
; 1009 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1009				; 000003f1H
	lea	rdx, OFFSET FLAT:$SG4294951302
	lea	rcx, OFFSET FLAT:$SG4294951301
	vzeroupper
	call	QWORD PTR __imp__wassert
	vmovdqu	ymm8, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040
	vmovdqu	ymm9, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	jmp	$LN2@bipred_ave
$LN25@bipred_ave:

; 1006 :         case 32: // Same as case 64
; 1007 :         case 64: _mm256_storeu_si256((__m256i*)&dst[y * dst_stride + x], avg); break;

	imul	eax, r14d
	add	eax, edx
	vmovdqu	YMMWORD PTR [rax+rsi], ymm2
	jmp	$LN2@bipred_ave
$LN24@bipred_ave:

; 1005 :         case 16: scatter_ymm_16x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	imul	eax, r14d
	lea	ecx, DWORD PTR [rdx+rax]
	add	rcx, rsi

; 801  :   _mm_storeu_si128((__m128i *)dst, ymm_lo); dst += dst_stride;

	vmovdqu	XMMWORD PTR [rcx], xmm2
	vextracti128 XMMWORD PTR [r14+rcx], ymm2, 1

; 1005 :         case 16: scatter_ymm_16x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	jmp	SHORT $LN2@bipred_ave
$LN23@bipred_ave:

; 1004 :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	imul	eax, r14d

; 790  :   __m128d ymm_hi = _mm256_extractf128_pd(ymm_as_m256d, 1);

	vextractf128 xmm0, ymm2, 1

; 1004 :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	lea	ecx, DWORD PTR [rdx+rax]
	add	rcx, rsi

; 791  :   _mm_storel_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovlpd	QWORD PTR [rcx], xmm2
	add	rcx, r14

; 792  :   _mm_storeh_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovhpd	QWORD PTR [rcx], xmm2
	add	rcx, r14

; 793  :   _mm_storel_pd((double*)dst, ymm_hi); dst += dst_stride;

	vmovlpd	QWORD PTR [rcx], xmm0

; 794  :   _mm_storeh_pd((double*)dst, ymm_hi);

	vmovhpd	QWORD PTR [r14+rcx], xmm0

; 1004 :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	jmp	SHORT $LN2@bipred_ave
$LN22@bipred_ave:

; 775  :   __m128i ymm_hi = _mm256_extracti128_si256(ymm, 1);

	vextracti128 xmm0, ymm2, 1

; 1003 :         case  4: scatter_ymm_4x8_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	imul	eax, r14d
	add	edx, eax
	add	rdx, rsi

; 776  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_lo); dst += dst_stride;

	vmovd	DWORD PTR [rdx], xmm2
	add	rdx, r14

; 777  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 1); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm2, 1
	add	rdx, r14

; 778  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 2); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm2, 2
	add	rdx, r14

; 779  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 3); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm2, 3
	add	rdx, r14

; 780  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_hi); dst += dst_stride;

	vmovd	DWORD PTR [rdx], xmm0
	add	rdx, r14

; 781  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 1); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm0, 1
	add	rdx, r14

; 782  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 2); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm0, 2

; 783  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 3);

	vpextrd	DWORD PTR [r14+rdx], xmm0, 3
$LN2@bipred_ave:

; 967  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	add	ebx, 32					; 00000020H
	add	r12, 64					; 00000040H
	cmp	ebx, r15d
	jb	$LL4@bipred_ave

; 1010 :           break;
; 1011 :       }
; 1012 :     }
; 1013 :   } else if (area_mod_32 == 0) {

	jmp	$LN18@bipred_ave
$LN20@bipred_ave:

; 1014 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r15d, r15d
	je	$LN18@bipred_ave

; 1015 : 
; 1016 :       int y = i / pu_w;
; 1017 :       int x = i % pu_w;
; 1018 : 
; 1019 :       // Last 64 bits of the 256 are not used to simplify the loop
; 1020 :       __m256i mask              = _mm256_setr_epi64x(-1, -1, -1, 0);

	vmovdqu	ymm9, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovaps	XMMWORD PTR [rsp+32], xmm10
	lea	r12, QWORD PTR [r14+32]
	vmovdqu	ymm10, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	sub	r13, r14
	mov	r14d, DWORD PTR dst_stride$[rsp]
	npad	1
$LL9@bipred_ave:

; 1021 :       __m256i sample_L0_a_16bit = _mm256_loadu_si256((__m256i*)&im_L0[i]);
; 1022 :       __m256i sample_L1_a_16bit = _mm256_loadu_si256((__m256i*)&im_L1[i]);
; 1023 :       __m256i sample_L0_b_16bit = _mm256_castsi128_si256(_mm_loadu_si128((__m128i*)&im_L0[i + 16]));

	vmovups	xmm5, XMMWORD PTR [r12+r13]

; 1024 :       __m256i sample_L1_b_16bit = _mm256_castsi128_si256(_mm_loadu_si128((__m128i*)&im_L1[i + 16]));

	vmovups	xmm3, XMMWORD PTR [r12]
	vmovdqu	ymm7, YMMWORD PTR [r12+r13-32]

; 1025 : 
; 1026 :       __m256i sample_L0_L1_a_lo = _mm256_unpacklo_epi16(sample_L0_a_16bit, sample_L1_a_16bit);
; 1027 :       __m256i sample_L0_L1_a_hi = _mm256_unpackhi_epi16(sample_L0_a_16bit, sample_L1_a_16bit);
; 1028 :       __m256i sample_L0_L1_b_lo = _mm256_unpacklo_epi16(sample_L0_b_16bit, sample_L1_b_16bit);
; 1029 :       __m256i sample_L0_L1_b_hi = _mm256_unpackhi_epi16(sample_L0_b_16bit, sample_L1_b_16bit);

	vpunpckhwd ymm0, ymm5, ymm3

; 1030 : 
; 1031 :       __m256i all_ones = _mm256_set1_epi16(1);
; 1032 :       __m256i avg_a_lo = _mm256_madd_epi16(sample_L0_L1_a_lo, all_ones);
; 1033 :       __m256i avg_a_hi = _mm256_madd_epi16(sample_L0_L1_a_hi, all_ones);
; 1034 :       __m256i avg_b_lo = _mm256_madd_epi16(sample_L0_L1_b_lo, all_ones);
; 1035 :       __m256i avg_b_hi = _mm256_madd_epi16(sample_L0_L1_b_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm10

; 1036 : 
; 1037 :       avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);
; 1038 :       avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);
; 1039 :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);
; 1040 :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vpaddd	ymm2, ymm1, ymm8

; 1041 : 
; 1042 :       avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);
; 1043 :       avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);
; 1044 :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);
; 1045 :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vpsrad	ymm4, ymm2, 7
	vpunpcklwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm10
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [r12-32]
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm10
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [r12-32]
	vpaddd	ymm2, ymm1, ymm8

; 1046 : 
; 1047 :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);
; 1048 :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vpackusdw ymm5, ymm3, ymm4
	vpmaddwd ymm1, ymm0, ymm10
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	xor	edx, edx
	mov	eax, ebx
	vpackusdw ymm0, ymm3, ymm4
	div	edi

; 1049 :       __m256i avg0213 = _mm256_packus_epi16(avg_01, avg_23);

	vpackuswb ymm1, ymm0, ymm5

; 1050 :       __m256i avg     = _mm256_permute4x64_epi64(avg0213, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm2, ymm1, 216				; 000000d8H

; 1051 : 
; 1052 :       switch (pu_w) {

	cmp	edi, 12
	je	SHORT $LN29@bipred_ave
	cmp	edi, 24
	je	SHORT $LN30@bipred_ave
	cmp	edi, 48					; 00000030H
	je	SHORT $LN30@bipred_ave

; 1056 :         default:
; 1057 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1057				; 00000421H
	lea	rdx, OFFSET FLAT:$SG4294951300
	lea	rcx, OFFSET FLAT:$SG4294951299
	vzeroupper
	call	QWORD PTR __imp__wassert
	vmovdqu	ymm8, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040
	vmovdqu	ymm9, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	ymm10, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	jmp	SHORT $LN7@bipred_ave
$LN30@bipred_ave:

; 1054 :         case 24: // Same as case 48
; 1055 :         case 48: _mm256_maskstore_epi64((long long*)&dst[y * dst_stride + x], mask, avg); break;

	imul	eax, r14d
	add	eax, edx
	vpmaskmovq YMMWORD PTR [rax+rsi], ymm9, ymm2
	jmp	SHORT $LN7@bipred_ave
$LN29@bipred_ave:

; 807  :   __m256i mask_a = _mm256_setr_epi32(-1, -1, -1, 0, 0, 0, 0, 0);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000000000000000000000000000000000000ffffffffffffffffffffffff

; 808  :   __m256i mask_b = _mm256_setr_epi32(0, 0, 0, -1, -1, -1, 0, 0);

	vmovdqu	ymm1, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffff000000000000000000000000

; 1053 :         case 12: scatter_ymm_12x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	imul	eax, r14d
	add	edx, eax

; 809  :   _mm256_maskstore_epi32((int32_t*)dst, mask_a, ymm); dst += dst_stride - 3 * 4;

	lea	eax, DWORD PTR [r14-12]

; 1053 :         case 12: scatter_ymm_12x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	add	rdx, rsi

; 809  :   _mm256_maskstore_epi32((int32_t*)dst, mask_a, ymm); dst += dst_stride - 3 * 4;

	vpmaskmovd YMMWORD PTR [rdx], ymm0, ymm2

; 810  :   _mm256_maskstore_epi32((int32_t*)dst, mask_b, ymm);

	vpmaskmovd YMMWORD PTR [rax+rdx], ymm1, ymm2
$LN7@bipred_ave:

; 1014 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	add	ebx, 24
	add	r12, 48					; 00000030H
	cmp	ebx, r15d
	jb	$LL9@bipred_ave

; 1058 :           break;
; 1059 :       }
; 1060 :     }
; 1061 :   } else {

	vmovaps	xmm10, XMMWORD PTR [rsp+32]
	jmp	$LN18@bipred_ave
$LN27@bipred_ave:

; 1062 :     // 8x2, 8x6, 6x8 blocks (and maybe 2x8 in the future)
; 1063 :     switch (pu_w) {

	cmp	edi, 6
	je	$LN33@bipred_ave
	cmp	edi, 8
	je	SHORT $LN32@bipred_ave

; 1129 :         }
; 1130 :         break;
; 1131 :       default:
; 1132 :         assert(0 && "Unexpected block width.");

	mov	r8d, 1132				; 0000046cH
	lea	rdx, OFFSET FLAT:$SG4294951298
	lea	rcx, OFFSET FLAT:$SG4294951297
	vzeroupper
	call	QWORD PTR __imp__wassert
	jmp	$LN18@bipred_ave
$LN32@bipred_ave:

; 1064 :       case 8: // 8x2, 8x6
; 1065 :         for (int i = 0; i < pu_w * pu_h; i += 16) {

	xor	ebx, ebx
	test	r15d, r15d
	je	$LN18@bipred_ave

; 1074 : 
; 1075 :           __m256i all_ones = _mm256_set1_epi16(1);

	vmovdqu	ymm6, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	mov	r8d, DWORD PTR dst_stride$[rsp]
	sub	r13, r14
	npad	12
$LL16@bipred_ave:

; 1066 : 
; 1067 :           int y = i / pu_w;
; 1068 : 
; 1069 :           __m256i sample_L0_16bit = _mm256_loadu_si256((__m256i*)&im_L0[i]);
; 1070 :           __m256i sample_L1_16bit = _mm256_loadu_si256((__m256i*)&im_L1[i]);
; 1071 : 
; 1072 :           __m256i sample_L0_L1_lo = _mm256_unpacklo_epi16(sample_L0_16bit, sample_L1_16bit);
; 1073 :           __m256i sample_L0_L1_hi = _mm256_unpackhi_epi16(sample_L0_16bit, sample_L1_16bit);

	vmovdqu	ymm5, YMMWORD PTR [r14+r13]
	vpunpckhwd ymm0, ymm5, YMMWORD PTR [r14]

; 1076 :           __m256i avg_lo   = _mm256_madd_epi16(sample_L0_L1_lo, all_ones);
; 1077 :           __m256i avg_hi   = _mm256_madd_epi16(sample_L0_L1_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm6
	vpunpcklwd ymm0, ymm5, YMMWORD PTR [r14]

; 1078 : 
; 1079 :           avg_lo = _mm256_add_epi32(avg_lo, offset);
; 1080 :           avg_hi = _mm256_add_epi32(avg_hi, offset);

	vpaddd	ymm2, ymm1, ymm8
	vpmaddwd ymm1, ymm0, ymm6
	mov	edx, ebx
	lea	r14, QWORD PTR [r14+32]
	shr	edx, 3
	add	ebx, 16

; 1081 : 
; 1082 :           avg_lo = _mm256_srai_epi32(avg_lo, shift);
; 1083 :           avg_hi = _mm256_srai_epi32(avg_hi, shift);
; 1084 : 
; 1085 :           __m256i avg256 = _mm256_packus_epi32(avg_lo, avg_hi);
; 1086 :           avg256         = _mm256_packus_epi16(avg256, avg256);
; 1087 :           avg256         = _mm256_permute4x64_epi64(avg256, _MM_SHUFFLE(3, 1, 2, 0));
; 1088 :           __m128i avg    = _mm256_castsi256_si128(avg256);
; 1089 : 
; 1090 :           _mm_storel_epi64((__m128i*)&dst[y * dst_stride], avg);

	mov	eax, edx
	imul	eax, r8d
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm0, ymm0, ymm0
	vpermq	ymm1, ymm0, 216				; 000000d8H
	vmovq	QWORD PTR [rax+rsi], xmm1

; 1091 :           _mm_storeh_pd((double*)&dst[(y + 1) * dst_stride], _mm_castsi128_pd(avg));

	lea	eax, DWORD PTR [rdx+1]
	imul	eax, r8d
	vmovhpd	QWORD PTR [rax+rsi], xmm1
	cmp	ebx, r15d
	jb	SHORT $LL16@bipred_ave

; 1092 :         }
; 1093 :         break;

	jmp	$LN18@bipred_ave
$LN33@bipred_ave:

; 1094 :       case 6: // 6x8
; 1095 :         for (int i = 0; i < pu_w * pu_h; i += 12) {

	xor	ebx, ebx
	test	r15d, r15d
	je	$LN18@bipred_ave

; 1102 : 
; 1103 :           __m256i sample_L0_L1_lo = _mm256_unpacklo_epi16(sample_L0_16bit, sample_L1_16bit);
; 1104 :           __m256i sample_L0_L1_hi = _mm256_unpackhi_epi16(sample_L0_16bit, sample_L1_16bit);
; 1105 : 
; 1106 :           __m256i all_ones = _mm256_set1_epi16(1);

	vmovdqu	ymm6, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	ymm7, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	mov	r9d, DWORD PTR dst_stride$[rsp]
	npad	5
$LL19@bipred_ave:

; 1096 : 
; 1097 :           int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH
	mul	ebx

; 1098 : 
; 1099 :           __m256i mask            = _mm256_setr_epi64x(-1, -1, -1, 0);
; 1100 :           __m256i sample_L0_16bit = _mm256_maskload_epi64((const long long*)(&im_L0[i]), mask);

	movsxd	rax, ebx
	add	ebx, 12
	mov	r8d, edx
	shr	r8d, 2

; 1101 :           __m256i sample_L1_16bit = _mm256_maskload_epi64((const long long*)(&im_L1[i]), mask);

	vpmaskmovq ymm3, ymm6, YMMWORD PTR [r14+rax*2]
	vpmaskmovq ymm5, ymm6, YMMWORD PTR [r13+rax*2]

; 1107 :           __m256i avg_a_lo = _mm256_madd_epi16(sample_L0_L1_lo, all_ones);
; 1108 :           __m256i avg_a_hi = _mm256_madd_epi16(sample_L0_L1_hi, all_ones);
; 1109 : 
; 1110 :           avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);
; 1111 :           avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);
; 1112 : 
; 1113 :           avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);
; 1114 :           avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);
; 1115 : 
; 1116 :           __m256i avg256 = _mm256_packus_epi32(avg_a_lo, avg_a_hi);
; 1117 :           avg256         = _mm256_packus_epi16(avg256, avg256);
; 1118 :           avg256         = _mm256_permute4x64_epi64(avg256, _MM_SHUFFLE(3, 1, 2, 0));
; 1119 :           __m128i avg    = _mm256_castsi256_si128(avg256);
; 1120 : 
; 1121 :           uint32_t elements_0123 = _mm_cvtsi128_si32(avg);
; 1122 :           uint16_t elements_45   = _mm_extract_epi16(avg, 2);
; 1123 :           uint16_t elements_67   = _mm_extract_epi16(avg, 3);
; 1124 :           uint32_t elements_89ab = _mm_extract_epi32(avg, 2);
; 1125 :           *(uint32_t*)&dst[(y + 0) * dst_stride + 0] = elements_0123;

	mov	eax, r8d

; 1126 :           *(uint16_t*)&dst[(y + 0) * dst_stride + 4] = elements_45;
; 1127 :           *(uint16_t*)&dst[(y + 1) * dst_stride + 0] = elements_67;

	lea	ecx, DWORD PTR [r8+1]
	imul	eax, r9d
	vpunpckhwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm7
	vpaddd	ymm2, ymm1, ymm8
	vpunpcklwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm7
	imul	ecx, r9d
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm0, ymm0, ymm0
	vpermq	ymm1, ymm0, 216				; 000000d8H
	vmovd	DWORD PTR [rax+rsi], xmm1
	add	eax, 4
	vpextrw	WORD PTR [rax+rsi], xmm1, 2

; 1128 :           *(uint32_t*)&dst[(y + 1) * dst_stride + 2] = elements_89ab;

	lea	eax, DWORD PTR [rcx+2]
	vpextrw	WORD PTR [rcx+rsi], xmm1, 3
	vpextrd	DWORD PTR [rax+rsi], xmm1, 2
	cmp	ebx, r15d
	jb	$LL19@bipred_ave
$LN18@bipred_ave:
	vzeroupper
	vmovaps	xmm9, XMMWORD PTR [rsp+48]

; 1133 :         break;
; 1134 :     }
; 1135 :   }
; 1136 : }

	lea	r11, QWORD PTR [rsp+112]
	mov	rbx, QWORD PTR [r11+32]
	mov	rsi, QWORD PTR [r11+40]
	mov	rdi, QWORD PTR [r11+48]
	mov	r12, QWORD PTR [r11+56]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm7, XMMWORD PTR [rsp+80]
	vmovaps	xmm6, XMMWORD PTR [rsp+96]
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r13
	ret	0
bipred_average_im_im_template_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\picture-generic.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
dst$ = 160
im_L0$ = 168
im_L1$ = 176
pu_w$ = 184
pu_h$ = 192
dst_stride$ = 200
bipred_average_im_im_avx2 PROC

; 1144 : {

	mov	rax, rsp
	mov	QWORD PTR [rax+8], rbx
	mov	QWORD PTR [rax+16], rsi
	mov	QWORD PTR [rax+24], rdi
	push	r12
	push	r14
	push	r15
	sub	rsp, 128				; 00000080H
	vmovaps	XMMWORD PTR [rax-40], xmm6
	mov	rbx, r8
	mov	r8d, DWORD PTR pu_h$[rsp]
	mov	r11d, r9d
	vmovaps	XMMWORD PTR [rax-56], xmm7
	mov	rdi, rdx
	vmovaps	XMMWORD PTR [rax-72], xmm8
	mov	rsi, rcx
	vmovaps	XMMWORD PTR [rax-88], xmm9
	vmovaps	XMMWORD PTR [rax-104], xmm10

; 1145 :   // Use scalar code for yet unoptimized block sizes (4x4, 2x8)
; 1146 :   if (!(pu_w == 4 && pu_h == 4) && pu_w > 2) {

	cmp	r9d, 4
	jne	SHORT $LN9@bipred_ave
	cmp	r8d, r9d
	je	$LN7@bipred_ave
	add	r11d, -4				; fffffffcH
	jmp	SHORT $LN687@bipred_ave
$LN9@bipred_ave:
	cmp	r11d, 2
	jbe	$LN7@bipred_ave

; 1147 :     switch (pu_w) {

	add	r11d, -4				; fffffffcH
	cmp	r11d, 60				; 0000003cH
	ja	$LN19@bipred_ave
$LN687@bipred_ave:
	lea	rdx, OFFSET FLAT:__ImageBase
	movzx	eax, BYTE PTR $LN688@bipred_ave[rdx+r11]
	mov	ecx, DWORD PTR $LN689@bipred_ave[rdx+rax*4]
	add	rcx, rdx
	jmp	rcx
$LN10@bipred_ave:

; 959  :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovdqu	ymm9, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 960  : 
; 961  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 4

; 962  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	r14d, DWORD PTR [r8*4]
	popcnt	eax, eax
	cmp	eax, 1
	mov	r15d, r14d
	sete	r12b
	and	r15d, 31

; 963  :   assert(!(pu_w == 4 && pu_h == 4) && "Branch for 4x4 not yet implemented.");

	cmp	r8d, 4
	jne	SHORT $LN62@bipred_ave
	mov	r8d, 963				; 000003c3H
	lea	rdx, OFFSET FLAT:$SG4294951307
	lea	rcx, OFFSET FLAT:$SG4294951306
	vzeroupper
	call	QWORD PTR __imp__wassert
	vmovdqu	ymm9, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040
$LN62@bipred_ave:

; 964  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 965  : 
; 966  :   if (has_pow2_width && area_mod_32 == 0) {

	test	r15d, r15d
	jne	$LN48@bipred_ave
	test	r12b, r12b
	je	$LN41@bipred_ave

; 967  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	xor	ecx, ecx
	test	r14d, r14d
	je	$LL5@bipred_ave
	vmovdqu	ymm8, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 980  : 
; 981  :       __m256i all_ones = _mm256_set1_epi16(1);

	mov	r10d, DWORD PTR dst_stride$[rsp]
	lea	r9, QWORD PTR [rbx+32]
	sub	rdi, rbx
	npad	8
$LL25@bipred_ave:

; 968  :       int y = i / pu_w;
; 969  :       int x = i % pu_w;
; 970  : 
; 971  :       __m256i sample_L0_a_16bit = _mm256_loadu_si256((__m256i*)&im_L0[i]);

	vmovdqu	ymm5, YMMWORD PTR [rdi+r9-32]

; 974  :       __m256i sample_L1_b_16bit = _mm256_loadu_si256((__m256i*)&im_L1[i + 16]);
; 975  : 
; 976  :       __m256i sample_L0_L1_a_lo = _mm256_unpacklo_epi16(sample_L0_a_16bit, sample_L1_a_16bit);
; 977  :       __m256i sample_L0_L1_a_hi = _mm256_unpackhi_epi16(sample_L0_a_16bit, sample_L1_a_16bit);

	vpunpckhwd ymm0, ymm5, YMMWORD PTR [r9-32]
	vmovdqu	ymm7, YMMWORD PTR [rdi+r9]

; 982  :       __m256i avg_a_lo = _mm256_madd_epi16(sample_L0_L1_a_lo, all_ones);
; 983  :       __m256i avg_a_hi = _mm256_madd_epi16(sample_L0_L1_a_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm8
	vpunpcklwd ymm0, ymm5, YMMWORD PTR [r9-32]

; 986  : 
; 987  :       avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);
; 988  :       avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);

	vpaddd	ymm2, ymm1, ymm9
	vpmaddwd ymm1, ymm0, ymm8
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [r9]

; 991  : 
; 992  :       avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);
; 993  :       avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);

	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm9
	vpmaddwd ymm1, ymm0, ymm8
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [r9]
	vpsrad	ymm3, ymm2, 7
	vpaddd	ymm2, ymm1, ymm9
	vpmaddwd ymm1, ymm0, ymm8
	mov	edx, ecx
	lea	r9, QWORD PTR [r9+64]
	shr	edx, 2
	mov	eax, ecx

; 999  :       __m256i avg0213 = _mm256_packus_epi16(avg_01, avg_23);
; 1000 :       __m256i avg     = _mm256_permute4x64_epi64(avg0213, _MM_SHUFFLE(3, 1, 2, 0));
; 1001 : 
; 1002 :       switch (pu_w) {
; 1003 :         case  4: scatter_ymm_4x8_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	imul	edx, r10d
	and	eax, 3
	vpackusdw ymm5, ymm3, ymm4
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm9
	vpsrad	ymm3, ymm2, 7
	add	edx, eax
	add	ecx, 32					; 00000020H
	add	rdx, rsi
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm1, ymm5, ymm0
	vpermq	ymm2, ymm1, 216				; 000000d8H

; 776  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_lo); dst += dst_stride;

	vmovd	DWORD PTR [rdx], xmm2
	add	rdx, r10
	vextracti128 xmm0, ymm2, 1

; 777  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 1); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm2, 1
	add	rdx, r10

; 778  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 2); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm2, 2
	add	rdx, r10

; 779  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 3); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm2, 3
	add	rdx, r10

; 780  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_hi); dst += dst_stride;

	vmovd	DWORD PTR [rdx], xmm0
	add	rdx, r10

; 781  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 1); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm0, 1
	add	rdx, r10

; 782  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 2); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm0, 2

; 783  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 3);

	vpextrd	DWORD PTR [r10+rdx], xmm0, 3

; 967  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	cmp	ecx, r14d
	jb	$LL25@bipred_ave

; 1010 :           break;
; 1011 :       }
; 1012 :     }
; 1013 :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN41@bipred_ave:

; 1014 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r14d, r14d
	je	$LL5@bipred_ave
	lea	ecx, DWORD PTR [r14-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
	npad	6
$LL30@bipred_ave:

; 1056 :         default:
; 1057 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1057				; 00000421H
	lea	rdx, OFFSET FLAT:$SG4294951300
	lea	rcx, OFFSET FLAT:$SG4294951299
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL30@bipred_ave

; 1058 :           break;
; 1059 :       }
; 1060 :     }
; 1061 :   } else {

	jmp	$LL5@bipred_ave
$LN48@bipred_ave:

; 1174 :     }
; 1175 :   }
; 1176 : }

	mov	r8d, 1132				; 0000046cH
	lea	rdx, OFFSET FLAT:$SG4294951298
	lea	rcx, OFFSET FLAT:$SG4294951297
	vzeroupper
	call	QWORD PTR __imp__wassert
	jmp	$LL5@bipred_ave
$LN11@bipred_ave:

; 959  :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovdqu	ymm8, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 960  : 
; 961  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 8

; 962  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	r9d, DWORD PTR [r8*8]
	popcnt	eax, eax
	cmp	eax, 1
	sete	al
	test	r9b, 31

; 964  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 965  : 
; 966  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN100@bipred_ave
	test	al, al
	je	$LN93@bipred_ave

; 967  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	xor	ecx, ecx
	test	r9d, r9d
	je	$LL5@bipred_ave
	vmovdqu	ymm9, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 980  : 
; 981  :       __m256i all_ones = _mm256_set1_epi16(1);

	mov	r10d, DWORD PTR dst_stride$[rsp]
	lea	r8, QWORD PTR [rbx+32]
	sub	rdi, rbx
	npad	3
$LL77@bipred_ave:

; 972  :       __m256i sample_L1_a_16bit = _mm256_loadu_si256((__m256i*)&im_L1[i]);
; 973  :       __m256i sample_L0_b_16bit = _mm256_loadu_si256((__m256i*)&im_L0[i + 16]);

	vmovdqu	ymm5, YMMWORD PTR [r8+rdi]

; 978  :       __m256i sample_L0_L1_b_lo = _mm256_unpacklo_epi16(sample_L0_b_16bit, sample_L1_b_16bit);
; 979  :       __m256i sample_L0_L1_b_hi = _mm256_unpackhi_epi16(sample_L0_b_16bit, sample_L1_b_16bit);

	vpunpckhwd ymm0, ymm5, YMMWORD PTR [r8]
	vmovdqu	ymm7, YMMWORD PTR [r8+rdi-32]

; 984  :       __m256i avg_b_lo = _mm256_madd_epi16(sample_L0_L1_b_lo, all_ones);
; 985  :       __m256i avg_b_hi = _mm256_madd_epi16(sample_L0_L1_b_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm9
	vpunpcklwd ymm0, ymm5, YMMWORD PTR [r8]

; 989  :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);
; 990  :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vpaddd	ymm2, ymm1, ymm8
	vpmaddwd ymm1, ymm0, ymm9
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [r8-32]

; 994  :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);
; 995  :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	vpmaddwd ymm1, ymm0, ymm9
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [r8-32]
	vpsrad	ymm3, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	vpmaddwd ymm1, ymm0, ymm9

; 996  : 
; 997  :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);
; 998  :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vpackusdw ymm5, ymm3, ymm4
	vpsrad	ymm4, ymm2, 7
	mov	eax, ecx
	lea	r8, QWORD PTR [r8+64]
	and	eax, 7
	mov	edx, ecx
	vpaddd	ymm2, ymm1, ymm8
	shr	edx, 3
	add	ecx, 32					; 00000020H

; 1004 :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	imul	edx, r10d
	vpsrad	ymm3, ymm2, 7
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm1, ymm0, ymm5
	vpermq	ymm2, ymm1, 216				; 000000d8H
	add	eax, edx
	add	rax, rsi

; 790  :   __m128d ymm_hi = _mm256_extractf128_pd(ymm_as_m256d, 1);

	vextractf128 xmm0, ymm2, 1

; 791  :   _mm_storel_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovlpd	QWORD PTR [rax], xmm2
	add	rax, r10

; 792  :   _mm_storeh_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovhpd	QWORD PTR [rax], xmm2
	add	rax, r10

; 793  :   _mm_storel_pd((double*)dst, ymm_hi); dst += dst_stride;

	vmovlpd	QWORD PTR [rax], xmm0

; 794  :   _mm_storeh_pd((double*)dst, ymm_hi);

	vmovhpd	QWORD PTR [rax+r10], xmm0

; 967  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	cmp	ecx, r9d
	jb	$LL77@bipred_ave

; 1010 :           break;
; 1011 :       }
; 1012 :     }
; 1013 :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN93@bipred_ave:

; 1014 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r9d, r9d
	je	$LL5@bipred_ave
	lea	ecx, DWORD PTR [r9-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
$LL82@bipred_ave:

; 1056 :         default:
; 1057 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1057				; 00000421H
	lea	rdx, OFFSET FLAT:$SG4294951300
	lea	rcx, OFFSET FLAT:$SG4294951299
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL82@bipred_ave

; 1058 :           break;
; 1059 :       }
; 1060 :     }
; 1061 :   } else {

	jmp	$LL5@bipred_ave
$LN100@bipred_ave:

; 1062 :     // 8x2, 8x6, 6x8 blocks (and maybe 2x8 in the future)
; 1063 :     switch (pu_w) {
; 1064 :       case 8: // 8x2, 8x6
; 1065 :         for (int i = 0; i < pu_w * pu_h; i += 16) {

	xor	ecx, ecx
	test	r9d, r9d
	je	$LL5@bipred_ave
	vmovdqu	ymm6, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 1074 : 
; 1075 :           __m256i all_ones = _mm256_set1_epi16(1);

	mov	r10d, DWORD PTR dst_stride$[rsp]
	sub	rdi, rbx
	npad	7
$LL89@bipred_ave:

; 1066 : 
; 1067 :           int y = i / pu_w;
; 1068 : 
; 1069 :           __m256i sample_L0_16bit = _mm256_loadu_si256((__m256i*)&im_L0[i]);

	vmovdqu	ymm5, YMMWORD PTR [rbx+rdi]

; 1070 :           __m256i sample_L1_16bit = _mm256_loadu_si256((__m256i*)&im_L1[i]);
; 1071 : 
; 1072 :           __m256i sample_L0_L1_lo = _mm256_unpacklo_epi16(sample_L0_16bit, sample_L1_16bit);
; 1073 :           __m256i sample_L0_L1_hi = _mm256_unpackhi_epi16(sample_L0_16bit, sample_L1_16bit);

	vpunpckhwd ymm0, ymm5, YMMWORD PTR [rbx]

; 1076 :           __m256i avg_lo   = _mm256_madd_epi16(sample_L0_L1_lo, all_ones);
; 1077 :           __m256i avg_hi   = _mm256_madd_epi16(sample_L0_L1_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm6
	vpunpcklwd ymm0, ymm5, YMMWORD PTR [rbx]

; 1078 : 
; 1079 :           avg_lo = _mm256_add_epi32(avg_lo, offset);
; 1080 :           avg_hi = _mm256_add_epi32(avg_hi, offset);

	vpaddd	ymm2, ymm1, ymm8
	vpmaddwd ymm1, ymm0, ymm6
	mov	r8d, ecx
	lea	rbx, QWORD PTR [rbx+32]
	shr	r8d, 3
	add	ecx, 16

; 1081 : 
; 1082 :           avg_lo = _mm256_srai_epi32(avg_lo, shift);
; 1083 :           avg_hi = _mm256_srai_epi32(avg_hi, shift);
; 1084 : 
; 1085 :           __m256i avg256 = _mm256_packus_epi32(avg_lo, avg_hi);
; 1086 :           avg256         = _mm256_packus_epi16(avg256, avg256);
; 1087 :           avg256         = _mm256_permute4x64_epi64(avg256, _MM_SHUFFLE(3, 1, 2, 0));
; 1088 :           __m128i avg    = _mm256_castsi256_si128(avg256);
; 1089 : 
; 1090 :           _mm_storel_epi64((__m128i*)&dst[y * dst_stride], avg);

	mov	eax, r8d
	imul	eax, r10d
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm0, ymm0, ymm0
	vpermq	ymm1, ymm0, 216				; 000000d8H
	vmovq	QWORD PTR [rax+rsi], xmm1

; 1091 :           _mm_storeh_pd((double*)&dst[(y + 1) * dst_stride], _mm_castsi128_pd(avg));

	lea	eax, DWORD PTR [r8+1]
	imul	eax, r10d
	vmovhpd	QWORD PTR [rax+rsi], xmm1
	cmp	ecx, r9d
	jb	SHORT $LL89@bipred_ave

; 1148 :       case  4: bipred_average_im_im_template_avx2(dst, im_L0, im_L1,  4, pu_h, dst_stride); break;
; 1149 :       case  8: bipred_average_im_im_template_avx2(dst, im_L0, im_L1,  8, pu_h, dst_stride); break;

	jmp	$LL5@bipred_ave
$LN12@bipred_ave:

; 959  :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovdqu	ymm9, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 960  : 
; 961  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 16
	popcnt	eax, eax
	cmp	eax, 1
	sete	cl

; 962  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	imul	eax, r8d, -16				; fffffff0H
	test	al, 31

; 964  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 965  : 
; 966  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN48@bipred_ave
	shl	r8d, 4
	test	cl, cl
	je	$LN145@bipred_ave

; 967  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	xor	ecx, ecx
	test	r8d, r8d
	je	$LL5@bipred_ave
	vmovdqu	ymm8, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 980  : 
; 981  :       __m256i all_ones = _mm256_set1_epi16(1);

	mov	r10d, DWORD PTR dst_stride$[rsp]
	lea	r9, QWORD PTR [rbx+32]
	sub	rdi, rbx
	npad	6
$LL129@bipred_ave:

; 972  :       __m256i sample_L1_a_16bit = _mm256_loadu_si256((__m256i*)&im_L1[i]);
; 973  :       __m256i sample_L0_b_16bit = _mm256_loadu_si256((__m256i*)&im_L0[i + 16]);

	vmovdqu	ymm5, YMMWORD PTR [rdi+r9]

; 978  :       __m256i sample_L0_L1_b_lo = _mm256_unpacklo_epi16(sample_L0_b_16bit, sample_L1_b_16bit);
; 979  :       __m256i sample_L0_L1_b_hi = _mm256_unpackhi_epi16(sample_L0_b_16bit, sample_L1_b_16bit);

	vpunpckhwd ymm0, ymm5, YMMWORD PTR [r9]
	vmovdqu	ymm7, YMMWORD PTR [rdi+r9-32]

; 984  :       __m256i avg_b_lo = _mm256_madd_epi16(sample_L0_L1_b_lo, all_ones);
; 985  :       __m256i avg_b_hi = _mm256_madd_epi16(sample_L0_L1_b_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm8
	vpunpcklwd ymm0, ymm5, YMMWORD PTR [r9]

; 989  :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);
; 990  :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vpaddd	ymm2, ymm1, ymm9

; 994  :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);
; 995  :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vpsrad	ymm4, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm8
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [r9-32]
	vpaddd	ymm2, ymm1, ymm9
	vpsrad	ymm3, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm8
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [r9-32]
	vpaddd	ymm2, ymm1, ymm9

; 996  : 
; 997  :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);
; 998  :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vpackusdw ymm5, ymm3, ymm4
	vpmaddwd ymm1, ymm0, ymm8
	vpsrad	ymm4, ymm2, 7
	mov	eax, ecx
	lea	r9, QWORD PTR [r9+64]
	vpaddd	ymm2, ymm1, ymm9
	and	eax, 15
	mov	edx, ecx
	shr	edx, 4
	add	ecx, 32					; 00000020H

; 1005 :         case 16: scatter_ymm_16x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	imul	edx, r10d
	vpsrad	ymm3, ymm2, 7
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm1, ymm0, ymm5
	vpermq	ymm2, ymm1, 216				; 000000d8H
	add	eax, edx
	add	rax, rsi

; 801  :   _mm_storeu_si128((__m128i *)dst, ymm_lo); dst += dst_stride;

	vmovdqu	XMMWORD PTR [rax], xmm2
	vextracti128 XMMWORD PTR [rax+r10], ymm2, 1

; 967  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	cmp	ecx, r8d
	jb	$LL129@bipred_ave

; 1010 :           break;
; 1011 :       }
; 1012 :     }
; 1013 :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN145@bipred_ave:

; 1014 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r8d, r8d
	je	$LL5@bipred_ave
	lea	ecx, DWORD PTR [r8-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
$LL134@bipred_ave:

; 1056 :         default:
; 1057 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1057				; 00000421H
	lea	rdx, OFFSET FLAT:$SG4294951300
	lea	rcx, OFFSET FLAT:$SG4294951299
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL134@bipred_ave

; 1058 :           break;
; 1059 :       }
; 1060 :     }
; 1061 :   } else {

	jmp	$LL5@bipred_ave
$LN13@bipred_ave:

; 959  :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovdqu	ymm9, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 960  : 
; 961  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 32					; 00000020H
	popcnt	eax, eax
	cmp	eax, 1
	sete	al
	shl	r8d, 5

; 964  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 965  : 
; 966  :   if (has_pow2_width && area_mod_32 == 0) {

	test	al, al
	je	$LN652@bipred_ave

; 967  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	xor	ecx, ecx
	test	r8d, r8d
	je	$LL5@bipred_ave
	vmovdqu	ymm8, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 980  : 
; 981  :       __m256i all_ones = _mm256_set1_epi16(1);

	mov	r10d, DWORD PTR dst_stride$[rsp]
	lea	r9, QWORD PTR [rbx+32]
	sub	rdi, rbx
	npad	3
$LL181@bipred_ave:

; 972  :       __m256i sample_L1_a_16bit = _mm256_loadu_si256((__m256i*)&im_L1[i]);
; 973  :       __m256i sample_L0_b_16bit = _mm256_loadu_si256((__m256i*)&im_L0[i + 16]);

	vmovdqu	ymm5, YMMWORD PTR [rdi+r9]

; 978  :       __m256i sample_L0_L1_b_lo = _mm256_unpacklo_epi16(sample_L0_b_16bit, sample_L1_b_16bit);
; 979  :       __m256i sample_L0_L1_b_hi = _mm256_unpackhi_epi16(sample_L0_b_16bit, sample_L1_b_16bit);

	vpunpckhwd ymm0, ymm5, YMMWORD PTR [r9]
	vmovdqu	ymm7, YMMWORD PTR [rdi+r9-32]

; 984  :       __m256i avg_b_lo = _mm256_madd_epi16(sample_L0_L1_b_lo, all_ones);
; 985  :       __m256i avg_b_hi = _mm256_madd_epi16(sample_L0_L1_b_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm8
	vpunpcklwd ymm0, ymm5, YMMWORD PTR [r9]

; 989  :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);
; 990  :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vpaddd	ymm2, ymm1, ymm9

; 994  :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);
; 995  :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vpsrad	ymm4, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm8
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [r9-32]
	vpaddd	ymm2, ymm1, ymm9
	vpsrad	ymm3, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm8
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [r9-32]
	vpaddd	ymm2, ymm1, ymm9
	mov	edx, ecx
	lea	r9, QWORD PTR [r9+64]
	shr	edx, 5
	mov	eax, ecx

; 996  : 
; 997  :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);
; 998  :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vpackusdw ymm5, ymm3, ymm4
	vpmaddwd ymm1, ymm0, ymm8
	vpsrad	ymm4, ymm2, 7

; 1006 :         case 32: // Same as case 64
; 1007 :         case 64: _mm256_storeu_si256((__m256i*)&dst[y * dst_stride + x], avg); break;

	imul	edx, r10d
	and	eax, 31
	vpaddd	ymm2, ymm1, ymm9
	vpsrad	ymm3, ymm2, 7
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm1, ymm0, ymm5
	add	edx, eax
	add	ecx, 32					; 00000020H
	vpermq	ymm2, ymm1, 216				; 000000d8H
	vmovdqu	YMMWORD PTR [rdx+rsi], ymm2
	cmp	ecx, r8d
	jb	$LL181@bipred_ave

; 1010 :           break;
; 1011 :       }
; 1012 :     }
; 1013 :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN652@bipred_ave:

; 1014 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r8d, r8d
	je	$LL5@bipred_ave
	lea	ecx, DWORD PTR [r8-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
	npad	9
$LL186@bipred_ave:

; 1056 :         default:
; 1057 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1057				; 00000421H
	lea	rdx, OFFSET FLAT:$SG4294951300
	lea	rcx, OFFSET FLAT:$SG4294951299
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL186@bipred_ave

; 1150 :       case 16: bipred_average_im_im_template_avx2(dst, im_L0, im_L1, 16, pu_h, dst_stride); break;
; 1151 :       case 32: bipred_average_im_im_template_avx2(dst, im_L0, im_L1, 32, pu_h, dst_stride); break;

	jmp	$LL5@bipred_ave
$LN14@bipred_ave:

; 959  :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovdqu	ymm9, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 960  : 
; 961  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 64					; 00000040H
	popcnt	eax, eax
	cmp	eax, 1
	sete	al
	shl	r8d, 6

; 964  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 965  : 
; 966  :   if (has_pow2_width && area_mod_32 == 0) {

	test	al, al
	je	$LN658@bipred_ave

; 967  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	xor	ecx, ecx
	test	r8d, r8d
	je	$LL5@bipred_ave
	vmovdqu	ymm8, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 980  : 
; 981  :       __m256i all_ones = _mm256_set1_epi16(1);

	mov	r10d, DWORD PTR dst_stride$[rsp]
	lea	r9, QWORD PTR [rbx+32]
	sub	rdi, rbx
	npad	3
$LL233@bipred_ave:

; 972  :       __m256i sample_L1_a_16bit = _mm256_loadu_si256((__m256i*)&im_L1[i]);
; 973  :       __m256i sample_L0_b_16bit = _mm256_loadu_si256((__m256i*)&im_L0[i + 16]);

	vmovdqu	ymm5, YMMWORD PTR [rdi+r9]

; 978  :       __m256i sample_L0_L1_b_lo = _mm256_unpacklo_epi16(sample_L0_b_16bit, sample_L1_b_16bit);
; 979  :       __m256i sample_L0_L1_b_hi = _mm256_unpackhi_epi16(sample_L0_b_16bit, sample_L1_b_16bit);

	vpunpckhwd ymm0, ymm5, YMMWORD PTR [r9]
	vmovdqu	ymm7, YMMWORD PTR [rdi+r9-32]

; 984  :       __m256i avg_b_lo = _mm256_madd_epi16(sample_L0_L1_b_lo, all_ones);
; 985  :       __m256i avg_b_hi = _mm256_madd_epi16(sample_L0_L1_b_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm8
	vpunpcklwd ymm0, ymm5, YMMWORD PTR [r9]

; 989  :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);
; 990  :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vpaddd	ymm2, ymm1, ymm9

; 994  :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);
; 995  :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vpsrad	ymm4, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm8
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [r9-32]
	vpaddd	ymm2, ymm1, ymm9
	vpsrad	ymm3, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm8
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [r9-32]
	vpaddd	ymm2, ymm1, ymm9
	mov	edx, ecx
	lea	r9, QWORD PTR [r9+64]
	shr	edx, 6
	mov	eax, ecx

; 996  : 
; 997  :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);
; 998  :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vpackusdw ymm5, ymm3, ymm4
	vpmaddwd ymm1, ymm0, ymm8
	vpsrad	ymm4, ymm2, 7

; 1006 :         case 32: // Same as case 64
; 1007 :         case 64: _mm256_storeu_si256((__m256i*)&dst[y * dst_stride + x], avg); break;

	imul	edx, r10d
	and	eax, 63					; 0000003fH
	vpaddd	ymm2, ymm1, ymm9
	vpsrad	ymm3, ymm2, 7
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm1, ymm0, ymm5
	add	edx, eax
	add	ecx, 32					; 00000020H
	vpermq	ymm2, ymm1, 216				; 000000d8H
	vmovdqu	YMMWORD PTR [rdx+rsi], ymm2
	cmp	ecx, r8d
	jb	$LL233@bipred_ave

; 1010 :           break;
; 1011 :       }
; 1012 :     }
; 1013 :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN658@bipred_ave:

; 1014 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r8d, r8d
	je	$LL5@bipred_ave
	lea	ecx, DWORD PTR [r8-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
	npad	9
$LL238@bipred_ave:

; 1056 :         default:
; 1057 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1057				; 00000421H
	lea	rdx, OFFSET FLAT:$SG4294951300
	lea	rcx, OFFSET FLAT:$SG4294951299
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL238@bipred_ave

; 1152 :       case 64: bipred_average_im_im_template_avx2(dst, im_L0, im_L1, 64, pu_h, dst_stride); break;

	jmp	$LL5@bipred_ave
$LN15@bipred_ave:

; 959  :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovdqu	ymm8, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 960  : 
; 961  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 6
	popcnt	eax, eax
	cmp	eax, 1

; 962  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	eax, DWORD PTR [r8+r8*2]
	lea	r10d, DWORD PTR [rax+rax]
	sete	cl
	test	r10b, 31

; 964  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 965  : 
; 966  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	SHORT $LN308@bipred_ave

; 967  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r10d, r10d
	je	$LL5@bipred_ave

; 964  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 965  : 
; 966  :   if (has_pow2_width && area_mod_32 == 0) {

	test	cl, cl
	je	SHORT $LN301@bipred_ave
	lea	ebx, DWORD PTR [r10-1]
	shr	ebx, 5
	inc	ebx
$LL285@bipred_ave:

; 1008 :         default:
; 1009 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1009				; 000003f1H
	lea	rdx, OFFSET FLAT:$SG4294951302
	lea	rcx, OFFSET FLAT:$SG4294951301
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL285@bipred_ave

; 1010 :           break;
; 1011 :       }
; 1012 :     }
; 1013 :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN301@bipred_ave:
	lea	ecx, DWORD PTR [r10-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
	npad	4
$LL290@bipred_ave:

; 1056 :         default:
; 1057 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1057				; 00000421H
	lea	rdx, OFFSET FLAT:$SG4294951300
	lea	rcx, OFFSET FLAT:$SG4294951299
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL290@bipred_ave

; 1058 :           break;
; 1059 :       }
; 1060 :     }
; 1061 :   } else {

	jmp	$LL5@bipred_ave
$LN308@bipred_ave:

; 1092 :         }
; 1093 :         break;
; 1094 :       case 6: // 6x8
; 1095 :         for (int i = 0; i < pu_w * pu_h; i += 12) {

	xor	ecx, ecx
	test	r10d, r10d
	je	$LL5@bipred_ave
	vmovdqu	ymm6, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	ymm7, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 1102 : 
; 1103 :           __m256i sample_L0_L1_lo = _mm256_unpacklo_epi16(sample_L0_16bit, sample_L1_16bit);
; 1104 :           __m256i sample_L0_L1_hi = _mm256_unpackhi_epi16(sample_L0_16bit, sample_L1_16bit);
; 1105 : 
; 1106 :           __m256i all_ones = _mm256_set1_epi16(1);

	mov	r11d, DWORD PTR dst_stride$[rsp]
	npad	5
$LL300@bipred_ave:

; 1096 : 
; 1097 :           int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx

; 1098 : 
; 1099 :           __m256i mask            = _mm256_setr_epi64x(-1, -1, -1, 0);
; 1100 :           __m256i sample_L0_16bit = _mm256_maskload_epi64((const long long*)(&im_L0[i]), mask);

	movsxd	rax, ecx
	add	ecx, 12
	mov	r9d, edx
	shr	r9d, 2

; 1101 :           __m256i sample_L1_16bit = _mm256_maskload_epi64((const long long*)(&im_L1[i]), mask);

	vpmaskmovq ymm3, ymm6, YMMWORD PTR [rbx+rax*2]
	vpmaskmovq ymm5, ymm6, YMMWORD PTR [rdi+rax*2]

; 1107 :           __m256i avg_a_lo = _mm256_madd_epi16(sample_L0_L1_lo, all_ones);
; 1108 :           __m256i avg_a_hi = _mm256_madd_epi16(sample_L0_L1_hi, all_ones);
; 1109 : 
; 1110 :           avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);
; 1111 :           avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);
; 1112 : 
; 1113 :           avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);
; 1114 :           avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);
; 1115 : 
; 1116 :           __m256i avg256 = _mm256_packus_epi32(avg_a_lo, avg_a_hi);
; 1117 :           avg256         = _mm256_packus_epi16(avg256, avg256);
; 1118 :           avg256         = _mm256_permute4x64_epi64(avg256, _MM_SHUFFLE(3, 1, 2, 0));
; 1119 :           __m128i avg    = _mm256_castsi256_si128(avg256);
; 1120 : 
; 1121 :           uint32_t elements_0123 = _mm_cvtsi128_si32(avg);
; 1122 :           uint16_t elements_45   = _mm_extract_epi16(avg, 2);
; 1123 :           uint16_t elements_67   = _mm_extract_epi16(avg, 3);
; 1124 :           uint32_t elements_89ab = _mm_extract_epi32(avg, 2);
; 1125 :           *(uint32_t*)&dst[(y + 0) * dst_stride + 0] = elements_0123;

	mov	eax, r9d

; 1126 :           *(uint16_t*)&dst[(y + 0) * dst_stride + 4] = elements_45;
; 1127 :           *(uint16_t*)&dst[(y + 1) * dst_stride + 0] = elements_67;

	lea	edx, DWORD PTR [r9+1]
	imul	eax, r11d
	vpunpckhwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm7
	vpaddd	ymm2, ymm1, ymm8
	vpunpcklwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm7
	imul	edx, r11d
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm0, ymm0, ymm0
	vpermq	ymm1, ymm0, 216				; 000000d8H
	vmovd	DWORD PTR [rax+rsi], xmm1
	add	eax, 4
	vpextrw	WORD PTR [rax+rsi], xmm1, 2

; 1128 :           *(uint32_t*)&dst[(y + 1) * dst_stride + 2] = elements_89ab;

	lea	eax, DWORD PTR [rdx+2]
	vpextrw	WORD PTR [rdx+rsi], xmm1, 3
	vpextrd	DWORD PTR [rax+rsi], xmm1, 2
	cmp	ecx, r10d
	jb	$LL300@bipred_ave

; 1153 : 
; 1154 :       case  6: bipred_average_im_im_template_avx2(dst, im_L0, im_L1,  6, pu_h, dst_stride); break;

	jmp	$LL5@bipred_ave
$LN16@bipred_ave:

; 959  :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovdqu	ymm8, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 960  : 
; 961  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 12
	popcnt	eax, eax
	cmp	eax, 1

; 962  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	eax, DWORD PTR [r8+r8*2]
	lea	r9d, DWORD PTR [rax*4]
	sete	cl
	test	r9b, 31

; 964  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 965  : 
; 966  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN48@bipred_ave
	test	cl, cl
	je	SHORT $LN353@bipred_ave

; 967  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r9d, r9d
	je	$LL5@bipred_ave
	lea	ebx, DWORD PTR [r9-1]
	shr	ebx, 5
	inc	ebx
$LL337@bipred_ave:

; 1008 :         default:
; 1009 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1009				; 000003f1H
	lea	rdx, OFFSET FLAT:$SG4294951302
	lea	rcx, OFFSET FLAT:$SG4294951301
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL337@bipred_ave

; 1010 :           break;
; 1011 :       }
; 1012 :     }
; 1013 :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN353@bipred_ave:

; 1014 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	xor	ecx, ecx
	test	r9d, r9d
	je	$LL5@bipred_ave

; 1030 : 
; 1031 :       __m256i all_ones = _mm256_set1_epi16(1);

	mov	r11d, DWORD PTR dst_stride$[rsp]
	lea	r10, QWORD PTR [rbx+32]
	vmovdqu	ymm9, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vmovdqu	ymm10, YMMWORD PTR __ymm@0000000000000000000000000000000000000000ffffffffffffffffffffffff
	vmovaps	XMMWORD PTR [rsp+32], xmm11
	sub	rdi, rbx
	vmovdqu	ymm11, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffff000000000000000000000000
	lea	r14d, DWORD PTR [r11-12]
	npad	9
$LL342@bipred_ave:

; 1021 :       __m256i sample_L0_a_16bit = _mm256_loadu_si256((__m256i*)&im_L0[i]);
; 1022 :       __m256i sample_L1_a_16bit = _mm256_loadu_si256((__m256i*)&im_L1[i]);
; 1023 :       __m256i sample_L0_b_16bit = _mm256_castsi128_si256(_mm_loadu_si128((__m128i*)&im_L0[i + 16]));

	vmovups	xmm5, XMMWORD PTR [rdi+r10]

; 1024 :       __m256i sample_L1_b_16bit = _mm256_castsi128_si256(_mm_loadu_si128((__m128i*)&im_L1[i + 16]));

	vmovups	xmm3, XMMWORD PTR [r10]
	vmovdqu	ymm7, YMMWORD PTR [rdi+r10-32]

; 1025 : 
; 1026 :       __m256i sample_L0_L1_a_lo = _mm256_unpacklo_epi16(sample_L0_a_16bit, sample_L1_a_16bit);
; 1027 :       __m256i sample_L0_L1_a_hi = _mm256_unpackhi_epi16(sample_L0_a_16bit, sample_L1_a_16bit);
; 1028 :       __m256i sample_L0_L1_b_lo = _mm256_unpacklo_epi16(sample_L0_b_16bit, sample_L1_b_16bit);
; 1029 :       __m256i sample_L0_L1_b_hi = _mm256_unpackhi_epi16(sample_L0_b_16bit, sample_L1_b_16bit);

	vpunpckhwd ymm0, ymm5, ymm3

; 1032 :       __m256i avg_a_lo = _mm256_madd_epi16(sample_L0_L1_a_lo, all_ones);
; 1033 :       __m256i avg_a_hi = _mm256_madd_epi16(sample_L0_L1_a_hi, all_ones);
; 1034 :       __m256i avg_b_lo = _mm256_madd_epi16(sample_L0_L1_b_lo, all_ones);
; 1035 :       __m256i avg_b_hi = _mm256_madd_epi16(sample_L0_L1_b_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm9

; 1036 : 
; 1037 :       avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);
; 1038 :       avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);
; 1039 :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);
; 1040 :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vpaddd	ymm2, ymm1, ymm8

; 1041 : 
; 1042 :       avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);
; 1043 :       avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);
; 1044 :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);
; 1045 :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vpsrad	ymm4, ymm2, 7
	vpunpcklwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm9
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [r10-32]
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm9
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [r10-32]
	vpaddd	ymm2, ymm1, ymm8
	mov	r8d, ecx
	lea	r10, QWORD PTR [r10+48]

; 1046 : 
; 1047 :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);
; 1048 :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vpackusdw ymm5, ymm3, ymm4
	vpmaddwd ymm1, ymm0, ymm9
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	vpsrad	ymm3, ymm2, 7
	shr	edx, 3
	add	ecx, 24
	vpackusdw ymm0, ymm3, ymm4

; 1049 :       __m256i avg0213 = _mm256_packus_epi16(avg_01, avg_23);

	vpackuswb ymm1, ymm0, ymm5

; 1050 :       __m256i avg     = _mm256_permute4x64_epi64(avg0213, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm2, ymm1, 216				; 000000d8H
	lea	eax, DWORD PTR [rdx+rdx*2]

; 1051 : 
; 1052 :       switch (pu_w) {
; 1053 :         case 12: scatter_ymm_12x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	imul	edx, r11d
	shl	eax, 2
	sub	r8d, eax
	lea	eax, DWORD PTR [r8+rdx]
	add	rax, rsi

; 809  :   _mm256_maskstore_epi32((int32_t*)dst, mask_a, ymm); dst += dst_stride - 3 * 4;

	vpmaskmovd YMMWORD PTR [rax], ymm10, ymm2

; 810  :   _mm256_maskstore_epi32((int32_t*)dst, mask_b, ymm);

	vpmaskmovd YMMWORD PTR [r14+rax], ymm11, ymm2

; 1014 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	cmp	ecx, r9d
	jb	$LL342@bipred_ave

; 1058 :           break;
; 1059 :       }
; 1060 :     }
; 1061 :   } else {

	vmovaps	xmm11, XMMWORD PTR [rsp+32]
	jmp	$LL5@bipred_ave
$LN17@bipred_ave:

; 959  :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovdqu	ymm8, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 960  : 
; 961  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 24
	popcnt	eax, eax
	cmp	eax, 1
	sete	cl

; 962  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	imul	eax, r8d, -8				; fffffff8H
	test	al, 31

; 964  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 965  : 
; 966  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN48@bipred_ave
	lea	r9d, DWORD PTR [r8+r8*2]
	shl	r9d, 3
	test	cl, cl
	je	SHORT $LN405@bipred_ave

; 967  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r9d, r9d
	je	$LL5@bipred_ave
	lea	ebx, DWORD PTR [r9-1]
	shr	ebx, 5
	inc	ebx
	npad	2
$LL389@bipred_ave:

; 1008 :         default:
; 1009 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1009				; 000003f1H
	lea	rdx, OFFSET FLAT:$SG4294951302
	lea	rcx, OFFSET FLAT:$SG4294951301
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL389@bipred_ave

; 1010 :           break;
; 1011 :       }
; 1012 :     }
; 1013 :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN405@bipred_ave:

; 1014 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	xor	ecx, ecx
	test	r9d, r9d
	je	$LL5@bipred_ave
	vmovdqu	ymm10, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	ymm9, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 1015 : 
; 1016 :       int y = i / pu_w;
; 1017 :       int x = i % pu_w;
; 1018 : 
; 1019 :       // Last 64 bits of the 256 are not used to simplify the loop
; 1020 :       __m256i mask              = _mm256_setr_epi64x(-1, -1, -1, 0);

	mov	r11d, DWORD PTR dst_stride$[rsp]
	lea	r10, QWORD PTR [rbx+32]
	sub	rdi, rbx
$LL394@bipred_ave:

; 1021 :       __m256i sample_L0_a_16bit = _mm256_loadu_si256((__m256i*)&im_L0[i]);
; 1022 :       __m256i sample_L1_a_16bit = _mm256_loadu_si256((__m256i*)&im_L1[i]);
; 1023 :       __m256i sample_L0_b_16bit = _mm256_castsi128_si256(_mm_loadu_si128((__m128i*)&im_L0[i + 16]));

	vmovups	xmm5, XMMWORD PTR [rdi+r10]

; 1024 :       __m256i sample_L1_b_16bit = _mm256_castsi128_si256(_mm_loadu_si128((__m128i*)&im_L1[i + 16]));

	vmovups	xmm3, XMMWORD PTR [r10]
	vmovdqu	ymm7, YMMWORD PTR [rdi+r10-32]

; 1025 : 
; 1026 :       __m256i sample_L0_L1_a_lo = _mm256_unpacklo_epi16(sample_L0_a_16bit, sample_L1_a_16bit);
; 1027 :       __m256i sample_L0_L1_a_hi = _mm256_unpackhi_epi16(sample_L0_a_16bit, sample_L1_a_16bit);
; 1028 :       __m256i sample_L0_L1_b_lo = _mm256_unpacklo_epi16(sample_L0_b_16bit, sample_L1_b_16bit);
; 1029 :       __m256i sample_L0_L1_b_hi = _mm256_unpackhi_epi16(sample_L0_b_16bit, sample_L1_b_16bit);

	vpunpckhwd ymm0, ymm5, ymm3

; 1032 :       __m256i avg_a_lo = _mm256_madd_epi16(sample_L0_L1_a_lo, all_ones);
; 1033 :       __m256i avg_a_hi = _mm256_madd_epi16(sample_L0_L1_a_hi, all_ones);
; 1034 :       __m256i avg_b_lo = _mm256_madd_epi16(sample_L0_L1_b_lo, all_ones);
; 1035 :       __m256i avg_b_hi = _mm256_madd_epi16(sample_L0_L1_b_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm9

; 1036 : 
; 1037 :       avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);
; 1038 :       avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);
; 1039 :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);
; 1040 :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vpaddd	ymm2, ymm1, ymm8

; 1041 : 
; 1042 :       avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);
; 1043 :       avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);
; 1044 :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);
; 1045 :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vpsrad	ymm4, ymm2, 7
	vpunpcklwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm9
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [r10-32]
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm9
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [r10-32]
	vpaddd	ymm2, ymm1, ymm8

; 1046 : 
; 1047 :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);
; 1048 :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vpackusdw ymm5, ymm3, ymm4
	vpmaddwd ymm1, ymm0, ymm9
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	mov	r8d, ecx
	lea	r10, QWORD PTR [r10+48]
	vpsrad	ymm3, ymm2, 7
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	vpackusdw ymm0, ymm3, ymm4
	shr	edx, 4
	add	ecx, 24

; 1049 :       __m256i avg0213 = _mm256_packus_epi16(avg_01, avg_23);

	vpackuswb ymm1, ymm0, ymm5

; 1050 :       __m256i avg     = _mm256_permute4x64_epi64(avg0213, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm2, ymm1, 216				; 000000d8H
	lea	eax, DWORD PTR [rdx+rdx*2]

; 1054 :         case 24: // Same as case 48
; 1055 :         case 48: _mm256_maskstore_epi64((long long*)&dst[y * dst_stride + x], mask, avg); break;

	imul	edx, r11d
	shl	eax, 3
	sub	r8d, eax
	add	edx, r8d
	vpmaskmovq YMMWORD PTR [rdx+rsi], ymm10, ymm2
	cmp	ecx, r9d
	jb	$LL394@bipred_ave

; 1058 :           break;
; 1059 :       }
; 1060 :     }
; 1061 :   } else {

	jmp	$LL5@bipred_ave
$LN18@bipred_ave:

; 959  :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovdqu	ymm8, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 960  : 
; 961  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 48					; 00000030H
	popcnt	eax, eax
	cmp	eax, 1
	sete	cl

; 962  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	imul	eax, r8d, -16				; fffffff0H
	test	al, 31

; 964  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 965  : 
; 966  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN48@bipred_ave
	lea	r9d, DWORD PTR [r8+r8*2]
	shl	r9d, 4
	test	cl, cl
	je	SHORT $LN457@bipred_ave

; 967  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r9d, r9d
	je	$LL5@bipred_ave
	lea	ebx, DWORD PTR [r9-1]
	shr	ebx, 5
	inc	ebx
$LL441@bipred_ave:

; 1008 :         default:
; 1009 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1009				; 000003f1H
	lea	rdx, OFFSET FLAT:$SG4294951302
	lea	rcx, OFFSET FLAT:$SG4294951301
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL441@bipred_ave

; 1010 :           break;
; 1011 :       }
; 1012 :     }
; 1013 :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN457@bipred_ave:

; 1014 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	xor	ecx, ecx
	test	r9d, r9d
	je	$LL5@bipred_ave
	vmovdqu	ymm10, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	ymm9, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 1015 : 
; 1016 :       int y = i / pu_w;
; 1017 :       int x = i % pu_w;
; 1018 : 
; 1019 :       // Last 64 bits of the 256 are not used to simplify the loop
; 1020 :       __m256i mask              = _mm256_setr_epi64x(-1, -1, -1, 0);

	mov	r11d, DWORD PTR dst_stride$[rsp]
	lea	r10, QWORD PTR [rbx+32]
	sub	rdi, rbx
$LL446@bipred_ave:

; 1021 :       __m256i sample_L0_a_16bit = _mm256_loadu_si256((__m256i*)&im_L0[i]);
; 1022 :       __m256i sample_L1_a_16bit = _mm256_loadu_si256((__m256i*)&im_L1[i]);
; 1023 :       __m256i sample_L0_b_16bit = _mm256_castsi128_si256(_mm_loadu_si128((__m128i*)&im_L0[i + 16]));

	vmovups	xmm5, XMMWORD PTR [rdi+r10]

; 1024 :       __m256i sample_L1_b_16bit = _mm256_castsi128_si256(_mm_loadu_si128((__m128i*)&im_L1[i + 16]));

	vmovups	xmm3, XMMWORD PTR [r10]
	vmovdqu	ymm7, YMMWORD PTR [rdi+r10-32]

; 1025 : 
; 1026 :       __m256i sample_L0_L1_a_lo = _mm256_unpacklo_epi16(sample_L0_a_16bit, sample_L1_a_16bit);
; 1027 :       __m256i sample_L0_L1_a_hi = _mm256_unpackhi_epi16(sample_L0_a_16bit, sample_L1_a_16bit);
; 1028 :       __m256i sample_L0_L1_b_lo = _mm256_unpacklo_epi16(sample_L0_b_16bit, sample_L1_b_16bit);
; 1029 :       __m256i sample_L0_L1_b_hi = _mm256_unpackhi_epi16(sample_L0_b_16bit, sample_L1_b_16bit);

	vpunpckhwd ymm0, ymm5, ymm3

; 1032 :       __m256i avg_a_lo = _mm256_madd_epi16(sample_L0_L1_a_lo, all_ones);
; 1033 :       __m256i avg_a_hi = _mm256_madd_epi16(sample_L0_L1_a_hi, all_ones);
; 1034 :       __m256i avg_b_lo = _mm256_madd_epi16(sample_L0_L1_b_lo, all_ones);
; 1035 :       __m256i avg_b_hi = _mm256_madd_epi16(sample_L0_L1_b_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm9

; 1036 : 
; 1037 :       avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);
; 1038 :       avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);
; 1039 :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);
; 1040 :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vpaddd	ymm2, ymm1, ymm8

; 1041 : 
; 1042 :       avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);
; 1043 :       avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);
; 1044 :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);
; 1045 :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vpsrad	ymm4, ymm2, 7
	vpunpcklwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm9
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [r10-32]
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm9
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [r10-32]
	vpaddd	ymm2, ymm1, ymm8

; 1046 : 
; 1047 :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);
; 1048 :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vpackusdw ymm5, ymm3, ymm4
	vpmaddwd ymm1, ymm0, ymm9
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	mov	r8d, ecx
	lea	r10, QWORD PTR [r10+48]
	vpsrad	ymm3, ymm2, 7
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	vpackusdw ymm0, ymm3, ymm4
	shr	edx, 5
	add	ecx, 24

; 1049 :       __m256i avg0213 = _mm256_packus_epi16(avg_01, avg_23);

	vpackuswb ymm1, ymm0, ymm5

; 1050 :       __m256i avg     = _mm256_permute4x64_epi64(avg0213, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm2, ymm1, 216				; 000000d8H
	lea	eax, DWORD PTR [rdx+rdx*2]

; 1054 :         case 24: // Same as case 48
; 1055 :         case 48: _mm256_maskstore_epi64((long long*)&dst[y * dst_stride + x], mask, avg); break;

	imul	edx, r11d
	shl	eax, 4
	sub	r8d, eax
	add	edx, r8d
	vpmaskmovq YMMWORD PTR [rdx+rsi], ymm10, ymm2
	cmp	ecx, r9d
	jb	$LL446@bipred_ave

; 1058 :           break;
; 1059 :       }
; 1060 :     }
; 1061 :   } else {

	jmp	$LL5@bipred_ave
$LN19@bipred_ave:

; 1155 :       case 12: bipred_average_im_im_template_avx2(dst, im_L0, im_L1, 12, pu_h, dst_stride); break;
; 1156 :       case 24: bipred_average_im_im_template_avx2(dst, im_L0, im_L1, 24, pu_h, dst_stride); break;
; 1157 :       case 48: bipred_average_im_im_template_avx2(dst, im_L0, im_L1, 48, pu_h, dst_stride); break;
; 1158 :       default:
; 1159 :         assert(0 && "Unexpected block width.");

	mov	r8d, 1159				; 00000487H
	lea	rdx, OFFSET FLAT:$SG4294951296
	lea	rcx, OFFSET FLAT:$SG4294951295
	call	QWORD PTR __imp__wassert

; 1160 :         break;
; 1161 :     }
; 1162 :   } else {

	jmp	SHORT $LL5@bipred_ave
$LN7@bipred_ave:

; 1163 :     int32_t shift = 15 - KVZ_BIT_DEPTH; // TODO: defines
; 1164 :     int32_t offset = 1 << (shift - 1);
; 1165 : 
; 1166 :     for (int i = 0; i < pu_w * pu_h; ++i)

	mov	r14d, r11d
	xor	ecx, ecx
	imul	r14d, r8d
	test	r14d, r14d
	je	SHORT $LL5@bipred_ave
	mov	r15d, DWORD PTR dst_stride$[rsp]
	sub	rdi, rbx
	npad	10
$LL6@bipred_ave:

; 1167 :     {
; 1168 :       int y = i / pu_w;
; 1169 :       int x = i % pu_w;
; 1170 :       int16_t sample_L0 = im_L0[i];
; 1171 :       int16_t sample_L1 = im_L1[i];
; 1172 :       int32_t rounded = (sample_L0 + sample_L1 + offset) >> shift;

	movsx	r10d, WORD PTR [rbx]
	xor	edx, edx
	movsx	r9d, WORD PTR [rdi+rbx]
	add	r10d, 64				; 00000040H
	add	r10d, r9d
	mov	eax, ecx
	sar	r10d, 7
	div	r11d
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\picture-generic.c

; 69   :   if (value & ~PIXEL_MAX) {

	test	r10d, -256				; ffffffffffffff00H
	je	SHORT $LN491@bipred_ave

; 70   :     int32_t temp = (-value) >> 31;

	neg	r10d
	sar	r10d, 31
$LN491@bipred_ave:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 1173 :       dst[y * dst_stride + x] = kvz_fast_clip_32bit_to_pixel(rounded);

	imul	eax, r15d
	inc	ecx
	add	rbx, 2
	add	eax, edx
	mov	BYTE PTR [rax+rsi], r10b
	cmp	ecx, r14d
	jb	SHORT $LL6@bipred_ave
$LL5@bipred_ave:
	vzeroupper

; 1174 :     }
; 1175 :   }
; 1176 : }

	vmovaps	xmm6, XMMWORD PTR [rsp+112]
	lea	r11, QWORD PTR [rsp+128]
	mov	rbx, QWORD PTR [r11+32]
	mov	rsi, QWORD PTR [r11+40]
	mov	rdi, QWORD PTR [r11+48]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm9, XMMWORD PTR [r11-64]
	vmovaps	xmm10, XMMWORD PTR [r11-80]
	vmovaps	xmm7, XMMWORD PTR [rsp+96]
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r12
	ret	0
	npad	1
$LN689@bipred_ave:
	DD	$LN10@bipred_ave
	DD	$LN15@bipred_ave
	DD	$LN11@bipred_ave
	DD	$LN16@bipred_ave
	DD	$LN12@bipred_ave
	DD	$LN17@bipred_ave
	DD	$LN13@bipred_ave
	DD	$LN18@bipred_ave
	DD	$LN14@bipred_ave
	DD	$LN19@bipred_ave
$LN688@bipred_ave:
	DB	0
	DB	9
	DB	1
	DB	9
	DB	2
	DB	9
	DB	9
	DB	9
	DB	3
	DB	9
	DB	9
	DB	9
	DB	4
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	5
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	6
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	7
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	8
bipred_average_im_im_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
tv1436 = 32
dst$ = 160
px$ = 168
im$ = 176
pu_w$ = 184
pu_h$ = 192
dst_stride$ = 200
bipred_average_px_im_template_avx2 PROC

; 1184 : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rsi
	mov	QWORD PTR [rsp+24], rdi
	mov	QWORD PTR [rsp+32], r12
	push	r13
	push	r14
	push	r15
	sub	rsp, 128				; 00000080H

; 1185 :   int32_t shift = 15 - KVZ_BIT_DEPTH; // TODO: defines
; 1186 :   int32_t scalar_offset = 1 << (shift - 1);
; 1187 :   __m256i offset = _mm256_set1_epi32(scalar_offset);
; 1188 : 
; 1189 :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	popcnt	eax, r9d
	cmp	eax, 1
	vmovaps	XMMWORD PTR [rsp+80], xmm8

; 1190 :   bool area_mod_32 = (pu_w * pu_h) % 32;

	mov	eax, DWORD PTR pu_h$[rsp]
	mov	r13d, r9d
	vmovdqu	ymm8, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040
	sete	r15b
	mov	edi, r9d
	imul	r13d, eax
	mov	rsi, r8
	mov	r12, rdx
	mov	r14, rcx
	mov	ebx, r13d
	and	ebx, 31

; 1191 :   assert(!(pu_w == 4 && pu_h == 4) && "Branch for 4x4 not yet implemented.");

	cmp	r9d, 4
	jne	SHORT $LN38@bipred_ave
	cmp	eax, r9d
	jne	SHORT $LN38@bipred_ave
	mov	r8d, 1191				; 000004a7H
	lea	rdx, OFFSET FLAT:$SG4294951293
	lea	rcx, OFFSET FLAT:$SG4294951292
	vzeroupper
	call	QWORD PTR __imp__wassert
	vmovdqu	ymm8, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040
$LN38@bipred_ave:

; 1192 :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 1193 : 
; 1194 :   if (has_pow2_width && area_mod_32 == 0) {

	vmovaps	XMMWORD PTR [rsp+112], xmm6
	vmovaps	XMMWORD PTR [rsp+96], xmm7
	vmovaps	XMMWORD PTR [rsp+64], xmm9
	test	ebx, ebx
	jne	$LN27@bipred_ave
	test	r15b, r15b
	je	$LN20@bipred_ave

; 1195 :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r13d, r13d
	je	$LN18@bipred_ave

; 1211 : 
; 1212 :       __m256i all_ones  = _mm256_set1_epi16(1);

	vmovdqu	ymm9, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	mov	r15d, DWORD PTR dst_stride$[rsp]
	npad	9
$LL4@bipred_ave:

; 1196 : 
; 1197 :       int y = i / pu_w;
; 1198 :       int x = i % pu_w;
; 1199 : 
; 1200 :       __m256i sample_px_a_16bit = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&px[i]));

	vpmovzxbw ymm1, XMMWORD PTR [r12+16]
	vpmovzxbw ymm0, XMMWORD PTR [r12]

; 1201 :       __m256i sample_px_b_16bit = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&px[i + 16]));
; 1202 :       sample_px_a_16bit         = _mm256_slli_epi16(sample_px_a_16bit, 14 - KVZ_BIT_DEPTH);
; 1203 :       sample_px_b_16bit         = _mm256_slli_epi16(sample_px_b_16bit, 14 - KVZ_BIT_DEPTH);

	vpsllw	ymm5, ymm1, 6
	vpsllw	ymm7, ymm0, 6

; 1204 :       __m256i sample_im_a_16bit = _mm256_loadu_si256((__m256i*)&im[i]);
; 1205 :       __m256i sample_im_b_16bit = _mm256_loadu_si256((__m256i*)&im[i + 16]);
; 1206 : 
; 1207 :       __m256i sample_px_im_a_lo = _mm256_unpacklo_epi16(sample_px_a_16bit, sample_im_a_16bit);
; 1208 :       __m256i sample_px_im_a_hi = _mm256_unpackhi_epi16(sample_px_a_16bit, sample_im_a_16bit);
; 1209 :       __m256i sample_px_im_b_lo = _mm256_unpacklo_epi16(sample_px_b_16bit, sample_im_b_16bit);
; 1210 :       __m256i sample_px_im_b_hi = _mm256_unpackhi_epi16(sample_px_b_16bit, sample_im_b_16bit);

	vpunpckhwd ymm0, ymm5, YMMWORD PTR [rsi+32]

; 1213 :       __m256i avg_a_lo = _mm256_madd_epi16(sample_px_im_a_lo, all_ones);
; 1214 :       __m256i avg_a_hi = _mm256_madd_epi16(sample_px_im_a_hi, all_ones);
; 1215 :       __m256i avg_b_lo = _mm256_madd_epi16(sample_px_im_b_lo, all_ones);
; 1216 :       __m256i avg_b_hi = _mm256_madd_epi16(sample_px_im_b_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm9
	vpunpcklwd ymm0, ymm5, YMMWORD PTR [rsi+32]

; 1217 : 
; 1218 :       avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);
; 1219 :       avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);
; 1220 :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);
; 1221 :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vpaddd	ymm2, ymm1, ymm8

; 1222 : 
; 1223 :       avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);
; 1224 :       avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);
; 1225 :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);
; 1226 :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vpsrad	ymm4, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm9
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [rsi]
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm9
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [rsi]
	vpaddd	ymm2, ymm1, ymm8

; 1227 : 
; 1228 :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);
; 1229 :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vpackusdw ymm5, ymm3, ymm4
	vpmaddwd ymm1, ymm0, ymm9
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	xor	edx, edx
	mov	eax, ebx
	vpackusdw ymm0, ymm3, ymm4
	div	edi

; 1230 :       __m256i avg0213 = _mm256_packus_epi16(avg_01, avg_23);

	vpackuswb ymm1, ymm0, ymm5

; 1231 :       __m256i avg     = _mm256_permute4x64_epi64(avg0213, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm2, ymm1, 216				; 000000d8H

; 1232 : 
; 1233 :       switch (pu_w) {

	cmp	edi, 4
	je	$LN22@bipred_ave
	cmp	edi, 8
	je	SHORT $LN23@bipred_ave
	cmp	edi, 16
	je	SHORT $LN24@bipred_ave
	lea	ecx, DWORD PTR [rdi-32]
	test	ecx, -33				; ffffffdfH
	je	SHORT $LN25@bipred_ave

; 1239 :         default:
; 1240 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1240				; 000004d8H
	lea	rdx, OFFSET FLAT:$SG4294951288
	lea	rcx, OFFSET FLAT:$SG4294951287
	vzeroupper
	call	QWORD PTR __imp__wassert
	vmovdqu	ymm8, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040
	vmovdqu	ymm9, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	jmp	$LN2@bipred_ave
$LN25@bipred_ave:

; 1237 :         case 32: // Same as case 64
; 1238 :         case 64: _mm256_storeu_si256((__m256i*)&dst[y * dst_stride + x], avg); break;

	imul	eax, r15d
	add	eax, edx
	vmovdqu	YMMWORD PTR [rax+r14], ymm2
	jmp	$LN2@bipred_ave
$LN24@bipred_ave:

; 1236 :         case 16: scatter_ymm_16x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	imul	eax, r15d
	lea	ecx, DWORD PTR [rdx+rax]
	add	rcx, r14

; 801  :   _mm_storeu_si128((__m128i *)dst, ymm_lo); dst += dst_stride;

	vmovdqu	XMMWORD PTR [rcx], xmm2
	vextracti128 XMMWORD PTR [r15+rcx], ymm2, 1

; 1236 :         case 16: scatter_ymm_16x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	jmp	SHORT $LN2@bipred_ave
$LN23@bipred_ave:

; 1235 :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	imul	eax, r15d

; 790  :   __m128d ymm_hi = _mm256_extractf128_pd(ymm_as_m256d, 1);

	vextractf128 xmm0, ymm2, 1

; 1235 :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	lea	ecx, DWORD PTR [rdx+rax]
	add	rcx, r14

; 791  :   _mm_storel_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovlpd	QWORD PTR [rcx], xmm2
	add	rcx, r15

; 792  :   _mm_storeh_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovhpd	QWORD PTR [rcx], xmm2
	add	rcx, r15

; 793  :   _mm_storel_pd((double*)dst, ymm_hi); dst += dst_stride;

	vmovlpd	QWORD PTR [rcx], xmm0

; 794  :   _mm_storeh_pd((double*)dst, ymm_hi);

	vmovhpd	QWORD PTR [r15+rcx], xmm0

; 1235 :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	jmp	SHORT $LN2@bipred_ave
$LN22@bipred_ave:

; 775  :   __m128i ymm_hi = _mm256_extracti128_si256(ymm, 1);

	vextracti128 xmm0, ymm2, 1

; 1234 :         case  4: scatter_ymm_4x8_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	imul	eax, r15d
	add	edx, eax
	add	rdx, r14

; 776  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_lo); dst += dst_stride;

	vmovd	DWORD PTR [rdx], xmm2
	add	rdx, r15

; 777  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 1); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm2, 1
	add	rdx, r15

; 778  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 2); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm2, 2
	add	rdx, r15

; 779  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 3); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm2, 3
	add	rdx, r15

; 780  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_hi); dst += dst_stride;

	vmovd	DWORD PTR [rdx], xmm0
	add	rdx, r15

; 781  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 1); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm0, 1
	add	rdx, r15

; 782  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 2); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm0, 2

; 783  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 3);

	vpextrd	DWORD PTR [r15+rdx], xmm0, 3
$LN2@bipred_ave:

; 1195 :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	add	ebx, 32					; 00000020H
	add	r12, 32					; 00000020H
	add	rsi, 64					; 00000040H
	cmp	ebx, r13d
	jb	$LL4@bipred_ave

; 1241 :           break;
; 1242 :       }
; 1243 :     }
; 1244 :   } else if (area_mod_32 == 0) {

	jmp	$LN18@bipred_ave
$LN20@bipred_ave:

; 1245 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r13d, r13d
	je	$LN18@bipred_ave

; 1246 : 
; 1247 :       int y = i / pu_w;
; 1248 :       int x = i % pu_w;
; 1249 : 
; 1250 :       // Last 64 bits of the 256 / 32 bits of the 128 are not used to simplify the loop
; 1251 :       __m256i mask              = _mm256_setr_epi64x(-1, -1, -1, 0);

	vmovdqu	ymm9, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	mov	r15d, DWORD PTR dst_stride$[rsp]
	mov	rcx, r12
	vmovaps	XMMWORD PTR [rsp+48], xmm10
	vmovdqu	ymm10, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	mov	QWORD PTR tv1436[rsp], rcx
	npad	9
$LL9@bipred_ave:

; 1252 :       __m128i sample_px_a_8bit  = _mm_loadu_si128((__m128i*)&px[i]);
; 1253 :       __m128i sample_px_b_8bit  = _mm_loadl_epi64((__m128i*)&px[i + 16]);
; 1254 :       __m256i sample_px_a_16bit = _mm256_cvtepu8_epi16(sample_px_a_8bit);

	vpmovzxbw ymm0, XMMWORD PTR [rcx]

; 1255 :       __m256i sample_px_b_16bit = _mm256_cvtepu8_epi16(sample_px_b_8bit);
; 1256 :       sample_px_a_16bit         = _mm256_slli_epi16(sample_px_a_16bit, 14 - KVZ_BIT_DEPTH);
; 1257 :       sample_px_b_16bit         = _mm256_slli_epi16(sample_px_b_16bit, 14 - KVZ_BIT_DEPTH);
; 1258 :       __m256i sample_im_a_16bit = _mm256_loadu_si256((__m256i*)&im[i]);
; 1259 :       __m256i sample_im_b_16bit = _mm256_castsi128_si256(_mm_loadu_si128((__m128i*)&im[i + 16]));

	vmovups	xmm3, XMMWORD PTR [rsi+32]
	vpsllw	ymm7, ymm0, 6
	xor	edx, edx
	lea	ecx, DWORD PTR [rbx+16]
	movsxd	r8, ecx
	mov	eax, ebx
	div	edi
	vmovq	xmm0, QWORD PTR [r8+r12]
	vpmovzxbw ymm1, xmm0
	vpsllw	ymm5, ymm1, 6

; 1260 : 
; 1261 :       __m256i sample_px_im_a_lo = _mm256_unpacklo_epi16(sample_px_a_16bit, sample_im_a_16bit);
; 1262 :       __m256i sample_px_im_a_hi = _mm256_unpackhi_epi16(sample_px_a_16bit, sample_im_a_16bit);
; 1263 :       __m256i sample_px_im_b_lo = _mm256_unpacklo_epi16(sample_px_b_16bit, sample_im_b_16bit);
; 1264 :       __m256i sample_px_im_b_hi = _mm256_unpackhi_epi16(sample_px_b_16bit, sample_im_b_16bit);

	vpunpckhwd ymm0, ymm5, ymm3

; 1265 : 
; 1266 :       __m256i all_ones = _mm256_set1_epi16(1);
; 1267 :       __m256i avg_a_lo = _mm256_madd_epi16(sample_px_im_a_lo, all_ones);
; 1268 :       __m256i avg_a_hi = _mm256_madd_epi16(sample_px_im_a_hi, all_ones);
; 1269 :       __m256i avg_b_lo = _mm256_madd_epi16(sample_px_im_b_lo, all_ones);
; 1270 :       __m256i avg_b_hi = _mm256_madd_epi16(sample_px_im_b_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm10

; 1271 : 
; 1272 :       avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);
; 1273 :       avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);
; 1274 :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);
; 1275 :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vpaddd	ymm2, ymm1, ymm8

; 1276 : 
; 1277 :       avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);
; 1278 :       avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);
; 1279 :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);
; 1280 :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vpsrad	ymm4, ymm2, 7
	vpunpcklwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm10
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [rsi]
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm10
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [rsi]
	vpaddd	ymm2, ymm1, ymm8

; 1281 : 
; 1282 :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);
; 1283 :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vpackusdw ymm5, ymm3, ymm4
	vpmaddwd ymm1, ymm0, ymm10
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	vpackusdw ymm0, ymm3, ymm4

; 1284 :       __m256i avg0213 = _mm256_packus_epi16(avg_01, avg_23);

	vpackuswb ymm1, ymm0, ymm5

; 1285 :       __m256i avg     = _mm256_permute4x64_epi64(avg0213, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm2, ymm1, 216				; 000000d8H

; 1286 : 
; 1287 :       switch (pu_w) {

	cmp	edi, 12
	je	SHORT $LN29@bipred_ave
	cmp	edi, 24
	je	SHORT $LN30@bipred_ave
	cmp	edi, 48					; 00000030H
	je	SHORT $LN30@bipred_ave

; 1291 :         default:
; 1292 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1292				; 0000050cH
	lea	rdx, OFFSET FLAT:$SG4294951286
	lea	rcx, OFFSET FLAT:$SG4294951285
	vzeroupper
	call	QWORD PTR __imp__wassert
	vmovdqu	ymm8, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040
	vmovdqu	ymm9, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	ymm10, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	jmp	SHORT $LN7@bipred_ave
$LN30@bipred_ave:

; 1289 :         case 24: // Same as case 48
; 1290 :         case 48: _mm256_maskstore_epi64((long long*)&dst[y * dst_stride + x], mask, avg); break;

	imul	eax, r15d
	add	eax, edx
	vpmaskmovq YMMWORD PTR [rax+r14], ymm9, ymm2
	jmp	SHORT $LN7@bipred_ave
$LN29@bipred_ave:

; 807  :   __m256i mask_a = _mm256_setr_epi32(-1, -1, -1, 0, 0, 0, 0, 0);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000000000000000000000000000000000000ffffffffffffffffffffffff

; 808  :   __m256i mask_b = _mm256_setr_epi32(0, 0, 0, -1, -1, -1, 0, 0);

	vmovdqu	ymm1, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffff000000000000000000000000

; 1288 :         case 12: scatter_ymm_12x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	imul	eax, r15d
	add	edx, eax

; 809  :   _mm256_maskstore_epi32((int32_t*)dst, mask_a, ymm); dst += dst_stride - 3 * 4;

	lea	eax, DWORD PTR [r15-12]

; 1288 :         case 12: scatter_ymm_12x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	add	rdx, r14

; 809  :   _mm256_maskstore_epi32((int32_t*)dst, mask_a, ymm); dst += dst_stride - 3 * 4;

	vpmaskmovd YMMWORD PTR [rdx], ymm0, ymm2

; 810  :   _mm256_maskstore_epi32((int32_t*)dst, mask_b, ymm);

	vpmaskmovd YMMWORD PTR [rax+rdx], ymm1, ymm2
$LN7@bipred_ave:

; 1245 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	mov	rcx, QWORD PTR tv1436[rsp]
	add	ebx, 24
	add	rcx, 24
	add	rsi, 48					; 00000030H
	mov	QWORD PTR tv1436[rsp], rcx
	cmp	ebx, r13d
	jb	$LL9@bipred_ave

; 1293 :           break;
; 1294 :       }
; 1295 :     }
; 1296 :   } else {

	vmovaps	xmm10, XMMWORD PTR [rsp+48]
	jmp	$LN18@bipred_ave
$LN27@bipred_ave:

; 1297 :     // 8x2, 8x6, 6x8 blocks (and maybe 2x8 in the future)
; 1298 :     switch (pu_w) {

	cmp	edi, 6
	je	$LN33@bipred_ave
	cmp	edi, 8
	je	SHORT $LN32@bipred_ave

; 1370 :         }
; 1371 :         break;
; 1372 :       default:
; 1373 :         assert(0 && "Unexpected block width.");

	mov	r8d, 1373				; 0000055dH
	lea	rdx, OFFSET FLAT:$SG4294951284
	lea	rcx, OFFSET FLAT:$SG4294951283
	vzeroupper
	call	QWORD PTR __imp__wassert
	jmp	$LN18@bipred_ave
$LN32@bipred_ave:

; 1299 :       case 8: // 8x2, 8x6
; 1300 :         for (int i = 0; i < pu_w * pu_h; i += 16) {

	xor	ebx, ebx
	test	r13d, r13d
	je	$LN18@bipred_ave

; 1311 : 
; 1312 :           __m256i all_ones = _mm256_set1_epi16(1);

	vmovdqu	ymm6, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	mov	r8d, DWORD PTR dst_stride$[rsp]
	npad	5
$LL16@bipred_ave:

; 1301 : 
; 1302 :           int y = i / pu_w;
; 1303 : 
; 1304 :           __m128i sample_px_8bit  = _mm_loadu_si128((__m128i*)&px[i]);
; 1305 :           __m256i sample_px_16bit = _mm256_cvtepu8_epi16(sample_px_8bit);
; 1306 :           sample_px_16bit         = _mm256_slli_epi16(sample_px_16bit, 14 - KVZ_BIT_DEPTH);

	vpmovzxbw ymm0, XMMWORD PTR [r12]
	vpsllw	ymm5, ymm0, 6

; 1307 :           __m256i sample_im_16bit = _mm256_loadu_si256((__m256i*)&im[i]);
; 1308 : 
; 1309 :           __m256i sample_px_im_lo = _mm256_unpacklo_epi16(sample_px_16bit, sample_im_16bit);
; 1310 :           __m256i sample_px_im_hi = _mm256_unpackhi_epi16(sample_px_16bit, sample_im_16bit);

	vpunpckhwd ymm0, ymm5, YMMWORD PTR [rsi]

; 1313 :           __m256i avg_lo   = _mm256_madd_epi16(sample_px_im_lo, all_ones);
; 1314 :           __m256i avg_hi   = _mm256_madd_epi16(sample_px_im_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm6
	vpunpcklwd ymm0, ymm5, YMMWORD PTR [rsi]

; 1315 : 
; 1316 :           avg_lo = _mm256_add_epi32(avg_lo, offset);
; 1317 :           avg_hi = _mm256_add_epi32(avg_hi, offset);

	vpaddd	ymm2, ymm1, ymm8
	vpmaddwd ymm1, ymm0, ymm6
	mov	edx, ebx
	lea	r12, QWORD PTR [r12+16]
	shr	edx, 3
	lea	rsi, QWORD PTR [rsi+32]

; 1318 : 
; 1319 :           avg_lo = _mm256_srai_epi32(avg_lo, shift);
; 1320 :           avg_hi = _mm256_srai_epi32(avg_hi, shift);
; 1321 : 
; 1322 :           __m256i avg256 = _mm256_packus_epi32(avg_lo, avg_hi);
; 1323 :           avg256         = _mm256_packus_epi16(avg256, avg256);
; 1324 :           avg256         = _mm256_permute4x64_epi64(avg256, _MM_SHUFFLE(3, 1, 2, 0));
; 1325 :           __m128i avg    = _mm256_castsi256_si128(avg256);
; 1326 : 
; 1327 :           _mm_storel_epi64((__m128i*)&dst[y * dst_stride], avg);

	mov	eax, edx
	add	ebx, 16
	imul	eax, r8d
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm0, ymm0, ymm0
	vpermq	ymm1, ymm0, 216				; 000000d8H
	vmovq	QWORD PTR [rax+r14], xmm1

; 1328 :           _mm_storeh_pd((double*)&dst[(y + 1) * dst_stride], _mm_castsi128_pd(avg));

	lea	eax, DWORD PTR [rdx+1]
	imul	eax, r8d
	vmovhpd	QWORD PTR [rax+r14], xmm1
	cmp	ebx, r13d
	jb	SHORT $LL16@bipred_ave

; 1329 :         }
; 1330 :         break;

	jmp	$LN18@bipred_ave
$LN33@bipred_ave:

; 1331 :       case 6: // 6x8
; 1332 :         for (int i = 0; i < pu_w * pu_h; i += 12) {

	xor	ebx, ebx
	test	r13d, r13d
	je	$LN18@bipred_ave

; 1343 : 
; 1344 :           __m256i sample_px_im_lo = _mm256_unpacklo_epi16(sample_px_16bit, sample_im_16bit);
; 1345 :           __m256i sample_px_im_hi = _mm256_unpackhi_epi16(sample_px_16bit, sample_im_16bit);
; 1346 : 
; 1347 :           __m256i all_ones = _mm256_set1_epi16(1);

	vmovdqu	xmm7, XMMWORD PTR __xmm@00000000ffffffffffffffffffffffff
	vmovdqu	ymm9, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	ymm6, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	mov	r9d, DWORD PTR dst_stride$[rsp]
	npad	3
$LL19@bipred_ave:

; 1333 : 
; 1334 :           int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH
	mul	ebx

; 1335 : 
; 1336 :           __m128i mask128         = _mm_setr_epi32(-1, -1, -1, 0);
; 1337 :           __m128i sample_px_8bit  = _mm_maskload_epi32((const int*)(&px[i]), mask128);

	movsxd	rax, ebx
	add	ebx, 12
	mov	r8d, edx
	shr	r8d, 2

; 1338 : 
; 1339 :           __m256i mask            = _mm256_setr_epi64x(-1, -1, -1, 0);
; 1340 :           __m256i sample_px_16bit = _mm256_cvtepu8_epi16(sample_px_8bit);
; 1341 :           sample_px_16bit         = _mm256_slli_epi16(sample_px_16bit, 14 - KVZ_BIT_DEPTH);
; 1342 :           __m256i sample_im_16bit = _mm256_maskload_epi64((const long long*)(&im[i]), mask);

	vpmaskmovq ymm3, ymm9, YMMWORD PTR [rsi+rax*2]
	vpmaskmovd xmm0, xmm7, XMMWORD PTR [rax+r12]
	vpmovzxbw ymm1, xmm0
	vpsllw	ymm5, ymm1, 6

; 1348 :           __m256i avg_lo   = _mm256_madd_epi16(sample_px_im_lo, all_ones);
; 1349 :           __m256i avg_hi   = _mm256_madd_epi16(sample_px_im_hi, all_ones);
; 1350 : 
; 1351 :           avg_lo = _mm256_add_epi32(avg_lo, offset);
; 1352 :           avg_hi = _mm256_add_epi32(avg_hi, offset);
; 1353 : 
; 1354 :           avg_lo = _mm256_srai_epi32(avg_lo, shift);
; 1355 :           avg_hi = _mm256_srai_epi32(avg_hi, shift);
; 1356 : 
; 1357 :           __m256i avg256 = _mm256_packus_epi32(avg_lo, avg_hi);
; 1358 :           avg256         = _mm256_packus_epi16(avg256, avg256);
; 1359 :           avg256         = _mm256_permute4x64_epi64(avg256, _MM_SHUFFLE(3, 1, 2, 0));
; 1360 :           __m128i avg    = _mm256_castsi256_si128(avg256);
; 1361 : 
; 1362 :           uint32_t elements_0123 = _mm_cvtsi128_si32(avg);
; 1363 :           uint16_t elements_45   = _mm_extract_epi16(avg, 2);
; 1364 :           uint16_t elements_67   = _mm_extract_epi16(avg, 3);
; 1365 :           uint32_t elements_89ab = _mm_extract_epi32(avg, 2);
; 1366 :           *(uint32_t*)&dst[(y + 0) * dst_stride + 0] = elements_0123;

	mov	eax, r8d

; 1367 :           *(uint16_t*)&dst[(y + 0) * dst_stride + 4] = elements_45;
; 1368 :           *(uint16_t*)&dst[(y + 1) * dst_stride + 0] = elements_67;

	lea	ecx, DWORD PTR [r8+1]
	imul	eax, r9d
	vpunpckhwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm6
	vpaddd	ymm2, ymm1, ymm8
	vpunpcklwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm6
	imul	ecx, r9d
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm0, ymm0, ymm0
	vpermq	ymm1, ymm0, 216				; 000000d8H
	vmovd	DWORD PTR [rax+r14], xmm1
	add	eax, 4
	vpextrw	WORD PTR [rax+r14], xmm1, 2

; 1369 :           *(uint32_t*)&dst[(y + 1) * dst_stride + 2] = elements_89ab;

	lea	eax, DWORD PTR [rcx+2]
	vpextrw	WORD PTR [rcx+r14], xmm1, 3
	vpextrd	DWORD PTR [rax+r14], xmm1, 2
	cmp	ebx, r13d
	jb	$LL19@bipred_ave
$LN18@bipred_ave:
	vzeroupper
	vmovaps	xmm9, XMMWORD PTR [rsp+64]

; 1374 :         break;
; 1375 :     }
; 1376 :   }
; 1377 : }

	lea	r11, QWORD PTR [rsp+128]
	mov	rbx, QWORD PTR [r11+32]
	mov	rsi, QWORD PTR [r11+40]
	mov	rdi, QWORD PTR [r11+48]
	mov	r12, QWORD PTR [r11+56]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm7, XMMWORD PTR [rsp+96]
	vmovaps	xmm6, XMMWORD PTR [rsp+112]
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r13
	ret	0
bipred_average_px_im_template_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\picture-generic.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
dst$ = 160
px$ = 168
im$ = 176
pu_w$ = 184
pu_h$ = 192
dst_stride$ = 200
bipred_average_px_im_avx2 PROC

; 1385 : {

	mov	rax, rsp
	mov	QWORD PTR [rax+8], rbx
	mov	QWORD PTR [rax+16], rsi
	mov	QWORD PTR [rax+24], rdi
	push	r12
	push	r14
	push	r15
	sub	rsp, 128				; 00000080H
	vmovaps	XMMWORD PTR [rax-40], xmm6
	mov	rbx, r8
	mov	r8d, DWORD PTR pu_h$[rsp]
	mov	r10d, r9d
	vmovaps	XMMWORD PTR [rax-56], xmm7
	mov	rdi, rdx
	vmovaps	XMMWORD PTR [rax-72], xmm8
	mov	rsi, rcx
	vmovaps	XMMWORD PTR [rax-88], xmm9
	vmovaps	XMMWORD PTR [rax-104], xmm10

; 1386 :   // Use scalar code for yet unoptimized block sizes (4x4, 2x8)
; 1387 :   if (!(pu_w == 4 && pu_h == 4) && pu_w > 2) {

	cmp	r9d, 4
	jne	SHORT $LN9@bipred_ave
	cmp	r8d, r9d
	je	$LN7@bipred_ave
	add	r10d, -4				; fffffffcH
	jmp	SHORT $LN687@bipred_ave
$LN9@bipred_ave:
	cmp	r10d, 2
	jbe	$LN7@bipred_ave

; 1388 :     switch (pu_w) {

	add	r10d, -4				; fffffffcH
	cmp	r10d, 60				; 0000003cH
	ja	$LN19@bipred_ave
$LN687@bipred_ave:
	lea	rdx, OFFSET FLAT:__ImageBase
	movzx	eax, BYTE PTR $LN688@bipred_ave[rdx+r10]
	mov	ecx, DWORD PTR $LN689@bipred_ave[rdx+rax*4]
	add	rcx, rdx
	jmp	rcx
$LN10@bipred_ave:

; 1187 :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovdqu	ymm9, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 1188 : 
; 1189 :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 4

; 1190 :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	r14d, DWORD PTR [r8*4]
	popcnt	eax, eax
	cmp	eax, 1
	mov	r15d, r14d
	sete	r12b
	and	r15d, 31

; 1191 :   assert(!(pu_w == 4 && pu_h == 4) && "Branch for 4x4 not yet implemented.");

	cmp	r8d, 4
	jne	SHORT $LN62@bipred_ave
	mov	r8d, 1191				; 000004a7H
	lea	rdx, OFFSET FLAT:$SG4294951293
	lea	rcx, OFFSET FLAT:$SG4294951292
	vzeroupper
	call	QWORD PTR __imp__wassert
	vmovdqu	ymm9, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040
$LN62@bipred_ave:

; 1192 :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 1193 : 
; 1194 :   if (has_pow2_width && area_mod_32 == 0) {

	test	r15d, r15d
	jne	$LN48@bipred_ave
	test	r12b, r12b
	je	$LN41@bipred_ave

; 1195 :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	xor	ecx, ecx
	test	r14d, r14d
	je	$LL5@bipred_ave
	vmovdqu	ymm8, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 1211 : 
; 1212 :       __m256i all_ones  = _mm256_set1_epi16(1);

	mov	r9d, DWORD PTR dst_stride$[rsp]
	npad	15
$LL25@bipred_ave:

; 1196 : 
; 1197 :       int y = i / pu_w;
; 1198 :       int x = i % pu_w;
; 1199 : 
; 1200 :       __m256i sample_px_a_16bit = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&px[i]));
; 1201 :       __m256i sample_px_b_16bit = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&px[i + 16]));

	vpmovzxbw ymm1, XMMWORD PTR [rdi+16]
	vpmovzxbw ymm0, XMMWORD PTR [rdi]

; 1202 :       sample_px_a_16bit         = _mm256_slli_epi16(sample_px_a_16bit, 14 - KVZ_BIT_DEPTH);
; 1203 :       sample_px_b_16bit         = _mm256_slli_epi16(sample_px_b_16bit, 14 - KVZ_BIT_DEPTH);

	vpsllw	ymm5, ymm1, 6
	vpsllw	ymm7, ymm0, 6

; 1204 :       __m256i sample_im_a_16bit = _mm256_loadu_si256((__m256i*)&im[i]);
; 1205 :       __m256i sample_im_b_16bit = _mm256_loadu_si256((__m256i*)&im[i + 16]);
; 1206 : 
; 1207 :       __m256i sample_px_im_a_lo = _mm256_unpacklo_epi16(sample_px_a_16bit, sample_im_a_16bit);
; 1208 :       __m256i sample_px_im_a_hi = _mm256_unpackhi_epi16(sample_px_a_16bit, sample_im_a_16bit);
; 1209 :       __m256i sample_px_im_b_lo = _mm256_unpacklo_epi16(sample_px_b_16bit, sample_im_b_16bit);
; 1210 :       __m256i sample_px_im_b_hi = _mm256_unpackhi_epi16(sample_px_b_16bit, sample_im_b_16bit);

	vpunpckhwd ymm0, ymm5, YMMWORD PTR [rbx+32]

; 1213 :       __m256i avg_a_lo = _mm256_madd_epi16(sample_px_im_a_lo, all_ones);
; 1214 :       __m256i avg_a_hi = _mm256_madd_epi16(sample_px_im_a_hi, all_ones);
; 1215 :       __m256i avg_b_lo = _mm256_madd_epi16(sample_px_im_b_lo, all_ones);
; 1216 :       __m256i avg_b_hi = _mm256_madd_epi16(sample_px_im_b_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm8
	vpunpcklwd ymm0, ymm5, YMMWORD PTR [rbx+32]

; 1217 : 
; 1218 :       avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);
; 1219 :       avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);
; 1220 :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);
; 1221 :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vpaddd	ymm2, ymm1, ymm9
	vpmaddwd ymm1, ymm0, ymm8
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [rbx]

; 1222 : 
; 1223 :       avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);
; 1224 :       avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);
; 1225 :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);
; 1226 :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm9
	vpmaddwd ymm1, ymm0, ymm8
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [rbx]
	vpsrad	ymm3, ymm2, 7
	vpaddd	ymm2, ymm1, ymm9
	vpmaddwd ymm1, ymm0, ymm8
	mov	edx, ecx
	lea	rdi, QWORD PTR [rdi+32]
	shr	edx, 2
	lea	rbx, QWORD PTR [rbx+64]

; 1230 :       __m256i avg0213 = _mm256_packus_epi16(avg_01, avg_23);
; 1231 :       __m256i avg     = _mm256_permute4x64_epi64(avg0213, _MM_SHUFFLE(3, 1, 2, 0));
; 1232 : 
; 1233 :       switch (pu_w) {
; 1234 :         case  4: scatter_ymm_4x8_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	imul	edx, r9d
	mov	eax, ecx
	vpackusdw ymm5, ymm3, ymm4
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm9
	and	eax, 3
	add	ecx, 32					; 00000020H
	add	edx, eax
	add	rdx, rsi
	vpsrad	ymm3, ymm2, 7
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm1, ymm0, ymm5
	vpermq	ymm2, ymm1, 216				; 000000d8H

; 776  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_lo); dst += dst_stride;

	vmovd	DWORD PTR [rdx], xmm2
	add	rdx, r9
	vextracti128 xmm0, ymm2, 1

; 777  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 1); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm2, 1
	add	rdx, r9

; 778  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 2); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm2, 2
	add	rdx, r9

; 779  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 3); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm2, 3
	add	rdx, r9

; 780  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_hi); dst += dst_stride;

	vmovd	DWORD PTR [rdx], xmm0
	add	rdx, r9

; 781  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 1); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm0, 1
	add	rdx, r9

; 782  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 2); dst += dst_stride;

	vpextrd	DWORD PTR [rdx], xmm0, 2

; 783  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 3);

	vpextrd	DWORD PTR [r9+rdx], xmm0, 3

; 1195 :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	cmp	ecx, r14d
	jb	$LL25@bipred_ave

; 1241 :           break;
; 1242 :       }
; 1243 :     }
; 1244 :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN41@bipred_ave:

; 1245 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r14d, r14d
	je	$LL5@bipred_ave
	lea	ecx, DWORD PTR [r14-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
$LL30@bipred_ave:

; 1291 :         default:
; 1292 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1292				; 0000050cH
	lea	rdx, OFFSET FLAT:$SG4294951286
	lea	rcx, OFFSET FLAT:$SG4294951285
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL30@bipred_ave

; 1293 :           break;
; 1294 :       }
; 1295 :     }
; 1296 :   } else {

	jmp	$LL5@bipred_ave
$LN48@bipred_ave:

; 1415 :     }
; 1416 :   }
; 1417 : }

	mov	r8d, 1373				; 0000055dH
	lea	rdx, OFFSET FLAT:$SG4294951284
	lea	rcx, OFFSET FLAT:$SG4294951283
	vzeroupper
	call	QWORD PTR __imp__wassert
	jmp	$LL5@bipred_ave
$LN11@bipred_ave:

; 1187 :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovdqu	ymm8, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 1188 : 
; 1189 :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 8

; 1190 :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	r9d, DWORD PTR [r8*8]
	popcnt	eax, eax
	cmp	eax, 1
	sete	al
	test	r9b, 31

; 1192 :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 1193 : 
; 1194 :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN100@bipred_ave
	test	al, al
	je	$LN93@bipred_ave

; 1195 :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	xor	ecx, ecx
	test	r9d, r9d
	je	$LL5@bipred_ave
	vmovdqu	ymm9, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 1211 : 
; 1212 :       __m256i all_ones  = _mm256_set1_epi16(1);

	mov	r8d, DWORD PTR dst_stride$[rsp]
	npad	8
$LL77@bipred_ave:

; 1196 : 
; 1197 :       int y = i / pu_w;
; 1198 :       int x = i % pu_w;
; 1199 : 
; 1200 :       __m256i sample_px_a_16bit = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&px[i]));
; 1201 :       __m256i sample_px_b_16bit = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&px[i + 16]));

	vpmovzxbw ymm1, XMMWORD PTR [rdi+16]
	vpmovzxbw ymm0, XMMWORD PTR [rdi]

; 1202 :       sample_px_a_16bit         = _mm256_slli_epi16(sample_px_a_16bit, 14 - KVZ_BIT_DEPTH);
; 1203 :       sample_px_b_16bit         = _mm256_slli_epi16(sample_px_b_16bit, 14 - KVZ_BIT_DEPTH);

	vpsllw	ymm5, ymm1, 6
	vpsllw	ymm7, ymm0, 6

; 1204 :       __m256i sample_im_a_16bit = _mm256_loadu_si256((__m256i*)&im[i]);
; 1205 :       __m256i sample_im_b_16bit = _mm256_loadu_si256((__m256i*)&im[i + 16]);
; 1206 : 
; 1207 :       __m256i sample_px_im_a_lo = _mm256_unpacklo_epi16(sample_px_a_16bit, sample_im_a_16bit);
; 1208 :       __m256i sample_px_im_a_hi = _mm256_unpackhi_epi16(sample_px_a_16bit, sample_im_a_16bit);
; 1209 :       __m256i sample_px_im_b_lo = _mm256_unpacklo_epi16(sample_px_b_16bit, sample_im_b_16bit);
; 1210 :       __m256i sample_px_im_b_hi = _mm256_unpackhi_epi16(sample_px_b_16bit, sample_im_b_16bit);

	vpunpckhwd ymm0, ymm5, YMMWORD PTR [rbx+32]

; 1213 :       __m256i avg_a_lo = _mm256_madd_epi16(sample_px_im_a_lo, all_ones);
; 1214 :       __m256i avg_a_hi = _mm256_madd_epi16(sample_px_im_a_hi, all_ones);
; 1215 :       __m256i avg_b_lo = _mm256_madd_epi16(sample_px_im_b_lo, all_ones);
; 1216 :       __m256i avg_b_hi = _mm256_madd_epi16(sample_px_im_b_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm9
	vpunpcklwd ymm0, ymm5, YMMWORD PTR [rbx+32]

; 1217 : 
; 1218 :       avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);
; 1219 :       avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);
; 1220 :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);
; 1221 :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vpaddd	ymm2, ymm1, ymm8
	vpmaddwd ymm1, ymm0, ymm9
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [rbx]

; 1222 : 
; 1223 :       avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);
; 1224 :       avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);
; 1225 :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);
; 1226 :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	vpmaddwd ymm1, ymm0, ymm9
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [rbx]
	vpsrad	ymm3, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	vpmaddwd ymm1, ymm0, ymm9

; 1227 : 
; 1228 :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);
; 1229 :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vpackusdw ymm5, ymm3, ymm4
	vpsrad	ymm4, ymm2, 7
	mov	eax, ecx
	lea	rdi, QWORD PTR [rdi+32]
	and	eax, 7
	lea	rbx, QWORD PTR [rbx+64]
	vpaddd	ymm2, ymm1, ymm8
	mov	edx, ecx
	add	ecx, 32					; 00000020H
	shr	edx, 3

; 1235 :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	imul	edx, r8d
	vpsrad	ymm3, ymm2, 7
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm1, ymm0, ymm5
	vpermq	ymm2, ymm1, 216				; 000000d8H
	add	eax, edx
	add	rax, rsi

; 790  :   __m128d ymm_hi = _mm256_extractf128_pd(ymm_as_m256d, 1);

	vextractf128 xmm0, ymm2, 1

; 791  :   _mm_storel_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovlpd	QWORD PTR [rax], xmm2
	add	rax, r8

; 792  :   _mm_storeh_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovhpd	QWORD PTR [rax], xmm2
	add	rax, r8

; 793  :   _mm_storel_pd((double*)dst, ymm_hi); dst += dst_stride;

	vmovlpd	QWORD PTR [rax], xmm0

; 794  :   _mm_storeh_pd((double*)dst, ymm_hi);

	vmovhpd	QWORD PTR [r8+rax], xmm0

; 1195 :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	cmp	ecx, r9d
	jb	$LL77@bipred_ave

; 1241 :           break;
; 1242 :       }
; 1243 :     }
; 1244 :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN93@bipred_ave:

; 1245 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r9d, r9d
	je	$LL5@bipred_ave
	lea	ecx, DWORD PTR [r9-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
	npad	5
$LL82@bipred_ave:

; 1291 :         default:
; 1292 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1292				; 0000050cH
	lea	rdx, OFFSET FLAT:$SG4294951286
	lea	rcx, OFFSET FLAT:$SG4294951285
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL82@bipred_ave

; 1293 :           break;
; 1294 :       }
; 1295 :     }
; 1296 :   } else {

	jmp	$LL5@bipred_ave
$LN100@bipred_ave:

; 1297 :     // 8x2, 8x6, 6x8 blocks (and maybe 2x8 in the future)
; 1298 :     switch (pu_w) {
; 1299 :       case 8: // 8x2, 8x6
; 1300 :         for (int i = 0; i < pu_w * pu_h; i += 16) {

	xor	ecx, ecx
	test	r9d, r9d
	je	$LL5@bipred_ave
	vmovdqu	ymm6, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 1311 : 
; 1312 :           __m256i all_ones = _mm256_set1_epi16(1);

	mov	r10d, DWORD PTR dst_stride$[rsp]
	npad	13
$LL89@bipred_ave:

; 1301 : 
; 1302 :           int y = i / pu_w;
; 1303 : 
; 1304 :           __m128i sample_px_8bit  = _mm_loadu_si128((__m128i*)&px[i]);
; 1305 :           __m256i sample_px_16bit = _mm256_cvtepu8_epi16(sample_px_8bit);

	vpmovzxbw ymm0, XMMWORD PTR [rdi]

; 1306 :           sample_px_16bit         = _mm256_slli_epi16(sample_px_16bit, 14 - KVZ_BIT_DEPTH);

	vpsllw	ymm5, ymm0, 6

; 1307 :           __m256i sample_im_16bit = _mm256_loadu_si256((__m256i*)&im[i]);
; 1308 : 
; 1309 :           __m256i sample_px_im_lo = _mm256_unpacklo_epi16(sample_px_16bit, sample_im_16bit);
; 1310 :           __m256i sample_px_im_hi = _mm256_unpackhi_epi16(sample_px_16bit, sample_im_16bit);

	vpunpckhwd ymm0, ymm5, YMMWORD PTR [rbx]

; 1313 :           __m256i avg_lo   = _mm256_madd_epi16(sample_px_im_lo, all_ones);
; 1314 :           __m256i avg_hi   = _mm256_madd_epi16(sample_px_im_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm6
	vpunpcklwd ymm0, ymm5, YMMWORD PTR [rbx]

; 1315 : 
; 1316 :           avg_lo = _mm256_add_epi32(avg_lo, offset);
; 1317 :           avg_hi = _mm256_add_epi32(avg_hi, offset);

	vpaddd	ymm2, ymm1, ymm8
	vpmaddwd ymm1, ymm0, ymm6
	mov	r8d, ecx
	lea	rdi, QWORD PTR [rdi+16]
	shr	r8d, 3
	lea	rbx, QWORD PTR [rbx+32]

; 1318 : 
; 1319 :           avg_lo = _mm256_srai_epi32(avg_lo, shift);
; 1320 :           avg_hi = _mm256_srai_epi32(avg_hi, shift);
; 1321 : 
; 1322 :           __m256i avg256 = _mm256_packus_epi32(avg_lo, avg_hi);
; 1323 :           avg256         = _mm256_packus_epi16(avg256, avg256);
; 1324 :           avg256         = _mm256_permute4x64_epi64(avg256, _MM_SHUFFLE(3, 1, 2, 0));
; 1325 :           __m128i avg    = _mm256_castsi256_si128(avg256);
; 1326 : 
; 1327 :           _mm_storel_epi64((__m128i*)&dst[y * dst_stride], avg);

	mov	eax, r8d
	add	ecx, 16
	imul	eax, r10d
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm0, ymm0, ymm0
	vpermq	ymm1, ymm0, 216				; 000000d8H
	vmovq	QWORD PTR [rax+rsi], xmm1

; 1328 :           _mm_storeh_pd((double*)&dst[(y + 1) * dst_stride], _mm_castsi128_pd(avg));

	lea	eax, DWORD PTR [r8+1]
	imul	eax, r10d
	vmovhpd	QWORD PTR [rax+rsi], xmm1
	cmp	ecx, r9d
	jb	SHORT $LL89@bipred_ave

; 1389 :       case  4: bipred_average_px_im_template_avx2(dst, px, im,  4, pu_h, dst_stride); break;
; 1390 :       case  8: bipred_average_px_im_template_avx2(dst, px, im,  8, pu_h, dst_stride); break;

	jmp	$LL5@bipred_ave
$LN12@bipred_ave:

; 1187 :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovdqu	ymm9, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 1188 : 
; 1189 :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 16
	popcnt	eax, eax
	cmp	eax, 1
	sete	cl

; 1190 :   bool area_mod_32 = (pu_w * pu_h) % 32;

	imul	eax, r8d, -16				; fffffff0H
	test	al, 31

; 1192 :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 1193 : 
; 1194 :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN48@bipred_ave
	shl	r8d, 4
	test	cl, cl
	je	$LN145@bipred_ave

; 1195 :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	xor	ecx, ecx
	test	r8d, r8d
	je	$LL5@bipred_ave
	vmovdqu	ymm8, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 1211 : 
; 1212 :       __m256i all_ones  = _mm256_set1_epi16(1);

	mov	r9d, DWORD PTR dst_stride$[rsp]
	npad	4
$LL129@bipred_ave:

; 1196 : 
; 1197 :       int y = i / pu_w;
; 1198 :       int x = i % pu_w;
; 1199 : 
; 1200 :       __m256i sample_px_a_16bit = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&px[i]));
; 1201 :       __m256i sample_px_b_16bit = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&px[i + 16]));

	vpmovzxbw ymm1, XMMWORD PTR [rdi+16]
	vpmovzxbw ymm0, XMMWORD PTR [rdi]

; 1202 :       sample_px_a_16bit         = _mm256_slli_epi16(sample_px_a_16bit, 14 - KVZ_BIT_DEPTH);
; 1203 :       sample_px_b_16bit         = _mm256_slli_epi16(sample_px_b_16bit, 14 - KVZ_BIT_DEPTH);

	vpsllw	ymm5, ymm1, 6
	vpsllw	ymm7, ymm0, 6

; 1204 :       __m256i sample_im_a_16bit = _mm256_loadu_si256((__m256i*)&im[i]);
; 1205 :       __m256i sample_im_b_16bit = _mm256_loadu_si256((__m256i*)&im[i + 16]);
; 1206 : 
; 1207 :       __m256i sample_px_im_a_lo = _mm256_unpacklo_epi16(sample_px_a_16bit, sample_im_a_16bit);
; 1208 :       __m256i sample_px_im_a_hi = _mm256_unpackhi_epi16(sample_px_a_16bit, sample_im_a_16bit);
; 1209 :       __m256i sample_px_im_b_lo = _mm256_unpacklo_epi16(sample_px_b_16bit, sample_im_b_16bit);
; 1210 :       __m256i sample_px_im_b_hi = _mm256_unpackhi_epi16(sample_px_b_16bit, sample_im_b_16bit);

	vpunpckhwd ymm0, ymm5, YMMWORD PTR [rbx+32]

; 1213 :       __m256i avg_a_lo = _mm256_madd_epi16(sample_px_im_a_lo, all_ones);
; 1214 :       __m256i avg_a_hi = _mm256_madd_epi16(sample_px_im_a_hi, all_ones);
; 1215 :       __m256i avg_b_lo = _mm256_madd_epi16(sample_px_im_b_lo, all_ones);
; 1216 :       __m256i avg_b_hi = _mm256_madd_epi16(sample_px_im_b_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm8
	vpunpcklwd ymm0, ymm5, YMMWORD PTR [rbx+32]

; 1217 : 
; 1218 :       avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);
; 1219 :       avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);
; 1220 :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);
; 1221 :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vpaddd	ymm2, ymm1, ymm9

; 1222 : 
; 1223 :       avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);
; 1224 :       avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);
; 1225 :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);
; 1226 :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vpsrad	ymm4, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm8
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [rbx]
	vpaddd	ymm2, ymm1, ymm9
	vpsrad	ymm3, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm8
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [rbx]
	vpaddd	ymm2, ymm1, ymm9

; 1227 : 
; 1228 :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);
; 1229 :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vpackusdw ymm5, ymm3, ymm4
	vpmaddwd ymm1, ymm0, ymm8
	vpsrad	ymm4, ymm2, 7
	mov	eax, ecx
	lea	rdi, QWORD PTR [rdi+32]
	vpaddd	ymm2, ymm1, ymm9
	and	eax, 15
	lea	rbx, QWORD PTR [rbx+64]
	mov	edx, ecx
	add	ecx, 32					; 00000020H
	shr	edx, 4

; 1236 :         case 16: scatter_ymm_16x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	imul	edx, r9d
	vpsrad	ymm3, ymm2, 7
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm1, ymm0, ymm5
	vpermq	ymm2, ymm1, 216				; 000000d8H
	add	eax, edx
	add	rax, rsi

; 801  :   _mm_storeu_si128((__m128i *)dst, ymm_lo); dst += dst_stride;

	vmovdqu	XMMWORD PTR [rax], xmm2
	vextracti128 XMMWORD PTR [r9+rax], ymm2, 1

; 1195 :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	cmp	ecx, r8d
	jb	$LL129@bipred_ave

; 1241 :           break;
; 1242 :       }
; 1243 :     }
; 1244 :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN145@bipred_ave:

; 1245 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r8d, r8d
	je	$LL5@bipred_ave
	lea	ecx, DWORD PTR [r8-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
	npad	8
$LL134@bipred_ave:

; 1291 :         default:
; 1292 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1292				; 0000050cH
	lea	rdx, OFFSET FLAT:$SG4294951286
	lea	rcx, OFFSET FLAT:$SG4294951285
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL134@bipred_ave

; 1293 :           break;
; 1294 :       }
; 1295 :     }
; 1296 :   } else {

	jmp	$LL5@bipred_ave
$LN13@bipred_ave:

; 1187 :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovdqu	ymm9, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 1188 : 
; 1189 :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 32					; 00000020H
	popcnt	eax, eax
	cmp	eax, 1
	sete	al
	shl	r8d, 5

; 1192 :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 1193 : 
; 1194 :   if (has_pow2_width && area_mod_32 == 0) {

	test	al, al
	je	$LN652@bipred_ave

; 1195 :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	xor	ecx, ecx
	test	r8d, r8d
	je	$LL5@bipred_ave
	vmovdqu	ymm8, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 1211 : 
; 1212 :       __m256i all_ones  = _mm256_set1_epi16(1);

	mov	r9d, DWORD PTR dst_stride$[rsp]
	npad	10
$LL181@bipred_ave:

; 1196 : 
; 1197 :       int y = i / pu_w;
; 1198 :       int x = i % pu_w;
; 1199 : 
; 1200 :       __m256i sample_px_a_16bit = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&px[i]));
; 1201 :       __m256i sample_px_b_16bit = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&px[i + 16]));

	vpmovzxbw ymm1, XMMWORD PTR [rdi+16]
	vpmovzxbw ymm0, XMMWORD PTR [rdi]

; 1202 :       sample_px_a_16bit         = _mm256_slli_epi16(sample_px_a_16bit, 14 - KVZ_BIT_DEPTH);
; 1203 :       sample_px_b_16bit         = _mm256_slli_epi16(sample_px_b_16bit, 14 - KVZ_BIT_DEPTH);

	vpsllw	ymm5, ymm1, 6
	vpsllw	ymm7, ymm0, 6

; 1204 :       __m256i sample_im_a_16bit = _mm256_loadu_si256((__m256i*)&im[i]);
; 1205 :       __m256i sample_im_b_16bit = _mm256_loadu_si256((__m256i*)&im[i + 16]);
; 1206 : 
; 1207 :       __m256i sample_px_im_a_lo = _mm256_unpacklo_epi16(sample_px_a_16bit, sample_im_a_16bit);
; 1208 :       __m256i sample_px_im_a_hi = _mm256_unpackhi_epi16(sample_px_a_16bit, sample_im_a_16bit);
; 1209 :       __m256i sample_px_im_b_lo = _mm256_unpacklo_epi16(sample_px_b_16bit, sample_im_b_16bit);
; 1210 :       __m256i sample_px_im_b_hi = _mm256_unpackhi_epi16(sample_px_b_16bit, sample_im_b_16bit);

	vpunpckhwd ymm0, ymm5, YMMWORD PTR [rbx+32]

; 1213 :       __m256i avg_a_lo = _mm256_madd_epi16(sample_px_im_a_lo, all_ones);
; 1214 :       __m256i avg_a_hi = _mm256_madd_epi16(sample_px_im_a_hi, all_ones);
; 1215 :       __m256i avg_b_lo = _mm256_madd_epi16(sample_px_im_b_lo, all_ones);
; 1216 :       __m256i avg_b_hi = _mm256_madd_epi16(sample_px_im_b_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm8
	vpunpcklwd ymm0, ymm5, YMMWORD PTR [rbx+32]

; 1217 : 
; 1218 :       avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);
; 1219 :       avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);
; 1220 :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);
; 1221 :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vpaddd	ymm2, ymm1, ymm9

; 1222 : 
; 1223 :       avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);
; 1224 :       avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);
; 1225 :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);
; 1226 :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vpsrad	ymm4, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm8
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [rbx]
	vpaddd	ymm2, ymm1, ymm9
	vpsrad	ymm3, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm8
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [rbx]
	vpaddd	ymm2, ymm1, ymm9
	mov	edx, ecx
	lea	rdi, QWORD PTR [rdi+32]
	shr	edx, 5
	lea	rbx, QWORD PTR [rbx+64]

; 1227 : 
; 1228 :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);
; 1229 :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vpackusdw ymm5, ymm3, ymm4
	vpmaddwd ymm1, ymm0, ymm8
	vpsrad	ymm4, ymm2, 7

; 1237 :         case 32: // Same as case 64
; 1238 :         case 64: _mm256_storeu_si256((__m256i*)&dst[y * dst_stride + x], avg); break;

	imul	edx, r9d
	mov	eax, ecx
	vpaddd	ymm2, ymm1, ymm9
	vpsrad	ymm3, ymm2, 7
	and	eax, 31
	add	ecx, 32					; 00000020H
	vpackusdw ymm0, ymm3, ymm4
	add	edx, eax
	vpackuswb ymm1, ymm0, ymm5
	vpermq	ymm2, ymm1, 216				; 000000d8H
	vmovdqu	YMMWORD PTR [rdx+rsi], ymm2
	cmp	ecx, r8d
	jb	$LL181@bipred_ave

; 1241 :           break;
; 1242 :       }
; 1243 :     }
; 1244 :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN652@bipred_ave:

; 1245 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r8d, r8d
	je	$LL5@bipred_ave
	lea	ecx, DWORD PTR [r8-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
	npad	1
$LL186@bipred_ave:

; 1291 :         default:
; 1292 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1292				; 0000050cH
	lea	rdx, OFFSET FLAT:$SG4294951286
	lea	rcx, OFFSET FLAT:$SG4294951285
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL186@bipred_ave

; 1391 :       case 16: bipred_average_px_im_template_avx2(dst, px, im, 16, pu_h, dst_stride); break;
; 1392 :       case 32: bipred_average_px_im_template_avx2(dst, px, im, 32, pu_h, dst_stride); break;

	jmp	$LL5@bipred_ave
$LN14@bipred_ave:

; 1187 :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovdqu	ymm9, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 1188 : 
; 1189 :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 64					; 00000040H
	popcnt	eax, eax
	cmp	eax, 1
	sete	al
	shl	r8d, 6

; 1192 :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 1193 : 
; 1194 :   if (has_pow2_width && area_mod_32 == 0) {

	test	al, al
	je	$LN658@bipred_ave

; 1195 :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	xor	ecx, ecx
	test	r8d, r8d
	je	$LL5@bipred_ave
	vmovdqu	ymm8, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 1211 : 
; 1212 :       __m256i all_ones  = _mm256_set1_epi16(1);

	mov	r9d, DWORD PTR dst_stride$[rsp]
	npad	10
$LL233@bipred_ave:

; 1196 : 
; 1197 :       int y = i / pu_w;
; 1198 :       int x = i % pu_w;
; 1199 : 
; 1200 :       __m256i sample_px_a_16bit = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&px[i]));
; 1201 :       __m256i sample_px_b_16bit = _mm256_cvtepu8_epi16(_mm_loadu_si128((__m128i*)&px[i + 16]));

	vpmovzxbw ymm1, XMMWORD PTR [rdi+16]
	vpmovzxbw ymm0, XMMWORD PTR [rdi]

; 1202 :       sample_px_a_16bit         = _mm256_slli_epi16(sample_px_a_16bit, 14 - KVZ_BIT_DEPTH);
; 1203 :       sample_px_b_16bit         = _mm256_slli_epi16(sample_px_b_16bit, 14 - KVZ_BIT_DEPTH);

	vpsllw	ymm5, ymm1, 6
	vpsllw	ymm7, ymm0, 6

; 1204 :       __m256i sample_im_a_16bit = _mm256_loadu_si256((__m256i*)&im[i]);
; 1205 :       __m256i sample_im_b_16bit = _mm256_loadu_si256((__m256i*)&im[i + 16]);
; 1206 : 
; 1207 :       __m256i sample_px_im_a_lo = _mm256_unpacklo_epi16(sample_px_a_16bit, sample_im_a_16bit);
; 1208 :       __m256i sample_px_im_a_hi = _mm256_unpackhi_epi16(sample_px_a_16bit, sample_im_a_16bit);
; 1209 :       __m256i sample_px_im_b_lo = _mm256_unpacklo_epi16(sample_px_b_16bit, sample_im_b_16bit);
; 1210 :       __m256i sample_px_im_b_hi = _mm256_unpackhi_epi16(sample_px_b_16bit, sample_im_b_16bit);

	vpunpckhwd ymm0, ymm5, YMMWORD PTR [rbx+32]

; 1213 :       __m256i avg_a_lo = _mm256_madd_epi16(sample_px_im_a_lo, all_ones);
; 1214 :       __m256i avg_a_hi = _mm256_madd_epi16(sample_px_im_a_hi, all_ones);
; 1215 :       __m256i avg_b_lo = _mm256_madd_epi16(sample_px_im_b_lo, all_ones);
; 1216 :       __m256i avg_b_hi = _mm256_madd_epi16(sample_px_im_b_hi, all_ones);

	vpmaddwd ymm1, ymm0, ymm8
	vpunpcklwd ymm0, ymm5, YMMWORD PTR [rbx+32]

; 1217 : 
; 1218 :       avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);
; 1219 :       avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);
; 1220 :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);
; 1221 :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);

	vpaddd	ymm2, ymm1, ymm9

; 1222 : 
; 1223 :       avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);
; 1224 :       avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);
; 1225 :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);
; 1226 :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);

	vpsrad	ymm4, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm8
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [rbx]
	vpaddd	ymm2, ymm1, ymm9
	vpsrad	ymm3, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm8
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [rbx]
	vpaddd	ymm2, ymm1, ymm9
	mov	edx, ecx
	lea	rdi, QWORD PTR [rdi+32]
	shr	edx, 6
	lea	rbx, QWORD PTR [rbx+64]

; 1227 : 
; 1228 :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);
; 1229 :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);

	vpackusdw ymm5, ymm3, ymm4
	vpmaddwd ymm1, ymm0, ymm8
	vpsrad	ymm4, ymm2, 7

; 1237 :         case 32: // Same as case 64
; 1238 :         case 64: _mm256_storeu_si256((__m256i*)&dst[y * dst_stride + x], avg); break;

	imul	edx, r9d
	mov	eax, ecx
	vpaddd	ymm2, ymm1, ymm9
	vpsrad	ymm3, ymm2, 7
	and	eax, 63					; 0000003fH
	add	ecx, 32					; 00000020H
	vpackusdw ymm0, ymm3, ymm4
	add	edx, eax
	vpackuswb ymm1, ymm0, ymm5
	vpermq	ymm2, ymm1, 216				; 000000d8H
	vmovdqu	YMMWORD PTR [rdx+rsi], ymm2
	cmp	ecx, r8d
	jb	$LL233@bipred_ave

; 1241 :           break;
; 1242 :       }
; 1243 :     }
; 1244 :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN658@bipred_ave:

; 1245 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r8d, r8d
	je	$LL5@bipred_ave
	lea	ecx, DWORD PTR [r8-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
	npad	1
$LL238@bipred_ave:

; 1291 :         default:
; 1292 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1292				; 0000050cH
	lea	rdx, OFFSET FLAT:$SG4294951286
	lea	rcx, OFFSET FLAT:$SG4294951285
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL238@bipred_ave

; 1393 :       case 64: bipred_average_px_im_template_avx2(dst, px, im, 64, pu_h, dst_stride); break;

	jmp	$LL5@bipred_ave
$LN15@bipred_ave:

; 1187 :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovdqu	ymm7, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 1188 : 
; 1189 :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 6
	popcnt	eax, eax
	cmp	eax, 1

; 1190 :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	eax, DWORD PTR [r8+r8*2]
	lea	r10d, DWORD PTR [rax+rax]
	sete	cl
	test	r10b, 31

; 1192 :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 1193 : 
; 1194 :   if (has_pow2_width && area_mod_32 == 0) {

	jne	SHORT $LN308@bipred_ave

; 1195 :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r10d, r10d
	je	$LL5@bipred_ave

; 1192 :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 1193 : 
; 1194 :   if (has_pow2_width && area_mod_32 == 0) {

	test	cl, cl
	je	SHORT $LN301@bipred_ave
	lea	ebx, DWORD PTR [r10-1]
	shr	ebx, 5
	inc	ebx
$LL285@bipred_ave:

; 1239 :         default:
; 1240 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1240				; 000004d8H
	lea	rdx, OFFSET FLAT:$SG4294951288
	lea	rcx, OFFSET FLAT:$SG4294951287
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL285@bipred_ave

; 1241 :           break;
; 1242 :       }
; 1243 :     }
; 1244 :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN301@bipred_ave:
	lea	ecx, DWORD PTR [r10-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
	npad	4
$LL290@bipred_ave:

; 1291 :         default:
; 1292 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1292				; 0000050cH
	lea	rdx, OFFSET FLAT:$SG4294951286
	lea	rcx, OFFSET FLAT:$SG4294951285
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL290@bipred_ave

; 1293 :           break;
; 1294 :       }
; 1295 :     }
; 1296 :   } else {

	jmp	$LL5@bipred_ave
$LN308@bipred_ave:

; 1329 :         }
; 1330 :         break;
; 1331 :       case 6: // 6x8
; 1332 :         for (int i = 0; i < pu_w * pu_h; i += 12) {

	xor	ecx, ecx
	test	r10d, r10d
	je	$LL5@bipred_ave
	vmovdqu	xmm8, XMMWORD PTR __xmm@00000000ffffffffffffffffffffffff
	vmovdqu	ymm9, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	ymm6, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 1343 : 
; 1344 :           __m256i sample_px_im_lo = _mm256_unpacklo_epi16(sample_px_16bit, sample_im_16bit);
; 1345 :           __m256i sample_px_im_hi = _mm256_unpackhi_epi16(sample_px_16bit, sample_im_16bit);
; 1346 : 
; 1347 :           __m256i all_ones = _mm256_set1_epi16(1);

	mov	r11d, DWORD PTR dst_stride$[rsp]
	npad	13
$LL300@bipred_ave:

; 1333 : 
; 1334 :           int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx

; 1335 : 
; 1336 :           __m128i mask128         = _mm_setr_epi32(-1, -1, -1, 0);
; 1337 :           __m128i sample_px_8bit  = _mm_maskload_epi32((const int*)(&px[i]), mask128);

	movsxd	rax, ecx
	add	ecx, 12
	mov	r9d, edx
	shr	r9d, 2

; 1338 : 
; 1339 :           __m256i mask            = _mm256_setr_epi64x(-1, -1, -1, 0);
; 1340 :           __m256i sample_px_16bit = _mm256_cvtepu8_epi16(sample_px_8bit);
; 1341 :           sample_px_16bit         = _mm256_slli_epi16(sample_px_16bit, 14 - KVZ_BIT_DEPTH);
; 1342 :           __m256i sample_im_16bit = _mm256_maskload_epi64((const long long*)(&im[i]), mask);

	vpmaskmovq ymm3, ymm9, YMMWORD PTR [rbx+rax*2]
	vpmaskmovd xmm0, xmm8, XMMWORD PTR [rax+rdi]
	vpmovzxbw ymm1, xmm0
	vpsllw	ymm5, ymm1, 6

; 1348 :           __m256i avg_lo   = _mm256_madd_epi16(sample_px_im_lo, all_ones);
; 1349 :           __m256i avg_hi   = _mm256_madd_epi16(sample_px_im_hi, all_ones);
; 1350 : 
; 1351 :           avg_lo = _mm256_add_epi32(avg_lo, offset);
; 1352 :           avg_hi = _mm256_add_epi32(avg_hi, offset);
; 1353 : 
; 1354 :           avg_lo = _mm256_srai_epi32(avg_lo, shift);
; 1355 :           avg_hi = _mm256_srai_epi32(avg_hi, shift);
; 1356 : 
; 1357 :           __m256i avg256 = _mm256_packus_epi32(avg_lo, avg_hi);
; 1358 :           avg256         = _mm256_packus_epi16(avg256, avg256);
; 1359 :           avg256         = _mm256_permute4x64_epi64(avg256, _MM_SHUFFLE(3, 1, 2, 0));
; 1360 :           __m128i avg    = _mm256_castsi256_si128(avg256);
; 1361 : 
; 1362 :           uint32_t elements_0123 = _mm_cvtsi128_si32(avg);
; 1363 :           uint16_t elements_45   = _mm_extract_epi16(avg, 2);
; 1364 :           uint16_t elements_67   = _mm_extract_epi16(avg, 3);
; 1365 :           uint32_t elements_89ab = _mm_extract_epi32(avg, 2);
; 1366 :           *(uint32_t*)&dst[(y + 0) * dst_stride + 0] = elements_0123;

	mov	eax, r9d

; 1367 :           *(uint16_t*)&dst[(y + 0) * dst_stride + 4] = elements_45;
; 1368 :           *(uint16_t*)&dst[(y + 1) * dst_stride + 0] = elements_67;

	lea	edx, DWORD PTR [r9+1]
	imul	eax, r11d
	vpunpckhwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm6
	vpaddd	ymm2, ymm1, ymm7
	vpunpcklwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm6
	imul	edx, r11d
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm7
	vpsrad	ymm3, ymm2, 7
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm0, ymm0, ymm0
	vpermq	ymm1, ymm0, 216				; 000000d8H
	vmovd	DWORD PTR [rax+rsi], xmm1
	add	eax, 4
	vpextrw	WORD PTR [rax+rsi], xmm1, 2

; 1369 :           *(uint32_t*)&dst[(y + 1) * dst_stride + 2] = elements_89ab;

	lea	eax, DWORD PTR [rdx+2]
	vpextrw	WORD PTR [rdx+rsi], xmm1, 3
	vpextrd	DWORD PTR [rax+rsi], xmm1, 2
	cmp	ecx, r10d
	jb	$LL300@bipred_ave

; 1394 : 
; 1395 :       case  6: bipred_average_px_im_template_avx2(dst, px, im,  6, pu_h, dst_stride); break;

	jmp	$LL5@bipred_ave
$LN16@bipred_ave:

; 1187 :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovdqu	ymm8, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 1188 : 
; 1189 :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 12
	popcnt	eax, eax
	cmp	eax, 1

; 1190 :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	eax, DWORD PTR [r8+r8*2]
	lea	r10d, DWORD PTR [rax*4]
	sete	cl
	test	r10b, 31

; 1192 :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 1193 : 
; 1194 :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN48@bipred_ave
	test	cl, cl
	je	SHORT $LN353@bipred_ave

; 1195 :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r10d, r10d
	je	$LL5@bipred_ave
	lea	ebx, DWORD PTR [r10-1]
	shr	ebx, 5
	inc	ebx
	npad	5
$LL337@bipred_ave:

; 1239 :         default:
; 1240 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1240				; 000004d8H
	lea	rdx, OFFSET FLAT:$SG4294951288
	lea	rcx, OFFSET FLAT:$SG4294951287
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL337@bipred_ave

; 1241 :           break;
; 1242 :       }
; 1243 :     }
; 1244 :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN353@bipred_ave:

; 1245 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	xor	ecx, ecx
	test	r10d, r10d
	je	$LL5@bipred_ave

; 1260 : 
; 1261 :       __m256i sample_px_im_a_lo = _mm256_unpacklo_epi16(sample_px_a_16bit, sample_im_a_16bit);
; 1262 :       __m256i sample_px_im_a_hi = _mm256_unpackhi_epi16(sample_px_a_16bit, sample_im_a_16bit);
; 1263 :       __m256i sample_px_im_b_lo = _mm256_unpacklo_epi16(sample_px_b_16bit, sample_im_b_16bit);
; 1264 :       __m256i sample_px_im_b_hi = _mm256_unpackhi_epi16(sample_px_b_16bit, sample_im_b_16bit);
; 1265 : 
; 1266 :       __m256i all_ones = _mm256_set1_epi16(1);

	mov	r11d, DWORD PTR dst_stride$[rsp]
	mov	r14, rdi
	vmovdqu	ymm9, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	vmovdqu	ymm10, YMMWORD PTR __ymm@0000000000000000000000000000000000000000ffffffffffffffffffffffff
	vmovaps	XMMWORD PTR [rsp+32], xmm11
	vmovdqu	ymm11, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffff000000000000000000000000
	lea	r15d, DWORD PTR [r11-12]
$LL342@bipred_ave:

; 1252 :       __m128i sample_px_a_8bit  = _mm_loadu_si128((__m128i*)&px[i]);
; 1253 :       __m128i sample_px_b_8bit  = _mm_loadl_epi64((__m128i*)&px[i + 16]);
; 1254 :       __m256i sample_px_a_16bit = _mm256_cvtepu8_epi16(sample_px_a_8bit);

	vpmovzxbw ymm0, XMMWORD PTR [r14]

; 1255 :       __m256i sample_px_b_16bit = _mm256_cvtepu8_epi16(sample_px_b_8bit);
; 1256 :       sample_px_a_16bit         = _mm256_slli_epi16(sample_px_a_16bit, 14 - KVZ_BIT_DEPTH);
; 1257 :       sample_px_b_16bit         = _mm256_slli_epi16(sample_px_b_16bit, 14 - KVZ_BIT_DEPTH);
; 1258 :       __m256i sample_im_a_16bit = _mm256_loadu_si256((__m256i*)&im[i]);
; 1259 :       __m256i sample_im_b_16bit = _mm256_castsi128_si256(_mm_loadu_si128((__m128i*)&im[i + 16]));

	vmovups	xmm3, XMMWORD PTR [rbx+32]
	vpsllw	ymm7, ymm0, 6
	mov	r9d, ecx
	lea	r14, QWORD PTR [r14+24]
	mov	eax, -1431655765			; aaaaaaabH
	lea	rbx, QWORD PTR [rbx+48]
	mul	ecx
	shr	edx, 3
	lea	eax, DWORD PTR [rdx+rdx*2]

; 1267 :       __m256i avg_a_lo = _mm256_madd_epi16(sample_px_im_a_lo, all_ones);
; 1268 :       __m256i avg_a_hi = _mm256_madd_epi16(sample_px_im_a_hi, all_ones);
; 1269 :       __m256i avg_b_lo = _mm256_madd_epi16(sample_px_im_b_lo, all_ones);
; 1270 :       __m256i avg_b_hi = _mm256_madd_epi16(sample_px_im_b_hi, all_ones);
; 1271 : 
; 1272 :       avg_a_lo = _mm256_add_epi32(avg_a_lo, offset);
; 1273 :       avg_a_hi = _mm256_add_epi32(avg_a_hi, offset);
; 1274 :       avg_b_lo = _mm256_add_epi32(avg_b_lo, offset);
; 1275 :       avg_b_hi = _mm256_add_epi32(avg_b_hi, offset);
; 1276 : 
; 1277 :       avg_a_lo = _mm256_srai_epi32(avg_a_lo, shift);
; 1278 :       avg_a_hi = _mm256_srai_epi32(avg_a_hi, shift);
; 1279 :       avg_b_lo = _mm256_srai_epi32(avg_b_lo, shift);
; 1280 :       avg_b_hi = _mm256_srai_epi32(avg_b_hi, shift);
; 1281 : 
; 1282 :       __m256i avg_01  = _mm256_packus_epi32(avg_a_lo, avg_a_hi);
; 1283 :       __m256i avg_23  = _mm256_packus_epi32(avg_b_lo, avg_b_hi);
; 1284 :       __m256i avg0213 = _mm256_packus_epi16(avg_01, avg_23);
; 1285 :       __m256i avg     = _mm256_permute4x64_epi64(avg0213, _MM_SHUFFLE(3, 1, 2, 0));
; 1286 : 
; 1287 :       switch (pu_w) {
; 1288 :         case 12: scatter_ymm_12x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	imul	edx, r11d
	shl	eax, 2
	sub	r9d, eax
	lea	eax, DWORD PTR [rcx+16]
	movsxd	r8, eax
	add	ecx, 24
	vmovq	xmm0, QWORD PTR [r8+rdi]
	vpmovzxbw ymm1, xmm0
	vpsllw	ymm5, ymm1, 6
	vpunpckhwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm9
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm4, ymm2, 7
	vpunpcklwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm9
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [rbx-48]
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm9
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [rbx-48]
	vpaddd	ymm2, ymm1, ymm8
	vpackusdw ymm5, ymm3, ymm4
	vpmaddwd ymm1, ymm0, ymm9
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm8
	vpsrad	ymm3, ymm2, 7
	lea	eax, DWORD PTR [r9+rdx]
	add	rax, rsi
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm1, ymm0, ymm5
	vpermq	ymm2, ymm1, 216				; 000000d8H

; 809  :   _mm256_maskstore_epi32((int32_t*)dst, mask_a, ymm); dst += dst_stride - 3 * 4;

	vpmaskmovd YMMWORD PTR [rax], ymm10, ymm2

; 810  :   _mm256_maskstore_epi32((int32_t*)dst, mask_b, ymm);

	vpmaskmovd YMMWORD PTR [rax+r15], ymm11, ymm2

; 1245 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	cmp	ecx, r10d
	jb	$LL342@bipred_ave

; 1293 :           break;
; 1294 :       }
; 1295 :     }
; 1296 :   } else {

	vmovaps	xmm11, XMMWORD PTR [rsp+32]
	jmp	$LL5@bipred_ave
$LN17@bipred_ave:

; 1187 :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovdqu	ymm9, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 1188 : 
; 1189 :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 24
	popcnt	eax, eax
	cmp	eax, 1
	sete	cl

; 1190 :   bool area_mod_32 = (pu_w * pu_h) % 32;

	imul	eax, r8d, -8				; fffffff8H
	test	al, 31

; 1192 :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 1193 : 
; 1194 :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN48@bipred_ave
	lea	r10d, DWORD PTR [r8+r8*2]
	shl	r10d, 3
	test	cl, cl
	je	SHORT $LN405@bipred_ave

; 1195 :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r10d, r10d
	je	$LL5@bipred_ave
	lea	ebx, DWORD PTR [r10-1]
	shr	ebx, 5
	inc	ebx
$LL389@bipred_ave:

; 1239 :         default:
; 1240 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1240				; 000004d8H
	lea	rdx, OFFSET FLAT:$SG4294951288
	lea	rcx, OFFSET FLAT:$SG4294951287
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL389@bipred_ave

; 1241 :           break;
; 1242 :       }
; 1243 :     }
; 1244 :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN405@bipred_ave:

; 1245 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	xor	ecx, ecx
	test	r10d, r10d
	je	$LL5@bipred_ave
	vmovdqu	ymm10, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	ymm8, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 1246 : 
; 1247 :       int y = i / pu_w;
; 1248 :       int x = i % pu_w;
; 1249 : 
; 1250 :       // Last 64 bits of the 256 / 32 bits of the 128 are not used to simplify the loop
; 1251 :       __m256i mask              = _mm256_setr_epi64x(-1, -1, -1, 0);

	mov	r14d, DWORD PTR dst_stride$[rsp]
	mov	r11, rdi
$LL394@bipred_ave:

; 1252 :       __m128i sample_px_a_8bit  = _mm_loadu_si128((__m128i*)&px[i]);
; 1253 :       __m128i sample_px_b_8bit  = _mm_loadl_epi64((__m128i*)&px[i + 16]);
; 1254 :       __m256i sample_px_a_16bit = _mm256_cvtepu8_epi16(sample_px_a_8bit);

	vpmovzxbw ymm0, XMMWORD PTR [r11]

; 1255 :       __m256i sample_px_b_16bit = _mm256_cvtepu8_epi16(sample_px_b_8bit);
; 1256 :       sample_px_a_16bit         = _mm256_slli_epi16(sample_px_a_16bit, 14 - KVZ_BIT_DEPTH);
; 1257 :       sample_px_b_16bit         = _mm256_slli_epi16(sample_px_b_16bit, 14 - KVZ_BIT_DEPTH);
; 1258 :       __m256i sample_im_a_16bit = _mm256_loadu_si256((__m256i*)&im[i]);
; 1259 :       __m256i sample_im_b_16bit = _mm256_castsi128_si256(_mm_loadu_si128((__m128i*)&im[i + 16]));

	vmovups	xmm3, XMMWORD PTR [rbx+32]
	vpsllw	ymm7, ymm0, 6
	mov	r9d, ecx
	lea	r11, QWORD PTR [r11+24]
	mov	eax, -1431655765			; aaaaaaabH
	lea	rbx, QWORD PTR [rbx+48]
	mul	ecx
	shr	edx, 4
	lea	eax, DWORD PTR [rdx+rdx*2]

; 1289 :         case 24: // Same as case 48
; 1290 :         case 48: _mm256_maskstore_epi64((long long*)&dst[y * dst_stride + x], mask, avg); break;

	imul	edx, r14d
	shl	eax, 3
	sub	r9d, eax
	lea	eax, DWORD PTR [rcx+16]
	movsxd	r8, eax
	add	ecx, 24
	add	edx, r9d
	vmovq	xmm0, QWORD PTR [r8+rdi]
	vpmovzxbw ymm1, xmm0
	vpsllw	ymm5, ymm1, 6
	vpunpckhwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm8
	vpaddd	ymm2, ymm1, ymm9
	vpsrad	ymm4, ymm2, 7
	vpunpcklwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm8
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [rbx-48]
	vpaddd	ymm2, ymm1, ymm9
	vpsrad	ymm3, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm8
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [rbx-48]
	vpaddd	ymm2, ymm1, ymm9
	vpackusdw ymm5, ymm3, ymm4
	vpmaddwd ymm1, ymm0, ymm8
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm9
	vpsrad	ymm3, ymm2, 7
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm1, ymm0, ymm5
	vpermq	ymm2, ymm1, 216				; 000000d8H
	vpmaskmovq YMMWORD PTR [rdx+rsi], ymm10, ymm2
	cmp	ecx, r10d
	jb	$LL394@bipred_ave

; 1293 :           break;
; 1294 :       }
; 1295 :     }
; 1296 :   } else {

	jmp	$LL5@bipred_ave
$LN18@bipred_ave:

; 1187 :   __m256i offset = _mm256_set1_epi32(scalar_offset);

	vmovdqu	ymm9, YMMWORD PTR __ymm@0000004000000040000000400000004000000040000000400000004000000040

; 1188 : 
; 1189 :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 48					; 00000030H
	popcnt	eax, eax
	cmp	eax, 1
	sete	cl

; 1190 :   bool area_mod_32 = (pu_w * pu_h) % 32;

	imul	eax, r8d, -16				; fffffff0H
	test	al, 31

; 1192 :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 1193 : 
; 1194 :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN48@bipred_ave
	lea	r10d, DWORD PTR [r8+r8*2]
	shl	r10d, 4
	test	cl, cl
	je	SHORT $LN457@bipred_ave

; 1195 :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r10d, r10d
	je	$LL5@bipred_ave
	lea	ebx, DWORD PTR [r10-1]
	shr	ebx, 5
	inc	ebx
	npad	11
$LL441@bipred_ave:

; 1239 :         default:
; 1240 :           assert(0 && "Unexpected block width.");

	mov	r8d, 1240				; 000004d8H
	lea	rdx, OFFSET FLAT:$SG4294951288
	lea	rcx, OFFSET FLAT:$SG4294951287
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL441@bipred_ave

; 1241 :           break;
; 1242 :       }
; 1243 :     }
; 1244 :   } else if (area_mod_32 == 0) {

	jmp	$LL5@bipred_ave
$LN457@bipred_ave:

; 1245 :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	xor	ecx, ecx
	test	r10d, r10d
	je	$LL5@bipred_ave
	vmovdqu	ymm10, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	ymm8, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 1246 : 
; 1247 :       int y = i / pu_w;
; 1248 :       int x = i % pu_w;
; 1249 : 
; 1250 :       // Last 64 bits of the 256 / 32 bits of the 128 are not used to simplify the loop
; 1251 :       __m256i mask              = _mm256_setr_epi64x(-1, -1, -1, 0);

	mov	r14d, DWORD PTR dst_stride$[rsp]
	mov	r11, rdi
	npad	2
$LL446@bipred_ave:

; 1252 :       __m128i sample_px_a_8bit  = _mm_loadu_si128((__m128i*)&px[i]);
; 1253 :       __m128i sample_px_b_8bit  = _mm_loadl_epi64((__m128i*)&px[i + 16]);
; 1254 :       __m256i sample_px_a_16bit = _mm256_cvtepu8_epi16(sample_px_a_8bit);

	vpmovzxbw ymm0, XMMWORD PTR [r11]

; 1255 :       __m256i sample_px_b_16bit = _mm256_cvtepu8_epi16(sample_px_b_8bit);
; 1256 :       sample_px_a_16bit         = _mm256_slli_epi16(sample_px_a_16bit, 14 - KVZ_BIT_DEPTH);
; 1257 :       sample_px_b_16bit         = _mm256_slli_epi16(sample_px_b_16bit, 14 - KVZ_BIT_DEPTH);
; 1258 :       __m256i sample_im_a_16bit = _mm256_loadu_si256((__m256i*)&im[i]);
; 1259 :       __m256i sample_im_b_16bit = _mm256_castsi128_si256(_mm_loadu_si128((__m128i*)&im[i + 16]));

	vmovups	xmm3, XMMWORD PTR [rbx+32]
	vpsllw	ymm7, ymm0, 6
	mov	r9d, ecx
	lea	r11, QWORD PTR [r11+24]
	mov	eax, -1431655765			; aaaaaaabH
	lea	rbx, QWORD PTR [rbx+48]
	mul	ecx
	shr	edx, 5
	lea	eax, DWORD PTR [rdx+rdx*2]

; 1289 :         case 24: // Same as case 48
; 1290 :         case 48: _mm256_maskstore_epi64((long long*)&dst[y * dst_stride + x], mask, avg); break;

	imul	edx, r14d
	shl	eax, 4
	sub	r9d, eax
	lea	eax, DWORD PTR [rcx+16]
	movsxd	r8, eax
	add	ecx, 24
	add	edx, r9d
	vmovq	xmm0, QWORD PTR [r8+rdi]
	vpmovzxbw ymm1, xmm0
	vpsllw	ymm5, ymm1, 6
	vpunpckhwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm8
	vpaddd	ymm2, ymm1, ymm9
	vpsrad	ymm4, ymm2, 7
	vpunpcklwd ymm0, ymm5, ymm3
	vpmaddwd ymm1, ymm0, ymm8
	vpunpckhwd ymm0, ymm7, YMMWORD PTR [rbx-48]
	vpaddd	ymm2, ymm1, ymm9
	vpsrad	ymm3, ymm2, 7
	vpmaddwd ymm1, ymm0, ymm8
	vpunpcklwd ymm0, ymm7, YMMWORD PTR [rbx-48]
	vpaddd	ymm2, ymm1, ymm9
	vpackusdw ymm5, ymm3, ymm4
	vpmaddwd ymm1, ymm0, ymm8
	vpsrad	ymm4, ymm2, 7
	vpaddd	ymm2, ymm1, ymm9
	vpsrad	ymm3, ymm2, 7
	vpackusdw ymm0, ymm3, ymm4
	vpackuswb ymm1, ymm0, ymm5
	vpermq	ymm2, ymm1, 216				; 000000d8H
	vpmaskmovq YMMWORD PTR [rdx+rsi], ymm10, ymm2
	cmp	ecx, r10d
	jb	$LL446@bipred_ave

; 1293 :           break;
; 1294 :       }
; 1295 :     }
; 1296 :   } else {

	jmp	SHORT $LL5@bipred_ave
$LN19@bipred_ave:

; 1396 :       case 12: bipred_average_px_im_template_avx2(dst, px, im, 12, pu_h, dst_stride); break;
; 1397 :       case 24: bipred_average_px_im_template_avx2(dst, px, im, 24, pu_h, dst_stride); break;
; 1398 :       case 48: bipred_average_px_im_template_avx2(dst, px, im, 48, pu_h, dst_stride); break;
; 1399 :       default:
; 1400 :         assert(0 && "Unexpected block width.");

	mov	r8d, 1400				; 00000578H
	lea	rdx, OFFSET FLAT:$SG4294951282
	lea	rcx, OFFSET FLAT:$SG4294951281
	call	QWORD PTR __imp__wassert

; 1401 :         break;
; 1402 :     }
; 1403 :   } else {

	jmp	SHORT $LL5@bipred_ave
$LN7@bipred_ave:

; 1404 :     int32_t shift = 15 - KVZ_BIT_DEPTH; // TODO: defines
; 1405 :     int32_t offset = 1 << (shift - 1);
; 1406 : 
; 1407 :     for (int i = 0; i < pu_w * pu_h; ++i)

	mov	r11d, r10d
	xor	ecx, ecx
	imul	r11d, r8d
	test	r11d, r11d
	je	SHORT $LL5@bipred_ave
	mov	r14d, DWORD PTR dst_stride$[rsp]
$LL6@bipred_ave:

; 1408 :     {
; 1409 :       int y = i / pu_w;
; 1410 :       int x = i % pu_w;
; 1411 :       int16_t sample_px = px[i] << (14 - KVZ_BIT_DEPTH);
; 1412 :       int16_t sample_im = im[i];
; 1413 :       int32_t rounded = (sample_px + sample_im + offset) >> shift;

	movzx	r9d, BYTE PTR [rdi]
	xor	edx, edx
	movsx	r8d, WORD PTR [rbx]
	inc	r9d
	shl	r9d, 6
	mov	eax, ecx
	add	r9d, r8d
	sar	r9d, 7
	div	r10d
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\picture-generic.c

; 69   :   if (value & ~PIXEL_MAX) {

	test	r9d, -256				; ffffffffffffff00H
	je	SHORT $LN491@bipred_ave

; 70   :     int32_t temp = (-value) >> 31;

	neg	r9d
	sar	r9d, 31
$LN491@bipred_ave:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 1414 :       dst[y * dst_stride + x] = kvz_fast_clip_32bit_to_pixel(rounded);

	imul	eax, r14d
	inc	ecx
	inc	rdi
	add	rbx, 2
	add	eax, edx
	mov	BYTE PTR [rax+rsi], r9b
	cmp	ecx, r11d
	jb	SHORT $LL6@bipred_ave
$LL5@bipred_ave:
	vzeroupper

; 1415 :     }
; 1416 :   }
; 1417 : }

	vmovaps	xmm6, XMMWORD PTR [rsp+112]
	lea	r11, QWORD PTR [rsp+128]
	mov	rbx, QWORD PTR [r11+32]
	mov	rsi, QWORD PTR [r11+40]
	mov	rdi, QWORD PTR [r11+48]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm9, XMMWORD PTR [r11-64]
	vmovaps	xmm10, XMMWORD PTR [r11-80]
	vmovaps	xmm7, XMMWORD PTR [rsp+96]
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r12
	ret	0
	npad	2
$LN689@bipred_ave:
	DD	$LN10@bipred_ave
	DD	$LN15@bipred_ave
	DD	$LN11@bipred_ave
	DD	$LN16@bipred_ave
	DD	$LN12@bipred_ave
	DD	$LN17@bipred_ave
	DD	$LN13@bipred_ave
	DD	$LN18@bipred_ave
	DD	$LN14@bipred_ave
	DD	$LN19@bipred_ave
$LN688@bipred_ave:
	DB	0
	DB	9
	DB	1
	DB	9
	DB	2
	DB	9
	DB	9
	DB	9
	DB	3
	DB	9
	DB	9
	DB	9
	DB	4
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	5
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	6
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	7
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	8
bipred_average_px_im_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\picture-generic.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\picture-generic.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\picture-generic.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
tv23567 = 48
lcu$ = 128
px_L0$ = 136
px_L1$ = 144
im_L0$ = 152
im_L1$ = 160
pu_x$ = 168
pu_y$ = 176
pu_w$ = 184
pu_h$ = 192
im_flags_L0$ = 200
im_flags_L1$ = 208
tv24881 = 216
tv24875 = 216
tv24872 = 216
predict_luma$ = 216
predict_chroma$ = 224
bipred_average_avx2 PROC

; 1431 :   const bool predict_chroma) {

	mov	QWORD PTR [rsp+32], r9
	mov	QWORD PTR [rsp+24], r8
	mov	QWORD PTR [rsp+16], rdx
	mov	QWORD PTR [rsp+8], rcx
	push	rbx
	push	rsi
	push	rdi
	push	r12
	push	r13
	push	r14
	push	r15
	sub	rsp, 64					; 00000040H
	mov	r15d, DWORD PTR pu_h$[rsp]

; 1432 : 
; 1433 :   //After reconstruction, merge the predictors by taking an average of each pixel
; 1434 :   if (predict_luma) {

	lea	r11, OFFSET FLAT:__ImageBase
	mov	edi, DWORD PTR im_flags_L0$[rsp]
	xor	ebx, ebx
	mov	rsi, r9
	mov	r14d, DWORD PTR pu_y$[rsp]
	mov	r12, rcx
	mov	ecx, DWORD PTR im_flags_L1$[rsp]
	lea	r13d, QWORD PTR [rbx+8]
	lea	r10d, QWORD PTR [rbx+32]
	lea	r9d, QWORD PTR [rbx+64]
	cmp	BYTE PTR predict_luma$[rsp], bl
	je	$LN3591@bipred_ave

; 1435 :     unsigned pb_offset = SUB_SCU(pu_y) * LCU_WIDTH + SUB_SCU(pu_x);

	mov	eax, DWORD PTR pu_x$[rsp]
	mov	ecx, r14d
	mov	edx, DWORD PTR im_flags_L1$[rsp]
	and	eax, 63					; 0000003fH
	and	ecx, 63					; 0000003fH
	shl	ecx, 6
	add	ecx, eax
	mov	eax, edi
	or	eax, edx
	test	al, 1

; 1436 : 
; 1437 :     if (!(im_flags_L0 & 1) && !(im_flags_L1 & 1)) {

	jne	$LN3@bipred_ave

; 1438 :       bipred_average_px_px_avx2(lcu->rec.y + pb_offset, px_L0->y, px_L1->y, pu_w, pu_h, LCU_WIDTH);

	mov	rdx, QWORD PTR px_L0$[rsp]
	lea	r14, QWORD PTR [r12+6540]

; 918  :   if (!(pu_w == 4 && pu_h == 4) && pu_w > 2) {

	mov	r9d, DWORD PTR pu_w$[rsp]

; 1438 :       bipred_average_px_px_avx2(lcu->rec.y + pb_offset, px_L0->y, px_L1->y, pu_w, pu_h, LCU_WIDTH);

	mov	rsi, QWORD PTR [r8+8]
	mov	eax, ecx
	mov	rdi, QWORD PTR [rdx+8]
	add	r14, rax

; 918  :   if (!(pu_w == 4 && pu_h == 4) && pu_w > 2) {

	cmp	r9d, 4
	jne	SHORT $LN33@bipred_ave
	cmp	r15d, r9d
	je	$LN31@bipred_ave
	lea	eax, DWORD PTR [r9-4]
	jmp	SHORT $LN3564@bipred_ave
$LN33@bipred_ave:
	cmp	r9d, 2
	jbe	$LN31@bipred_ave

; 919  :     switch (pu_w) {

	lea	eax, DWORD PTR [r9-4]
	cmp	eax, 60					; 0000003cH
	ja	$LN43@bipred_ave
$LN3564@bipred_ave:
	movzx	eax, BYTE PTR $LN3569@bipred_ave[r11+rax]
	mov	ecx, DWORD PTR $LN3570@bipred_ave[r11+rax*4]
	add	rcx, r11
	jmp	rcx
$LN34@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 4

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	r13d, DWORD PTR [r15*4]
	popcnt	eax, eax
	cmp	eax, 1
	mov	r12d, r13d
	sete	al
	and	r12d, 31
	mov	BYTE PTR tv24881[rsp], al

; 822  :   assert(!(pu_w == 4 && pu_h == 4) && "Branch for 4x4 not yet implemented.");

	cmp	r15d, 4
	jne	SHORT $LN86@bipred_ave
	mov	r8d, 822				; 00000336H
	lea	rdx, OFFSET FLAT:$SG4294951321
	lea	rcx, OFFSET FLAT:$SG4294951320
	call	QWORD PTR __imp__wassert
	movzx	eax, BYTE PTR tv24881[rsp]
	mov	rdx, QWORD PTR px_L0$[rsp]
$LN86@bipred_ave:

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	test	r12d, r12d
	jne	$LN72@bipred_ave
	test	al, al
	je	$LN65@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	mov	r8d, ebx
	test	r13d, r13d
	je	$LN3590@bipred_ave
	sub	rsi, rdi
	npad	3
$LL49@bipred_ave:

; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rsi+rdi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rdi]

; 775  :   __m128i ymm_hi = _mm256_extracti128_si256(ymm, 1);

	vextracti128 xmm0, ymm1, 1

; 828  :       int y = i / pu_w;

	mov	edx, r8d
	lea	rdi, QWORD PTR [rdi+32]
	shr	edx, 2

; 829  :       int x = i % pu_w;

	mov	eax, r8d

; 834  : 
; 835  :       switch (pu_w) {
; 836  :         case  4: scatter_ymm_4x8_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	shl	edx, 6
	and	eax, 3
	add	edx, eax
	add	r8d, 32					; 00000020H
	add	rdx, r14

; 776  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_lo); dst += dst_stride;

	vmovd	DWORD PTR [rdx], xmm1

; 777  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 1); dst += dst_stride;

	vpextrd	DWORD PTR [rdx+64], xmm1, 1

; 778  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 2); dst += dst_stride;

	vpextrd	DWORD PTR [rdx+128], xmm1, 2

; 779  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 3); dst += dst_stride;

	vpextrd	DWORD PTR [rdx+192], xmm1, 3

; 780  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_hi); dst += dst_stride;

	vmovd	DWORD PTR [rdx+256], xmm0

; 781  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 1); dst += dst_stride;

	vpextrd	DWORD PTR [rdx+320], xmm0, 1

; 782  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 2); dst += dst_stride;

	vpextrd	DWORD PTR [rdx+384], xmm0, 2

; 783  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 3);

	vpextrd	DWORD PTR [rdx+448], xmm0, 3

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	cmp	r8d, r13d
	jb	SHORT $LL49@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	r12, QWORD PTR lcu$[rsp]
$LN3581@bipred_ave:
	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN65@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r13d, r13d
	je	$LN3590@bipred_ave
	lea	ecx, DWORD PTR [r13-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	edi, DWORD PTR [rdx+1]
	npad	12
$LL54@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL54@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	r12, QWORD PTR lcu$[rsp]
	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN72@bipred_ave:

; 904  :         assert(0 && "Unexpected block width.");

	mov	r8d, 904				; 00000388H
	lea	rdx, OFFSET FLAT:$SG4294951312
	lea	rcx, OFFSET FLAT:$SG4294951311
	call	QWORD PTR __imp__wassert

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	rdx, QWORD PTR px_L0$[rsp]
	mov	r12, QWORD PTR lcu$[rsp]
	jmp	$LL3571@bipred_ave
$LN35@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	popcnt	eax, r13d
	cmp	eax, 1

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	r8d, DWORD PTR [r15*8]
	sete	al
	test	r8b, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN124@bipred_ave
	test	al, al
	je	SHORT $LN117@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	mov	edx, ebx
	test	r8d, r8d
	je	$LN3581@bipred_ave
	sub	rsi, rdi
	npad	13
$LL101@bipred_ave:

; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rdi+rsi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rdi]
	mov	eax, edx
	lea	rdi, QWORD PTR [rdi+32]
	and	eax, 7
	mov	ecx, edx
	shr	ecx, 3
	add	edx, r10d

; 837  :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	shl	ecx, 6
	add	eax, ecx
	add	rax, r14

; 790  :   __m128d ymm_hi = _mm256_extractf128_pd(ymm_as_m256d, 1);

	vextractf128 xmm0, ymm1, 1

; 791  :   _mm_storel_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovlpd	QWORD PTR [rax], xmm1

; 792  :   _mm_storeh_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovhpd	QWORD PTR [rax+64], xmm1

; 793  :   _mm_storel_pd((double*)dst, ymm_hi); dst += dst_stride;

	vmovlpd	QWORD PTR [rax+128], xmm0

; 794  :   _mm_storeh_pd((double*)dst, ymm_hi);

	vmovhpd	QWORD PTR [rax+192], xmm0

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	cmp	edx, r8d
	jb	SHORT $LL101@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN117@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r8d, r8d
	je	$LL3571@bipred_ave
	lea	ecx, DWORD PTR [r8-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	edi, DWORD PTR [rdx+1]
	npad	3
$LL106@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL106@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN124@bipred_ave:

; 872  :         for (int i = 0; i < pu_w * pu_h; i += 16) {

	mov	edx, ebx
	test	r8d, r8d
	je	$LN3581@bipred_ave
	sub	rdi, rsi
	npad	5
$LL113@bipred_ave:

; 873  : 
; 874  :           int y = i / pu_w;
; 875  : 
; 876  :           sample_L0 = _mm_loadu_si128((__m128i*)&px_L0[i]);
; 877  :           sample_L1 = _mm_loadu_si128((__m128i*)&px_L1[i]);
; 878  :           avg       = _mm_avg_epu8(sample_L0, sample_L1);

	vmovdqu	xmm0, XMMWORD PTR [rdi+rsi]
	vpavgb	xmm1, xmm0, XMMWORD PTR [rsi]
	mov	eax, edx
	lea	rsi, QWORD PTR [rsi+16]
	shr	eax, 3
	add	edx, 16

; 879  :           _mm_storel_epi64((__m128i*)&dst[y * dst_stride], avg);

	shl	eax, 6
	vmovq	QWORD PTR [rax+r14], xmm1

; 880  :           _mm_storeh_pd((double*)&dst[(y + 1) * dst_stride], _mm_castsi128_pd(avg));

	add	eax, 64					; 00000040H
	vmovhpd	QWORD PTR [rax+r14], xmm1
	cmp	edx, r8d
	jb	SHORT $LL113@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN36@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	r10d, 16
	popcnt	eax, r10d
	cmp	eax, 1
	sete	cl

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	imul	eax, r15d, -16				; fffffff0H
	test	al, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN176@bipred_ave
	mov	r8d, r15d
	shl	r8d, 4
	test	cl, cl
	je	SHORT $LN169@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	mov	edx, ebx
	test	r8d, r8d
	je	SHORT $LN3584@bipred_ave
	sub	rsi, rdi
	npad	5
$LL153@bipred_ave:

; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rdi+rsi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rdi]
	mov	ecx, edx
	lea	rdi, QWORD PTR [rdi+32]
	shr	ecx, 4
	mov	eax, edx

; 838  :         case 16: scatter_ymm_16x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	shl	ecx, 6
	and	eax, 15
	add	ecx, eax
	add	edx, 32					; 00000020H

; 801  :   _mm_storeu_si128((__m128i *)dst, ymm_lo); dst += dst_stride;

	vmovdqu	XMMWORD PTR [rcx+r14], xmm1
	vextracti128 XMMWORD PTR [rcx+r14+64], ymm1, 1

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	cmp	edx, r8d
	jb	SHORT $LL153@bipred_ave
$LN3584@bipred_ave:

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	r14d, DWORD PTR pu_y$[rsp]
	mov	rsi, QWORD PTR im_L0$[rsp]
	mov	edi, DWORD PTR im_flags_L0$[rsp]
	mov	ecx, DWORD PTR im_flags_L1$[rsp]
	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LN3612@bipred_ave
$LN169@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r8d, r8d
	je	$LN3585@bipred_ave
	lea	ecx, DWORD PTR [r8-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	edi, DWORD PTR [rdx+1]
	npad	9
$LL158@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL158@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN176@bipred_ave:
	mov	r8d, 904				; 00000388H
	lea	rdx, OFFSET FLAT:$SG4294951312
	lea	rcx, OFFSET FLAT:$SG4294951311
	call	QWORD PTR __imp__wassert
	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN37@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	popcnt	eax, r10d
	cmp	eax, 1
	mov	r8d, r15d
	sete	al
	shl	r8d, 5

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	test	al, al
	je	SHORT $LN3437@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	mov	edx, ebx
	test	r8d, r8d
	je	$LN3581@bipred_ave
	sub	rsi, rdi
	npad	8
$LL205@bipred_ave:

; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rdi+rsi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rdi]
	mov	ecx, edx
	lea	rdi, QWORD PTR [rdi+32]
	shr	ecx, 5
	mov	eax, edx

; 839  :         case 32: // Same as case 64
; 840  :         case 64: _mm256_storeu_si256((__m256i *)&dst[y * dst_stride + x], avg); break;

	shl	ecx, 6
	and	eax, 31
	add	ecx, eax
	add	edx, r10d
	vmovdqu	YMMWORD PTR [rcx+r14], ymm1
	cmp	edx, r8d
	jb	SHORT $LL205@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN3437@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r8d, r8d
	je	$LL3571@bipred_ave
	lea	ecx, DWORD PTR [r8-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	edi, DWORD PTR [rdx+1]
$LL210@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL210@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN38@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 64					; 00000040H
	mov	r8d, r15d
	popcnt	eax, eax
	cmp	eax, 1
	sete	al
	shl	r8d, 6

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	test	al, al
	je	SHORT $LN3443@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	mov	edx, ebx
	test	r8d, r8d
	je	$LN3581@bipred_ave
	sub	rsi, rdi
	npad	10
$LL257@bipred_ave:

; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rdi+rsi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rdi]

; 839  :         case 32: // Same as case 64
; 840  :         case 64: _mm256_storeu_si256((__m256i *)&dst[y * dst_stride + x], avg); break;

	mov	eax, edx
	lea	rdi, QWORD PTR [rdi+32]
	and	eax, -64				; ffffffc0H
	mov	ecx, edx
	and	ecx, 63					; 0000003fH
	add	edx, r10d
	add	eax, ecx
	vmovdqu	YMMWORD PTR [rax+r14], ymm1
	cmp	edx, r8d
	jb	SHORT $LL257@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN3443@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r8d, r8d
	je	$LL3571@bipred_ave
	lea	ecx, DWORD PTR [r8-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	edi, DWORD PTR [rdx+1]
	npad	2
$LL262@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL262@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN39@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 6
	popcnt	eax, eax
	cmp	eax, 1

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	eax, DWORD PTR [r15+r15*2]
	lea	r9d, DWORD PTR [rax+rax]
	sete	cl
	test	r9b, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN332@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r9d, r9d
	je	$LL3571@bipred_ave

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	test	cl, cl
	je	SHORT $LN325@bipred_ave
	lea	edi, DWORD PTR [r9-1]
	shr	edi, 5
	inc	edi
	npad	12
$LL309@bipred_ave:

; 841  :         default:
; 842  :           assert(0 && "Unexpected block width.");

	mov	r8d, 842				; 0000034aH
	lea	rdx, OFFSET FLAT:$SG4294951316
	lea	rcx, OFFSET FLAT:$SG4294951315
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL309@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN325@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	lea	ecx, DWORD PTR [r9-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	edi, DWORD PTR [rdx+1]
	npad	2
$LL314@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL314@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN332@bipred_ave:

; 884  :         for (int i = 0; i < pu_w * pu_h; i += 12) {

	mov	r10d, ebx
	test	r9d, r9d
	je	$LL3571@bipred_ave
	vmovdqu	xmm3, XMMWORD PTR __xmm@00000000ffffffffffffffffffffffff
$LL324@bipred_ave:

; 885  : 
; 886  :           int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH
	mul	r10d

; 887  : 
; 888  :           __m128i mask      = _mm_setr_epi32(-1, -1, -1, 0);
; 889  :           __m128i sample_L0 = _mm_maskload_epi32((const int*)(&px_L0[i]), mask);

	movsxd	rax, r10d
	add	r10d, 12
	shr	edx, 2

; 890  :           __m128i sample_L1 = _mm_maskload_epi32((const int*)(&px_L1[i]), mask);
; 891  :           __m128i avg       = _mm_avg_epu8(sample_L0, sample_L1);
; 892  : 
; 893  :           uint32_t elements_0123 = _mm_cvtsi128_si32(avg);
; 894  :           uint16_t elements_45   = _mm_extract_epi16(avg, 2);
; 895  :           uint16_t elements_67   = _mm_extract_epi16(avg, 3);
; 896  :           uint32_t elements_89ab = _mm_extract_epi32(avg, 2);
; 897  :           *(uint32_t*)&dst[(y + 0) * dst_stride + 0] = elements_0123;

	shl	edx, 6
	mov	r8d, edx
	vpmaskmovd xmm1, xmm3, XMMWORD PTR [rax+rdi]
	vpmaskmovd xmm0, xmm3, XMMWORD PTR [rax+rsi]
	vpavgb	xmm2, xmm1, xmm0

; 898  :           *(uint16_t*)&dst[(y + 0) * dst_stride + 4] = elements_45;

	lea	eax, DWORD PTR [r8+4]
	vmovd	DWORD PTR [rdx+r14], xmm2
	vpextrw	WORD PTR [rax+r14], xmm2, 2

; 899  :           *(uint16_t*)&dst[(y + 1) * dst_stride + 0] = elements_67;

	lea	eax, DWORD PTR [r8+64]
	vpextrw	WORD PTR [rax+r14], xmm2, 3

; 900  :           *(uint32_t*)&dst[(y + 1) * dst_stride + 2] = elements_89ab;

	lea	eax, DWORD PTR [r8+66]
	vpextrd	DWORD PTR [rax+r14], xmm2, 2
	cmp	r10d, r9d
	jb	SHORT $LL324@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN40@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	r11d, 12
	popcnt	eax, r11d
	cmp	eax, 1

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	eax, DWORD PTR [r15+r15*2]
	lea	r9d, DWORD PTR [rax*4]
	sete	cl
	test	r9b, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN176@bipred_ave
	test	cl, cl
	je	SHORT $LN377@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r9d, r9d
	je	$LN3576@bipred_ave
	lea	edi, DWORD PTR [r9-1]
	shr	edi, 5
	inc	edi
	npad	1
$LL361@bipred_ave:

; 841  :         default:
; 842  :           assert(0 && "Unexpected block width.");

	mov	r8d, 842				; 0000034aH
	lea	rdx, OFFSET FLAT:$SG4294951316
	lea	rcx, OFFSET FLAT:$SG4294951315
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL361@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN377@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	mov	r8d, ebx
	test	r9d, r9d
	je	$LN3576@bipred_ave
	vmovdqu	ymm3, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	ymm4, YMMWORD PTR __ymm@0000000000000000000000000000000000000000ffffffffffffffffffffffff
	vmovdqu	ymm5, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffff000000000000000000000000
$LL366@bipred_ave:

; 848  : 
; 849  :       int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH

; 850  :       int x = i % pu_w;

	mov	ecx, r8d
	mul	r8d
	shr	edx, 3
	lea	eax, DWORD PTR [rdx+rdx*2]

; 851  : 
; 852  :       // Last 64 bits of the 256 are not used to simplify the loop
; 853  :       __m256i mask      = _mm256_setr_epi64x(-1, -1, -1, 0);
; 854  :       __m256i sample_L0 = _mm256_maskload_epi64((const long long*)&px_L0[i], mask);
; 855  :       __m256i sample_L1 = _mm256_maskload_epi64((const long long*)&px_L1[i], mask);
; 856  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);
; 857  : 
; 858  :       switch (pu_w) {
; 859  :         case 12: scatter_ymm_12x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	shl	edx, 6
	shl	eax, 2
	sub	ecx, eax
	movsxd	rax, r8d
	add	edx, ecx
	add	r8d, 24
	vpmaskmovq ymm1, ymm3, YMMWORD PTR [rax+rsi]
	vpmaskmovq ymm0, ymm3, YMMWORD PTR [rax+rdi]
	vpavgb	ymm2, ymm0, ymm1

; 809  :   _mm256_maskstore_epi32((int32_t*)dst, mask_a, ymm); dst += dst_stride - 3 * 4;

	vpmaskmovd YMMWORD PTR [rdx+r14], ymm4, ymm2

; 810  :   _mm256_maskstore_epi32((int32_t*)dst, mask_b, ymm);

	vpmaskmovd YMMWORD PTR [rdx+r14+52], ymm5, ymm2

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	cmp	r8d, r9d
	jb	SHORT $LL366@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	mov	r14d, DWORD PTR pu_y$[rsp]
	mov	r10d, 16
	mov	rsi, QWORD PTR im_L0$[rsp]
	mov	edi, DWORD PTR im_flags_L0$[rsp]
	mov	ecx, DWORD PTR im_flags_L1$[rsp]
	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LN3613@bipred_ave
$LN41@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	r8d, 24
	popcnt	eax, r8d
	cmp	eax, 1
	sete	cl

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	imul	eax, r15d, -8				; fffffff8H
	test	al, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN176@bipred_ave
	lea	r9d, DWORD PTR [r15+r15*2]
	shl	r9d, 3
	test	cl, cl
	je	SHORT $LN429@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r9d, r9d
	je	$LN3574@bipred_ave
	lea	edi, DWORD PTR [r9-1]
	shr	edi, 5
	inc	edi
$LL413@bipred_ave:

; 841  :         default:
; 842  :           assert(0 && "Unexpected block width.");

	mov	r8d, 842				; 0000034aH
	lea	rdx, OFFSET FLAT:$SG4294951316
	lea	rcx, OFFSET FLAT:$SG4294951315
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL413@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN429@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	mov	r8d, ebx
	test	r9d, r9d
	je	$LL3571@bipred_ave
	vmovdqu	ymm3, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
$LL418@bipred_ave:

; 848  : 
; 849  :       int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH

; 850  :       int x = i % pu_w;

	mov	ecx, r8d
	mul	r8d
	shr	edx, 4
	lea	eax, DWORD PTR [rdx+rdx*2]

; 860  :         case 24: // Same as case 48
; 861  :         case 48: _mm256_maskstore_epi64((long long*)&dst[y * dst_stride + x], mask, avg); break;

	shl	edx, 6
	shl	eax, 3
	sub	ecx, eax
	movsxd	rax, r8d
	add	edx, ecx
	add	r8d, 24
	vpmaskmovq ymm1, ymm3, YMMWORD PTR [rax+rsi]
	vpmaskmovq ymm0, ymm3, YMMWORD PTR [rax+rdi]
	vpavgb	ymm2, ymm0, ymm1
	vpmaskmovq YMMWORD PTR [rdx+r14], ymm3, ymm2
	cmp	r8d, r9d
	jb	SHORT $LL418@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN42@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 48					; 00000030H
	popcnt	eax, eax
	cmp	eax, 1
	sete	cl

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	imul	eax, r15d, -16				; fffffff0H
	test	al, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN176@bipred_ave
	lea	r9d, DWORD PTR [r15+r15*2]
	shl	r9d, 4
	test	cl, cl
	je	SHORT $LN481@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r9d, r9d
	je	$LL3571@bipred_ave
	lea	edi, DWORD PTR [r9-1]
	shr	edi, 5
	inc	edi
	npad	11
$LL465@bipred_ave:

; 841  :         default:
; 842  :           assert(0 && "Unexpected block width.");

	mov	r8d, 842				; 0000034aH
	lea	rdx, OFFSET FLAT:$SG4294951316
	lea	rcx, OFFSET FLAT:$SG4294951315
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL465@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN481@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	mov	r8d, ebx
	test	r9d, r9d
	je	$LL3571@bipred_ave
	vmovdqu	ymm3, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
$LL470@bipred_ave:

; 848  : 
; 849  :       int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH

; 850  :       int x = i % pu_w;

	mov	ecx, r8d
	mul	r8d
	shr	edx, 5
	lea	eax, DWORD PTR [rdx+rdx*2]

; 860  :         case 24: // Same as case 48
; 861  :         case 48: _mm256_maskstore_epi64((long long*)&dst[y * dst_stride + x], mask, avg); break;

	shl	edx, 6
	shl	eax, 4
	sub	ecx, eax
	movsxd	rax, r8d
	add	edx, ecx
	add	r8d, 24
	vpmaskmovq ymm1, ymm3, YMMWORD PTR [rax+rsi]
	vpmaskmovq ymm0, ymm3, YMMWORD PTR [rax+rdi]
	vpavgb	ymm2, ymm0, ymm1
	vpmaskmovq YMMWORD PTR [rdx+r14], ymm3, ymm2
	cmp	r8d, r9d
	jb	SHORT $LL470@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN43@bipred_ave:

; 931  :         assert(0 && "Unexpected block width.");

	mov	r8d, 931				; 000003a3H
	lea	rdx, OFFSET FLAT:$SG4294951310
	lea	rcx, OFFSET FLAT:$SG4294951309

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	call	QWORD PTR __imp__wassert
	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN31@bipred_ave:

; 938  :     for (int i = 0; i < pu_w * pu_h; ++i)

	mov	r11d, r9d
	mov	r10d, ebx
	imul	r11d, r15d
	test	r11d, r11d
	je	$LL3571@bipred_ave
	sub	rdi, rsi
	mov	r12d, r9d
	npad	5
$LL30@bipred_ave:

; 939  :     {
; 940  :       int y = i / pu_w;
; 941  :       int x = i % pu_w;
; 942  :       int16_t sample_L0 = px_L0[i] << (14 - KVZ_BIT_DEPTH);
; 943  :       int16_t sample_L1 = px_L1[i] << (14 - KVZ_BIT_DEPTH);
; 944  :       int32_t rounded = (sample_L0 + sample_L1 + offset) >> shift;

	movzx	r8d, BYTE PTR [rsi]
	xor	edx, edx
	movzx	ecx, BYTE PTR [rsi+rdi]
	mov	eax, r10d
	shl	r8d, 6
	div	r12d
	add	r8d, 64					; 00000040H
	shl	ecx, 6
	add	r8d, ecx
	mov	r9d, eax
	shr	r8d, 7
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\picture-generic.c

; 69   :   if (value & ~PIXEL_MAX) {

	test	r8d, -256				; ffffff00H
	je	SHORT $LN515@bipred_ave

; 70   :     int32_t temp = (-value) >> 31;

	neg	r8d
	sar	r8d, 31
$LN515@bipred_ave:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 945  :       dst[y * dst_stride + x] = kvz_fast_clip_32bit_to_pixel(rounded);

	shl	r9d, 6
	inc	r10d
	add	r9d, edx
	inc	rsi
	mov	BYTE PTR [r9+r14], r8b
	cmp	r10d, r11d
	jb	SHORT $LL30@bipred_ave

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	r12, QWORD PTR lcu$[rsp]
	mov	rdx, QWORD PTR px_L0$[rsp]
	jmp	$LL3571@bipred_ave
$LN3@bipred_ave:

; 1439 : 
; 1440 :     } else if ((im_flags_L0 & 1) && (im_flags_L1 & 1)) {

	mov	r13, QWORD PTR im_L1$[rsp]
	mov	eax, edi
	and	eax, edx
	test	al, 1
	je	SHORT $LN5@bipred_ave

; 1441 :       bipred_average_im_im_avx2(lcu->rec.y + pb_offset, im_L0->y, im_L1->y, pu_w, pu_h, LCU_WIDTH);

	mov	r8, QWORD PTR [r13+8]
	mov	rdx, QWORD PTR [rsi+8]
	mov	eax, ecx
	lea	rcx, QWORD PTR [r12+6540]
	mov	DWORD PTR [rsp+40], r9d
	add	rcx, rax
	mov	r9d, DWORD PTR pu_w$[rsp]
	mov	DWORD PTR [rsp+32], r15d
	call	bipred_average_im_im_avx2
$LN3617@bipred_ave:

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	ecx, DWORD PTR im_flags_L1$[rsp]
	mov	r10d, 16
	mov	rdx, QWORD PTR px_L0$[rsp]
	lea	r11d, QWORD PTR [r10-4]
	jmp	$LN3614@bipred_ave
$LN5@bipred_ave:

; 1442 : 
; 1443 :     } else {
; 1444 :       kvz_pixel *src_px    = (im_flags_L0 & 1) ? px_L1->y : px_L0->y;

	test	dil, 1
	je	SHORT $LN13@bipred_ave
	mov	rdx, QWORD PTR [r8+8]

; 1445 :       kvz_pixel_im *src_im = (im_flags_L0 & 1) ? im_L0->y : im_L1->y;

	mov	r8, QWORD PTR [rsi+8]
	jmp	SHORT $LN16@bipred_ave
$LN13@bipred_ave:

; 1442 : 
; 1443 :     } else {
; 1444 :       kvz_pixel *src_px    = (im_flags_L0 & 1) ? px_L1->y : px_L0->y;

	mov	rax, QWORD PTR px_L0$[rsp]

; 1445 :       kvz_pixel_im *src_im = (im_flags_L0 & 1) ? im_L0->y : im_L1->y;

	mov	r8, QWORD PTR [r13+8]
	mov	rdx, QWORD PTR [rax+8]
$LN16@bipred_ave:

; 1446 :       bipred_average_px_im_avx2(lcu->rec.y + pb_offset, src_px, src_im, pu_w, pu_h, LCU_WIDTH);

	mov	eax, ecx
	lea	rcx, QWORD PTR [r12+6540]
	mov	DWORD PTR [rsp+40], r9d
	add	rcx, rax
	mov	r9d, DWORD PTR pu_w$[rsp]
	mov	DWORD PTR [rsp+32], r15d
	call	bipred_average_px_im_avx2

; 938  :     for (int i = 0; i < pu_w * pu_h; ++i)

	jmp	SHORT $LN3617@bipred_ave
$LN3574@bipred_ave:

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	mov	r14d, DWORD PTR pu_y$[rsp]
	mov	r10d, 16
	mov	rsi, QWORD PTR im_L0$[rsp]
	mov	r13, QWORD PTR im_L1$[rsp]
	mov	edi, DWORD PTR im_flags_L0$[rsp]
	mov	ecx, DWORD PTR im_flags_L1$[rsp]
	lea	r11d, QWORD PTR [r10-4]
	jmp	$LN6@bipred_ave
$LN3576@bipred_ave:

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	r14d, DWORD PTR pu_y$[rsp]
	mov	r10d, 16
	mov	rsi, QWORD PTR im_L0$[rsp]
	mov	edi, DWORD PTR im_flags_L0$[rsp]
	mov	ecx, DWORD PTR im_flags_L1$[rsp]
	jmp	SHORT $LN3613@bipred_ave
$LN3585@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	mov	r14d, DWORD PTR pu_y$[rsp]
	mov	rsi, QWORD PTR im_L0$[rsp]
	mov	edi, DWORD PTR im_flags_L0$[rsp]
	mov	ecx, DWORD PTR im_flags_L1$[rsp]
	jmp	SHORT $LN3612@bipred_ave
$LN3590@bipred_ave:

; 1447 :     }
; 1448 :   }
; 1449 :   if (predict_chroma) {

	mov	r12, QWORD PTR lcu$[rsp]
	npad	13
$LL3571@bipred_ave:
	mov	edi, DWORD PTR im_flags_L0$[rsp]
	mov	rsi, QWORD PTR im_L0$[rsp]
	mov	r14d, DWORD PTR pu_y$[rsp]
	mov	ecx, DWORD PTR im_flags_L1$[rsp]
$LN3591@bipred_ave:
	mov	r10d, 16
$LN3612@bipred_ave:
	mov	r11d, 12
$LN3613@bipred_ave:
	mov	r13, QWORD PTR im_L1$[rsp]
$LN3614@bipred_ave:
	mov	r8d, 24
$LN6@bipred_ave:
	cmp	BYTE PTR predict_chroma$[rsp], bl
	je	$LL11@bipred_ave

; 1450 :     unsigned pb_offset = SUB_SCU(pu_y) / 2 * LCU_WIDTH_C + SUB_SCU(pu_x) / 2;
; 1451 :     unsigned pb_w = pu_w / 2;

	mov	r9d, DWORD PTR pu_w$[rsp]
	mov	eax, DWORD PTR pu_x$[rsp]
	shr	r14d, 1
	shr	eax, 1
	and	r14d, 31
	and	eax, 31
	shl	r14d, 5
	add	r14d, eax
	shr	r9d, 1
	mov	eax, edi

; 1452 :     unsigned pb_h = pu_h / 2;

	shr	r15d, 1
	or	eax, ecx
	mov	DWORD PTR pu_w$[rsp], r9d
	test	al, 2

; 1453 : 
; 1454 :     if (!(im_flags_L0 & 2) && !(im_flags_L1 & 2)) {

	jne	$LN8@bipred_ave

; 1455 :       bipred_average_px_px_avx2(lcu->rec.u + pb_offset, px_L0->u, px_L1->u, pb_w, pb_h, LCU_WIDTH_C);

	mov	rdi, QWORD PTR [rdx+16]
	mov	rsi, QWORD PTR px_L1$[rsp]
	mov	edx, r14d
	add	rdx, r12
	mov	QWORD PTR tv23567[rsp], rdx
	mov	rsi, QWORD PTR [rsi+16]
	lea	r14, QWORD PTR [rdx+10636]

; 918  :   if (!(pu_w == 4 && pu_h == 4) && pu_w > 2) {

	cmp	r9d, 4
	jne	SHORT $LN1512@bipred_ave
	cmp	r15d, r9d
	je	$LN1510@bipred_ave
	lea	eax, DWORD PTR [r9-4]
	jmp	SHORT $LN3563@bipred_ave
$LN1512@bipred_ave:
	cmp	r9d, 2
	jbe	$LN1510@bipred_ave

; 919  :     switch (pu_w) {

	lea	eax, DWORD PTR [r9-4]
	cmp	eax, 60					; 0000003cH
	ja	$LN1522@bipred_ave
$LN3563@bipred_ave:
	lea	rdx, OFFSET FLAT:__ImageBase
	movzx	eax, BYTE PTR $LN3567@bipred_ave[rdx+rax]
	mov	ecx, DWORD PTR $LN3568@bipred_ave[rdx+rax*4]
	add	rcx, rdx
	jmp	rcx
$LN1513@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 4

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	r13d, DWORD PTR [r15*4]
	popcnt	eax, eax
	cmp	eax, 1
	mov	r12d, r13d
	sete	al
	and	r12d, 31
	mov	BYTE PTR tv24875[rsp], al

; 822  :   assert(!(pu_w == 4 && pu_h == 4) && "Branch for 4x4 not yet implemented.");

	cmp	r15d, 4
	jne	SHORT $LN1565@bipred_ave
	mov	r8d, 822				; 00000336H
	lea	rdx, OFFSET FLAT:$SG4294951321
	lea	rcx, OFFSET FLAT:$SG4294951320
	vzeroupper
	call	QWORD PTR __imp__wassert
	movzx	eax, BYTE PTR tv24875[rsp]
$LN1565@bipred_ave:

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	test	r12d, r12d
	jne	$LN1551@bipred_ave
	test	al, al
	je	$LN1544@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	mov	r8d, ebx
	test	r13d, r13d
	je	$LN3609@bipred_ave
	sub	rsi, rdi
	npad	6
$LL1528@bipred_ave:

; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rsi+rdi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rdi]

; 775  :   __m128i ymm_hi = _mm256_extracti128_si256(ymm, 1);

	vextracti128 xmm0, ymm1, 1

; 828  :       int y = i / pu_w;

	mov	edx, r8d
	lea	rdi, QWORD PTR [rdi+32]
	shr	edx, 2

; 829  :       int x = i % pu_w;

	mov	eax, r8d

; 834  : 
; 835  :       switch (pu_w) {
; 836  :         case  4: scatter_ymm_4x8_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	shl	edx, 5
	and	eax, 3
	add	edx, eax
	add	r8d, 32					; 00000020H
	add	rdx, r14

; 776  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_lo); dst += dst_stride;

	vmovd	DWORD PTR [rdx], xmm1

; 777  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 1); dst += dst_stride;

	vpextrd	DWORD PTR [rdx+32], xmm1, 1

; 778  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 2); dst += dst_stride;

	vpextrd	DWORD PTR [rdx+64], xmm1, 2

; 779  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 3); dst += dst_stride;

	vpextrd	DWORD PTR [rdx+96], xmm1, 3

; 780  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_hi); dst += dst_stride;

	vmovd	DWORD PTR [rdx+128], xmm0

; 781  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 1); dst += dst_stride;

	vpextrd	DWORD PTR [rdx+160], xmm0, 1

; 782  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 2); dst += dst_stride;

	vpextrd	DWORD PTR [rdx+192], xmm0, 2

; 783  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 3);

	vpextrd	DWORD PTR [rdx+224], xmm0, 3

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	cmp	r8d, r13d
	jb	SHORT $LL1528@bipred_ave

; 1456 :       bipred_average_px_px_avx2(lcu->rec.v + pb_offset, px_L0->v, px_L1->v, pb_w, pb_h, LCU_WIDTH_C);

	jmp	$LN3609@bipred_ave
$LN1544@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r13d, r13d
	je	$LN3609@bipred_ave
	lea	ecx, DWORD PTR [r13-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	edi, DWORD PTR [rdx+1]
	npad	2
$LL1533@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL1533@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LN3609@bipred_ave
$LN1551@bipred_ave:

; 1456 :       bipred_average_px_px_avx2(lcu->rec.v + pb_offset, px_L0->v, px_L1->v, pb_w, pb_h, LCU_WIDTH_C);

	mov	r8d, 904				; 00000388H
	lea	rdx, OFFSET FLAT:$SG4294951312
	lea	rcx, OFFSET FLAT:$SG4294951311
	vzeroupper
	call	QWORD PTR __imp__wassert
	jmp	$LN3609@bipred_ave
$LN1514@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	r13d, 8

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	r8d, DWORD PTR [r15*8]
	popcnt	eax, r13d
	cmp	eax, 1
	sete	al
	test	r8b, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN1603@bipred_ave
	test	al, al
	je	SHORT $LN1596@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	mov	edx, ebx
	test	r8d, r8d
	je	SHORT $LN3594@bipred_ave
	sub	rsi, rdi
	npad	5
$LL1580@bipred_ave:

; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rdi+rsi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rdi]
	mov	eax, edx
	lea	rdi, QWORD PTR [rdi+32]
	and	eax, 7
	mov	ecx, edx
	shr	ecx, 3
	add	edx, 32					; 00000020H

; 837  :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	shl	ecx, 5
	add	eax, ecx
	add	rax, r14

; 790  :   __m128d ymm_hi = _mm256_extractf128_pd(ymm_as_m256d, 1);

	vextractf128 xmm0, ymm1, 1

; 791  :   _mm_storel_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovlpd	QWORD PTR [rax], xmm1

; 792  :   _mm_storeh_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovhpd	QWORD PTR [rax+32], xmm1

; 793  :   _mm_storel_pd((double*)dst, ymm_hi); dst += dst_stride;

	vmovlpd	QWORD PTR [rax+64], xmm0

; 794  :   _mm_storeh_pd((double*)dst, ymm_hi);

	vmovhpd	QWORD PTR [rax+96], xmm0

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	cmp	edx, r8d
	jb	SHORT $LL1580@bipred_ave
$LN3594@bipred_ave:

; 1456 :       bipred_average_px_px_avx2(lcu->rec.v + pb_offset, px_L0->v, px_L1->v, pb_w, pb_h, LCU_WIDTH_C);

	mov	r12d, 32				; 00000020H
	jmp	$LN3618@bipred_ave
$LN1596@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r8d, r8d
	je	SHORT $LN3594@bipred_ave
	lea	ecx, DWORD PTR [r8-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	edi, DWORD PTR [rdx+1]
$LL1585@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL1585@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	lea	r12d, QWORD PTR [rdi+32]
	jmp	$LN3618@bipred_ave
$LN1603@bipred_ave:

; 868  :     // 8x2, 8x6, 6x8 blocks (and maybe 2x8 in the future)
; 869  :     switch (pu_w) {
; 870  :       __m128i sample_L0, sample_L1, avg;
; 871  :       case 8: // 8x2, 8x6
; 872  :         for (int i = 0; i < pu_w * pu_h; i += 16) {

	mov	edx, ebx
	test	r8d, r8d
	je	SHORT $LN3594@bipred_ave
	sub	rdi, rsi
	npad	9
$LL1592@bipred_ave:

; 873  : 
; 874  :           int y = i / pu_w;
; 875  : 
; 876  :           sample_L0 = _mm_loadu_si128((__m128i*)&px_L0[i]);
; 877  :           sample_L1 = _mm_loadu_si128((__m128i*)&px_L1[i]);
; 878  :           avg       = _mm_avg_epu8(sample_L0, sample_L1);

	vmovdqu	xmm0, XMMWORD PTR [rdi+rsi]
	vpavgb	xmm1, xmm0, XMMWORD PTR [rsi]
	mov	eax, edx
	lea	rsi, QWORD PTR [rsi+16]
	shr	eax, 3
	add	edx, 16

; 879  :           _mm_storel_epi64((__m128i*)&dst[y * dst_stride], avg);

	shl	eax, 5
	vmovq	QWORD PTR [rax+r14], xmm1

; 880  :           _mm_storeh_pd((double*)&dst[(y + 1) * dst_stride], _mm_castsi128_pd(avg));

	add	eax, 32					; 00000020H
	vmovhpd	QWORD PTR [rax+r14], xmm1
	cmp	edx, r8d
	jb	SHORT $LL1592@bipred_ave

; 921  :       case  8: bipred_average_px_px_template_avx2(dst, px_L0, px_L1,  8, pu_h, dst_stride); break;

	mov	r12d, 32				; 00000020H
	jmp	$LN3618@bipred_ave
$LN1515@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	popcnt	eax, r10d
	cmp	eax, 1
	sete	cl

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	imul	eax, r15d, -16				; fffffff0H
	test	al, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN1551@bipred_ave
	test	cl, cl
	je	SHORT $LN1648@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	mov	r8d, r15d
	mov	edx, ebx
	shl	r8d, 4
	test	r8d, r8d
	je	$LN3609@bipred_ave
	sub	rsi, rdi
	npad	9
$LL1632@bipred_ave:

; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rdi+rsi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rdi]
	mov	ecx, edx
	lea	rdi, QWORD PTR [rdi+32]
	shr	ecx, 4
	mov	eax, edx

; 838  :         case 16: scatter_ymm_16x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	shl	ecx, 5
	and	eax, 15
	add	ecx, eax
	add	edx, 32					; 00000020H

; 801  :   _mm_storeu_si128((__m128i *)dst, ymm_lo); dst += dst_stride;

	vmovdqu	XMMWORD PTR [rcx+r14], xmm1
	vextracti128 XMMWORD PTR [rcx+r14+32], ymm1, 1

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	cmp	edx, r8d
	jb	SHORT $LL1632@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LN3609@bipred_ave
$LN1648@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	mov	ecx, r15d
	shl	ecx, 4
	test	ecx, ecx
	je	$LN3609@bipred_ave
	dec	ecx
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	edi, DWORD PTR [rdx+1]
	npad	12
$LL1637@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL1637@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LN3609@bipred_ave
$LN1516@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	r12d, 32				; 00000020H
	popcnt	eax, r12d
	cmp	eax, 1
	sete	al

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	test	al, al
	je	SHORT $LN3482@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	mov	r8d, r15d
	mov	edx, ebx
	shl	r8d, 5
	test	r8d, r8d
	je	$LN3611@bipred_ave
	sub	rsi, rdi
	npad	14
$LL1684@bipred_ave:

; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rdi+rsi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rdi]

; 839  :         case 32: // Same as case 64
; 840  :         case 64: _mm256_storeu_si256((__m256i *)&dst[y * dst_stride + x], avg); break;

	mov	eax, edx
	lea	rdi, QWORD PTR [rdi+32]
	and	eax, -32				; ffffffe0H
	mov	ecx, edx
	and	ecx, 31
	add	edx, r12d
	add	eax, ecx
	vmovdqu	YMMWORD PTR [rax+r14], ymm1
	cmp	edx, r8d
	jb	SHORT $LL1684@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LN3611@bipred_ave
$LN3482@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	mov	ecx, r15d
	shl	ecx, 5
	test	ecx, ecx
	je	$LN3611@bipred_ave
	dec	ecx
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	edi, DWORD PTR [rdx+1]
	npad	7
$LL1689@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL1689@bipred_ave

; 923  :       case 32: bipred_average_px_px_template_avx2(dst, px_L0, px_L1, 32, pu_h, dst_stride); break;

	jmp	$LN3611@bipred_ave
$LN1517@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	r9d, 64					; 00000040H
	popcnt	eax, r9d
	cmp	eax, 1
	sete	al

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	test	al, al
	je	SHORT $LN3488@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	mov	r8d, r15d
	mov	edx, ebx
	shl	r8d, 6
	test	r8d, r8d
	je	SHORT $LN3599@bipred_ave
	sub	rsi, rdi
	npad	2
$LL1736@bipred_ave:

; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rdi+rsi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rdi]
	mov	ecx, edx
	lea	rdi, QWORD PTR [rdi+32]
	shr	ecx, 6
	mov	eax, edx

; 839  :         case 32: // Same as case 64
; 840  :         case 64: _mm256_storeu_si256((__m256i *)&dst[y * dst_stride + x], avg); break;

	shl	ecx, 5
	and	eax, 63					; 0000003fH
	add	ecx, eax
	add	edx, 32					; 00000020H
	vmovdqu	YMMWORD PTR [rcx+r14], ymm1
	cmp	edx, r8d
	jb	SHORT $LL1736@bipred_ave
$LN3599@bipred_ave:

; 1456 :       bipred_average_px_px_avx2(lcu->rec.v + pb_offset, px_L0->v, px_L1->v, pb_w, pb_h, LCU_WIDTH_C);

	mov	r12d, 32				; 00000020H
	lea	r13d, QWORD PTR [r12-24]
	jmp	$LN3619@bipred_ave
$LN3488@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	mov	ecx, r15d
	shl	ecx, 6
	test	ecx, ecx
	je	SHORT $LN3599@bipred_ave
	dec	ecx
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	edi, DWORD PTR [rdx+1]
$LL1741@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL1741@bipred_ave

; 924  :       case 64: bipred_average_px_px_template_avx2(dst, px_L0, px_L1, 64, pu_h, dst_stride); break;

	jmp	$LN3609@bipred_ave
$LN1518@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	r11d, 6
	popcnt	eax, r11d
	cmp	eax, 1

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	eax, DWORD PTR [r15+r15*2]
	lea	r9d, DWORD PTR [rax+rax]
	sete	cl
	test	r9b, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	SHORT $LN1811@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r9d, r9d
	je	$LN3601@bipred_ave

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	test	cl, cl
	je	SHORT $LN1804@bipred_ave
	lea	edi, DWORD PTR [r9-1]
	shr	edi, 5
	inc	edi
$LL1788@bipred_ave:

; 841  :         default:
; 842  :           assert(0 && "Unexpected block width.");

	mov	r8d, 842				; 0000034aH
	lea	rdx, OFFSET FLAT:$SG4294951316
	lea	rcx, OFFSET FLAT:$SG4294951315
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL1788@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LN3609@bipred_ave
$LN1804@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	lea	ecx, DWORD PTR [r9-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	edi, DWORD PTR [rdx+1]
	npad	7
$LL1793@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL1793@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LN3609@bipred_ave
$LN1811@bipred_ave:

; 881  :         }
; 882  :         break;
; 883  :       case 6: // 6x8
; 884  :         for (int i = 0; i < pu_w * pu_h; i += 12) {

	mov	r10d, ebx
	test	r9d, r9d
	je	SHORT $LN3601@bipred_ave
	vmovdqu	xmm3, XMMWORD PTR __xmm@00000000ffffffffffffffffffffffff
	npad	8
$LL1803@bipred_ave:

; 885  : 
; 886  :           int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH
	mul	r10d

; 887  : 
; 888  :           __m128i mask      = _mm_setr_epi32(-1, -1, -1, 0);
; 889  :           __m128i sample_L0 = _mm_maskload_epi32((const int*)(&px_L0[i]), mask);

	movsxd	rax, r10d
	add	r10d, 12
	shr	edx, 2

; 890  :           __m128i sample_L1 = _mm_maskload_epi32((const int*)(&px_L1[i]), mask);
; 891  :           __m128i avg       = _mm_avg_epu8(sample_L0, sample_L1);
; 892  : 
; 893  :           uint32_t elements_0123 = _mm_cvtsi128_si32(avg);
; 894  :           uint16_t elements_45   = _mm_extract_epi16(avg, 2);
; 895  :           uint16_t elements_67   = _mm_extract_epi16(avg, 3);
; 896  :           uint32_t elements_89ab = _mm_extract_epi32(avg, 2);
; 897  :           *(uint32_t*)&dst[(y + 0) * dst_stride + 0] = elements_0123;

	shl	edx, 5
	mov	r8d, edx
	vpmaskmovd xmm1, xmm3, XMMWORD PTR [rdi+rax]
	vpmaskmovd xmm0, xmm3, XMMWORD PTR [rsi+rax]
	vpavgb	xmm2, xmm1, xmm0

; 898  :           *(uint16_t*)&dst[(y + 0) * dst_stride + 4] = elements_45;

	lea	eax, DWORD PTR [r8+4]
	vmovd	DWORD PTR [rdx+r14], xmm2
	vpextrw	WORD PTR [rax+r14], xmm2, 2

; 899  :           *(uint16_t*)&dst[(y + 1) * dst_stride + 0] = elements_67;

	lea	eax, DWORD PTR [r8+32]
	vpextrw	WORD PTR [rax+r14], xmm2, 3

; 900  :           *(uint32_t*)&dst[(y + 1) * dst_stride + 2] = elements_89ab;

	lea	eax, DWORD PTR [r8+34]
	vpextrd	DWORD PTR [rax+r14], xmm2, 2
	cmp	r10d, r9d
	jb	SHORT $LL1803@bipred_ave
$LN3601@bipred_ave:

; 1456 :       bipred_average_px_px_avx2(lcu->rec.v + pb_offset, px_L0->v, px_L1->v, pb_w, pb_h, LCU_WIDTH_C);

	mov	r12d, 32				; 00000020H
	lea	r9d, QWORD PTR [r12+32]
	lea	r13d, QWORD PTR [r12-24]
	jmp	$LN3620@bipred_ave
$LN1519@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	popcnt	eax, r11d
	cmp	eax, 1

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	eax, DWORD PTR [r15+r15*2]
	lea	r9d, DWORD PTR [rax*4]
	sete	cl
	test	r9b, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN1551@bipred_ave
	test	cl, cl
	je	SHORT $LN1856@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r9d, r9d
	je	$LN3609@bipred_ave
	lea	edi, DWORD PTR [r9-1]
	shr	edi, 5
	inc	edi
$LL1840@bipred_ave:

; 841  :         default:
; 842  :           assert(0 && "Unexpected block width.");

	mov	r8d, 842				; 0000034aH
	lea	rdx, OFFSET FLAT:$SG4294951316
	lea	rcx, OFFSET FLAT:$SG4294951315
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL1840@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LN3609@bipred_ave
$LN1856@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	mov	r8d, ebx
	test	r9d, r9d
	je	$LN3609@bipred_ave
	vmovdqu	ymm3, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	ymm4, YMMWORD PTR __ymm@0000000000000000000000000000000000000000ffffffffffffffffffffffff
	vmovdqu	ymm5, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffff000000000000000000000000
	npad	4
$LL1845@bipred_ave:

; 848  : 
; 849  :       int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH

; 850  :       int x = i % pu_w;

	mov	ecx, r8d
	mul	r8d
	shr	edx, 3
	lea	eax, DWORD PTR [rdx+rdx*2]

; 851  : 
; 852  :       // Last 64 bits of the 256 are not used to simplify the loop
; 853  :       __m256i mask      = _mm256_setr_epi64x(-1, -1, -1, 0);
; 854  :       __m256i sample_L0 = _mm256_maskload_epi64((const long long*)&px_L0[i], mask);
; 855  :       __m256i sample_L1 = _mm256_maskload_epi64((const long long*)&px_L1[i], mask);
; 856  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);
; 857  : 
; 858  :       switch (pu_w) {
; 859  :         case 12: scatter_ymm_12x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	shl	edx, 5
	shl	eax, 2
	sub	ecx, eax
	movsxd	rax, r8d
	add	edx, ecx
	add	r8d, 24
	vpmaskmovq ymm1, ymm3, YMMWORD PTR [rsi+rax]
	vpmaskmovq ymm0, ymm3, YMMWORD PTR [rdi+rax]
	vpavgb	ymm2, ymm0, ymm1

; 809  :   _mm256_maskstore_epi32((int32_t*)dst, mask_a, ymm); dst += dst_stride - 3 * 4;

	vpmaskmovd YMMWORD PTR [rdx+r14], ymm4, ymm2

; 810  :   _mm256_maskstore_epi32((int32_t*)dst, mask_b, ymm);

	vpmaskmovd YMMWORD PTR [rdx+r14+20], ymm5, ymm2

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	cmp	r8d, r9d
	jb	SHORT $LL1845@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LN3609@bipred_ave
$LN1520@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	popcnt	eax, r8d
	cmp	eax, 1
	sete	cl

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	imul	eax, r15d, -8				; fffffff8H
	test	al, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN1551@bipred_ave
	lea	eax, DWORD PTR [r15+r15*2]
	test	cl, cl
	je	SHORT $LN1908@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	lea	eax, DWORD PTR [rax*8]
	test	eax, eax
	je	$LN3609@bipred_ave
	lea	edi, DWORD PTR [rax-1]
	shr	edi, 5
	inc	edi
	npad	1
$LL1892@bipred_ave:

; 841  :         default:
; 842  :           assert(0 && "Unexpected block width.");

	mov	r8d, 842				; 0000034aH
	lea	rdx, OFFSET FLAT:$SG4294951316
	lea	rcx, OFFSET FLAT:$SG4294951315
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL1892@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LN3609@bipred_ave
$LN1908@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	lea	r9d, DWORD PTR [rax*8]
	mov	r8d, ebx
	test	r9d, r9d
	je	$LN3609@bipred_ave
	vmovdqu	ymm3, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	npad	12
$LL1897@bipred_ave:

; 848  : 
; 849  :       int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH

; 850  :       int x = i % pu_w;

	mov	ecx, r8d
	mul	r8d
	shr	edx, 4
	lea	eax, DWORD PTR [rdx+rdx*2]

; 860  :         case 24: // Same as case 48
; 861  :         case 48: _mm256_maskstore_epi64((long long*)&dst[y * dst_stride + x], mask, avg); break;

	shl	edx, 5
	shl	eax, 3
	sub	ecx, eax
	movsxd	rax, r8d
	add	edx, ecx
	add	r8d, 24
	vpmaskmovq ymm1, ymm3, YMMWORD PTR [rsi+rax]
	vpmaskmovq ymm0, ymm3, YMMWORD PTR [rdi+rax]
	vpavgb	ymm2, ymm0, ymm1
	vpmaskmovq YMMWORD PTR [rdx+r14], ymm3, ymm2
	cmp	r8d, r9d
	jb	SHORT $LL1897@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LN3609@bipred_ave
$LN1521@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	r10d, 48				; 00000030H
	popcnt	eax, r10d
	cmp	eax, 1
	sete	cl

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	imul	eax, r15d, -16				; fffffff0H
	test	al, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN1551@bipred_ave
	test	cl, cl
	je	SHORT $LN1960@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	lea	eax, DWORD PTR [r15+r15*2]
	shl	eax, 4
	test	eax, eax
	je	$LN3607@bipred_ave
	lea	edi, DWORD PTR [rax-1]
	shr	edi, 5
	inc	edi
	npad	6
$LL1944@bipred_ave:

; 841  :         default:
; 842  :           assert(0 && "Unexpected block width.");

	mov	r8d, 842				; 0000034aH
	lea	rdx, OFFSET FLAT:$SG4294951316
	lea	rcx, OFFSET FLAT:$SG4294951315
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rdi, 1
	jne	SHORT $LL1944@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LN3609@bipred_ave
$LN1960@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	lea	r9d, DWORD PTR [r15+r15*2]
	mov	r8d, ebx
	shl	r9d, 4
	test	r9d, r9d
	je	SHORT $LN3607@bipred_ave
	vmovdqu	ymm3, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
$LL1949@bipred_ave:

; 848  : 
; 849  :       int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH

; 850  :       int x = i % pu_w;

	mov	ecx, r8d
	mul	r8d
	shr	edx, 5
	lea	eax, DWORD PTR [rdx+rdx*2]

; 860  :         case 24: // Same as case 48
; 861  :         case 48: _mm256_maskstore_epi64((long long*)&dst[y * dst_stride + x], mask, avg); break;

	shl	edx, 5
	shl	eax, 4
	sub	ecx, eax
	movsxd	rax, r8d
	add	edx, ecx
	add	r8d, 24
	vpmaskmovq ymm1, ymm3, YMMWORD PTR [rax+rsi]
	vpmaskmovq ymm0, ymm3, YMMWORD PTR [rax+rdi]
	vpavgb	ymm2, ymm0, ymm1
	vpmaskmovq YMMWORD PTR [rdx+r14], ymm3, ymm2
	cmp	r8d, r9d
	jb	SHORT $LL1949@bipred_ave
$LN3607@bipred_ave:

; 1456 :       bipred_average_px_px_avx2(lcu->rec.v + pb_offset, px_L0->v, px_L1->v, pb_w, pb_h, LCU_WIDTH_C);

	mov	r12d, 32				; 00000020H
	lea	r9d, QWORD PTR [r12+32]
	lea	r13d, QWORD PTR [r12-24]
	lea	r11d, QWORD PTR [r12-26]
	jmp	$LN1508@bipred_ave
$LN1522@bipred_ave:

; 931  :         assert(0 && "Unexpected block width.");

	mov	r8d, 931				; 000003a3H
	lea	rdx, OFFSET FLAT:$SG4294951310
	lea	rcx, OFFSET FLAT:$SG4294951309
	vzeroupper
	call	QWORD PTR __imp__wassert

; 932  :         break;
; 933  :     }
; 934  :   } else {

	jmp	SHORT $LN3609@bipred_ave
$LN1510@bipred_ave:

; 938  :     for (int i = 0; i < pu_w * pu_h; ++i)

	mov	r11d, r15d
	mov	r10d, ebx
	imul	r11d, r9d
	test	r11d, r11d
	je	SHORT $LN3609@bipred_ave
	sub	rdi, rsi
	mov	ebx, r9d
	npad	5
$LL1509@bipred_ave:

; 939  :     {
; 940  :       int y = i / pu_w;
; 941  :       int x = i % pu_w;
; 942  :       int16_t sample_L0 = px_L0[i] << (14 - KVZ_BIT_DEPTH);
; 943  :       int16_t sample_L1 = px_L1[i] << (14 - KVZ_BIT_DEPTH);
; 944  :       int32_t rounded = (sample_L0 + sample_L1 + offset) >> shift;

	movzx	r8d, BYTE PTR [rsi]
	xor	edx, edx
	movzx	ecx, BYTE PTR [rsi+rdi]
	mov	eax, r10d
	shl	r8d, 6
	div	ebx
	add	r8d, 64					; 00000040H
	shl	ecx, 6
	add	r8d, ecx
	mov	r9d, eax
	shr	r8d, 7
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\picture-generic.c

; 69   :   if (value & ~PIXEL_MAX) {

	test	r8d, -256				; ffffff00H
	je	SHORT $LN1994@bipred_ave

; 70   :     int32_t temp = (-value) >> 31;

	neg	r8d
	sar	r8d, 31
$LN1994@bipred_ave:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 945  :       dst[y * dst_stride + x] = kvz_fast_clip_32bit_to_pixel(rounded);

	shl	r9d, 5
	inc	r10d
	add	r9d, edx
	inc	rsi
	mov	BYTE PTR [r9+r14], r8b
	cmp	r10d, r11d
	jb	SHORT $LL1509@bipred_ave
	xor	ebx, ebx
$LN3609@bipred_ave:

; 1456 :       bipred_average_px_px_avx2(lcu->rec.v + pb_offset, px_L0->v, px_L1->v, pb_w, pb_h, LCU_WIDTH_C);

	mov	r12d, 32				; 00000020H
$LN3611@bipred_ave:
	mov	r13d, 8
$LN3618@bipred_ave:
	mov	r9d, 64					; 00000040H
$LN3619@bipred_ave:
	mov	r11d, 6
$LN3620@bipred_ave:
	mov	r10d, 48				; 00000030H
$LN1508@bipred_ave:
	mov	rax, QWORD PTR px_L1$[rsp]
	mov	r14, QWORD PTR tv23567[rsp]

; 918  :   if (!(pu_w == 4 && pu_h == 4) && pu_w > 2) {

	mov	r8d, DWORD PTR pu_w$[rsp]

; 1456 :       bipred_average_px_px_avx2(lcu->rec.v + pb_offset, px_L0->v, px_L1->v, pb_w, pb_h, LCU_WIDTH_C);

	add	r14, 11660				; 00002d8cH
	mov	rdi, QWORD PTR [rax+24]
	mov	rax, QWORD PTR px_L0$[rsp]
	mov	rsi, QWORD PTR [rax+24]

; 918  :   if (!(pu_w == 4 && pu_h == 4) && pu_w > 2) {

	cmp	r8d, 4
	jne	SHORT $LN2005@bipred_ave
	cmp	r15d, r8d
	je	$LN2003@bipred_ave
	lea	eax, DWORD PTR [r8-4]
	jmp	SHORT $LN3562@bipred_ave
$LN2005@bipred_ave:
	cmp	r8d, 2
	jbe	$LN2003@bipred_ave

; 919  :     switch (pu_w) {

	lea	eax, DWORD PTR [r8-4]
	cmp	eax, 60					; 0000003cH
	ja	$LN2015@bipred_ave
$LN3562@bipred_ave:
	lea	rdx, OFFSET FLAT:__ImageBase
	movzx	eax, BYTE PTR $LN3565@bipred_ave[rdx+rax]
	mov	ecx, DWORD PTR $LN3566@bipred_ave[rdx+rax*4]
	add	rcx, rdx
	jmp	rcx
$LN2006@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 4

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	r12d, DWORD PTR [r15*4]
	popcnt	eax, eax
	cmp	eax, 1
	mov	r13d, r12d
	sete	al
	and	r13d, 31
	mov	BYTE PTR tv24872[rsp], al

; 822  :   assert(!(pu_w == 4 && pu_h == 4) && "Branch for 4x4 not yet implemented.");

	cmp	r15d, 4
	jne	SHORT $LN2058@bipred_ave
	mov	r8d, 822				; 00000336H
	lea	rdx, OFFSET FLAT:$SG4294951321
	lea	rcx, OFFSET FLAT:$SG4294951320
	vzeroupper
	call	QWORD PTR __imp__wassert
	movzx	eax, BYTE PTR tv24872[rsp]
$LN2058@bipred_ave:

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	test	r13d, r13d
	jne	$LN2460@bipred_ave
	test	al, al
	je	$LN2037@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r12d, r12d
	je	$LL11@bipred_ave
	sub	rsi, rdi
	npad	6
$LL2021@bipred_ave:

; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rsi+rdi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rdi]

; 775  :   __m128i ymm_hi = _mm256_extracti128_si256(ymm, 1);

	vextracti128 xmm0, ymm1, 1

; 828  :       int y = i / pu_w;

	mov	edx, ebx
	lea	rdi, QWORD PTR [rdi+32]
	shr	edx, 2

; 829  :       int x = i % pu_w;

	mov	eax, ebx

; 834  : 
; 835  :       switch (pu_w) {
; 836  :         case  4: scatter_ymm_4x8_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	shl	edx, 5
	and	eax, 3
	add	edx, eax
	add	ebx, 32					; 00000020H
	add	rdx, r14

; 776  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_lo); dst += dst_stride;

	vmovd	DWORD PTR [rdx], xmm1

; 777  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 1); dst += dst_stride;

	vpextrd	DWORD PTR [rdx+32], xmm1, 1

; 778  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 2); dst += dst_stride;

	vpextrd	DWORD PTR [rdx+64], xmm1, 2

; 779  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_lo, 3); dst += dst_stride;

	vpextrd	DWORD PTR [rdx+96], xmm1, 3

; 780  :   *(uint32_t *)dst = _mm_cvtsi128_si32(ymm_hi); dst += dst_stride;

	vmovd	DWORD PTR [rdx+128], xmm0

; 781  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 1); dst += dst_stride;

	vpextrd	DWORD PTR [rdx+160], xmm0, 1

; 782  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 2); dst += dst_stride;

	vpextrd	DWORD PTR [rdx+192], xmm0, 2

; 783  :   *(uint32_t *)dst = _mm_extract_epi32(ymm_hi, 3);

	vpextrd	DWORD PTR [rdx+224], xmm0, 3

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	cmp	ebx, r12d
	jb	SHORT $LL2021@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LL11@bipred_ave
$LN2037@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r12d, r12d
	je	$LL11@bipred_ave
	npad	6
$LL2026@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	vzeroupper
	call	QWORD PTR __imp__wassert
	add	ebx, 24
	cmp	ebx, r12d
	jb	SHORT $LL2026@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LL11@bipred_ave
$LN2007@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	popcnt	eax, r13d
	cmp	eax, 1

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	r15d, DWORD PTR [r15*8]
	sete	al
	test	r15b, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN2096@bipred_ave
	test	al, al
	je	SHORT $LN2089@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r15d, r15d
	je	$LL11@bipred_ave
	sub	rsi, rdi
	npad	9
$LL2073@bipred_ave:

; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rdi+rsi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rdi]
	mov	eax, ebx
	lea	rdi, QWORD PTR [rdi+32]
	and	eax, 7
	mov	ecx, ebx
	shr	ecx, 3
	add	ebx, 32					; 00000020H

; 837  :         case  8: scatter_ymm_8x4_8bit( &dst[y * dst_stride + x], avg, dst_stride); break;

	shl	ecx, 5
	add	eax, ecx
	add	rax, r14

; 790  :   __m128d ymm_hi = _mm256_extractf128_pd(ymm_as_m256d, 1);

	vextractf128 xmm0, ymm1, 1

; 791  :   _mm_storel_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovlpd	QWORD PTR [rax], xmm1

; 792  :   _mm_storeh_pd((double*)dst, ymm_lo); dst += dst_stride;

	vmovhpd	QWORD PTR [rax+32], xmm1

; 793  :   _mm_storel_pd((double*)dst, ymm_hi); dst += dst_stride;

	vmovlpd	QWORD PTR [rax+64], xmm0

; 794  :   _mm_storeh_pd((double*)dst, ymm_hi);

	vmovhpd	QWORD PTR [rax+96], xmm0

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	cmp	ebx, r15d
	jb	SHORT $LL2073@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LL11@bipred_ave
$LN2089@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r15d, r15d
	je	$LL11@bipred_ave
	npad	2
$LL2078@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	vzeroupper
	call	QWORD PTR __imp__wassert
	add	ebx, 24
	cmp	ebx, r15d
	jb	SHORT $LL2078@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LL11@bipred_ave
$LN2096@bipred_ave:

; 868  :     // 8x2, 8x6, 6x8 blocks (and maybe 2x8 in the future)
; 869  :     switch (pu_w) {
; 870  :       __m128i sample_L0, sample_L1, avg;
; 871  :       case 8: // 8x2, 8x6
; 872  :         for (int i = 0; i < pu_w * pu_h; i += 16) {

	test	r15d, r15d
	je	$LL11@bipred_ave
	sub	rsi, rdi
	npad	10
$LL2085@bipred_ave:

; 873  : 
; 874  :           int y = i / pu_w;
; 875  : 
; 876  :           sample_L0 = _mm_loadu_si128((__m128i*)&px_L0[i]);
; 877  :           sample_L1 = _mm_loadu_si128((__m128i*)&px_L1[i]);
; 878  :           avg       = _mm_avg_epu8(sample_L0, sample_L1);

	vmovdqu	xmm0, XMMWORD PTR [rdi+rsi]
	vpavgb	xmm1, xmm0, XMMWORD PTR [rdi]
	mov	eax, ebx
	lea	rdi, QWORD PTR [rdi+16]
	shr	eax, 3
	add	ebx, 16

; 879  :           _mm_storel_epi64((__m128i*)&dst[y * dst_stride], avg);

	shl	eax, 5
	vmovq	QWORD PTR [rax+r14], xmm1

; 880  :           _mm_storeh_pd((double*)&dst[(y + 1) * dst_stride], _mm_castsi128_pd(avg));

	add	eax, 32					; 00000020H
	vmovhpd	QWORD PTR [rax+r14], xmm1
	cmp	ebx, r15d
	jb	SHORT $LL2085@bipred_ave

; 921  :       case  8: bipred_average_px_px_template_avx2(dst, px_L0, px_L1,  8, pu_h, dst_stride); break;

	jmp	$LL11@bipred_ave
$LN2008@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 16
	popcnt	eax, eax
	cmp	eax, 1
	sete	cl

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	imul	eax, r15d, -16				; fffffff0H
	test	al, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN2460@bipred_ave
	shl	r15d, 4
	test	cl, cl
	je	SHORT $LN2141@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r15d, r15d
	je	$LL11@bipred_ave
	sub	rsi, rdi
$LL2125@bipred_ave:

; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rdi+rsi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rdi]
	mov	ecx, ebx
	lea	rdi, QWORD PTR [rdi+32]
	shr	ecx, 4
	mov	eax, ebx

; 838  :         case 16: scatter_ymm_16x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	shl	ecx, 5
	and	eax, 15
	add	ecx, eax
	add	ebx, 32					; 00000020H

; 801  :   _mm_storeu_si128((__m128i *)dst, ymm_lo); dst += dst_stride;

	vmovdqu	XMMWORD PTR [rcx+r14], xmm1
	vextracti128 XMMWORD PTR [rcx+r14+32], ymm1, 1

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	cmp	ebx, r15d
	jb	SHORT $LL2125@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LL11@bipred_ave
$LN2141@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r15d, r15d
	je	$LL11@bipred_ave
	lea	ecx, DWORD PTR [r15-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
$LL2130@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL2130@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LL11@bipred_ave
$LN2009@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	popcnt	eax, r12d
	cmp	eax, 1
	sete	al

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	shl	r15d, 5
	test	al, al
	je	SHORT $LN3527@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r15d, r15d
	je	$LL11@bipred_ave
	sub	rsi, rdi
	npad	8
$LL2177@bipred_ave:

; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rsi+rdi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rdi]

; 839  :         case 32: // Same as case 64
; 840  :         case 64: _mm256_storeu_si256((__m256i *)&dst[y * dst_stride + x], avg); break;

	mov	eax, ebx
	lea	rdi, QWORD PTR [rdi+32]
	and	eax, -32				; ffffffe0H
	mov	ecx, ebx
	and	ecx, 31
	add	ebx, 32					; 00000020H
	add	eax, ecx
	vmovdqu	YMMWORD PTR [rax+r14], ymm1
	cmp	ebx, r15d
	jb	SHORT $LL2177@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LL11@bipred_ave
$LN3527@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r15d, r15d
	je	$LL11@bipred_ave
	lea	ecx, DWORD PTR [r15-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
	npad	10
$LL2182@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL2182@bipred_ave

; 923  :       case 32: bipred_average_px_px_template_avx2(dst, px_L0, px_L1, 32, pu_h, dst_stride); break;

	jmp	$LL11@bipred_ave
$LN2010@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	popcnt	eax, r9d
	cmp	eax, 1
	sete	al

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	shl	r15d, 6
	test	al, al
	je	SHORT $LN3533@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r15d, r15d
	je	$LL11@bipred_ave
	sub	rsi, rdi
	npad	9
$LL2229@bipred_ave:

; 830  : 
; 831  :       __m256i sample_L0 = _mm256_loadu_si256((__m256i*)&px_L0[i]);
; 832  :       __m256i sample_L1 = _mm256_loadu_si256((__m256i*)&px_L1[i]);
; 833  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);

	vmovdqu	ymm0, YMMWORD PTR [rsi+rdi]
	vpavgb	ymm1, ymm0, YMMWORD PTR [rdi]
	mov	ecx, ebx
	lea	rdi, QWORD PTR [rdi+32]
	shr	ecx, 6
	mov	eax, ebx

; 839  :         case 32: // Same as case 64
; 840  :         case 64: _mm256_storeu_si256((__m256i *)&dst[y * dst_stride + x], avg); break;

	shl	ecx, 5
	and	eax, 63					; 0000003fH
	add	ecx, eax
	add	ebx, 32					; 00000020H
	vmovdqu	YMMWORD PTR [rcx+r14], ymm1
	cmp	ebx, r15d
	jb	SHORT $LL2229@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LL11@bipred_ave
$LN3533@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r15d, r15d
	je	$LL11@bipred_ave
	lea	ecx, DWORD PTR [r15-1]
	mov	eax, -1431655765			; aaaaaaabH
	mul	ecx
	shr	edx, 4
	lea	ebx, DWORD PTR [rdx+1]
	npad	7
$LL2234@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL2234@bipred_ave

; 924  :       case 64: bipred_average_px_px_template_avx2(dst, px_L0, px_L1, 64, pu_h, dst_stride); break;

	jmp	$LL11@bipred_ave
$LN2011@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	popcnt	eax, r11d
	cmp	eax, 1

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	eax, DWORD PTR [r15+r15*2]
	lea	r15d, DWORD PTR [rax+rax]
	sete	cl
	test	r15b, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	SHORT $LN2304@bipred_ave
	test	cl, cl
	je	SHORT $LN2297@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r15d, r15d
	je	$LL11@bipred_ave
	lea	ebx, DWORD PTR [r15-1]
	shr	ebx, 5
	inc	ebx
	npad	9
$LL2281@bipred_ave:

; 841  :         default:
; 842  :           assert(0 && "Unexpected block width.");

	mov	r8d, 842				; 0000034aH
	lea	rdx, OFFSET FLAT:$SG4294951316
	lea	rcx, OFFSET FLAT:$SG4294951315
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL2281@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LL11@bipred_ave
$LN2297@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r15d, r15d
	je	$LL11@bipred_ave
$LL2286@bipred_ave:

; 862  :         default:
; 863  :           assert(0 && "Unexpected block width.");

	mov	r8d, 863				; 0000035fH
	lea	rdx, OFFSET FLAT:$SG4294951314
	lea	rcx, OFFSET FLAT:$SG4294951313
	vzeroupper
	call	QWORD PTR __imp__wassert
	add	ebx, 24
	cmp	ebx, r15d
	jb	SHORT $LL2286@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LL11@bipred_ave
$LN2304@bipred_ave:

; 881  :         }
; 882  :         break;
; 883  :       case 6: // 6x8
; 884  :         for (int i = 0; i < pu_w * pu_h; i += 12) {

	test	r15d, r15d
	je	$LL11@bipred_ave
	vmovdqu	xmm3, XMMWORD PTR __xmm@00000000ffffffffffffffffffffffff
	npad	4
$LL2296@bipred_ave:

; 885  : 
; 886  :           int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH
	mul	ebx

; 887  : 
; 888  :           __m128i mask      = _mm_setr_epi32(-1, -1, -1, 0);
; 889  :           __m128i sample_L0 = _mm_maskload_epi32((const int*)(&px_L0[i]), mask);

	movsxd	rax, ebx
	add	ebx, 12
	shr	edx, 2

; 890  :           __m128i sample_L1 = _mm_maskload_epi32((const int*)(&px_L1[i]), mask);
; 891  :           __m128i avg       = _mm_avg_epu8(sample_L0, sample_L1);
; 892  : 
; 893  :           uint32_t elements_0123 = _mm_cvtsi128_si32(avg);
; 894  :           uint16_t elements_45   = _mm_extract_epi16(avg, 2);
; 895  :           uint16_t elements_67   = _mm_extract_epi16(avg, 3);
; 896  :           uint32_t elements_89ab = _mm_extract_epi32(avg, 2);
; 897  :           *(uint32_t*)&dst[(y + 0) * dst_stride + 0] = elements_0123;

	shl	edx, 5
	mov	r8d, edx
	vpmaskmovd xmm1, xmm3, XMMWORD PTR [rsi+rax]
	vpmaskmovd xmm0, xmm3, XMMWORD PTR [rdi+rax]
	vpavgb	xmm2, xmm1, xmm0

; 898  :           *(uint16_t*)&dst[(y + 0) * dst_stride + 4] = elements_45;

	lea	eax, DWORD PTR [r8+4]
	vmovd	DWORD PTR [rdx+r14], xmm2
	vpextrw	WORD PTR [rax+r14], xmm2, 2

; 899  :           *(uint16_t*)&dst[(y + 1) * dst_stride + 0] = elements_67;

	lea	eax, DWORD PTR [r8+32]
	vpextrw	WORD PTR [rax+r14], xmm2, 3

; 900  :           *(uint32_t*)&dst[(y + 1) * dst_stride + 2] = elements_89ab;

	lea	eax, DWORD PTR [r8+34]
	vpextrd	DWORD PTR [rax+r14], xmm2, 2
	cmp	ebx, r15d
	jb	SHORT $LL2296@bipred_ave

; 926  :       case  6: bipred_average_px_px_template_avx2(dst, px_L0, px_L1,  6, pu_h, dst_stride); break;

	jmp	$LL11@bipred_ave
$LN2012@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 12
	popcnt	eax, eax
	cmp	eax, 1

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	lea	eax, DWORD PTR [r15+r15*2]
	lea	r8d, DWORD PTR [rax*4]
	sete	cl
	test	r8b, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN2460@bipred_ave
	test	cl, cl
	je	SHORT $LN2349@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	test	r8d, r8d
	je	$LL11@bipred_ave
	lea	ebx, DWORD PTR [r8-1]
	shr	ebx, 5
	inc	ebx
$LL2333@bipred_ave:

; 841  :         default:
; 842  :           assert(0 && "Unexpected block width.");

	mov	r8d, 842				; 0000034aH
	lea	rdx, OFFSET FLAT:$SG4294951316
	lea	rcx, OFFSET FLAT:$SG4294951315
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL2333@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LL11@bipred_ave
$LN2349@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	test	r8d, r8d
	je	$LL11@bipred_ave
	vmovdqu	ymm3, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	vmovdqu	ymm4, YMMWORD PTR __ymm@0000000000000000000000000000000000000000ffffffffffffffffffffffff
	vmovdqu	ymm5, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffff000000000000000000000000
	npad	5
$LL2338@bipred_ave:

; 848  : 
; 849  :       int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH

; 850  :       int x = i % pu_w;

	mov	ecx, ebx
	mul	ebx
	shr	edx, 3
	lea	eax, DWORD PTR [rdx+rdx*2]

; 851  : 
; 852  :       // Last 64 bits of the 256 are not used to simplify the loop
; 853  :       __m256i mask      = _mm256_setr_epi64x(-1, -1, -1, 0);
; 854  :       __m256i sample_L0 = _mm256_maskload_epi64((const long long*)&px_L0[i], mask);
; 855  :       __m256i sample_L1 = _mm256_maskload_epi64((const long long*)&px_L1[i], mask);
; 856  :       __m256i avg       = _mm256_avg_epu8(sample_L0, sample_L1);
; 857  : 
; 858  :       switch (pu_w) {
; 859  :         case 12: scatter_ymm_12x2_8bit(&dst[y * dst_stride + x], avg, dst_stride); break;

	shl	edx, 5
	shl	eax, 2
	sub	ecx, eax
	movsxd	rax, ebx
	add	edx, ecx
	add	ebx, 24
	vpmaskmovq ymm1, ymm3, YMMWORD PTR [rsi+rax]
	vpmaskmovq ymm0, ymm3, YMMWORD PTR [rdi+rax]
	vpavgb	ymm2, ymm0, ymm1

; 809  :   _mm256_maskstore_epi32((int32_t*)dst, mask_a, ymm); dst += dst_stride - 3 * 4;

	vpmaskmovd YMMWORD PTR [rdx+r14], ymm4, ymm2

; 810  :   _mm256_maskstore_epi32((int32_t*)dst, mask_b, ymm);

	vpmaskmovd YMMWORD PTR [rdx+r14+20], ymm5, ymm2

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	cmp	ebx, r8d
	jb	SHORT $LL2338@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LL11@bipred_ave
$LN2013@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	mov	eax, 24
	popcnt	eax, eax
	cmp	eax, 1
	sete	cl

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	imul	eax, r15d, -8				; fffffff8H
	test	al, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN2460@bipred_ave
	test	cl, cl
	je	SHORT $LN2401@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	lea	eax, DWORD PTR [r15+r15*2]
	lea	eax, DWORD PTR [rax*8]
	test	eax, eax
	je	$LL11@bipred_ave
	lea	ebx, DWORD PTR [rax-1]
	shr	ebx, 5
	inc	ebx
$LL2385@bipred_ave:

; 841  :         default:
; 842  :           assert(0 && "Unexpected block width.");

	mov	r8d, 842				; 0000034aH
	lea	rdx, OFFSET FLAT:$SG4294951316
	lea	rcx, OFFSET FLAT:$SG4294951315
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL2385@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LL11@bipred_ave
$LN2401@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	lea	r8d, DWORD PTR [r15+r15*2]
	shl	r8d, 3
	test	r8d, r8d
	je	$LL11@bipred_ave
	vmovdqu	ymm3, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
$LL2390@bipred_ave:

; 848  : 
; 849  :       int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH

; 850  :       int x = i % pu_w;

	mov	ecx, ebx
	mul	ebx
	shr	edx, 4
	lea	eax, DWORD PTR [rdx+rdx*2]

; 860  :         case 24: // Same as case 48
; 861  :         case 48: _mm256_maskstore_epi64((long long*)&dst[y * dst_stride + x], mask, avg); break;

	shl	edx, 5
	shl	eax, 3
	sub	ecx, eax
	movsxd	rax, ebx
	add	edx, ecx
	add	ebx, 24
	vpmaskmovq ymm1, ymm3, YMMWORD PTR [rsi+rax]
	vpmaskmovq ymm0, ymm3, YMMWORD PTR [rdi+rax]
	vpavgb	ymm2, ymm0, ymm1
	vpmaskmovq YMMWORD PTR [rdx+r14], ymm3, ymm2
	cmp	ebx, r8d
	jb	SHORT $LL2390@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LL11@bipred_ave
$LN2014@bipred_ave:

; 820  :   bool has_pow2_width = _mm_popcnt_u32(pu_w) == 1;

	popcnt	eax, r10d
	cmp	eax, 1
	sete	cl

; 821  :   bool area_mod_32 = (pu_w * pu_h) % 32;

	imul	eax, r15d, -16				; fffffff0H
	test	al, 31

; 823  :   assert(!(pu_w == 2 && pu_h == 8) && "Branch for 2x8 not yet implemented.");
; 824  : 
; 825  :   if (has_pow2_width && area_mod_32 == 0) {

	jne	$LN2460@bipred_ave
	test	cl, cl
	je	SHORT $LN2453@bipred_ave

; 826  :     for (int i = 0; i < pu_w * pu_h; i += 32) {

	lea	eax, DWORD PTR [r15+r15*2]
	shl	eax, 4
	test	eax, eax
	je	$LL11@bipred_ave
	lea	ebx, DWORD PTR [rax-1]
	shr	ebx, 5
	inc	ebx
$LL2437@bipred_ave:

; 841  :         default:
; 842  :           assert(0 && "Unexpected block width.");

	mov	r8d, 842				; 0000034aH
	lea	rdx, OFFSET FLAT:$SG4294951316
	lea	rcx, OFFSET FLAT:$SG4294951315
	vzeroupper
	call	QWORD PTR __imp__wassert
	sub	rbx, 1
	jne	SHORT $LL2437@bipred_ave

; 843  :           break;
; 844  :       }
; 845  :     }
; 846  :   } else if (area_mod_32 == 0) {

	jmp	$LL11@bipred_ave
$LN2453@bipred_ave:

; 847  :     for (int i = 0; i < pu_w * pu_h; i += 24) {

	lea	r8d, DWORD PTR [r15+r15*2]
	shl	r8d, 4
	test	r8d, r8d
	je	$LL11@bipred_ave
	vmovdqu	ymm3, YMMWORD PTR __ymm@0000000000000000ffffffffffffffffffffffffffffffffffffffffffffffff
	npad	13
$LL2442@bipred_ave:

; 848  : 
; 849  :       int y = i / pu_w;

	mov	eax, -1431655765			; aaaaaaabH

; 850  :       int x = i % pu_w;

	mov	ecx, ebx
	mul	ebx
	shr	edx, 5
	lea	eax, DWORD PTR [rdx+rdx*2]

; 860  :         case 24: // Same as case 48
; 861  :         case 48: _mm256_maskstore_epi64((long long*)&dst[y * dst_stride + x], mask, avg); break;

	shl	edx, 5
	shl	eax, 4
	sub	ecx, eax
	movsxd	rax, ebx
	add	edx, ecx
	add	ebx, 24
	vpmaskmovq ymm1, ymm3, YMMWORD PTR [rsi+rax]
	vpmaskmovq ymm0, ymm3, YMMWORD PTR [rdi+rax]
	vpavgb	ymm2, ymm0, ymm1
	vpmaskmovq YMMWORD PTR [rdx+r14], ymm3, ymm2
	cmp	ebx, r8d
	jb	SHORT $LL2442@bipred_ave

; 864  :           break;
; 865  :       }
; 866  :     }
; 867  :   } else {

	jmp	$LL11@bipred_ave
$LN2460@bipred_ave:

; 1469 :     }
; 1470 :   }
; 1471 : }

	mov	r8d, 904				; 00000388H
	lea	rdx, OFFSET FLAT:$SG4294951312
	lea	rcx, OFFSET FLAT:$SG4294951311
	vzeroupper
	call	QWORD PTR __imp__wassert
	jmp	$LL11@bipred_ave
$LN2015@bipred_ave:

; 931  :         assert(0 && "Unexpected block width.");

	mov	r8d, 931				; 000003a3H
	lea	rdx, OFFSET FLAT:$SG4294951310
	lea	rcx, OFFSET FLAT:$SG4294951309
	vzeroupper
	call	QWORD PTR __imp__wassert

; 932  :         break;
; 933  :     }
; 934  :   } else {

	jmp	$LL11@bipred_ave
$LN2003@bipred_ave:

; 938  :     for (int i = 0; i < pu_w * pu_h; ++i)

	imul	r15d, r8d
	test	r15d, r15d
	je	$LL11@bipred_ave
	sub	rdi, rsi
	npad	13
$LL2002@bipred_ave:

; 939  :     {
; 940  :       int y = i / pu_w;
; 941  :       int x = i % pu_w;
; 942  :       int16_t sample_L0 = px_L0[i] << (14 - KVZ_BIT_DEPTH);
; 943  :       int16_t sample_L1 = px_L1[i] << (14 - KVZ_BIT_DEPTH);
; 944  :       int32_t rounded = (sample_L0 + sample_L1 + offset) >> shift;

	movzx	r9d, BYTE PTR [rsi]
	xor	edx, edx
	shl	r9d, 6
	mov	eax, ebx
	div	r8d
	movzx	r8d, BYTE PTR [rdi+rsi]
	add	r9d, 64					; 00000040H
	shl	r8d, 6
	add	r9d, r8d
	shr	r9d, 7
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\generic\picture-generic.c

; 69   :   if (value & ~PIXEL_MAX) {

	test	r9d, -256				; ffffff00H
	je	SHORT $LN2487@bipred_ave

; 70   :     int32_t temp = (-value) >> 31;

	neg	r9d
	sar	r9d, 31
$LN2487@bipred_ave:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 938  :     for (int i = 0; i < pu_w * pu_h; ++i)

	mov	r8d, DWORD PTR pu_w$[rsp]
	inc	ebx

; 945  :       dst[y * dst_stride + x] = kvz_fast_clip_32bit_to_pixel(rounded);

	shl	eax, 5
	inc	rsi
	add	eax, edx
	mov	BYTE PTR [rax+r14], r9b
	cmp	ebx, r15d
	jb	SHORT $LL2002@bipred_ave

; 946  :     }
; 947  :   }
; 948  : }

	jmp	$LL11@bipred_ave
$LN8@bipred_ave:
	mov	eax, edi
	and	eax, ecx
	test	al, 2

; 1457 : 
; 1458 :     } else if ((im_flags_L0 & 2) && (im_flags_L1 & 2)) {

	je	SHORT $LN10@bipred_ave

; 1459 :       bipred_average_im_im_avx2(lcu->rec.u + pb_offset, im_L0->u, im_L1->u, pb_w, pb_h, LCU_WIDTH_C);

	mov	r8, QWORD PTR [r13+16]
	mov	rdx, QWORD PTR [rsi+16]
	mov	ebx, r14d
	add	rbx, r12
	mov	r12d, 32				; 00000020H
	mov	DWORD PTR [rsp+40], r12d
	mov	DWORD PTR [rsp+32], r15d
	lea	rcx, QWORD PTR [rbx+10636]
	vzeroupper
	call	bipred_average_im_im_avx2

; 1460 :       bipred_average_im_im_avx2(lcu->rec.v + pb_offset, im_L0->v, im_L1->v, pb_w, pb_h, LCU_WIDTH_C);

	mov	r9d, DWORD PTR pu_w$[rsp]
	lea	rcx, QWORD PTR [rbx+11660]
	mov	r8, QWORD PTR [r13+24]
	mov	rdx, QWORD PTR [rsi+24]
	mov	DWORD PTR [rsp+40], r12d
	mov	DWORD PTR [rsp+32], r15d
	call	bipred_average_im_im_avx2

; 1461 : 
; 1462 :     } else {

	jmp	$LL11@bipred_ave
$LN10@bipred_ave:

; 1463 :       kvz_pixel    *src_px_u = (im_flags_L0 & 2) ? px_L1->u : px_L0->u;

	test	dil, 2
	je	SHORT $LN17@bipred_ave
	mov	rax, QWORD PTR px_L1$[rsp]

; 1464 :       kvz_pixel_im *src_im_u = (im_flags_L0 & 2) ? im_L0->u : im_L1->u;

	mov	r8, QWORD PTR [rsi+16]

; 1466 :       kvz_pixel_im *src_im_v = (im_flags_L0 & 2) ? im_L0->v : im_L1->v;

	mov	rsi, QWORD PTR [rsi+24]
	mov	rdx, QWORD PTR [rax+16]
	jmp	SHORT $LN3621@bipred_ave
$LN17@bipred_ave:

; 1463 :       kvz_pixel    *src_px_u = (im_flags_L0 & 2) ? px_L1->u : px_L0->u;

	mov	rdx, QWORD PTR [rdx+16]

; 1464 :       kvz_pixel_im *src_im_u = (im_flags_L0 & 2) ? im_L0->u : im_L1->u;

	mov	r8, QWORD PTR [r13+16]

; 1465 :       kvz_pixel    *src_px_v = (im_flags_L0 & 2) ? px_L1->v : px_L0->v;

	mov	rax, QWORD PTR px_L0$[rsp]

; 1466 :       kvz_pixel_im *src_im_v = (im_flags_L0 & 2) ? im_L0->v : im_L1->v;

	mov	rsi, QWORD PTR [r13+24]
$LN3621@bipred_ave:

; 1467 :       bipred_average_px_im_avx2(lcu->rec.u + pb_offset, src_px_u, src_im_u, pb_w, pb_h, LCU_WIDTH_C);

	mov	rdi, QWORD PTR [rax+24]
	mov	ebx, r14d
	add	rbx, r12
	mov	r12d, 32				; 00000020H
	mov	DWORD PTR [rsp+40], r12d
	mov	DWORD PTR [rsp+32], r15d
	lea	rcx, QWORD PTR [rbx+10636]
	vzeroupper
	call	bipred_average_px_im_avx2

; 1468 :       bipred_average_px_im_avx2(lcu->rec.v + pb_offset, src_px_v, src_im_v, pb_w, pb_h, LCU_WIDTH_C);

	mov	r9d, DWORD PTR pu_w$[rsp]
	lea	rcx, QWORD PTR [rbx+11660]
	mov	r8, rsi
	mov	DWORD PTR [rsp+40], r12d
	mov	rdx, rdi
	mov	DWORD PTR [rsp+32], r15d
	call	bipred_average_px_im_avx2
$LL11@bipred_ave:
	vzeroupper

; 1469 :     }
; 1470 :   }
; 1471 : }

	add	rsp, 64					; 00000040H
	pop	r15
	pop	r14
	pop	r13
	pop	r12
	pop	rdi
	pop	rsi
	pop	rbx
	ret	0
	npad	1
$LN3570@bipred_ave:
	DD	$LN34@bipred_ave
	DD	$LN39@bipred_ave
	DD	$LN35@bipred_ave
	DD	$LN40@bipred_ave
	DD	$LN36@bipred_ave
	DD	$LN41@bipred_ave
	DD	$LN37@bipred_ave
	DD	$LN42@bipred_ave
	DD	$LN38@bipred_ave
	DD	$LN43@bipred_ave
$LN3569@bipred_ave:
	DB	0
	DB	9
	DB	1
	DB	9
	DB	2
	DB	9
	DB	9
	DB	9
	DB	3
	DB	9
	DB	9
	DB	9
	DB	4
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	5
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	6
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	7
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	8
	npad	3
$LN3568@bipred_ave:
	DD	$LN1513@bipred_ave
	DD	$LN1518@bipred_ave
	DD	$LN1514@bipred_ave
	DD	$LN1519@bipred_ave
	DD	$LN1515@bipred_ave
	DD	$LN1520@bipred_ave
	DD	$LN1516@bipred_ave
	DD	$LN1521@bipred_ave
	DD	$LN1517@bipred_ave
	DD	$LN1522@bipred_ave
$LN3567@bipred_ave:
	DB	0
	DB	9
	DB	1
	DB	9
	DB	2
	DB	9
	DB	9
	DB	9
	DB	3
	DB	9
	DB	9
	DB	9
	DB	4
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	5
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	6
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	7
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	8
	npad	3
$LN3566@bipred_ave:
	DD	$LN2006@bipred_ave
	DD	$LN2011@bipred_ave
	DD	$LN2007@bipred_ave
	DD	$LN2012@bipred_ave
	DD	$LN2008@bipred_ave
	DD	$LN2013@bipred_ave
	DD	$LN2009@bipred_ave
	DD	$LN2014@bipred_ave
	DD	$LN2010@bipred_ave
	DD	$LN2015@bipred_ave
$LN3565@bipred_ave:
	DB	0
	DB	9
	DB	1
	DB	9
	DB	2
	DB	9
	DB	9
	DB	9
	DB	3
	DB	9
	DB	9
	DB	9
	DB	4
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	5
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	6
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	7
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	9
	DB	8
bipred_average_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
width$ = 8
get_optimized_sad_avx2 PROC

; 1475 :   if (width == 0)

	test	ecx, ecx
	jne	SHORT $LN2@get_optimi

; 1476 :     return reg_sad_w0;

	lea	rax, OFFSET FLAT:reg_sad_w0

; 1490 :     return reg_sad_w64;
; 1491 :   else
; 1492 :     return NULL;
; 1493 : }

	ret	0
$LN2@get_optimi:

; 1477 :   if (width == 4)

	cmp	ecx, 4
	jne	SHORT $LN3@get_optimi

; 1478 :     return reg_sad_w4;

	lea	rax, OFFSET FLAT:reg_sad_w4

; 1490 :     return reg_sad_w64;
; 1491 :   else
; 1492 :     return NULL;
; 1493 : }

	ret	0
$LN3@get_optimi:

; 1479 :   if (width == 8)

	cmp	ecx, 8
	jne	SHORT $LN4@get_optimi

; 1480 :     return reg_sad_w8;

	lea	rax, OFFSET FLAT:reg_sad_w8

; 1490 :     return reg_sad_w64;
; 1491 :   else
; 1492 :     return NULL;
; 1493 : }

	ret	0
$LN4@get_optimi:

; 1481 :   if (width == 12)

	cmp	ecx, 12
	jne	SHORT $LN5@get_optimi

; 1482 :     return reg_sad_w12;

	lea	rax, OFFSET FLAT:reg_sad_w12

; 1490 :     return reg_sad_w64;
; 1491 :   else
; 1492 :     return NULL;
; 1493 : }

	ret	0
$LN5@get_optimi:

; 1483 :   if (width == 16)

	cmp	ecx, 16
	jne	SHORT $LN6@get_optimi

; 1484 :     return reg_sad_w16;

	lea	rax, OFFSET FLAT:reg_sad_w16

; 1490 :     return reg_sad_w64;
; 1491 :   else
; 1492 :     return NULL;
; 1493 : }

	ret	0
$LN6@get_optimi:

; 1485 :   if (width == 24)

	cmp	ecx, 24
	jne	SHORT $LN7@get_optimi

; 1486 :     return reg_sad_w24;

	lea	rax, OFFSET FLAT:reg_sad_w24

; 1490 :     return reg_sad_w64;
; 1491 :   else
; 1492 :     return NULL;
; 1493 : }

	ret	0
$LN7@get_optimi:

; 1487 :   if (width == 32)

	cmp	ecx, 32					; 00000020H
	jne	SHORT $LN8@get_optimi

; 1488 :     return reg_sad_w32;

	lea	rax, OFFSET FLAT:reg_sad_w32

; 1490 :     return reg_sad_w64;
; 1491 :   else
; 1492 :     return NULL;
; 1493 : }

	ret	0
$LN8@get_optimi:

; 1489 :   if (width == 64)

	xor	eax, eax
	lea	rdx, OFFSET FLAT:reg_sad_w64
	cmp	ecx, 64					; 00000040H
	cmove	rax, rdx

; 1490 :     return reg_sad_w64;
; 1491 :   else
; 1492 :     return NULL;
; 1493 : }

	ret	0
get_optimized_sad_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
pic_data$ = 64
ref_data$ = 72
width$ = 80
height$ = 88
stride$ = 96
ver_sad_avx2 PROC

; 1497 : {

	sub	rsp, 56					; 00000038H
	mov	r11d, r9d
	mov	r10, rcx

; 1498 :   if (width == 0)

	test	r8d, r8d
	jne	SHORT $LN2@ver_sad_av

; 1499 :     return 0;

	xor	eax, eax

; 1510 : }

	add	rsp, 56					; 00000038H
	ret	0
$LN2@ver_sad_av:
	mov	QWORD PTR [rsp+64], rbx
	mov	QWORD PTR [rsp+48], rdi

; 1500 :   if (width == 4)

	cmp	r8d, 4
	jne	$LN3@ver_sad_av
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 355  :   __m128i ref_row = _mm_set1_epi32(*(const uint32_t *)ref_data);

	vpbroadcastd xmm5, DWORD PTR [rdx]

; 356  :   __m128i sse_inc = _mm_setzero_si128();
; 357  :   int32_t y;
; 358  : 
; 359  :   const int32_t height_fourline_groups = height & ~3;
; 360  :   const int32_t height_residual_lines  = height &  3;
; 361  : 
; 362  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	edi, DWORD PTR stride$[rsp]
	xor	eax, eax
	mov	QWORD PTR [rsp+72], rbp
	mov	ebp, r11d
	mov	QWORD PTR [rsp+80], rsi
	and	ebp, 3
	mov	esi, r11d
	and	esi, -4
	vpxor	xmm4, xmm4, xmm4
	test	esi, esi
	jle	SHORT $LN11@ver_sad_av

; 355  :   __m128i ref_row = _mm_set1_epi32(*(const uint32_t *)ref_data);

	mov	ebx, 2
	npad	8
$LL12@ver_sad_av:

; 363  :     __m128i a = _mm_cvtsi32_si128(*(uint32_t *)(pic_data + y * stride));

	mov	ecx, eax

; 364  : 
; 365  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 1) * stride), 1);

	lea	edx, DWORD PTR [rbx-1]

; 366  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 2) * stride), 2);

	mov	r8d, ebx
	imul	ecx, edi

; 367  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 3) * stride), 3);

	lea	r9d, DWORD PTR [rbx+1]
	imul	edx, edi
	imul	r8d, edi
	add	eax, 4
	imul	r9d, edi
	add	ebx, 4
	vmovd	xmm0, DWORD PTR [rcx+r10]
	vpinsrd	xmm1, xmm0, DWORD PTR [rdx+r10], 1
	vpinsrd	xmm2, xmm1, DWORD PTR [r8+r10], 2
	vpinsrd	xmm3, xmm2, DWORD PTR [r9+r10], 3

; 368  : 
; 369  :     __m128i curr_sads = _mm_sad_epu8(a, ref_row);

	vpsadbw	xmm0, xmm3, xmm5

; 370  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm4, xmm0, xmm4
	cmp	eax, esi
	jl	SHORT $LL12@ver_sad_av
$LN11@ver_sad_av:
	mov	rsi, QWORD PTR [rsp+80]

; 371  :   }
; 372  :   if (height_residual_lines) {

	test	ebp, ebp
	mov	rbp, QWORD PTR [rsp+72]
	je	SHORT $LN14@ver_sad_av

; 373  :     // Only pick the last dword, because we're comparing single dwords (lines)
; 374  :     ref_row = _mm_bsrli_si128(ref_row, 12);

	vpsrldq	xmm2, xmm5, 12

; 375  : 
; 376  :     for (; y < height; y++) {

	cmp	eax, r11d
	jge	SHORT $LN14@ver_sad_av
	npad	1
$LL15@ver_sad_av:

; 377  :       __m128i a = _mm_cvtsi32_si128(*(const uint32_t *)(pic_data + y * stride));

	mov	ecx, eax
	inc	eax
	imul	ecx, edi
	vmovd	xmm0, DWORD PTR [rcx+r10]

; 378  : 
; 379  :       __m128i curr_sads = _mm_sad_epu8(a, ref_row);

	vpsadbw	xmm1, xmm0, xmm2

; 380  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm4, xmm1, xmm4
	cmp	eax, r11d
	jl	SHORT $LL15@ver_sad_av
$LN14@ver_sad_av:
	mov	rbx, QWORD PTR [rsp+64]
	mov	rdi, QWORD PTR [rsp+48]

; 381  :     }
; 382  :   }
; 383  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm4, 78				; 0000004eH

; 384  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vpaddq	xmm1, xmm0, xmm4

; 385  : 
; 386  :   return _mm_cvtsi128_si32(sad);

	vmovd	eax, xmm1
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 1510 : }

	add	rsp, 56					; 00000038H
	ret	0
$LN3@ver_sad_av:

; 1501 :     return ver_sad_w4(pic_data, ref_data, height, stride);
; 1502 :   if (width == 8)

	cmp	r8d, 8
	jne	$LN4@ver_sad_av
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 392  :   const __m128i ref_row = _mm_set1_epi64x(*(const uint64_t *)ref_data);

	vpbroadcastq xmm3, QWORD PTR [rdx]

; 393  :   __m128i sse_inc = _mm_setzero_si128();
; 394  :   int32_t y;
; 395  : 
; 396  :   const int32_t height_fourline_groups = height & ~3;
; 397  :   const int32_t height_residual_lines  = height &  3;
; 398  : 
; 399  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r8d, DWORD PTR stride$[rsp]
	mov	edi, r11d
	and	edi, 3
	and	r9d, -4
	xor	eax, eax
	vpxor	xmm2, xmm2, xmm2
	test	r9d, r9d
	jle	SHORT $LN20@ver_sad_av

; 392  :   const __m128i ref_row = _mm_set1_epi64x(*(const uint64_t *)ref_data);

	mov	ebx, 2
	npad	12
$LL21@ver_sad_av:

; 400  :     __m128d a_d = _mm_setzero_pd();
; 401  :     __m128d c_d = _mm_setzero_pd();
; 402  : 
; 403  :     a_d = _mm_loadl_pd(a_d, (const double *)(pic_data + (y + 0) * stride));

	mov	ecx, eax
	add	eax, 4
	imul	ecx, r8d
	vmovddup xmm0, QWORD PTR [rcx+r10]

; 404  :     a_d = _mm_loadh_pd(a_d, (const double *)(pic_data + (y + 1) * stride));

	lea	ecx, DWORD PTR [rbx-1]
	imul	ecx, r8d
	vmovhpd	xmm0, xmm0, QWORD PTR [rcx+r10]

; 405  : 
; 406  :     c_d = _mm_loadl_pd(c_d, (const double *)(pic_data + (y + 2) * stride));

	mov	ecx, ebx
	imul	ecx, r8d

; 407  :     c_d = _mm_loadh_pd(c_d, (const double *)(pic_data + (y + 3) * stride));
; 408  : 
; 409  :     __m128i a = _mm_castpd_si128(a_d);
; 410  :     __m128i c = _mm_castpd_si128(c_d);
; 411  : 
; 412  :     __m128i curr_sads_ab = _mm_sad_epu8(a, ref_row);

	vpsadbw	xmm0, xmm0, xmm3

; 413  :     __m128i curr_sads_cd = _mm_sad_epu8(c, ref_row);
; 414  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm2, xmm0, xmm2
	vmovddup xmm1, QWORD PTR [rcx+r10]
	lea	ecx, DWORD PTR [rbx+1]
	add	ebx, 4
	imul	ecx, r8d
	vmovhpd	xmm1, xmm1, QWORD PTR [rcx+r10]
	vpsadbw	xmm1, xmm1, xmm3

; 415  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	vpaddq	xmm2, xmm2, xmm1
	cmp	eax, r9d
	jl	SHORT $LL21@ver_sad_av
$LN20@ver_sad_av:

; 416  :   }
; 417  :   if (height_residual_lines) {

	test	edi, edi
	je	$LN37@ver_sad_av

; 418  :     __m128i b = _mm_move_epi64(ref_row);

	vmovq	xmm4, xmm3

; 419  : 
; 420  :     for (; y < height; y++) {

	cmp	eax, r11d
	jge	$LN37@ver_sad_av
$LL24@ver_sad_av:

; 421  :       __m128i a = _mm_loadl_epi64((__m128i *)(pic_data + y * stride));

	mov	ecx, eax
	inc	eax
	imul	ecx, r8d
	vmovq	xmm0, QWORD PTR [rcx+r10]

; 422  : 
; 423  :       __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	vpsadbw	xmm1, xmm0, xmm4

; 424  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	vpaddq	xmm2, xmm1, xmm2
	cmp	eax, r11d
	jl	SHORT $LL24@ver_sad_av
	mov	rbx, QWORD PTR [rsp+64]
	mov	rdi, QWORD PTR [rsp+48]

; 425  :     }
; 426  :   }
; 427  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm2, 78				; 0000004eH

; 428  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vpaddq	xmm1, xmm0, xmm2

; 429  : 
; 430  :   return _mm_cvtsi128_si32(sad);

	vmovd	eax, xmm1
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 1510 : }

	add	rsp, 56					; 00000038H
	ret	0
$LN4@ver_sad_av:

; 1503 :     return ver_sad_w8(pic_data, ref_data, height, stride);
; 1504 :   if (width == 12)

	cmp	r8d, 12
	jne	SHORT $LN5@ver_sad_av
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 436  :   const __m128i ref_row = _mm_loadu_si128((__m128i *)ref_data);

	vmovdqu	xmm3, XMMWORD PTR [rdx]
	xor	eax, eax
	vpxor	xmm2, xmm2, xmm2

; 437  :   __m128i sse_inc = _mm_setzero_si128();
; 438  :   int32_t y;
; 439  : 
; 440  :   for (y = 0; y < height; y++) {

	test	r11d, r11d
	jle	$LN37@ver_sad_av
	mov	edx, DWORD PTR stride$[rsp]
	npad	10
$LL30@ver_sad_av:

; 441  :     __m128i a = _mm_loadu_si128((const __m128i *)(pic_data + y * stride));

	mov	ecx, eax
	inc	eax
	imul	ecx, edx

; 442  : 
; 443  :     __m128i a_masked  = _mm_blend_epi16(ref_row, a, 0x3f);

	vpblendw xmm0, xmm3, XMMWORD PTR [rcx+r10], 63	; 0000003fH

; 444  :     __m128i curr_sads = _mm_sad_epu8   (ref_row, a_masked);

	vpsadbw	xmm1, xmm3, xmm0

; 445  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm2, xmm1, xmm2
	cmp	eax, r11d
	jl	SHORT $LL30@ver_sad_av
	mov	rbx, QWORD PTR [rsp+64]
	mov	rdi, QWORD PTR [rsp+48]

; 446  :   }
; 447  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm2, 78				; 0000004eH

; 448  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vpaddq	xmm1, xmm0, xmm2

; 449  :   return _mm_cvtsi128_si32(sad);

	vmovd	eax, xmm1
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 1510 : }

	add	rsp, 56					; 00000038H
	ret	0
$LN5@ver_sad_av:

; 1505 :     return ver_sad_w12(pic_data, ref_data, height, stride);
; 1506 :   if (width == 16)

	cmp	r8d, 16
	jne	$LN6@ver_sad_av
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 455  :   const __m128i ref_row = _mm_loadu_si128((__m128i *)ref_data);

	vmovdqu	xmm4, XMMWORD PTR [rdx]

; 456  :   __m128i sse_inc       = _mm_setzero_si128();
; 457  :   int32_t y;
; 458  : 
; 459  :   const int32_t height_fourline_groups = height & ~3;
; 460  :   const int32_t height_residual_lines  = height &  3;
; 461  : 
; 462  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	edx, DWORD PTR stride$[rsp]
	mov	r8d, r11d
	and	r8d, -4
	and	r9d, 3
	xor	eax, eax
	vpxor	xmm2, xmm2, xmm2
	test	r8d, r8d
	jle	SHORT $LN34@ver_sad_av

; 455  :   const __m128i ref_row = _mm_loadu_si128((__m128i *)ref_data);

	lea	ebx, QWORD PTR [rax+2]
$LL35@ver_sad_av:

; 463  :     __m128i pic_row_1   = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * stride));

	mov	ecx, eax
	add	eax, 4
	imul	ecx, edx

; 464  :     __m128i pic_row_2   = _mm_loadu_si128((__m128i *)(pic_data + (y + 1) * stride));
; 465  :     __m128i pic_row_3   = _mm_loadu_si128((__m128i *)(pic_data + (y + 2) * stride));
; 466  :     __m128i pic_row_4   = _mm_loadu_si128((__m128i *)(pic_data + (y + 3) * stride));
; 467  : 
; 468  :     __m128i curr_sads_1 = _mm_sad_epu8   (pic_row_1, ref_row);

	vpsadbw	xmm1, xmm4, XMMWORD PTR [rcx+r10]

; 469  :     __m128i curr_sads_2 = _mm_sad_epu8   (pic_row_2, ref_row);
; 470  :     __m128i curr_sads_3 = _mm_sad_epu8   (pic_row_3, ref_row);
; 471  :     __m128i curr_sads_4 = _mm_sad_epu8   (pic_row_4, ref_row);
; 472  : 
; 473  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_1);

	vpaddq	xmm2, xmm1, xmm2
	lea	ecx, DWORD PTR [rbx-1]
	imul	ecx, edx
	vpsadbw	xmm1, xmm4, XMMWORD PTR [rcx+r10]
	mov	ecx, ebx
	imul	ecx, edx

; 474  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_2);

	vpaddq	xmm3, xmm2, xmm1
	vpsadbw	xmm1, xmm4, XMMWORD PTR [rcx+r10]
	lea	ecx, DWORD PTR [rbx+1]
	add	ebx, 4
	imul	ecx, edx

; 475  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_3);

	vpaddq	xmm2, xmm3, xmm1
	vpsadbw	xmm1, xmm4, XMMWORD PTR [rcx+r10]

; 476  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_4);

	vpaddq	xmm2, xmm2, xmm1
	cmp	eax, r8d
	jl	SHORT $LL35@ver_sad_av
$LN34@ver_sad_av:

; 477  :   }
; 478  :   if (height_residual_lines) {

	test	r9d, r9d
	je	SHORT $LN37@ver_sad_av

; 479  :     for (; y < height; y++) {

	cmp	eax, r11d
	jge	SHORT $LN37@ver_sad_av
	npad	11
$LL38@ver_sad_av:

; 480  :       __m128i pic_row   = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * stride));

	mov	ecx, eax
	inc	eax
	imul	ecx, edx

; 481  :       __m128i curr_sads = _mm_sad_epu8   (pic_row, ref_row);

	vpsadbw	xmm1, xmm4, XMMWORD PTR [rcx+r10]

; 482  : 
; 483  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	vpaddq	xmm2, xmm1, xmm2
	cmp	eax, r11d
	jl	SHORT $LL38@ver_sad_av
$LN37@ver_sad_av:
	mov	rbx, QWORD PTR [rsp+64]
	mov	rdi, QWORD PTR [rsp+48]

; 484  :     }
; 485  :   }
; 486  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm2, 78				; 0000004eH

; 487  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vpaddq	xmm1, xmm0, xmm2

; 488  : 
; 489  :   return _mm_cvtsi128_si32(sad);

	vmovd	eax, xmm1
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 1510 : }

	add	rsp, 56					; 00000038H
	ret	0
$LN6@ver_sad_av:

; 1507 :     return ver_sad_w16(pic_data, ref_data, height, stride);
; 1508 :   else
; 1509 :     return ver_sad_arbitrary(pic_data, ref_data, width, height, stride);

	mov	rbx, QWORD PTR [rsp+64]
	mov	rdi, QWORD PTR [rsp+48]

; 1510 : }

	add	rsp, 56					; 00000038H

; 1507 :     return ver_sad_w16(pic_data, ref_data, height, stride);
; 1508 :   else
; 1509 :     return ver_sad_arbitrary(pic_data, ref_data, width, height, stride);

	jmp	ver_sad_arbitrary
ver_sad_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
a_off$1$ = 64
is_left_bm$1$ = 68
invec_lstart$1$ = 72
invec_linc$1$ = 76
left_offset$1$ = 80
x$2$ = 84
tv1935 = 84
invec_lend$1$ = 88
a_off$1$ = 92
outside_vecs$1$ = 96
y$1$ = 100
tv1918 = 104
height_fourline_groups$1$ = 108
tv1910 = 112
tv1936 = 116
tv1926 = 120
tv1908 = 120
tv1925 = 128
tv1909 = 128
tv1933 = 136
tv1915 = 136
tv1922 = 144
tv1907 = 144
old_f$1$ = 160
tv1906 = 160
old_d$1$ = 176
tv1921 = 176
old_b$1$ = 192
tv1905 = 192
tv2041 = 208
tv1930 = 216
move_old_to_b_imask$1$ = 224
shufmask1$1$ = 240
tv1914 = 256
tv1913 = 264
old_h$1$ = 272
blk_widths$1$ = 288
is_left$1$ = 304
pic_data$ = 544
ref_data$ = 552
border_off$1$ = 560
width$ = 560
height$ = 568
pic_stride$ = 576
ref_stride$ = 584
left$ = 592
right$ = 600
hor_sad_avx2 PROC

; 1515 : {

	mov	DWORD PTR [rsp+32], r9d
	mov	QWORD PTR [rsp+16], rdx
	push	rbx
	sub	rsp, 528				; 00000210H
	mov	ebx, r9d
	mov	r11, rdx
	mov	r10, rcx

; 1516 :   if (width == 4)

	cmp	r8d, 4
	jne	SHORT $LN2@hor_sad_av

; 1517 :     return hor_sad_sse41_w4(pic_data, ref_data, height,

	mov	eax, DWORD PTR right$[rsp]
	mov	r8d, ebx
	mov	r9d, DWORD PTR pic_stride$[rsp]
	mov	DWORD PTR [rsp+48], eax
	mov	eax, DWORD PTR left$[rsp]
	mov	DWORD PTR [rsp+40], eax
	mov	eax, DWORD PTR ref_stride$[rsp]
	mov	DWORD PTR [rsp+32], eax
	call	hor_sad_sse41_w4

; 1531 : }

	add	rsp, 528				; 00000210H
	pop	rbx
	ret	0
$LN2@hor_sad_av:

; 1518 :                             pic_stride, ref_stride, left, right);
; 1519 :   if (width == 8)

	cmp	r8d, 8
	jne	SHORT $LN3@hor_sad_av

; 1520 :     return hor_sad_sse41_w8(pic_data, ref_data, height,

	mov	eax, DWORD PTR right$[rsp]
	mov	r8d, ebx
	mov	r9d, DWORD PTR pic_stride$[rsp]
	mov	DWORD PTR [rsp+48], eax
	mov	eax, DWORD PTR left$[rsp]
	mov	DWORD PTR [rsp+40], eax
	mov	eax, DWORD PTR ref_stride$[rsp]
	mov	DWORD PTR [rsp+32], eax
	call	hor_sad_sse41_w8

; 1531 : }

	add	rsp, 528				; 00000210H
	pop	rbx
	ret	0
$LN3@hor_sad_av:

; 1521 :                             pic_stride, ref_stride, left, right);
; 1522 :   if (width == 16)

	cmp	r8d, 16
	jne	SHORT $LN4@hor_sad_av

; 1523 :     return hor_sad_sse41_w16(pic_data, ref_data, height,

	mov	eax, DWORD PTR right$[rsp]
	mov	r8d, ebx
	mov	r9d, DWORD PTR pic_stride$[rsp]
	mov	DWORD PTR [rsp+48], eax
	mov	eax, DWORD PTR left$[rsp]
	mov	DWORD PTR [rsp+40], eax
	mov	eax, DWORD PTR ref_stride$[rsp]
	mov	DWORD PTR [rsp+32], eax
	call	hor_sad_sse41_w16

; 1531 : }

	add	rsp, 528				; 00000210H
	pop	rbx
	ret	0
$LN4@hor_sad_av:

; 1524 :                              pic_stride, ref_stride, left, right);
; 1525 :   if (width == 32)

	cmp	r8d, 32					; 00000020H
	jne	SHORT $LN5@hor_sad_av

; 1526 :     return hor_sad_avx2_w32 (pic_data, ref_data, height,

	mov	eax, DWORD PTR right$[rsp]
	mov	r8d, ebx
	mov	r9d, DWORD PTR pic_stride$[rsp]
	mov	DWORD PTR [rsp+48], eax
	mov	eax, DWORD PTR left$[rsp]
	mov	DWORD PTR [rsp+40], eax
	mov	eax, DWORD PTR ref_stride$[rsp]
	mov	DWORD PTR [rsp+32], eax
	call	hor_sad_avx2_w32

; 1531 : }

	add	rsp, 528				; 00000210H
	pop	rbx
	ret	0
$LN5@hor_sad_av:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 852  :   const __m128i rights     = _mm_set1_epi8((uint8_t)right);

	mov	r9d, DWORD PTR right$[rsp]
	mov	eax, ebx

; 854  :   const __m128i vec_widths = _mm_set1_epi8((uint8_t)vec_width);

	vmovdqu	xmm5, XMMWORD PTR __xmm@10101010101010101010101010101010
	mov	QWORD PTR [rsp+544], rbp

; 849  :   const int32_t height_fourline_groups = height & ~3;

	and	eax, -4
	mov	QWORD PTR [rsp+520], rsi
	mov	QWORD PTR [rsp+512], rdi
	mov	QWORD PTR [rsp+504], r12
	mov	QWORD PTR [rsp+496], r13

; 856  : 
; 857  :   uint32_t outside_vecs,  inside_vecs,  left_offset, is_left_bm;
; 858  :   int32_t  outside_width, inside_width, border_off,  invec_lstart,
; 859  :            invec_lend,    invec_linc;
; 860  :   if (left) {

	mov	r13d, DWORD PTR left$[rsp]
	mov	QWORD PTR [rsp+488], r14
	mov	QWORD PTR [rsp+480], r15
	vmovaps	XMMWORD PTR [rsp+464], xmm6
	vmovaps	XMMWORD PTR [rsp+448], xmm7
	vmovaps	XMMWORD PTR [rsp+432], xmm8

; 849  :   const int32_t height_fourline_groups = height & ~3;

	mov	DWORD PTR height_fourline_groups$1$[rsp], eax

; 850  :   const int32_t height_residual_lines  = height &  3;

	mov	eax, ebx
	and	eax, 3
	vmovaps	XMMWORD PTR [rsp+416], xmm9

; 855  :   const __m128i nslo       = _mm_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);

	vmovdqu	xmm9, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	mov	DWORD PTR tv1936[rsp], eax
	movsx	eax, r9b
	vmovd	xmm0, eax
	vmovaps	XMMWORD PTR [rsp+400], xmm10
	movsxd	rdx, r8d
	add	rdx, 15
	movsx	eax, r8b
	vmovd	xmm8, eax
	shr	rdx, 4
	vmovaps	XMMWORD PTR [rsp+368], xmm12
	vmovaps	XMMWORD PTR [rsp+320], xmm15

; 853  :   const __m128i blk_widths = _mm_set1_epi8((uint8_t)width);

	vpbroadcastb xmm8, xmm8
	vmovdqu	XMMWORD PTR blk_widths$1$[rsp], xmm8
	vpxor	xmm7, xmm7, xmm7
	vpxor	xmm15, xmm15, xmm15

; 852  :   const __m128i rights     = _mm_set1_epi8((uint8_t)right);

	vpbroadcastb xmm0, xmm0

; 856  : 
; 857  :   uint32_t outside_vecs,  inside_vecs,  left_offset, is_left_bm;
; 858  :   int32_t  outside_width, inside_width, border_off,  invec_lstart,
; 859  :            invec_lend,    invec_linc;
; 860  :   if (left) {

	test	r13d, r13d
	je	SHORT $LN27@hor_sad_av

; 861  :     outside_vecs  =    left                              >> vec_width_log2;

	mov	ebx, r13d

; 862  :     inside_vecs   = (( width           + vecwid_bitmask) >> vec_width_log2) - outside_vecs;
; 863  :     outside_width =    outside_vecs * vec_width;
; 864  :     inside_width  =    inside_vecs  * vec_width;
; 865  :     left_offset   =    left;
; 866  :     border_off    =    left;

	mov	DWORD PTR border_off$1$[rsp], r13d
	shr	ebx, 4
	mov	r14d, r13d
	sub	edx, ebx

; 867  :     invec_lstart  =    0;
; 868  :     invec_lend    =    inside_vecs;
; 869  :     invec_linc    =    1;

	mov	DWORD PTR invec_linc$1$[rsp], 1
	mov	ebp, edx
	xor	eax, eax
	mov	r15d, edx

; 870  :     is_left_bm    =    -1;

	mov	r12d, -1				; ffffffffH

; 871  :   } else {

	jmp	SHORT $LN61@hor_sad_av
$LN27@hor_sad_av:

; 872  :     inside_vecs   =  ((width - right) + vecwid_bitmask)  >> vec_width_log2;

	mov	eax, r8d

; 873  :     outside_vecs  = (( width          + vecwid_bitmask)  >> vec_width_log2) - inside_vecs;

	mov	ebx, edx
	sub	eax, r9d

; 874  :     outside_width =    outside_vecs * vec_width;
; 875  :     inside_width  =    inside_vecs  * vec_width;
; 876  :     left_offset   =    right - width;

	mov	r13d, r9d
	sub	r13d, r8d

; 877  :     border_off    =    width - 1 - right;
; 878  :     invec_lstart  =    inside_vecs - 1;
; 879  :     invec_lend    =    -1;

	mov	r15d, -1

; 880  :     invec_linc    =    -1;

	mov	DWORD PTR invec_linc$1$[rsp], r15d
	lea	rbp, QWORD PTR [rax+15]
	shr	rbp, 4
	lea	r14d, DWORD PTR [rax-1]
	sub	ebx, ebp
	mov	DWORD PTR border_off$1$[rsp], r14d

; 881  :     is_left_bm    =    0;

	xor	r12d, r12d
	lea	eax, DWORD PTR [rbp-1]
$LN61@hor_sad_av:

; 901  : 
; 902  :   const int32_t outvec_offset = (~is_left_bm) & inside_width;
; 903  :   int32_t x, y;
; 904  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	edx, DWORD PTR ref_stride$[rsp]
	and	r13d, 15
	mov	esi, DWORD PTR pic_stride$[rsp]
	xor	r9d, r9d
	mov	DWORD PTR invec_lstart$1$[rsp], eax
	mov	edi, ebx
	vpcmpeqb xmm10, xmm7, xmm0
	movsx	eax, r13b
	vmovd	xmm4, eax
	mov	eax, ebp
	shl	edi, 4
	shl	eax, 4
	movsxd	r8, eax
	mov	eax, r12d
	vpbroadcastb xmm4, xmm4
	vpxor	xmm0, xmm10, xmm4
	vpand	xmm1, xmm10, xmm5
	vpor	xmm2, xmm1, xmm9
	vpsubb	xmm3, xmm0, xmm10
	vpaddb	xmm6, xmm3, xmm2
	not	eax
	mov	DWORD PTR is_left_bm$1$[rsp], r12d
	vpcmpgtb xmm0, xmm4, xmm9
	cdqe
	and	r8, rax
	mov	DWORD PTR invec_lend$1$[rsp], r15d
	vpcmpeqb xmm2, xmm0, xmm7
	vpcmpgtb xmm1, xmm5, xmm6
	vpblendvb xmm12, xmm1, xmm2, xmm10
	mov	DWORD PTR outside_vecs$1$[rsp], ebx
	mov	QWORD PTR tv2041[rsp], rbp
	vmovaps	XMMWORD PTR [rsp+384], xmm11

; 882  :   }
; 883  :   left_offset &= vecwid_bitmask;

	mov	DWORD PTR a_off$1$[rsp], edi
	mov	DWORD PTR left_offset$1$[rsp], r13d

; 884  : 
; 885  :   const __m128i left_offsets = _mm_set1_epi8 ((uint8_t)left_offset);
; 886  :   const __m128i is_left      = _mm_cmpeq_epi8(rights, _mm_setzero_si128());

	vmovdqu	XMMWORD PTR is_left$1$[rsp], xmm10

; 887  :   const __m128i vw_for_left  = _mm_and_si128 (is_left, vec_widths);
; 888  : 
; 889  :   // -x == (x ^ 0xff) + 1 = (x ^ 0xff) - 0xff. Also x == (x ^ 0x00) - 0x00.
; 890  :   // in other words, calculate inverse of left_offsets if is_left is true.
; 891  :   const __m128i offs_neg            = _mm_xor_si128 (left_offsets, is_left);
; 892  :   const __m128i offs_for_sm1        = _mm_sub_epi8  (offs_neg,     is_left);
; 893  : 
; 894  :   const __m128i ns_for_sm1          = _mm_or_si128  (vw_for_left,  nslo);
; 895  :   const __m128i shufmask1           = _mm_add_epi8  (ns_for_sm1,   offs_for_sm1);

	vmovdqu	XMMWORD PTR shufmask1$1$[rsp], xmm6

; 896  : 
; 897  :   const __m128i mo2bmask_l          = _mm_cmpgt_epi8(left_offsets, nslo);
; 898  :   const __m128i mo2bimask_l         = _mm_cmpeq_epi8(mo2bmask_l,   _mm_setzero_si128());
; 899  :   const __m128i mo2bimask_r         = _mm_cmpgt_epi8(vec_widths,   shufmask1);
; 900  :   const __m128i move_old_to_b_imask = _mm_blendv_epi8(mo2bimask_r, mo2bimask_l, is_left);

	vmovdqu	XMMWORD PTR move_old_to_b_imask$1$[rsp], xmm12

; 901  : 
; 902  :   const int32_t outvec_offset = (~is_left_bm) & inside_width;
; 903  :   int32_t x, y;
; 904  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	DWORD PTR y$1$[rsp], r9d
	mov	QWORD PTR tv1930[rsp], r8
	cmp	DWORD PTR height_fourline_groups$1$[rsp], r9d
	jle	$LN10@hor_sad_av
	mov	eax, edi

; 882  :   }
; 883  :   left_offset &= vecwid_bitmask;

	vmovaps	XMMWORD PTR [rsp+352], xmm13
	and	eax, r12d
	vmovaps	XMMWORD PTR [rsp+336], xmm14
	mov	edi, 2
	mov	DWORD PTR a_off$1$[rsp], eax
	mov	DWORD PTR tv1918[rsp], edi

; 901  : 
; 902  :   const int32_t outvec_offset = (~is_left_bm) & inside_width;
; 903  :   int32_t x, y;
; 904  :   for (y = 0; y < height_fourline_groups; y += 4) {

	jmp	SHORT $LN11@hor_sad_av
	npad	6
$LL60@hor_sad_av:
	mov	r14d, DWORD PTR border_off$1$[rsp]
$LN11@hor_sad_av:

; 905  :     __m128i borderpx_vec_b = _mm_set1_epi8(ref_data[(int32_t)((y + 0) * ref_stride + border_off)]);

	mov	r12d, r9d

; 906  :     __m128i borderpx_vec_d = _mm_set1_epi8(ref_data[(int32_t)((y + 1) * ref_stride + border_off)]);
; 907  :     __m128i borderpx_vec_f = _mm_set1_epi8(ref_data[(int32_t)((y + 2) * ref_stride + border_off)]);

	mov	r13d, edi
	imul	r12d, edx
	imul	r13d, edx
	mov	DWORD PTR tv1935[rsp], r12d
	lea	eax, DWORD PTR [r12+r14]
	movsxd	rcx, eax
	movsx	eax, BYTE PTR [rcx+r11]
	vmovd	xmm3, eax
	lea	eax, DWORD PTR [rdi-1]
	imul	eax, edx
	vpbroadcastb xmm3, xmm3
	vmovdqu	XMMWORD PTR old_b$1$[rsp], xmm3
	add	eax, r14d
	cdqe
	movsx	ecx, BYTE PTR [rax+r11]
	lea	eax, DWORD PTR [r14+r13]
	vmovd	xmm4, ecx
	movsxd	rcx, eax

; 908  :     __m128i borderpx_vec_h = _mm_set1_epi8(ref_data[(int32_t)((y + 3) * ref_stride + border_off)]);

	lea	r14d, DWORD PTR [rdi+1]
	vpbroadcastb xmm4, xmm4
	vmovdqu	XMMWORD PTR old_d$1$[rsp], xmm4
	movsx	eax, BYTE PTR [rcx+r11]
	vmovd	xmm5, eax
	mov	eax, r14d
	imul	eax, edx

; 909  : 
; 910  :     for (x = 0; x < outside_vecs; x++) {

	xor	edx, edx
	vpbroadcastb xmm5, xmm5
	vmovdqu	XMMWORD PTR old_f$1$[rsp], xmm5
	mov	DWORD PTR tv1910[rsp], eax
	add	eax, DWORD PTR border_off$1$[rsp]
	movsxd	rcx, eax
	movsx	eax, BYTE PTR [rcx+r11]
	vmovd	xmm14, eax
	vpbroadcastb xmm14, xmm14
	vmovdqu	XMMWORD PTR old_h$1$[rsp], xmm14
	test	ebx, ebx
	je	$LN50@hor_sad_av

; 911  :       __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + outvec_offset));

	mov	eax, r9d
	imul	eax, esi
	vmovdqu	xmm11, xmm3
	vmovdqu	xmm12, xmm4
	vmovdqu	xmm13, xmm5
	mov	QWORD PTR tv1926[rsp], rax
	lea	eax, DWORD PTR [rdi-1]
	imul	eax, esi
	mov	QWORD PTR tv1909[rsp], rax
	mov	eax, edi
	mov	r11, QWORD PTR tv1909[rsp]
	imul	eax, esi
	mov	QWORD PTR tv1915[rsp], rax
	mov	eax, r14d
	mov	r15, QWORD PTR tv1915[rsp]
	imul	eax, esi
	mov	rsi, QWORD PTR tv1926[rsp]
	mov	QWORD PTR tv1907[rsp], rax
	mov	r12d, eax
	npad	6
$LL14@hor_sad_av:
	movsxd	rcx, edx
	add	rcx, rcx
	lea	rax, QWORD PTR [rsi+rcx*8]
	add	rax, r8
	vmovdqu	xmm3, XMMWORD PTR [rax+r10]

; 912  :       __m128i c = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 1) * pic_stride + outvec_offset));

	lea	rax, QWORD PTR [r11+rcx*8]
	add	rax, r8
	vmovdqu	xmm4, XMMWORD PTR [rax+r10]

; 913  :       __m128i e = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 2) * pic_stride + outvec_offset));

	lea	rax, QWORD PTR [r15+rcx*8]
	add	rax, r8
	vmovdqu	xmm5, XMMWORD PTR [rax+r10]

; 914  :       __m128i g = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 3) * pic_stride + outvec_offset));

	lea	rax, QWORD PTR [r12+rcx*8]
	add	rax, r8
	vmovdqu	xmm6, XMMWORD PTR [rax+r10]

; 915  : 
; 916  :       __m128i startoffs  = _mm_set1_epi8  ((x + inside_vecs) << vec_width_log2);

	lea	eax, DWORD PTR [rdx+rbp]
	inc	edx
	shl	al, 4
	movsx	eax, al
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0

; 917  :       __m128i ns         = _mm_add_epi8   (startoffs, nslo);

	vpaddb	xmm0, xmm0, xmm9

; 918  : 
; 919  :       // Unread imask is (is_left NOR unrd_imask_for_right), do the maths etc
; 920  :       __m128i unrd_imask = _mm_cmpgt_epi8 (blk_widths, ns);

	vpcmpgtb xmm1, xmm8, xmm0

; 921  :               unrd_imask = _mm_or_si128   (unrd_imask, is_left);

	vpor	xmm2, xmm1, xmm10

; 922  :       __m128i unrd_mask  = _mm_cmpeq_epi8 (unrd_imask, _mm_setzero_si128());

	vpcmpeqb xmm0, xmm2, xmm7

; 923  : 
; 924  :       __m128i b_unread   = _mm_blendv_epi8(borderpx_vec_b, a, unrd_mask);

	vpblendvb xmm1, xmm11, xmm3, xmm0

; 925  :       __m128i d_unread   = _mm_blendv_epi8(borderpx_vec_d, c, unrd_mask);
; 926  :       __m128i f_unread   = _mm_blendv_epi8(borderpx_vec_f, e, unrd_mask);
; 927  :       __m128i h_unread   = _mm_blendv_epi8(borderpx_vec_h, g, unrd_mask);
; 928  : 
; 929  :       __m128i sad_ab     = _mm_sad_epu8   (a, b_unread);

	vpsadbw	xmm2, xmm3, xmm1

; 930  :       __m128i sad_cd     = _mm_sad_epu8   (c, d_unread);
; 931  :       __m128i sad_ef     = _mm_sad_epu8   (e, f_unread);
; 932  :       __m128i sad_gh     = _mm_sad_epu8   (g, h_unread);
; 933  : 
; 934  :       sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	vpaddq	xmm3, xmm2, xmm15
	vpblendvb xmm1, xmm12, xmm4, xmm0
	vpsadbw	xmm2, xmm4, xmm1

; 935  :       sse_inc = _mm_add_epi64(sse_inc, sad_cd);

	vpaddq	xmm4, xmm3, xmm2
	vpblendvb xmm1, xmm13, xmm5, xmm0
	vpsadbw	xmm2, xmm5, xmm1

; 936  :       sse_inc = _mm_add_epi64(sse_inc, sad_ef);

	vpaddq	xmm3, xmm4, xmm2
	vpblendvb xmm1, xmm14, xmm6, xmm0
	vpsadbw	xmm2, xmm6, xmm1

; 937  :       sse_inc = _mm_add_epi64(sse_inc, sad_gh);

	vpaddq	xmm15, xmm3, xmm2
	cmp	edx, ebx
	jb	$LL14@hor_sad_av
	vmovdqu	xmm3, XMMWORD PTR old_b$1$[rsp]
	vmovdqu	xmm4, XMMWORD PTR old_d$1$[rsp]
	vmovdqu	xmm5, XMMWORD PTR old_f$1$[rsp]
	vmovdqu	xmm6, XMMWORD PTR shufmask1$1$[rsp]
	mov	r11, QWORD PTR ref_data$[rsp]
	mov	esi, DWORD PTR pic_stride$[rsp]
	mov	r15d, DWORD PTR invec_lend$1$[rsp]
	mov	r12d, DWORD PTR tv1935[rsp]
$LN50@hor_sad_av:

; 941  : 
; 942  :     __m128i old_b = borderpx_vec_b;
; 943  :     __m128i old_d = borderpx_vec_d;
; 944  :     __m128i old_f = borderpx_vec_f;
; 945  :     __m128i old_h = borderpx_vec_h;
; 946  : 
; 947  :     for (x = invec_lstart; x != invec_lend; x += invec_linc) {

	mov	eax, DWORD PTR invec_lstart$1$[rsp]
	mov	DWORD PTR x$2$[rsp], eax
	cmp	eax, r15d
	je	$LN59@hor_sad_av

; 938  :     }
; 939  :     int32_t a_off = outside_width & is_left_bm;
; 940  :     int32_t leftoff_with_sign_neg = (left_offset ^ is_left_bm) - is_left_bm;

	mov	eax, DWORD PTR is_left_bm$1$[rsp]
	xor	eax, DWORD PTR left_offset$1$[rsp]
	sub	eax, DWORD PTR is_left_bm$1$[rsp]
	mov	r8d, DWORD PTR invec_linc$1$[rsp]

; 949  :       __m128i c = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 1) * pic_stride + a_off));
; 950  :       __m128i e = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 2) * pic_stride + a_off));
; 951  :       __m128i g = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 3) * pic_stride + a_off));
; 952  :       __m128i b = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 0) * ref_stride + a_off - leftoff_with_sign_neg));

	movsxd	rcx, eax
	mov	eax, r9d
	imul	eax, esi
	imul	r14d, esi
	mov	QWORD PTR tv1933[rsp], rcx
	lea	ecx, DWORD PTR [rdi-1]
	mov	QWORD PTR tv1925[rsp], rax
	movsxd	rax, DWORD PTR a_off$1$[rsp]
	mov	QWORD PTR tv1922[rsp], rax
	mov	eax, ecx
	imul	eax, esi
	mov	QWORD PTR tv1908[rsp], rax
	mov	eax, edi
	mov	rdx, QWORD PTR tv1908[rsp]
	imul	eax, esi
	mov	rsi, QWORD PTR tv1925[rsp]
	mov	QWORD PTR tv1914[rsp], rax
	mov	rbp, QWORD PTR tv1914[rsp]
	mov	eax, r14d
	mov	r14, QWORD PTR tv1922[rsp]
	mov	QWORD PTR tv1906[rsp], rax
	mov	rbx, QWORD PTR tv1906[rsp]
	mov	eax, r12d
	mov	r12d, DWORD PTR x$2$[rsp]
	mov	QWORD PTR tv1921[rsp], rax

; 948  :       __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + a_off));

	mov	eax, DWORD PTR ref_stride$[rsp]
	mov	r9, QWORD PTR tv1921[rsp]
	imul	eax, ecx
	mov	QWORD PTR tv1905[rsp], rax
	mov	rdi, QWORD PTR tv1905[rsp]
	mov	eax, r13d
	mov	r13, QWORD PTR tv1933[rsp]
	mov	QWORD PTR tv1913[rsp], rax
	npad	7
$LL17@hor_sad_av:

; 953  :       __m128i d = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 1) * ref_stride + a_off - leftoff_with_sign_neg));
; 954  :       __m128i f = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 2) * ref_stride + a_off - leftoff_with_sign_neg));
; 955  :       __m128i h = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 3) * ref_stride + a_off - leftoff_with_sign_neg));
; 956  : 
; 957  :       __m128i b_shifted    = _mm_shuffle_epi8(b,     shufmask1);
; 958  :       __m128i d_shifted    = _mm_shuffle_epi8(d,     shufmask1);
; 959  :       __m128i f_shifted    = _mm_shuffle_epi8(f,     shufmask1);
; 960  :       __m128i h_shifted    = _mm_shuffle_epi8(h,     shufmask1);
; 961  : 
; 962  :       __m128i b_with_old   = _mm_blendv_epi8 (old_b, b_shifted, move_old_to_b_imask);
; 963  :       __m128i d_with_old   = _mm_blendv_epi8 (old_d, d_shifted, move_old_to_b_imask);
; 964  :       __m128i f_with_old   = _mm_blendv_epi8 (old_f, f_shifted, move_old_to_b_imask);
; 965  :       __m128i h_with_old   = _mm_blendv_epi8 (old_h, h_shifted, move_old_to_b_imask);
; 966  : 
; 967  :       uint8_t startoff     = (x << vec_width_log2) + a_off;
; 968  :       __m128i startoffs    = _mm_set1_epi8   (startoff);
; 969  :       __m128i curr_ns      = _mm_add_epi8    (startoffs,    nslo);
; 970  :       __m128i unrd_imask   = _mm_cmpgt_epi8  (blk_widths,   curr_ns);

	vmovdqu	xmm1, XMMWORD PTR blk_widths$1$[rsp]
	movsxd	rcx, r12d
	lea	rax, QWORD PTR [r14+rsi]
	shl	rcx, 4
	add	rax, rcx
	vmovdqu	xmm14, XMMWORD PTR [rax+r10]
	lea	rax, QWORD PTR [rdx+r14]
	add	rax, rcx
	vmovdqu	xmm13, XMMWORD PTR [rax+r10]
	lea	rax, QWORD PTR [r14+rbp]
	add	rax, rcx
	vmovdqu	xmm12, XMMWORD PTR [rax+r10]
	lea	rax, QWORD PTR [rbx+r14]
	add	rax, rcx
	vmovdqu	xmm11, XMMWORD PTR [rax+r10]
	mov	rax, r9
	sub	rax, r13
	add	rax, r14
	add	rax, rcx
	vmovdqu	xmm0, XMMWORD PTR [rax+r11]
	vpshufb	xmm10, xmm0, xmm6
	mov	rax, r14
	sub	rax, r13
	add	rax, rcx
	add	rax, r11

; 977  : 
; 978  :       old_b = b_shifted;

	vmovdqu	XMMWORD PTR old_b$1$[rsp], xmm10
	vmovdqu	xmm0, XMMWORD PTR [rax+rdi]
	mov	rax, QWORD PTR tv1913[rsp]
	vpshufb	xmm9, xmm0, xmm6
	sub	rax, r13
	add	rax, r14
	add	rax, rcx

; 979  :       old_d = d_shifted;

	vmovdqu	XMMWORD PTR old_d$1$[rsp], xmm9
	vmovdqu	xmm0, XMMWORD PTR [rax+r11]
	vpshufb	xmm8, xmm0, xmm6
	mov	rax, r14
	sub	rax, r13
	add	rax, rcx
	mov	ecx, DWORD PTR tv1910[rsp]
	add	rax, rcx

; 980  :       old_f = f_shifted;

	vmovdqu	XMMWORD PTR old_f$1$[rsp], xmm8
	vmovdqu	xmm0, XMMWORD PTR [rax+r11]
	vpshufb	xmm7, xmm0, xmm6
	movzx	eax, r12b
	add	r12d, r8d
	shl	al, 4
	add	al, BYTE PTR a_off$1$[rsp]
	movsx	eax, al
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0
	vpaddb	xmm0, xmm0, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	vpcmpgtb xmm1, xmm1, xmm0
	vpxor	xmm0, xmm0, xmm0

; 971  :       __m128i unrd_mask    = _mm_cmpeq_epi8  (unrd_imask,   _mm_setzero_si128());

	vpcmpeqb xmm2, xmm1, xmm0
	vmovdqu	xmm0, XMMWORD PTR move_old_to_b_imask$1$[rsp]
	vpblendvb xmm1, xmm3, xmm10, xmm0

; 972  : 
; 973  :       __m128i b_unread     = _mm_blendv_epi8 (b_with_old,   a, unrd_mask);

	vpblendvb xmm6, xmm1, xmm14, xmm2
	vpblendvb xmm1, xmm4, xmm9, xmm0

; 974  :       __m128i d_unread     = _mm_blendv_epi8 (d_with_old,   c, unrd_mask);

	vpblendvb xmm3, xmm1, xmm13, xmm2
	vpblendvb xmm1, xmm5, xmm8, xmm0

; 975  :       __m128i f_unread     = _mm_blendv_epi8 (f_with_old,   e, unrd_mask);

	vpblendvb xmm4, xmm1, xmm12, xmm2
	vmovdqu	xmm1, XMMWORD PTR old_h$1$[rsp]
	vpblendvb xmm1, xmm1, xmm7, xmm0

; 976  :       __m128i h_unread     = _mm_blendv_epi8 (h_with_old,   g, unrd_mask);

	vpblendvb xmm5, xmm1, xmm11, xmm2

; 981  :       old_h = h_shifted;
; 982  : 
; 983  :       __m128i sad_ab     = _mm_sad_epu8(a, b_unread);

	vpsadbw	xmm0, xmm6, xmm14
	vmovdqu	xmm6, XMMWORD PTR shufmask1$1$[rsp]

; 984  :       __m128i sad_cd     = _mm_sad_epu8(c, d_unread);
; 985  :       __m128i sad_ef     = _mm_sad_epu8(e, f_unread);
; 986  :       __m128i sad_gh     = _mm_sad_epu8(g, h_unread);
; 987  : 
; 988  :       sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	vpaddq	xmm2, xmm0, xmm15
	vpsadbw	xmm1, xmm3, xmm13

; 989  :       sse_inc = _mm_add_epi64(sse_inc, sad_cd);

	vpaddq	xmm3, xmm2, xmm1
	vpsadbw	xmm0, xmm4, xmm12

; 990  :       sse_inc = _mm_add_epi64(sse_inc, sad_ef);

	vpaddq	xmm4, xmm3, xmm0
	vpsadbw	xmm1, xmm5, xmm11

; 991  :       sse_inc = _mm_add_epi64(sse_inc, sad_gh);

	vpaddq	xmm15, xmm4, xmm1
	vmovdqu	xmm4, xmm9
	vmovdqu	XMMWORD PTR old_h$1$[rsp], xmm7
	vmovdqu	xmm3, xmm10
	vmovdqu	xmm5, xmm8
	cmp	r12d, r15d
	jne	$LL17@hor_sad_av
	vmovdqu	xmm8, XMMWORD PTR blk_widths$1$[rsp]
	vmovdqu	xmm9, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	vmovdqu	xmm10, XMMWORD PTR is_left$1$[rsp]
	mov	r8, QWORD PTR tv1930[rsp]
	mov	r9d, DWORD PTR y$1$[rsp]
	mov	ebx, DWORD PTR outside_vecs$1$[rsp]
	mov	edi, DWORD PTR tv1918[rsp]
	mov	esi, DWORD PTR pic_stride$[rsp]
	mov	rbp, QWORD PTR tv2041[rsp]
	vpxor	xmm7, xmm7, xmm7
$LN59@hor_sad_av:

; 901  : 
; 902  :   const int32_t outvec_offset = (~is_left_bm) & inside_width;
; 903  :   int32_t x, y;
; 904  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	edx, DWORD PTR ref_stride$[rsp]
	add	r9d, 4
	vmovdqu	xmm6, XMMWORD PTR shufmask1$1$[rsp]
	add	edi, 4
	mov	DWORD PTR y$1$[rsp], r9d
	mov	DWORD PTR tv1918[rsp], edi
	cmp	r9d, DWORD PTR height_fourline_groups$1$[rsp]
	jl	$LL60@hor_sad_av
	vmovdqu	xmm12, XMMWORD PTR move_old_to_b_imask$1$[rsp]
	mov	edi, DWORD PTR a_off$1$[rsp]
	mov	r12d, DWORD PTR is_left_bm$1$[rsp]
	mov	r13d, DWORD PTR left_offset$1$[rsp]
	vmovaps	xmm14, XMMWORD PTR [rsp+336]
	vmovaps	xmm13, XMMWORD PTR [rsp+352]
$LN10@hor_sad_av:

; 992  :     }
; 993  :   }
; 994  :   if (height_residual_lines) {

	cmp	DWORD PTR tv1936[rsp], 0
	je	$LN19@hor_sad_av

; 995  :     for (; y < height; y++) {

	cmp	r9d, DWORD PTR height$[rsp]
	jge	$LN19@hor_sad_av
	vmovdqu	xmm11, XMMWORD PTR shufmask1$1$[rsp]
	and	edi, r12d
	mov	DWORD PTR a_off$1$[rsp], edi
$LL20@hor_sad_av:

; 996  :       __m128i borderpx_vec = _mm_set1_epi8(ref_data[(int32_t)((y + 0) * ref_stride + border_off)]);

	mov	eax, DWORD PTR border_off$1$[rsp]
	mov	r14d, r9d
	imul	r14d, edx
	add	eax, r14d
	movsxd	rcx, eax
	movsx	eax, BYTE PTR [rcx+r11]
	vmovd	xmm6, eax

; 997  :       for (x = 0; x < outside_vecs; x++) {

	xor	eax, eax
	vpbroadcastb xmm6, xmm6
	test	ebx, ebx
	je	SHORT $LN52@hor_sad_av

; 998  :         __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + outvec_offset));

	mov	edx, r9d
	imul	edx, esi
	npad	1
$LL23@hor_sad_av:
	movsxd	rcx, eax
	shl	rcx, 4
	add	rcx, rdx
	add	rcx, r8
	vmovdqu	xmm3, XMMWORD PTR [rcx+r10]

; 999  : 
; 1000 :         __m128i startoffs  = _mm_set1_epi8  ((x + inside_vecs) << vec_width_log2);

	lea	ecx, DWORD PTR [rax+rbp]
	inc	eax
	shl	cl, 4
	movsx	ecx, cl
	vmovd	xmm0, ecx
	vpbroadcastb xmm0, xmm0

; 1001 :         __m128i ns         = _mm_add_epi8   (startoffs, nslo);

	vpaddb	xmm0, xmm0, xmm9

; 1002 : 
; 1003 :         // Unread imask is (is_left NOR unrd_imask_for_right), do the maths etc
; 1004 :         __m128i unrd_imask = _mm_cmpgt_epi8 (blk_widths, ns);

	vpcmpgtb xmm1, xmm8, xmm0

; 1005 :                 unrd_imask = _mm_or_si128   (unrd_imask, is_left);

	vpor	xmm2, xmm1, xmm10

; 1006 :         __m128i unrd_mask  = _mm_cmpeq_epi8 (unrd_imask, _mm_setzero_si128());

	vpcmpeqb xmm0, xmm2, xmm7

; 1007 :         __m128i b_unread   = _mm_blendv_epi8(borderpx_vec, a, unrd_mask);

	vpblendvb xmm1, xmm6, xmm3, xmm0

; 1008 : 
; 1009 :         __m128i sad_ab     = _mm_sad_epu8   (a, b_unread);

	vpsadbw	xmm2, xmm3, xmm1

; 1010 :         sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	vpaddq	xmm15, xmm2, xmm15
	cmp	eax, ebx
	jb	SHORT $LL23@hor_sad_av
$LN52@hor_sad_av:

; 1014 : 
; 1015 :       __m128i old_b = borderpx_vec;
; 1016 :       for (x = invec_lstart; x != invec_lend; x += invec_linc) {

	mov	edx, DWORD PTR invec_lstart$1$[rsp]
	cmp	edx, r15d
	je	$LN18@hor_sad_av
	mov	r8d, DWORD PTR invec_linc$1$[rsp]

; 1011 :       }
; 1012 :       int32_t a_off = outside_width & is_left_bm;
; 1013 :       int32_t leftoff_with_sign_neg = (left_offset ^ is_left_bm) - is_left_bm;

	mov	eax, r12d
	xor	eax, r13d
	movsxd	rdi, edi
	sub	eax, r12d

; 1017 :         __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + a_off));

	mov	r13d, r9d
	imul	r13d, esi
	mov	esi, DWORD PTR a_off$1$[rsp]

; 1018 :         __m128i b = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 0) * ref_stride + a_off - leftoff_with_sign_neg));

	movsxd	r12, eax
	npad	8
$LL26@hor_sad_av:

; 1017 :         __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + a_off));

	movsxd	rcx, edx
	lea	rax, QWORD PTR [rdi+r13]
	shl	rcx, 4
	add	rax, rcx
	vmovdqu	xmm5, XMMWORD PTR [rax+r10]

; 1019 : 
; 1020 :         __m128i b_shifted    = _mm_shuffle_epi8(b,     shufmask1);

	mov	rax, rdi
	sub	rax, r12
	add	rax, r14
	add	rax, rcx
	vmovdqu	xmm0, XMMWORD PTR [rax+r11]
	vpshufb	xmm4, xmm0, xmm11

; 1021 :         __m128i b_with_old   = _mm_blendv_epi8 (old_b, b_shifted, move_old_to_b_imask);
; 1022 : 
; 1023 :         uint8_t startoff     = (x << vec_width_log2) + a_off;

	movzx	eax, dl
	add	edx, r8d
	shl	al, 4
	add	al, sil

; 1024 :         __m128i startoffs    = _mm_set1_epi8   (startoff);

	movsx	eax, al
	vpblendvb xmm2, xmm6, xmm4, xmm12
	vmovd	xmm0, eax
	vpbroadcastb xmm0, xmm0

; 1025 :         __m128i curr_ns      = _mm_add_epi8    (startoffs,    nslo);

	vpaddb	xmm0, xmm0, xmm9

; 1026 :         __m128i unrd_imask   = _mm_cmpgt_epi8  (blk_widths,   curr_ns);

	vpcmpgtb xmm1, xmm8, xmm0

; 1027 :         __m128i unrd_mask    = _mm_cmpeq_epi8  (unrd_imask,   _mm_setzero_si128());

	vpcmpeqb xmm3, xmm1, xmm7

; 1028 :         __m128i b_unread     = _mm_blendv_epi8 (b_with_old,   a, unrd_mask);

	vpblendvb xmm1, xmm2, xmm5, xmm3

; 1029 : 
; 1030 :         old_b = b_shifted;
; 1031 : 
; 1032 :         __m128i sad_ab     = _mm_sad_epu8(a, b_unread);

	vpsadbw	xmm0, xmm1, xmm5

; 1033 :         sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	vpaddq	xmm15, xmm0, xmm15
	vmovdqu	xmm6, xmm4
	cmp	edx, r15d
	jne	SHORT $LL26@hor_sad_av
	mov	r8, QWORD PTR tv1930[rsp]
	mov	esi, DWORD PTR pic_stride$[rsp]
	mov	edi, DWORD PTR a_off$1$[rsp]
	mov	r12d, DWORD PTR is_left_bm$1$[rsp]
$LN18@hor_sad_av:

; 995  :     for (; y < height; y++) {

	mov	edx, DWORD PTR ref_stride$[rsp]
	inc	r9d
	mov	r13d, DWORD PTR left_offset$1$[rsp]
	cmp	r9d, DWORD PTR height$[rsp]
	jl	$LL20@hor_sad_av
$LN19@hor_sad_av:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 1529 :     return hor_sad_sse41_arbitrary(pic_data, ref_data, width, height,

	vmovaps	xmm12, XMMWORD PTR [rsp+368]
	vmovaps	xmm11, XMMWORD PTR [rsp+384]
	vmovaps	xmm10, XMMWORD PTR [rsp+400]
	vmovaps	xmm9, XMMWORD PTR [rsp+416]
	vmovaps	xmm8, XMMWORD PTR [rsp+432]
	vmovaps	xmm7, XMMWORD PTR [rsp+448]
	vmovaps	xmm6, XMMWORD PTR [rsp+464]
	mov	r15, QWORD PTR [rsp+480]
	mov	r14, QWORD PTR [rsp+488]
	mov	r13, QWORD PTR [rsp+496]
	mov	r12, QWORD PTR [rsp+504]
	mov	rdi, QWORD PTR [rsp+512]
	mov	rsi, QWORD PTR [rsp+520]
	mov	rbp, QWORD PTR [rsp+544]
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 1037 :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm15, 78				; 0000004eH

; 1038 :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	vpaddq	xmm1, xmm0, xmm15
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 1529 :     return hor_sad_sse41_arbitrary(pic_data, ref_data, width, height,

	vmovaps	xmm15, XMMWORD PTR [rsp+320]
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 1039 :   return _mm_cvtsi128_si32(sad);

	vmovd	eax, xmm1
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c

; 1531 : }

	add	rsp, 528				; 00000210H
	pop	rbx
	ret	0
hor_sad_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
buf$ = 96
len$ = 104
sum$ = 112
pixel_var_avx2_largebuf PROC

; 1534 : {

	sub	rsp, 88					; 00000058H
	vmovaps	XMMWORD PTR [rsp+48], xmm7

; 1536 :   const __m256i zero = _mm256_setzero_si256();
; 1537 : 
; 1538 :   int64_t sum;
; 1539 :   size_t i;
; 1540 :   __m256i sums = zero;
; 1541 :   for (i = 0; i + 31 < len; i += 32) {

	xor	r9d, r9d
	vmovaps	XMMWORD PTR [rsp], xmm10
	mov	r8d, r9d
	mov	r10d, edx
	mov	eax, edx
	vmovaps	XMMWORD PTR [rsp+32], xmm8
	vmovaps	XMMWORD PTR [rsp+16], xmm9
	vxorps	xmm4, xmm4, xmm4
	vcvtsi2ss xmm4, xmm4, rax
	vpxor	xmm3, xmm3, xmm3
	vpxor	xmm2, xmm2, xmm2
	cmp	r10, 31
	jbe	SHORT $LN3@pixel_var_

; 1535 :   const float len_f  = (float)len;

	mov	eax, 31
$LL4@pixel_var_:

; 1542 :     __m256i curr = _mm256_loadu_si256((const __m256i *)(buf + i));
; 1543 :     __m256i curr_sum = _mm256_sad_epu8(curr, zero);

	vpsadbw	ymm1, ymm3, YMMWORD PTR [rcx+rax-31]
	add	rax, 32					; 00000020H
	add	r8, 32					; 00000020H

; 1544 :             sums = _mm256_add_epi64(sums, curr_sum);

	vpaddq	ymm2, ymm1, ymm2
	cmp	rax, r10
	jb	SHORT $LL4@pixel_var_
$LN3@pixel_var_:

; 1545 :   }
; 1546 :   __m128i sum_lo = _mm256_castsi256_si128  (sums);
; 1547 :   __m128i sum_hi = _mm256_extracti128_si256(sums,   1);

	vextracti128 xmm0, ymm2, 1

; 1548 :   __m128i sum_3  = _mm_add_epi64           (sum_lo, sum_hi);

	vpaddq	xmm1, xmm0, xmm2

; 1549 :   __m128i sum_4  = _mm_shuffle_epi32       (sum_3,  _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	xmm0, xmm1, 78				; 0000004eH

; 1550 :   __m128i sum_5  = _mm_add_epi64           (sum_3,  sum_4);

	vpaddq	xmm1, xmm0, xmm1

; 1551 : 
; 1552 :   _mm_storel_epi64((__m128i *)&sum, sum_5);

	vmovq	QWORD PTR sum$[rsp], xmm1

; 1553 : 
; 1554 :   // Remaining len mod 32 pixels
; 1555 :   for (; i < len; ++i) {

	mov	rdx, QWORD PTR sum$[rsp]
	cmp	r8, r10
	jae	SHORT $LN6@pixel_var_
	npad	4
$LL7@pixel_var_:

; 1556 :     sum += buf[i];

	movzx	eax, BYTE PTR [r8+rcx]
	inc	r8
	add	rdx, rax
	cmp	r8, r10
	jb	SHORT $LL7@pixel_var_
$LN6@pixel_var_:

; 1557 :   }
; 1558 : 
; 1559 :   float   mean_f = (float)sum / len_f;

	vmovss	xmm0, DWORD PTR __real@3f800000
	vxorps	xmm1, xmm1, xmm1
	vdivss	xmm10, xmm0, xmm4
	vcvtsi2ss xmm1, xmm1, rdx
	vmulss	xmm7, xmm1, xmm10

; 1560 :   __m256  mean   = _mm256_set1_ps(mean_f);

	vmovaps	xmm8, xmm7
	vbroadcastss ymm8, xmm7
	vxorps	xmm9, xmm9, xmm9

; 1561 :   __m256  accum  = _mm256_setzero_ps();
; 1562 : 
; 1563 :   for (i = 0; i + 31 < len; i += 32) {

	cmp	r10, 31
	jbe	$LN9@pixel_var_

; 1557 :   }
; 1558 : 
; 1559 :   float   mean_f = (float)sum / len_f;

	mov	r8d, 15
	vmovaps	XMMWORD PTR [rsp+64], xmm6
	sub	r8, rcx
	lea	rdx, QWORD PTR [rcx+16]
	npad	11
$LL10@pixel_var_:

; 1564 :     __m128i curr0    = _mm_loadl_epi64((const __m128i *)(buf + i +  0));

	vmovq	xmm0, QWORD PTR [rdx-16]

; 1565 :     __m128i curr1    = _mm_loadl_epi64((const __m128i *)(buf + i +  8));
; 1566 :     __m128i curr2    = _mm_loadl_epi64((const __m128i *)(buf + i + 16));
; 1567 :     __m128i curr3    = _mm_loadl_epi64((const __m128i *)(buf + i + 24));
; 1568 : 
; 1569 :     __m256i curr0_32 = _mm256_cvtepu8_epi32(curr0);

	vpmovzxbd ymm1, xmm0
	vmovq	xmm0, QWORD PTR [rdx-8]

; 1570 :     __m256i curr1_32 = _mm256_cvtepu8_epi32(curr1);
; 1571 :     __m256i curr2_32 = _mm256_cvtepu8_epi32(curr2);
; 1572 :     __m256i curr3_32 = _mm256_cvtepu8_epi32(curr3);
; 1573 : 
; 1574 :     __m256  curr0_f  = _mm256_cvtepi32_ps  (curr0_32);

	vcvtdq2ps ymm2, ymm1
	vpmovzxbd ymm1, xmm0
	vmovq	xmm0, QWORD PTR [rdx]

; 1575 :     __m256  curr1_f  = _mm256_cvtepi32_ps  (curr1_32);
; 1576 :     __m256  curr2_f  = _mm256_cvtepi32_ps  (curr2_32);
; 1577 :     __m256  curr3_f  = _mm256_cvtepi32_ps  (curr3_32);
; 1578 : 
; 1579 :     __m256  curr0_sd = _mm256_sub_ps       (curr0_f,  mean);

	vsubps	ymm6, ymm2, ymm8
	vcvtdq2ps ymm2, ymm1
	vpmovzxbd ymm1, xmm0
	vmovq	xmm0, QWORD PTR [rdx+8]

; 1580 :     __m256  curr1_sd = _mm256_sub_ps       (curr1_f,  mean);

	vsubps	ymm5, ymm2, ymm8
	vcvtdq2ps ymm2, ymm1
	vpmovzxbd ymm1, xmm0

; 1581 :     __m256  curr2_sd = _mm256_sub_ps       (curr2_f,  mean);

	vsubps	ymm4, ymm2, ymm8

; 1582 :     __m256  curr3_sd = _mm256_sub_ps       (curr3_f,  mean);
; 1583 : 
; 1584 :     __m256  curr0_v  = _mm256_mul_ps       (curr0_sd, curr0_sd);

	vmulps	ymm0, ymm6, ymm6

; 1585 :     __m256  curr1_v  = _mm256_mul_ps       (curr1_sd, curr1_sd);
; 1586 :     __m256  curr2_v  = _mm256_mul_ps       (curr2_sd, curr2_sd);
; 1587 :     __m256  curr3_v  = _mm256_mul_ps       (curr3_sd, curr3_sd);
; 1588 : 
; 1589 :     __m256  curr01   = _mm256_add_ps       (curr0_v,  curr1_v);

	vfmadd231ps ymm0, ymm5, ymm5
	vcvtdq2ps ymm2, ymm1
	lea	rdx, QWORD PTR [rdx+32]
	add	r9, 32					; 00000020H

; 1590 :     __m256  curr23   = _mm256_add_ps       (curr2_v,  curr3_v);
; 1591 :     __m256  curr     = _mm256_add_ps       (curr01,   curr23);

	vfmadd231ps ymm0, ymm4, ymm4
	vsubps	ymm3, ymm2, ymm8
	lea	rax, QWORD PTR [r8+rdx]
	vfmadd231ps ymm0, ymm3, ymm3

; 1592 :             accum    = _mm256_add_ps       (accum,    curr);

	vaddps	ymm9, ymm9, ymm0
	cmp	rax, r10
	jb	SHORT $LL10@pixel_var_

; 1561 :   __m256  accum  = _mm256_setzero_ps();
; 1562 : 
; 1563 :   for (i = 0; i + 31 < len; i += 32) {

	vmovaps	xmm6, XMMWORD PTR [rsp+64]
$LN9@pixel_var_:

; 1593 :   }
; 1594 :   __m256d accum_d  = _mm256_castps_pd     (accum);
; 1595 :   __m256d accum2_d = _mm256_permute4x64_pd(accum_d, _MM_SHUFFLE(1, 0, 3, 2));
; 1596 :   __m256  accum2   = _mm256_castpd_ps     (accum2_d);
; 1597 : 
; 1598 :   __m256  accum3   = _mm256_add_ps        (accum,  accum2);
; 1599 :   __m256  accum4   = _mm256_permute_ps    (accum3, _MM_SHUFFLE(1, 0, 3, 2));
; 1600 :   __m256  accum5   = _mm256_add_ps        (accum3, accum4);
; 1601 :   __m256  accum6   = _mm256_permute_ps    (accum5, _MM_SHUFFLE(2, 3, 0, 1));
; 1602 :   __m256  accum7   = _mm256_add_ps        (accum5, accum6);

	vmovaps	xmm8, XMMWORD PTR [rsp+32]
	vpermpd	ymm0, ymm9, 78				; 0000004eH
	vaddps	ymm1, ymm0, ymm9
	vmovaps	xmm9, XMMWORD PTR [rsp+16]
	vpermilps ymm0, ymm1, 78			; 0000004eH
	vaddps	ymm2, ymm0, ymm1
	vpermilps ymm1, ymm2, 177			; 000000b1H
	vaddps	ymm3, ymm1, ymm2

; 1603 : 
; 1604 :   __m128  accum8   = _mm256_castps256_ps128(accum7);
; 1605 :   float   var_sum  = _mm_cvtss_f32         (accum8);
; 1606 : 
; 1607 :   // Remaining len mod 32 pixels
; 1608 :   for (; i < len; ++i) {

	cmp	r9, r10
	jae	$LN29@pixel_var_
	mov	rax, r10
	sub	rax, r9
	cmp	rax, 4
	jb	$LL43@pixel_var_
	mov	rdx, r10
	lea	r8, QWORD PTR [rcx+2]
	sub	rdx, r9
	add	r8, r9
	sub	rdx, 4
	shr	rdx, 2
	inc	rdx
	lea	r9, QWORD PTR [r9+rdx*4]
	npad	7
$LL31@pixel_var_:

; 1609 :     float diff = buf[i] - mean_f;

	movzx	eax, BYTE PTR [r8-2]

; 1610 :     var_sum += diff * diff;

	lea	r8, QWORD PTR [r8+4]
	vxorps	xmm0, xmm0, xmm0
	vcvtsi2ss xmm0, xmm0, eax
	movzx	eax, BYTE PTR [r8-5]
	vsubss	xmm1, xmm0, xmm7
	vfmadd231ss xmm3, xmm1, xmm1
	vxorps	xmm0, xmm0, xmm0
	vcvtsi2ss xmm0, xmm0, eax
	movzx	eax, BYTE PTR [r8-4]
	vsubss	xmm1, xmm0, xmm7
	vfmadd231ss xmm3, xmm1, xmm1
	vxorps	xmm0, xmm0, xmm0
	vcvtsi2ss xmm0, xmm0, eax
	movzx	eax, BYTE PTR [r8-3]
	vsubss	xmm1, xmm0, xmm7
	vfmadd231ss xmm3, xmm1, xmm1
	vxorps	xmm0, xmm0, xmm0
	vcvtsi2ss xmm0, xmm0, eax
	vsubss	xmm1, xmm0, xmm7
	vfmadd231ss xmm3, xmm1, xmm1
	sub	rdx, 1
	jne	SHORT $LL31@pixel_var_

; 1603 : 
; 1604 :   __m128  accum8   = _mm256_castps256_ps128(accum7);
; 1605 :   float   var_sum  = _mm_cvtss_f32         (accum8);
; 1606 : 
; 1607 :   // Remaining len mod 32 pixels
; 1608 :   for (; i < len; ++i) {

	cmp	r9, r10
	jae	SHORT $LN29@pixel_var_
	npad	9
$LL43@pixel_var_:

; 1609 :     float diff = buf[i] - mean_f;

	movzx	eax, BYTE PTR [r9+rcx]
	inc	r9
	vxorps	xmm0, xmm0, xmm0
	vcvtsi2ss xmm0, xmm0, eax
	vsubss	xmm1, xmm0, xmm7

; 1610 :     var_sum += diff * diff;

	vfmadd231ss xmm3, xmm1, xmm1
	cmp	r9, r10
	jb	SHORT $LL43@pixel_var_
$LN29@pixel_var_:

; 1611 :   }
; 1612 : 
; 1613 :   return  var_sum / len_f;

	vmulss	xmm0, xmm10, xmm3
	vcvtss2sd xmm0, xmm0, xmm0
	vzeroupper

; 1614 : }

	vmovaps	xmm7, XMMWORD PTR [rsp+48]
	vmovaps	xmm10, XMMWORD PTR [rsp]
	add	rsp, 88					; 00000058H
	ret	0
pixel_var_avx2_largebuf ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\picture-avx2.c
_TEXT	SEGMENT
buf$ = 8
len$ = 16
pixel_var_avx2 PROC

; 1707 :   return pixel_var_avx2_largebuf(buf, len);

	jmp	pixel_var_avx2_largebuf
pixel_var_avx2 ENDP
_TEXT	ENDS
END
