; Listing generated by Microsoft (R) Optimizing Compiler Version 19.26.28806.0 

include listing.inc

INCLUDELIB OLDNAMES

cbf_masks DW	01fH
	DW	0fH
	DW	07H
	DW	03H
	DW	01H
	ORG $+6
g_sig_last_scan_16x16 DD 00H
	DD	04H
	DD	01H
	DD	08H
	DD	05H
	DD	02H
	DD	0cH
	DD	09H
	DD	06H
	DD	03H
	DD	0dH
	DD	0aH
	DD	07H
	DD	0eH
	DD	0bH
	DD	0fH
g_group_idx DB	00H
	DB	01H
	DB	02H
	DB	03H
	DB	04H
	DB	04H
	DB	05H
	DB	05H
	DB	06H
	DB	06H
	DB	06H
	DB	06H
	DB	07H
	DB	07H
	DB	07H
	DB	07H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
strategies_to_select DQ FLAT:$SG4294950327
	DQ	FLAT:kvz_array_checksum
	DQ	FLAT:$SG4294950326
	DQ	FLAT:kvz_array_md5
	DQ	FLAT:$SG4294950325
	DQ	FLAT:kvz_reg_sad
	DQ	FLAT:$SG4294950324
	DQ	FLAT:kvz_sad_4x4
	DQ	FLAT:$SG4294950323
	DQ	FLAT:kvz_sad_8x8
	DQ	FLAT:$SG4294950322
	DQ	FLAT:kvz_sad_16x16
	DQ	FLAT:$SG4294950321
	DQ	FLAT:kvz_sad_32x32
	DQ	FLAT:$SG4294950320
	DQ	FLAT:kvz_sad_64x64
	DQ	FLAT:$SG4294950319
	DQ	FLAT:kvz_satd_4x4
	DQ	FLAT:$SG4294950318
	DQ	FLAT:kvz_satd_8x8
	DQ	FLAT:$SG4294950317
	DQ	FLAT:kvz_satd_16x16
	DQ	FLAT:$SG4294950316
	DQ	FLAT:kvz_satd_32x32
	DQ	FLAT:$SG4294950315
	DQ	FLAT:kvz_satd_64x64
	DQ	FLAT:$SG4294950314
	DQ	FLAT:kvz_satd_any_size
	DQ	FLAT:$SG4294950313
	DQ	FLAT:kvz_sad_4x4_dual
	DQ	FLAT:$SG4294950312
	DQ	FLAT:kvz_sad_8x8_dual
	DQ	FLAT:$SG4294950311
	DQ	FLAT:kvz_sad_16x16_dual
	DQ	FLAT:$SG4294950310
	DQ	FLAT:kvz_sad_32x32_dual
	DQ	FLAT:$SG4294950309
	DQ	FLAT:kvz_sad_64x64_dual
	DQ	FLAT:$SG4294950308
	DQ	FLAT:kvz_satd_4x4_dual
	DQ	FLAT:$SG4294950307
	DQ	FLAT:kvz_satd_8x8_dual
	DQ	FLAT:$SG4294950306
	DQ	FLAT:kvz_satd_16x16_dual
	DQ	FLAT:$SG4294950305
	DQ	FLAT:kvz_satd_32x32_dual
	DQ	FLAT:$SG4294950304
	DQ	FLAT:kvz_satd_64x64_dual
	DQ	FLAT:$SG4294950303
	DQ	FLAT:kvz_satd_any_size_quad
	DQ	FLAT:$SG4294950302
	DQ	FLAT:kvz_pixels_calc_ssd
	DQ	FLAT:$SG4294950301
	DQ	FLAT:kvz_bipred_average
	DQ	FLAT:$SG4294950300
	DQ	FLAT:kvz_get_optimized_sad
	DQ	FLAT:$SG4294950299
	DQ	FLAT:kvz_ver_sad
	DQ	FLAT:$SG4294950298
	DQ	FLAT:kvz_hor_sad
	DQ	FLAT:$SG4294950297
	DQ	FLAT:kvz_pixel_var
	DQ	FLAT:$SG4294950296
	DQ	FLAT:kvz_fast_forward_dst_4x4
	DQ	FLAT:$SG4294950295
	DQ	FLAT:kvz_dct_4x4
	DQ	FLAT:$SG4294950294
	DQ	FLAT:kvz_dct_8x8
	DQ	FLAT:$SG4294950293
	DQ	FLAT:kvz_dct_16x16
	DQ	FLAT:$SG4294950292
	DQ	FLAT:kvz_dct_32x32
	DQ	FLAT:$SG4294950291
	DQ	FLAT:kvz_fast_inverse_dst_4x4
	DQ	FLAT:$SG4294950290
	DQ	FLAT:kvz_idct_4x4
	DQ	FLAT:$SG4294950289
	DQ	FLAT:kvz_idct_8x8
	DQ	FLAT:$SG4294950288
	DQ	FLAT:kvz_idct_16x16
	DQ	FLAT:$SG4294950287
	DQ	FLAT:kvz_idct_32x32
	DQ	FLAT:$SG4294950286
	DQ	FLAT:kvz_filter_hpel_blocks_hor_ver_luma
	DQ	FLAT:$SG4294950285
	DQ	FLAT:kvz_filter_hpel_blocks_diag_luma
	DQ	FLAT:$SG4294950284
	DQ	FLAT:kvz_filter_qpel_blocks_hor_ver_luma
	DQ	FLAT:$SG4294950283
	DQ	FLAT:kvz_filter_qpel_blocks_diag_luma
	DQ	FLAT:$SG4294950282
	DQ	FLAT:kvz_sample_quarterpel_luma
	DQ	FLAT:$SG4294950281
	DQ	FLAT:kvz_sample_octpel_chroma
	DQ	FLAT:$SG4294950280
	DQ	FLAT:kvz_sample_quarterpel_luma_hi
	DQ	FLAT:$SG4294950279
	DQ	FLAT:kvz_sample_octpel_chroma_hi
	DQ	FLAT:$SG4294950278
	DQ	FLAT:kvz_get_extended_block
	DQ	FLAT:$SG4294950277
	DQ	FLAT:kvz_quant
	DQ	FLAT:$SG4294950276
	DQ	FLAT:kvz_quantize_residual
	DQ	FLAT:$SG4294950275
	DQ	FLAT:kvz_dequant
	DQ	FLAT:$SG4294950274
	DQ	FLAT:kvz_coeff_abs_sum
	DQ	FLAT:$SG4294950273
	DQ	FLAT:kvz_fast_coeff_cost
	DQ	FLAT:$SG4294950272
	DQ	FLAT:kvz_angular_pred
	DQ	FLAT:$SG4294950271
	DQ	FLAT:kvz_intra_pred_planar
	DQ	FLAT:$SG4294950270
	DQ	FLAT:kvz_intra_pred_filtered_dc
	DQ	FLAT:$SG4294950269
	DQ	FLAT:kvz_sao_edge_ddistortion
	DQ	FLAT:$SG4294950268
	DQ	FLAT:kvz_calc_sao_edge_dir
	DQ	FLAT:$SG4294950267
	DQ	FLAT:kvz_sao_reconstruct_color
	DQ	FLAT:$SG4294950266
	DQ	FLAT:kvz_sao_band_ddistortion
	DQ	FLAT:$SG4294950265
	DQ	FLAT:kvz_encode_coeff_nxn
	DQ	0000000000000000H
	DQ	0000000000000000H
g_sig_last_scan_32x32 DD 00H
	DD	08H
	DD	01H
	DD	010H
	DD	09H
	DD	02H
	DD	018H
	DD	011H
	DD	0aH
	DD	03H
	DD	020H
	DD	019H
	DD	012H
	DD	0bH
	DD	04H
	DD	028H
	DD	021H
	DD	01aH
	DD	013H
	DD	0cH
	DD	05H
	DD	030H
	DD	029H
	DD	022H
	DD	01bH
	DD	014H
	DD	0dH
	DD	06H
	DD	038H
	DD	031H
	DD	02aH
	DD	023H
	DD	01cH
	DD	015H
	DD	0eH
	DD	07H
	DD	039H
	DD	032H
	DD	02bH
	DD	024H
	DD	01dH
	DD	016H
	DD	0fH
	DD	03aH
	DD	033H
	DD	02cH
	DD	025H
	DD	01eH
	DD	017H
	DD	03bH
	DD	034H
	DD	02dH
	DD	026H
	DD	01fH
	DD	03cH
	DD	035H
	DD	02eH
	DD	027H
	DD	03dH
	DD	036H
	DD	02fH
	DD	03eH
	DD	037H
	DD	03fH
g_min_in_group DB 00H
	DB	01H
	DB	02H
	DB	03H
	DB	04H
	DB	06H
	DB	08H
	DB	0cH
	DB	010H
	DB	018H
	ORG $+6
default_fast_coeff_cost_wts DD 03e282e88r	; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e26c094r			; 0.162844
	DD	04081ca43r			; 4.05594
	DD	04064203ar			; 3.56447
	DD	040db915ar			; 6.86149
	DD	03e03d189r			; 0.128729
	DD	04089fbafr			; 4.31197
	DD	0407c5771r			; 3.94284
	DD	040ddeed2r			; 6.9354
	DD	03de33ce6r			; 0.110956
	DD	0408ddcb1r			; 4.43319
	DD	0407c8738r			; 3.94575
	DD	040dc1618r			; 6.8777
	DD	03dc29cfer			; 0.095026
	DD	0408f7938r			; 4.48355
	DD	0408636aar			; 4.19417
	DD	040d90260r			; 6.78154
	DD	03d99b1b8r			; 0.075046
	DD	04094474cr			; 4.6337
	DD	04082b1b6r			; 4.08419
	DD	040d65aeer			; 6.6986
	DD	03d56bca5r			; 0.052426
	DD	0409ef37er			; 4.96722
	DD	04080dee8r			; 4.02721
	DD	040d19306r			; 6.5492
	DD	03d24bcaer			; 0.040219
	DD	040a489car			; 5.14182
	DD	0407ee3bdr			; 3.98265
	DD	040cec513r			; 6.46156
	DD	03d0fba88r			; 0.03509
	DD	040a628e7r			; 5.19249
	DD	040752e49r			; 3.83095
	DD	040cd642ar			; 6.41848
	DD	03cf47d80r			; 0.029845
	DD	040a6c5d0r			; 5.21165
	DD	040743073r			; 3.81546
	DD	040cb0dd8r			; 6.34544
	DD	03cc0b136r			; 0.023522
	DD	040aa4f92r			; 5.32221
	DD	040744224r			; 3.81654
	DD	040cb8aaar			; 6.36068
	DD	03cae87d3r			; 0.021305
	DD	040a73ac3r			; 5.22592
	DD	04075eeccr			; 3.8427
	DD	040ca6cd9r			; 6.32579
	DD	03c821294r			; 0.015878
	DD	040a5dbe0r			; 5.18309
	DD	0407d2f27r			; 3.956
	DD	040ca8cbdr			; 6.32968
	DD	03c2ae297r			; 0.01043
	DD	040a32ce4r			; 5.09923
	DD	04085a85fr			; 4.1768
	DD	040c9c5d6r			; 6.3054
	DD	03c0a2a91r			; 0.008433
	DD	040a0f7der			; 5.03026
	DD	040879a50r			; 4.23759
	DD	040c8a4eer			; 6.27013
	DD	03bd4fdf4r			; 0.0065
	DD	0409f0412r			; 4.96925
	DD	0408adc57r			; 4.3394
	DD	040c6f870r			; 6.21783
	DD	03ba18373r			; 0.004929
	DD	0409d8d50r			; 4.9235
	DD	0408e283fr			; 4.44241
	DD	040c5df6cr			; 6.18352
	DD	03b73775cr			; 0.003715
	DD	0409d4c75r			; 4.91558
	DD	0408dbb1br			; 4.42909
	DD	040c4029fr			; 6.12532
	DD	03b4a70d2r			; 0.003089
	DD	0409c48f7r			; 4.88391
	DD	040920260r			; 4.56279
	DD	040c5019dr			; 6.15645
	DD	03b219c9dr			; 0.002466
	DD	0409c31abr			; 4.88106
	DD	040942800r			; 4.62988
	DD	040c49088r			; 6.14264
	DD	03b0e25c8r			; 0.002169
	DD	0409c3d62r			; 4.88249
	DD	04094ae99r			; 4.64631
	DD	040c415d1r			; 6.12766
	DD	03b26dacbr			; 0.002546
	DD	040996304r			; 4.79334
	DD	0409acc16r			; 4.83741
	DD	040c6606br			; 6.19927
	DD	03aac3a86r			; 0.001314
	DD	04099e220r			; 4.80885
	DD	0409a81bdr			; 4.82834
	DD	040c7ca3cr			; 6.24344
	DD	03a9741d1r			; 0.001154
	DD	0409b9a72r			; 4.8626
	DD	0409b19aar			; 4.84688
	DD	040c693a5r			; 6.20552
	DD	03a80f990r			; 0.000984
	DD	0409bb993r			; 4.8664
	DD	0409b7fa2r			; 4.85933
	DD	040c7b565r			; 6.24089
	DD	03a551f82r			; 0.000813
	DD	0409b698ar			; 4.85663
	DD	0409d95bar			; 4.92453
	DD	040c963a4r			; 6.29341
	DD	03a91c087r			; 0.001112
	DD	04099419er			; 4.78926
	DD	040a050f0r			; 5.00988
	DD	040cddf8fr			; 6.43354
	DD	03a10b418r			; 0.000552
	DD	04098580ar			; 4.76075
	DD	040a2e4f1r			; 5.09045
	DD	040d32e1fr			; 6.59938
	DD	039ccff22r			; 0.000391
	DD	0409ec42cr			; 4.96145
	DD	040a38d95r			; 5.11103
	DD	040d8342fr			; 6.75637
	DD	039ae1049r			; 0.000332
	DD	0409f63f8r			; 4.98095
	DD	040a46b89r			; 5.13813
	DD	040dbc1e8r			; 6.86742
	DD	03952c387r			; 0.000201
	DD	040a5d297r			; 5.18196
	DD	04097af64r			; 4.74016
	DD	040cec07dr			; 6.461
	DD	0397ba882r			; 0.00024
	DD	040a5eeb7r			; 5.18539
	DD	0409bfeb0r			; 4.87484
	DD	040da3603r			; 6.81909
	DD	03908509cr			; 0.00013
	DD	040a8a6b5r			; 5.27035
	DD	040977eacr			; 4.73421
	DD	040da708fr			; 6.82624
	DD	038da1a93r			; 0.000104
	DD	040abe6e8r			; 5.37194
	DD	040930af4r			; 4.59509
	DD	040d5189ar			; 6.65925
	DD	038ae1049r			; 8.3e-05
	DD	040ab9581r			; 5.362
	DD	04093c250r			; 4.61747
	DD	040dacf03r			; 6.83777
	DD	03890b418r			; 6.9e-05
	DD	040a926e3r			; 5.286
	DD	0409828e7r			; 4.75499
	DD	040e516e1r			; 7.15904
	DD	0384d8559r			; 4.9e-05
	DD	040afa18cr			; 5.48847
	DD	0408cace9r			; 4.39611
	DD	040d74682r			; 6.72736
	DD	038734507r			; 5.8e-05
	DD	0409eafa3r			; 4.95894
	DD	040929321r			; 4.58046
	DD	040cf49a5r			; 6.47774
	DD	037eae18br			; 2.8e-05
	DD	040b0ae1br			; 5.52125
	DD	0408e1885r			; 4.44049
	DD	040e68f80r			; 7.20502
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	0379f6230r			; 1.9e-05
	DD	040b9f5d8r			; 5.81126
	DD	0408cc582r			; 4.39911
	DD	040eac30dr			; 7.33631
g_sao_edge_offsets DD 0ffffffffH
	DD	00H
	DD	01H
	DD	00H
	DD	00H
	DD	0ffffffffH
	DD	00H
	DD	01H
	DD	0ffffffffH
	DD	0ffffffffH
	DD	01H
	DD	01H
	DD	01H
	DD	0ffffffffH
	DD	0ffffffffH
	DD	01H
g_sig_last_scan_cg DQ FLAT:g_sig_last_scan_8x8
	DQ	FLAT:g_sig_last_scan_8x8+16
	DQ	FLAT:g_sig_last_scan_8x8+32
	DQ	FLAT:g_sig_last_scan_8x8
	DQ	FLAT:g_sig_last_scan_8x8+16
	DQ	FLAT:g_sig_last_scan_8x8+32
	DQ	FLAT:g_sig_last_scan_16x16
	DQ	0000000000000000H
	DQ	0000000000000000H
	DQ	FLAT:g_sig_last_scan_32x32
	DQ	0000000000000000H
	DQ	0000000000000000H
g_sig_last_scan_8x8 DD 00H
	DD	02H
	DD	01H
	DD	03H
	DD	00H
	DD	01H
	DD	02H
	DD	03H
	DD	00H
	DD	02H
	DD	01H
	DD	03H
	ORG $+3
$SG4294950277 DB 'quant', 00H
	ORG $+7
$SG4294950327 DB 'array_checksum', 00H
	ORG $+1
$SG4294950326 DB 'array_md5', 00H
	ORG $+6
$SG4294950325 DB 'reg_sad', 00H
$SG4294950324 DB 'sad_4x4', 00H
$SG4294950323 DB 'sad_8x8', 00H
$SG4294950322 DB 'sad_16x16', 00H
	ORG $+6
$SG4294950321 DB 'sad_32x32', 00H
	ORG $+6
$SG4294950320 DB 'sad_64x64', 00H
	ORG $+6
$SG4294950319 DB 'satd_4x4', 00H
	ORG $+7
$SG4294950318 DB 'satd_8x8', 00H
	ORG $+7
$SG4294950317 DB 'satd_16x16', 00H
	ORG $+5
$SG4294950316 DB 'satd_32x32', 00H
	ORG $+5
$SG4294950315 DB 'satd_64x64', 00H
	ORG $+5
$SG4294950314 DB 'satd_any_size', 00H
	ORG $+2
$SG4294950313 DB 'sad_4x4_dual', 00H
	ORG $+3
$SG4294950312 DB 'sad_8x8_dual', 00H
	ORG $+3
$SG4294950311 DB 'sad_16x16_dual', 00H
	ORG $+1
$SG4294950310 DB 'sad_32x32_dual', 00H
	ORG $+1
$SG4294950309 DB 'sad_64x64_dual', 00H
	ORG $+1
$SG4294950308 DB 'satd_4x4_dual', 00H
	ORG $+2
$SG4294950307 DB 'satd_8x8_dual', 00H
	ORG $+2
$SG4294950306 DB 'satd_16x16_dual', 00H
$SG4294950305 DB 'satd_32x32_dual', 00H
$SG4294950304 DB 'satd_64x64_dual', 00H
$SG4294950303 DB 'satd_any_size_quad', 00H
	ORG $+5
$SG4294950302 DB 'pixels_calc_ssd', 00H
$SG4294950301 DB 'bipred_average', 00H
	ORG $+1
$SG4294950300 DB 'get_optimized_sad', 00H
	ORG $+6
$SG4294950299 DB 'ver_sad', 00H
$SG4294950298 DB 'hor_sad', 00H
$SG4294950297 DB 'pixel_var', 00H
	ORG $+6
$SG4294950296 DB 'fast_forward_dst_4x4', 00H
	ORG $+3
$SG4294950295 DB 'dct_4x4', 00H
$SG4294950294 DB 'dct_8x8', 00H
$SG4294950293 DB 'dct_16x16', 00H
	ORG $+6
$SG4294950292 DB 'dct_32x32', 00H
	ORG $+6
$SG4294950291 DB 'fast_inverse_dst_4x4', 00H
	ORG $+3
$SG4294950290 DB 'idct_4x4', 00H
	ORG $+7
$SG4294950289 DB 'idct_8x8', 00H
	ORG $+7
$SG4294950288 DB 'idct_16x16', 00H
	ORG $+5
$SG4294950287 DB 'idct_32x32', 00H
	ORG $+5
$SG4294950286 DB 'filter_hpel_blocks_hor_ver_luma', 00H
$SG4294950285 DB 'filter_hpel_blocks_diag_luma', 00H
	ORG $+3
$SG4294950284 DB 'filter_qpel_blocks_hor_ver_luma', 00H
$SG4294950283 DB 'filter_qpel_blocks_diag_luma', 00H
	ORG $+3
$SG4294950282 DB 'sample_quarterpel_luma', 00H
	ORG $+1
$SG4294950281 DB 'sample_octpel_chroma', 00H
	ORG $+3
$SG4294950280 DB 'sample_quarterpel_luma_hi', 00H
	ORG $+6
$SG4294950279 DB 'sample_octpel_chroma_hi', 00H
$SG4294950278 DB 'get_extended_block', 00H
	ORG $+5
$SG4294950276 DB 'quantize_residual', 00H
	ORG $+6
$SG4294950275 DB 'dequant', 00H
$SG4294950274 DB 'coeff_abs_sum', 00H
	ORG $+2
$SG4294950273 DB 'fast_coeff_cost', 00H
$SG4294950272 DB 'angular_pred', 00H
	ORG $+3
$SG4294950271 DB 'intra_pred_planar', 00H
	ORG $+6
$SG4294950270 DB 'intra_pred_filtered_dc', 00H
	ORG $+1
$SG4294950269 DB 'sao_edge_ddistortion', 00H
	ORG $+3
$SG4294950268 DB 'calc_sao_edge_dir', 00H
	ORG $+6
$SG4294950267 DB 'sao_reconstruct_color', 00H
	ORG $+2
$SG4294950266 DB 'sao_band_ddistortion', 00H
	ORG $+3
$SG4294950265 DB 'encode_coeff_nxn', 00H
PUBLIC	kvz_dequant_avx2
PUBLIC	kvz_quantize_residual_avx2
PUBLIC	kvz_quant_avx2
PUBLIC	kvz_strategy_register_quant_avx2
pdata	SEGMENT
$pdata$fast_coeff_cost_avx2 DD imagerel fast_coeff_cost_avx2
	DD	imagerel fast_coeff_cost_avx2+299
	DD	imagerel $unwind$fast_coeff_cost_avx2
$pdata$coeff_abs_sum_avx2 DD imagerel coeff_abs_sum_avx2
	DD	imagerel coeff_abs_sum_avx2+161
	DD	imagerel $unwind$coeff_abs_sum_avx2
$pdata$kvz_dequant_avx2 DD imagerel $LN78
	DD	imagerel $LN78+54
	DD	imagerel $unwind$kvz_dequant_avx2
$pdata$0$kvz_dequant_avx2 DD imagerel $LN78+54
	DD	imagerel $LN78+239
	DD	imagerel $chain$0$kvz_dequant_avx2
$pdata$1$kvz_dequant_avx2 DD imagerel $LN78+239
	DD	imagerel $LN78+422
	DD	imagerel $chain$1$kvz_dequant_avx2
$pdata$2$kvz_dequant_avx2 DD imagerel $LN78+422
	DD	imagerel $LN78+444
	DD	imagerel $chain$2$kvz_dequant_avx2
$pdata$3$kvz_dequant_avx2 DD imagerel $LN78+444
	DD	imagerel $LN78+556
	DD	imagerel $chain$3$kvz_dequant_avx2
$pdata$5$kvz_dequant_avx2 DD imagerel $LN78+556
	DD	imagerel $LN78+618
	DD	imagerel $chain$5$kvz_dequant_avx2
$pdata$6$kvz_dequant_avx2 DD imagerel $LN78+618
	DD	imagerel $LN78+738
	DD	imagerel $chain$6$kvz_dequant_avx2
$pdata$7$kvz_dequant_avx2 DD imagerel $LN78+738
	DD	imagerel $LN78+749
	DD	imagerel $chain$7$kvz_dequant_avx2
$pdata$8$kvz_dequant_avx2 DD imagerel $LN78+749
	DD	imagerel $LN78+778
	DD	imagerel $chain$8$kvz_dequant_avx2
$pdata$kvz_quantize_residual_avx2 DD imagerel $LN107
	DD	imagerel $LN107+1198
	DD	imagerel $unwind$kvz_quantize_residual_avx2
$pdata$get_quantized_recon_avx2 DD imagerel get_quantized_recon_avx2
	DD	imagerel get_quantized_recon_avx2+50
	DD	imagerel $unwind$get_quantized_recon_avx2
$pdata$5$get_quantized_recon_avx2 DD imagerel get_quantized_recon_avx2+50
	DD	imagerel get_quantized_recon_avx2+262
	DD	imagerel $chain$5$get_quantized_recon_avx2
$pdata$6$get_quantized_recon_avx2 DD imagerel get_quantized_recon_avx2+262
	DD	imagerel get_quantized_recon_avx2+730
	DD	imagerel $chain$6$get_quantized_recon_avx2
$pdata$get_residual_avx2 DD imagerel get_residual_avx2
	DD	imagerel get_residual_avx2+45
	DD	imagerel $unwind$get_residual_avx2
$pdata$2$get_residual_avx2 DD imagerel get_residual_avx2+45
	DD	imagerel get_residual_avx2+222
	DD	imagerel $chain$2$get_residual_avx2
$pdata$3$get_residual_avx2 DD imagerel get_residual_avx2+222
	DD	imagerel get_residual_avx2+654
	DD	imagerel $chain$3$get_residual_avx2
$pdata$kvz_quant_avx2 DD imagerel $LN76
	DD	imagerel $LN76+2625
	DD	imagerel $unwind$kvz_quant_avx2
$pdata$hide_block_sign DD imagerel hide_block_sign
	DD	imagerel hide_block_sign+205
	DD	imagerel $unwind$hide_block_sign
$pdata$7$hide_block_sign DD imagerel hide_block_sign+205
	DD	imagerel hide_block_sign+865
	DD	imagerel $chain$7$hide_block_sign
$pdata$8$hide_block_sign DD imagerel hide_block_sign+865
	DD	imagerel hide_block_sign+909
	DD	imagerel $chain$8$hide_block_sign
$pdata$9$hide_block_sign DD imagerel hide_block_sign+909
	DD	imagerel hide_block_sign+955
	DD	imagerel $chain$9$hide_block_sign
$pdata$scanord_read_vector_32 DD imagerel scanord_read_vector_32
	DD	imagerel scanord_read_vector_32+341
	DD	imagerel $unwind$scanord_read_vector_32
$pdata$get_cheapest_alternative DD imagerel get_cheapest_alternative
	DD	imagerel get_cheapest_alternative+232
	DD	imagerel $unwind$get_cheapest_alternative
$pdata$scanord_read_vector DD imagerel scanord_read_vector
	DD	imagerel scanord_read_vector+422
	DD	imagerel $unwind$scanord_read_vector
$pdata$kvz_strategy_register_quant_avx2 DD imagerel $LN5
	DD	imagerel $LN5+247
	DD	imagerel $unwind$kvz_strategy_register_quant_avx2
xdata	SEGMENT
$unwind$fast_coeff_cost_avx2 DD 0b3901H
	DD	01a839H
	DD	029834H
	DD	038827H
	DD	047822H
	DD	05681dH
	DD	0c20bH
$unwind$coeff_abs_sum_avx2 DD 040a01H
	DD	06340aH
	DD	07006320aH
$unwind$kvz_dequant_avx2 DD 0a1901H
	DD	0d7419H
	DD	0c6419H
	DD	0b3419H
	DD	0f0155219H
	DD	0d011e013H
$chain$0$kvz_dequant_avx2 DD 020521H
	DD	0ac405H
	DD	imagerel $LN78
	DD	imagerel $LN78+54
	DD	imagerel $unwind$kvz_dequant_avx2
$chain$1$kvz_dequant_avx2 DD 021H
	DD	imagerel $LN78
	DD	imagerel $LN78+54
	DD	imagerel $unwind$kvz_dequant_avx2
$chain$2$kvz_dequant_avx2 DD 020021H
	DD	0ac400H
	DD	imagerel $LN78
	DD	imagerel $LN78+54
	DD	imagerel $unwind$kvz_dequant_avx2
$chain$3$kvz_dequant_avx2 DD 021H
	DD	imagerel $LN78
	DD	imagerel $LN78+54
	DD	imagerel $unwind$kvz_dequant_avx2
$chain$5$kvz_dequant_avx2 DD 040b21H
	DD	0880bH
	DD	017806H
	DD	imagerel $LN78+444
	DD	imagerel $LN78+556
	DD	imagerel $chain$3$kvz_dequant_avx2
$chain$6$kvz_dequant_avx2 DD 020621H
	DD	026806H
	DD	imagerel $LN78+556
	DD	imagerel $LN78+618
	DD	imagerel $chain$5$kvz_dequant_avx2
$chain$7$kvz_dequant_avx2 DD 021H
	DD	imagerel $LN78+556
	DD	imagerel $LN78+618
	DD	imagerel $chain$5$kvz_dequant_avx2
$chain$8$kvz_dequant_avx2 DD 021H
	DD	imagerel $LN78+444
	DD	imagerel $LN78+556
	DD	imagerel $chain$3$kvz_dequant_avx2
$unwind$kvz_quantize_residual_avx2 DD 0d3601H
	DD	0223742aH
	DD	0222642aH
	DD	0220342aH
	DD	021a012aH
	DD	0e01bf01dH
	DD	0c017d019H
	DD	05015H
$unwind$get_quantized_recon_avx2 DD 030801H
	DD	0e0042208H
	DD	05002H
$chain$5$get_quantized_recon_avx2 DD 0c2c21H
	DD	0f42cH
	DD	01d425H
	DD	02c41bH
	DD	087414H
	DD	07640cH
	DD	063405H
	DD	imagerel get_quantized_recon_avx2
	DD	imagerel get_quantized_recon_avx2+50
	DD	imagerel $unwind$get_quantized_recon_avx2
$chain$6$get_quantized_recon_avx2 DD 021H
	DD	imagerel get_quantized_recon_avx2
	DD	imagerel get_quantized_recon_avx2+50
	DD	imagerel $unwind$get_quantized_recon_avx2
$unwind$get_residual_avx2 DD 030601H
	DD	056406H
	DD	07006H
$chain$2$get_residual_avx2 DD 061d21H
	DD	04e41dH
	DD	03540eH
	DD	023405H
	DD	imagerel get_residual_avx2
	DD	imagerel get_residual_avx2+45
	DD	imagerel $unwind$get_residual_avx2
$chain$3$get_residual_avx2 DD 021H
	DD	imagerel get_residual_avx2
	DD	imagerel get_residual_avx2+45
	DD	imagerel $unwind$get_residual_avx2
$unwind$kvz_quant_avx2 DD 0216d01H
	DD	024f864H
	DD	025e85cH
	DD	026d854H
	DD	027c84cH
	DD	028b844H
	DD	029a83cH
	DD	02a9837H
	DD	02b8832H
	DD	02c782dH
	DD	02d6828H
	DD	0647423H
	DD	0636423H
	DD	0623423H
	DD	05c0123H
	DD	0e01af01cH
	DD	0c016d018H
	DD	05014H
$unwind$hide_block_sign DD 0b4e01H
	DD	01e84eH
	DD	05a827H
	DD	0176412H
	DD	0163412H
	DD	0140112H
	DD	0700bH
$chain$7$hide_block_sign DD 0109d21H
	DD	0f89dH
	DD	02d84cH
	DD	03c842H
	DD	04b83cH
	DD	06982eH
	DD	078820H
	DD	087816H
	DD	096809H
	DD	imagerel hide_block_sign
	DD	imagerel hide_block_sign+205
	DD	imagerel $unwind$hide_block_sign
$chain$8$hide_block_sign DD 020021H
	DD	0f800H
	DD	imagerel hide_block_sign
	DD	imagerel hide_block_sign+205
	DD	imagerel $unwind$hide_block_sign
$chain$9$hide_block_sign DD 021H
	DD	imagerel hide_block_sign
	DD	imagerel hide_block_sign+205
	DD	imagerel $unwind$hide_block_sign
$unwind$scanord_read_vector_32 DD 051601H
	DD	036340dH
	DD	034010dH
	DD	05006H
$unwind$get_cheapest_alternative DD 052b01H
	DD	0782bH
	DD	016821H
	DD	04204H
$unwind$scanord_read_vector DD 0b7801H
	DD	0c9878H
	DD	0d8854H
	DD	0e7849H
	DD	0f682dH
	DD	020010bH
	DD	03004H
$unwind$kvz_strategy_register_quant_avx2 DD 060f01H
	DD	09640fH
	DD	08340fH
	DD	0700b520fH
	ORG $+3
$SG4294950247 DB 'fast_coeff_cost', 00H
$SG4294950248 DB 'avx2', 00H
	ORG $+3
$SG4294950249 DB 'coeff_abs_sum', 00H
	ORG $+2
$SG4294950250 DB 'avx2', 00H
	ORG $+3
$SG4294950251 DB 'quant', 00H
	ORG $+2
$SG4294950252 DB 'avx2', 00H
	ORG $+3
$SG4294950253 DB 'dequant', 00H
$SG4294950254 DB 'avx2', 00H
	ORG $+3
$SG4294950255 DB 'quantize_residual', 00H
	ORG $+2
$SG4294950256 DB 'avx2', 00H
	ORG $+7
$SG4294950257 DB 'l', 00H, 'e', 00H, 'n', 00H, 'g', 00H, 't', 00H, 'h', 00H
	DB	' ', 00H, '%', 00H, ' ', 00H, '8', 00H, ' ', 00H, '=', 00H, '='
	DB	00H, ' ', 00H, '0', 00H, 00H, 00H
$SG4294950258 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'q', 00H, 'u', 00H, 'a', 00H, 'n', 00H, 't'
	DB	00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H, '2', 00H, '.', 00H
	DB	'c', 00H, 00H, 00H
	ORG $+2
$SG4294950259 DB 00H, 03H, 01H, 02H, 00H
	ORG $+7
$SG4294950260 DB 'w', 00H, 'i', 00H, 'd', 00H, 't', 00H, 'h', 00H, ' ', 00H
	DB	'>', 00H, '=', 00H, ' ', 00H, 'T', 00H, 'R', 00H, '_', 00H, 'M'
	DB	00H, 'I', 00H, 'N', 00H, '_', 00H, 'W', 00H, 'I', 00H, 'D', 00H
	DB	'T', 00H, 'H', 00H, 00H, 00H
	ORG $+4
$SG4294950261 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'q', 00H, 'u', 00H, 'a', 00H, 'n', 00H, 't'
	DB	00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H, '2', 00H, '.', 00H
	DB	'c', 00H, 00H, 00H
	ORG $+6
$SG4294950262 DB 'w', 00H, 'i', 00H, 'd', 00H, 't', 00H, 'h', 00H, ' ', 00H
	DB	'<', 00H, '=', 00H, ' ', 00H, 'T', 00H, 'R', 00H, '_', 00H, 'M'
	DB	00H, 'A', 00H, 'X', 00H, '_', 00H, 'W', 00H, 'I', 00H, 'D', 00H
	DB	'T', 00H, 'H', 00H, 00H, 00H
$SG4294950264 DB 00H, 03H, 01H, 02H, 00H
	ORG $+7
$SG4294950263 DB 'F', 00H, ':', 00H, '\', 00H, 'o', 00H, 'p', 00H, 'e', 00H
	DB	'n', 00H, '_', 00H, 'c', 00H, 'o', 00H, 'd', 00H, 'e', 00H, 'c'
	DB	00H, '_', 00H, 'l', 00H, 'e', 00H, 'a', 00H, 'r', 00H, 'n', 00H
	DB	'_', 00H, '2', 00H, '0', 00H, '2', 00H, '1', 00H, '\', 00H, 'k'
	DB	00H, 'v', 00H, 'a', 00H, 'z', 00H, 'a', 00H, 'a', 00H, 'r', 00H
	DB	'-', 00H, 'm', 00H, 'a', 00H, 's', 00H, 't', 00H, 'e', 00H, 'r'
	DB	00H, '\', 00H, 's', 00H, 'r', 00H, 'c', 00H, '\', 00H, 's', 00H
	DB	't', 00H, 'r', 00H, 'a', 00H, 't', 00H, 'e', 00H, 'g', 00H, 'i'
	DB	00H, 'e', 00H, 's', 00H, '\', 00H, 'a', 00H, 'v', 00H, 'x', 00H
	DB	'2', 00H, '\', 00H, 'q', 00H, 'u', 00H, 'a', 00H, 'n', 00H, 't'
	DB	00H, '-', 00H, 'a', 00H, 'v', 00H, 'x', 00H, '2', 00H, '.', 00H
	DB	'c', 00H, 00H, 00H
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
opaque$ = 64
bitdepth$dead$ = 72
kvz_strategy_register_quant_avx2 PROC

; 868  : {

$LN5:
	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rsi
	push	rdi
	sub	rsp, 48					; 00000030H

; 869  :   bool success = true;
; 870  : 
; 871  : #if COMPILE_INTEL_AVX2 && defined X86_64
; 872  : #if KVZ_BIT_DEPTH == 8
; 873  :   if (bitdepth == 8) {
; 874  :     success &= kvz_strategyselector_register(opaque, "quantize_residual", "avx2", 40, &kvz_quantize_residual_avx2);

	lea	rax, OFFSET FLAT:kvz_quantize_residual_avx2
	mov	r9d, 40					; 00000028H
	lea	r8, OFFSET FLAT:$SG4294950256
	mov	QWORD PTR [rsp+32], rax
	lea	rdx, OFFSET FLAT:$SG4294950255
	mov	rsi, rcx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 875  :     success &= kvz_strategyselector_register(opaque, "dequant", "avx2", 40, &kvz_dequant_avx2);

	lea	r8, OFFSET FLAT:$SG4294950254
	lea	rax, OFFSET FLAT:kvz_dequant_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294950253
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	ebx, 1
	call	kvz_strategyselector_register
	mov	edi, eax

; 876  :   }
; 877  : #endif // KVZ_BIT_DEPTH == 8
; 878  :   success &= kvz_strategyselector_register(opaque, "quant", "avx2", 40, &kvz_quant_avx2);

	lea	r8, OFFSET FLAT:$SG4294950252
	lea	rax, OFFSET FLAT:kvz_quant_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294950251
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	edi, ebx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 879  :   success &= kvz_strategyselector_register(opaque, "coeff_abs_sum", "avx2", 0, &coeff_abs_sum_avx2);

	lea	r8, OFFSET FLAT:$SG4294950250
	lea	rax, OFFSET FLAT:coeff_abs_sum_avx2
	xor	r9d, r9d
	lea	rdx, OFFSET FLAT:$SG4294950249
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	ebx, edi
	call	kvz_strategyselector_register
	mov	edi, eax

; 880  :   success &= kvz_strategyselector_register(opaque, "fast_coeff_cost", "avx2", 40, &fast_coeff_cost_avx2);

	lea	r8, OFFSET FLAT:$SG4294950248
	lea	rax, OFFSET FLAT:fast_coeff_cost_avx2
	mov	r9d, 40					; 00000028H
	lea	rdx, OFFSET FLAT:$SG4294950247
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	edi, ebx
	call	kvz_strategyselector_register

; 881  : #endif //COMPILE_INTEL_AVX2 && defined X86_64
; 882  : 
; 883  :   return success;
; 884  : }

	mov	rbx, QWORD PTR [rsp+64]
	and	eax, edi
	mov	rsi, QWORD PTR [rsp+72]
	add	rsp, 48					; 00000030H
	pop	rdi
	ret	0
kvz_strategy_register_quant_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
_TEXT	SEGMENT
low128_shuffle_masks$ = 0
blend_masks$ = 48
invec_rearr_masks_lower$ = 96
invec_rearr_masks_upper$ = 144
coeffs$ = 272
scan$ = 280
scan_mode$ = 288
subpos$ = 296
width$ = 304
result_vecs$ = 312
n_bufs$dead$ = 320
scanord_read_vector PROC

; 71   : {

	mov	rax, rsp
	push	rbx
	sub	rsp, 256				; 00000100H

; 72   :   // For vectorized reordering of coef and q_coef
; 73   :   const __m128i low128_shuffle_masks[3] = {

	vmovdqu	xmm2, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	vmovdqu	xmm0, XMMWORD PTR __xmm@030209080f0e070601000d0c05040b0a

; 74   :     _mm_setr_epi8(10,11,  4, 5, 12,13,  0, 1,  6, 7, 14,15,  8, 9,  2, 3),
; 75   :     _mm_setr_epi8( 0, 1,  2, 3,  4, 5,  6, 7,  8, 9, 10,11, 12,13, 14,15),
; 76   :     _mm_setr_epi8( 4, 5,  6, 7,  0, 1,  2, 3, 12,13, 14,15,  8, 9, 10,11),
; 77   :   };
; 78   : 
; 79   :   const __m128i blend_masks[3] = {

	vmovdqu	xmm1, XMMWORD PTR __xmm@ffffffff00000000ffff000000000000

; 80   :     _mm_setr_epi16( 0,  0,  0, -1,  0,  0, -1, -1),
; 81   :     _mm_setr_epi16( 0,  0,  0,  0,  0,  0,  0,  0),
; 82   :     _mm_setr_epi16( 0,  0, -1, -1,  0,  0, -1, -1),
; 83   :   };
; 84   : 
; 85   :   const __m128i invec_rearr_masks_upper[3] = {

	vmovdqu	XMMWORD PTR [rax-104], xmm2
	vmovaps	XMMWORD PTR [rax-24], xmm6
	mov	rbx, rcx

; 86   :     _mm_setr_epi8( 0, 1,  8, 9,  2, 3,  6, 7, 10,11,  4, 5, 12,13, 14,15),
; 87   :     _mm_setr_epi8( 0, 1,  2, 3,  4, 5,  6, 7,  8, 9, 10,11, 12,13, 14,15),
; 88   :     _mm_setr_epi8( 0, 1,  8, 9,  4, 5, 12,13,  2, 3, 10,11,  6, 7, 14,15),
; 89   :   };
; 90   : 
; 91   :   const __m128i invec_rearr_masks_lower[3] = {
; 92   :     _mm_setr_epi8(12,13,  6, 7,  0, 1,  2, 3, 14,15,  4, 5,  8, 9, 10,11),
; 93   :     _mm_setr_epi8( 0, 1,  2, 3,  4, 5,  6, 7,  8, 9, 10,11, 12,13, 14,15),
; 94   :     _mm_setr_epi8( 4, 5, 12,13,  0, 1,  8, 9,  6, 7, 14,15,  2, 3, 10,11),
; 95   :   };
; 96   : 
; 97   :   const size_t row_offsets[4] = {

	mov	ecx, DWORD PTR width$[rsp]
	vmovdqu	XMMWORD PTR low128_shuffle_masks$[rsp], xmm0
	vmovdqu	xmm0, XMMWORD PTR __xmm@0b0a09080f0e0d0c0302010007060504
	vmovaps	XMMWORD PTR [rax-40], xmm7
	vmovdqu	XMMWORD PTR low128_shuffle_masks$[rsp+32], xmm0
	vmovaps	XMMWORD PTR [rax-56], xmm8
	vpxor	xmm0, xmm0, xmm0
	vmovdqu	XMMWORD PTR blend_masks$[rsp+16], xmm0
	vmovdqu	xmm0, XMMWORD PTR __xmm@0f0e0d0c05040b0a0706030209080100
	vmovdqu	XMMWORD PTR [rax-120], xmm0
	vmovdqu	xmm0, XMMWORD PTR __xmm@0b0a090805040f0e0302010007060d0c
	vmovaps	XMMWORD PTR [rax-72], xmm9
	vmovdqu	XMMWORD PTR blend_masks$[rsp], xmm1
	vmovdqu	xmm1, XMMWORD PTR __xmm@ffffffff00000000ffffffff00000000
	vmovdqu	XMMWORD PTR invec_rearr_masks_lower$[rsp], xmm0
	vmovdqu	XMMWORD PTR invec_rearr_masks_lower$[rsp+16], xmm2
	vmovdqu	XMMWORD PTR blend_masks$[rsp+32], xmm1
	vmovdqu	xmm1, XMMWORD PTR __xmm@0f0e07060b0a03020d0c050409080100
	vmovdqu	XMMWORD PTR [rax-88], xmm1
	vmovdqu	xmm1, XMMWORD PTR __xmm@0b0a03020f0e0706090801000d0c0504
	vmovdqu	XMMWORD PTR [rax-136], xmm1
	vmovdqu	XMMWORD PTR low128_shuffle_masks$[rsp+16], xmm2
	movsxd	rax, r9d
	movsx	r11, r8b
	mov	r10d, DWORD PTR [rdx+rax*4]
	lea	r9d, DWORD PTR [r10+rcx]
	lea	r8d, DWORD PTR [r10+rcx*2]
	lea	eax, DWORD PTR [r10+rcx*2]
	add	ecx, eax

; 98   :     scan[subpos] + width * 0,
; 99   :     scan[subpos] + width * 1,
; 100  :     scan[subpos] + width * 2,
; 101  :     scan[subpos] + width * 3,
; 102  :   };
; 103  : 
; 104  :   for (int i = 0; i < n_bufs; i++) {
; 105  :     const int16_t *__restrict coeff = coeffs[i];
; 106  : 
; 107  :     // NOTE: Upper means "higher in pixel order inside block", which implies
; 108  :     // lower addresses (note the difference: HIGH and LOW vs UPPER and LOWER),
; 109  :     // so upper 128b vector actually becomes the lower part of a 256-bit coeff
; 110  :     // vector and lower vector the higher part!
; 111  :     __m128d coeffs_d_upper;
; 112  :     __m128d coeffs_d_lower;
; 113  : 
; 114  :     __m128i coeffs_upper;
; 115  :     __m128i coeffs_lower;
; 116  : 
; 117  :     __m128i coeffs_rearr1_upper;
; 118  :     __m128i coeffs_rearr1_lower;
; 119  : 
; 120  :     __m128i coeffs_rearr2_upper;
; 121  :     __m128i coeffs_rearr2_lower;
; 122  : 
; 123  :     // Zeroing these is actually unnecessary, but the compiler will whine
; 124  :     // about uninitialized values otherwise
; 125  :     coeffs_d_upper = _mm_setzero_pd();
; 126  :     coeffs_d_lower = _mm_setzero_pd();
; 127  : 
; 128  :     coeffs_d_upper = _mm_loadl_pd(coeffs_d_upper, (double *)(coeff + row_offsets[0]));
; 129  :     coeffs_d_upper = _mm_loadh_pd(coeffs_d_upper, (double *)(coeff + row_offsets[1]));
; 130  : 
; 131  :     coeffs_d_lower = _mm_loadl_pd(coeffs_d_lower, (double *)(coeff + row_offsets[2]));
; 132  :     coeffs_d_lower = _mm_loadh_pd(coeffs_d_lower, (double *)(coeff + row_offsets[3]));
; 133  : 
; 134  :     coeffs_upper   = _mm_castpd_si128(coeffs_d_upper);
; 135  :     coeffs_lower   = _mm_castpd_si128(coeffs_d_lower);
; 136  : 
; 137  :     coeffs_lower   = _mm_shuffle_epi8(coeffs_lower, low128_shuffle_masks[scan_mode]);

	mov	rax, r11
	add	rax, rax
	mov	edx, ecx

; 138  : 
; 139  :     coeffs_rearr1_upper = _mm_blendv_epi8(coeffs_upper, coeffs_lower, blend_masks[scan_mode]);
; 140  :     coeffs_rearr1_lower = _mm_blendv_epi8(coeffs_lower, coeffs_upper, blend_masks[scan_mode]);

	vmovdqu	xmm8, XMMWORD PTR blend_masks$[rsp+rax*8]
	vmovdqu	xmm6, XMMWORD PTR low128_shuffle_masks$[rsp+rax*8]

; 141  : 
; 142  :     coeffs_rearr2_upper = _mm_shuffle_epi8(coeffs_rearr1_upper, invec_rearr_masks_upper[scan_mode]);
; 143  :     coeffs_rearr2_lower = _mm_shuffle_epi8(coeffs_rearr1_lower, invec_rearr_masks_lower[scan_mode]);

	vmovdqu	xmm7, XMMWORD PTR invec_rearr_masks_lower$[rsp+rax*8]
	vmovdqu	xmm9, XMMWORD PTR invec_rearr_masks_upper$[rsp+rax*8]
	mov	rax, QWORD PTR [rbx]
	vmovddup xmm3, QWORD PTR [rax+r10*2]
	vmovhpd	xmm3, xmm3, QWORD PTR [rax+r9*2]
	vmovddup xmm0, QWORD PTR [rax+r8*2]
	vmovhpd	xmm0, xmm0, QWORD PTR [rax+rcx*2]
	mov	rax, QWORD PTR [rbx+8]

; 144  : 
; 145  :     // The Intel Intrinsics Guide talks about _mm256_setr_m128i but my headers
; 146  :     // lack such an instruction. What it does is essentially this anyway.
; 147  :     result_vecs[i] = _mm256_inserti128_si256(_mm256_castsi128_si256(coeffs_rearr2_upper),

	mov	rcx, QWORD PTR result_vecs$[rsp]
	vpshufb	xmm2, xmm0, xmm6
	vpblendvb xmm1, xmm2, xmm3, xmm8
	vmovddup xmm5, QWORD PTR [rax+r8*2]
	vmovhpd	xmm5, xmm5, QWORD PTR [rax+rdx*2]
	vpblendvb xmm2, xmm3, xmm2, xmm8
	vpshufb	xmm3, xmm2, xmm9
	vpshufb	xmm4, xmm1, xmm7
	vinserti128 ymm0, ymm3, xmm4, 1
	vmovddup xmm3, QWORD PTR [rax+r10*2]
	vmovhpd	xmm3, xmm3, QWORD PTR [rax+r9*2]
	vpshufb	xmm2, xmm5, xmm6
	vpblendvb xmm1, xmm2, xmm3, xmm8
	vpblendvb xmm2, xmm3, xmm2, xmm8
	vpshufb	xmm3, xmm2, xmm9
	vpshufb	xmm4, xmm1, xmm7
	vmovdqu	YMMWORD PTR [rcx], ymm0
	vinserti128 ymm0, ymm3, xmm4, 1
	vmovdqu	YMMWORD PTR [rcx+32], ymm0
	vzeroupper

; 148  :                                              coeffs_rearr2_lower,
; 149  :                                              1);
; 150  :   }
; 151  : }

	lea	r11, QWORD PTR [rsp+256]
	vmovaps	xmm6, XMMWORD PTR [r11-16]
	vmovaps	xmm7, XMMWORD PTR [r11-32]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm9, XMMWORD PTR [r11-64]
	mov	rsp, r11
	pop	rbx
	ret	0
scanord_read_vector ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
_TEXT	SEGMENT
ints$ = 8
first$ = 16
last$ = 24
get_first_last_nz_int16 PROC

; 155  : {

	vpxor	xmm1, xmm1, xmm1

; 156  :   // Note that nonzero_bytes will always have both bytes set for a set word
; 157  :   // even if said word only had one of its bytes set, because we're doing 16
; 158  :   // bit wide comparisons. No big deal, just shift results to the right by one
; 159  :   // bit to have the results represent indexes of first set words, not bytes.
; 160  :   // Another note, it has to use right shift instead of division to preserve
; 161  :   // behavior on an all-zero vector (-1 / 2 == 0, but -1 >> 1 == -1)
; 162  :   const __m256i zero = _mm256_setzero_si256();
; 163  : 
; 164  :   __m256i zeros = _mm256_cmpeq_epi16(ints, zero);

	vpcmpeqw ymm1, ymm1, YMMWORD PTR [rcx]

; 165  :   uint32_t nonzero_bytes = ~((uint32_t)_mm256_movemask_epi8(zeros));

	vpmovmskb ecx, ymm1
	not	ecx

; 166  :   *first = (    (int32_t)_tzcnt_u32(nonzero_bytes)) >> 1;

	tzcnt	eax, ecx
	sar	eax, 1
	mov	DWORD PTR [rdx], eax

; 167  :   *last = (31 - (int32_t)_lzcnt_u32(nonzero_bytes)) >> 1;

	lzcnt	eax, ecx
	mov	ecx, 31
	sub	ecx, eax
	sar	ecx, 1
	mov	DWORD PTR [r8], ecx
	vzeroupper

; 168  : }

	ret	0
get_first_last_nz_int16 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
src$ = 8
hsum32_8x32i PROC

; 59   :   __m128i a = _mm256_extracti128_si256(src, 0);
; 60   :   __m128i b = _mm256_extracti128_si256(src, 1);

	vmovdqu	ymm1, YMMWORD PTR [rcx]
	vextracti128 xmm0, ymm1, 1

; 61   : 
; 62   :   a = _mm_add_epi32(a, b);

	vpaddd	xmm1, xmm0, xmm1

; 63   :   b = _mm_shuffle_epi32(a, _MM_SHUFFLE(0, 1, 2, 3));

	vpshufd	xmm0, xmm1, 27

; 64   : 
; 65   :   a = _mm_add_epi32(a, b);

	vpaddd	xmm2, xmm0, xmm1

; 66   :   b = _mm_shuffle_epi32(a, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	xmm1, xmm2, 177				; 000000b1H

; 67   : 
; 68   :   a = _mm_add_epi32(a, b);

	vpaddd	xmm0, xmm1, xmm2

; 69   :   return _mm_cvtsi128_si32(a);

	vmovd	eax, xmm0
	vzeroupper

; 70   : }

	ret	0
hsum32_8x32i ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
src$ = 8
hsum32_16x16i PROC

; 74   :   __m128i a = _mm256_extracti128_si256(src, 0);

	vmovdqu	ymm0, YMMWORD PTR [rcx]

; 75   :   __m128i b = _mm256_extracti128_si256(src, 1);
; 76   :   __m256i c = _mm256_cvtepi16_epi32(a);

	vpmovsxwd ymm2, xmm0
	vextracti128 xmm0, ymm0, 1

; 77   :   __m256i d = _mm256_cvtepi16_epi32(b);

	vpmovsxwd ymm1, xmm0

; 78   : 
; 79   :   c = _mm256_add_epi32(c, d);

	vpaddd	ymm3, ymm1, ymm2

; 60   :   __m128i b = _mm256_extracti128_si256(src, 1);

	vextracti128 xmm0, ymm3, 1

; 61   : 
; 62   :   a = _mm_add_epi32(a, b);

	vpaddd	xmm1, xmm0, xmm3

; 63   :   b = _mm_shuffle_epi32(a, _MM_SHUFFLE(0, 1, 2, 3));

	vpshufd	xmm0, xmm1, 27

; 64   : 
; 65   :   a = _mm_add_epi32(a, b);

	vpaddd	xmm2, xmm0, xmm1

; 66   :   b = _mm_shuffle_epi32(a, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	xmm1, xmm2, 177				; 000000b1H

; 67   : 
; 68   :   a = _mm_add_epi32(a, b);

	vpaddd	xmm0, xmm1, xmm2

; 69   :   return _mm_cvtsi128_si32(a);

	vmovd	eax, xmm0
	vzeroupper

; 80   :   return hsum32_8x32i(c);
; 81   : }

	ret	0
hsum32_16x16i ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
hi$ = 8
lo$ = 16
rearrange_512 PROC

; 88   :   const __m256i perm8x32mask = _mm256_setr_epi32(0, 2, 4, 6, 1, 3, 5, 7);

	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000700000005000000030000000100000006000000040000000200000000

; 89   : 
; 90   :   __m256i tmphi = _mm256_permutevar8x32_epi32(*hi, perm8x32mask);

	vpermd	ymm2, ymm0, YMMWORD PTR [rcx]

; 91   :   __m256i tmplo = _mm256_permutevar8x32_epi32(*lo, perm8x32mask);

	vpermd	ymm1, ymm0, YMMWORD PTR [rdx]

; 92   : 
; 93   :   *hi = _mm256_permute2x128_si256(tmplo, tmphi, 0x31);

	vperm2i128 ymm0, ymm1, ymm2, 49			; 00000031H

; 94   :   *lo = _mm256_permute2x128_si256(tmplo, tmphi, 0x20);

	vperm2i128 ymm1, ymm1, ymm2, 32			; 00000020H
	vmovdqu	YMMWORD PTR [rcx], ymm0
	vmovdqu	YMMWORD PTR [rdx], ymm1
	vzeroupper

; 95   : }

	ret	0
rearrange_512 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
costs_hi$ = 48
costs_lo$ = 56
ns$ = 64
changes$ = 72
final_change$ = 80
min_pos$ = 88
get_cheapest_alternative PROC

; 100  : {

	sub	rsp, 40					; 00000028H

; 88   :   const __m256i perm8x32mask = _mm256_setr_epi32(0, 2, 4, 6, 1, 3, 5, 7);

	vmovdqu	ymm2, YMMWORD PTR __ymm@0000000700000005000000030000000100000006000000040000000200000000

; 91   :   __m256i tmplo = _mm256_permutevar8x32_epi32(*lo, perm8x32mask);

	vpermd	ymm0, ymm2, YMMWORD PTR [rdx]

; 101  :   // Interleave ns and lo into 32-bit variables and to two 256-bit wide vecs,
; 102  :   // to have the same data layout as in costs. Zero extend to 32b width, shift
; 103  :   // changes 16 bits to the left, and store them into the same vectors.
; 104  :   __m256i tmp1hi = _mm256_unpackhi_epi16(ns, changes);

	vmovdqu	ymm1, YMMWORD PTR [r8]

; 144  : 
; 145  :   *final_change = (int16_t)chng;

	mov	rax, QWORD PTR final_change$[rsp]
	vmovaps	XMMWORD PTR [rsp+16], xmm6
	vpunpcklwd ymm6, ymm1, YMMWORD PTR [r9]
	vmovaps	XMMWORD PTR [rsp], xmm7
	vpunpckhwd ymm7, ymm1, YMMWORD PTR [r9]

; 90   :   __m256i tmphi = _mm256_permutevar8x32_epi32(*hi, perm8x32mask);

	vpermd	ymm1, ymm2, YMMWORD PTR [rcx]

; 92   : 
; 93   :   *hi = _mm256_permute2x128_si256(tmplo, tmphi, 0x31);

	vperm2i128 ymm5, ymm0, ymm1, 49			; 00000031H

; 94   :   *lo = _mm256_permute2x128_si256(tmplo, tmphi, 0x20);

	vperm2i128 ymm4, ymm0, ymm1, 32			; 00000020H

; 105  :   __m256i tmp1lo = _mm256_unpacklo_epi16(ns, changes);
; 106  : 
; 107  :   __m256i pl1hi = _mm256_permute2x128_si256(tmp1lo, tmp1hi, 0x31);

	vperm2i128 ymm0, ymm6, ymm7, 49			; 00000031H

; 90   :   __m256i tmphi = _mm256_permutevar8x32_epi32(*hi, perm8x32mask);

	vpermd	ymm3, ymm2, ymm0

; 108  :   __m256i pl1lo = _mm256_permute2x128_si256(tmp1lo, tmp1hi, 0x20);

	vperm2i128 ymm1, ymm6, ymm7, 32			; 00000020H

; 91   :   __m256i tmplo = _mm256_permutevar8x32_epi32(*lo, perm8x32mask);

	vpermd	ymm0, ymm2, ymm1

; 94   :   *lo = _mm256_permute2x128_si256(tmplo, tmphi, 0x20);

	vperm2i128 ymm1, ymm0, ymm3, 32			; 00000020H

; 109  : 
; 110  :   // Reorder to afford result stability (if multiple atoms tie for cheapest,
; 111  :   // rightmost ie. the highest is the wanted one)
; 112  :   rearrange_512(&costs_hi, &costs_lo);
; 113  :   rearrange_512(&pl1hi, &pl1lo);
; 114  : 
; 115  :   // 0: pick hi, 1: pick lo (equality evaluates as 0)
; 116  :   __m256i cmpmask1 = _mm256_cmpgt_epi32(costs_hi, costs_lo);

	vpcmpgtd ymm2, ymm5, ymm4

; 117  :   __m256i cost1    = _mm256_blendv_epi8(costs_hi, costs_lo, cmpmask1);

	vpblendvb ymm4, ymm5, ymm4, ymm2

; 93   :   *hi = _mm256_permute2x128_si256(tmplo, tmphi, 0x31);

	vperm2i128 ymm0, ymm0, ymm3, 49			; 00000031H

; 118  :   __m256i pl1_1    = _mm256_blendv_epi8(pl1hi,    pl1lo,    cmpmask1);

	vpblendvb ymm3, ymm0, ymm1, ymm2

; 119  : 
; 120  :   __m256i cost2    = _mm256_shuffle_epi32(cost1, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm2, ymm4, 177				; 000000b1H

; 121  :   __m256i pl1_2    = _mm256_shuffle_epi32(pl1_1, _MM_SHUFFLE(2, 3, 0, 1));
; 122  : 
; 123  :   __m256i cmpmask2 = _mm256_cmpgt_epi32(cost2, cost1);

	vpcmpgtd ymm1, ymm2, ymm4

; 124  :   __m256i cost3    = _mm256_blendv_epi8(cost2, cost1, cmpmask2);

	vpblendvb ymm4, ymm2, ymm4, ymm1
	vpshufd	ymm0, ymm3, 177				; 000000b1H

; 125  :   __m256i pl1_3    = _mm256_blendv_epi8(pl1_2, pl1_1, cmpmask2);

	vpblendvb ymm5, ymm0, ymm3, ymm1

; 126  : 
; 127  :   __m256i cost4    = _mm256_shuffle_epi32(cost3, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	ymm1, ymm4, 78				; 0000004eH

; 128  :   __m256i pl1_4    = _mm256_shuffle_epi32(pl1_3, _MM_SHUFFLE(1, 0, 3, 2));
; 129  : 
; 130  :   __m256i cmpmask3 = _mm256_cmpgt_epi32(cost4, cost3);

	vpcmpgtd ymm2, ymm1, ymm4

; 131  :   __m256i cost5    = _mm256_blendv_epi8(cost4, cost3, cmpmask3);

	vpblendvb ymm4, ymm1, ymm4, ymm2
	vpshufd	ymm0, ymm5, 78				; 0000004eH

; 132  :   __m256i pl1_5    = _mm256_blendv_epi8(pl1_4, pl1_3, cmpmask3);

	vpblendvb ymm3, ymm0, ymm5, ymm2

; 133  : 
; 134  :   __m256i cost6    = _mm256_permute4x64_epi64(cost5, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm1, ymm4, 78				; 0000004eH

; 135  :   __m256i pl1_6    = _mm256_permute4x64_epi64(pl1_5, _MM_SHUFFLE(1, 0, 3, 2));
; 136  : 
; 137  :   __m256i cmpmask4 = _mm256_cmpgt_epi32(cost6, cost5);

	vpcmpgtd ymm2, ymm1, ymm4
	vpermq	ymm0, ymm3, 78				; 0000004eH

; 138  :   __m256i pl1_7    = _mm256_blendv_epi8(pl1_6, pl1_5, cmpmask4);

	vpblendvb ymm3, ymm0, ymm3, ymm2

; 139  : 
; 140  :   __m128i res1_128 = _mm256_castsi256_si128(pl1_7);
; 141  :   uint32_t tmp1 = (uint32_t)_mm_extract_epi32(res1_128, 0);

	vpextrd	edx, xmm3, 0

; 142  :   uint16_t n = (uint16_t)(tmp1 & 0xffff);
; 143  :   uint16_t chng = (uint16_t)(tmp1 >> 16);

	mov	ecx, edx
	shr	ecx, 16

; 144  : 
; 145  :   *final_change = (int16_t)chng;

	mov	WORD PTR [rax], cx

; 146  :   *min_pos = (int32_t)n;

	mov	rax, QWORD PTR min_pos$[rsp]
	movzx	ecx, dx
	mov	DWORD PTR [rax], ecx
	vzeroupper

; 147  : }

	vmovaps	xmm6, XMMWORD PTR [rsp+16]
	vmovaps	xmm7, XMMWORD PTR [rsp]
	add	rsp, 40					; 00000028H
	ret	0
get_cheapest_alternative ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
lo$ = 8
hi$ = 16
concatenate_2x128i PROC

; 151  :   __m256i v = _mm256_castsi128_si256(lo);

	vmovups	xmm0, XMMWORD PTR [rcx]

; 152  :   return _mm256_inserti128_si256(v, hi, 1);

	vinserti128 ymm0, ymm0, XMMWORD PTR [rdx], 1

; 153  : }

	ret	0
concatenate_2x128i ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
shufmasks$ = 0
blend_masks$ = 96
rearr_masks_lo$ = 192
rearr_masks_hi$ = 288
quant_coeff$ = 432
scan$ = 440
scan_mode$ = 448
subpos$ = 456
width$ = 464
v_quant_coeffs$ = 472
scanord_read_vector_32 PROC

; 161  : {

	mov	QWORD PTR [rsp+8], rbx
	push	rbp
	sub	rsp, 416				; 000001a0H
	lea	rbp, QWORD PTR [rsp+32]
	and	rbp, -32				; ffffffffffffffe0H

; 163  :     scan[subpos] + width * 0,
; 164  :     scan[subpos] + width * 1,
; 165  :     scan[subpos] + width * 2,
; 166  :     scan[subpos] + width * 3,
; 167  :   };
; 168  : 
; 169  :   const __m256i shufmasks[3] = {

	vmovdqu	ymm2, YMMWORD PTR __ymm@0000000700000006000000050000000400000003000000020000000100000000
	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000100000004000000070000000300000000000000060000000200000005

; 170  :     _mm256_setr_epi32(5, 2, 6, 0, 3, 7, 4, 1),
; 171  :     _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7),
; 172  :     _mm256_setr_epi32(2, 3, 0, 1, 6, 7, 4, 5),
; 173  :   };
; 174  : 
; 175  :   const __m256i blend_masks[3] = {

	vmovdqu	ymm1, YMMWORD PTR __ymm@ffffffffffffffff0000000000000000ffffffff000000000000000000000000
	vmovdqu	YMMWORD PTR shufmasks$[rbp], ymm0
	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000500000004000000070000000600000001000000000000000300000002
	vmovdqu	YMMWORD PTR shufmasks$[rbp+64], ymm0
	vmovdqu	YMMWORD PTR blend_masks$[rbp], ymm1
	vmovdqu	ymm1, YMMWORD PTR __ymm@ffffffffffffffff0000000000000000ffffffffffffffff0000000000000000
	vmovdqu	YMMWORD PTR blend_masks$[rbp+64], ymm1

; 176  :     _mm256_setr_epi32( 0,  0,  0, -1,  0,  0, -1, -1),
; 177  :     _mm256_setr_epi32( 0,  0,  0,  0,  0,  0,  0,  0),
; 178  :     _mm256_setr_epi32( 0,  0, -1, -1,  0,  0, -1, -1),
; 179  :   };
; 180  : 
; 181  :   const __m256i rearr_masks_lo[3] = {

	vmovdqu	ymm1, YMMWORD PTR __ymm@0000000700000003000000050000000100000006000000020000000400000000
	vpxor	xmm0, xmm0, xmm0
	vmovdqu	YMMWORD PTR blend_masks$[rbp+32], ymm0
	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000700000006000000020000000500000003000000010000000400000000
	vmovdqu	YMMWORD PTR rearr_masks_lo$[rbp+64], ymm1

; 182  :     _mm256_setr_epi32(0, 4, 1, 3, 5, 2, 6, 7),
; 183  :     _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7),
; 184  :     _mm256_setr_epi32(0, 4, 2, 6, 1, 5, 3, 7),
; 185  :   };
; 186  : 
; 187  :   const __m256i rearr_masks_hi[3] = {

	vmovdqu	ymm1, YMMWORD PTR __ymm@0000000500000001000000070000000300000004000000000000000600000002
	vmovdqu	YMMWORD PTR rearr_masks_lo$[rbp], ymm0
	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000500000004000000020000000700000001000000000000000300000006
	vmovdqu	YMMWORD PTR rearr_masks_hi$[rbp+64], ymm1
	vmovdqu	YMMWORD PTR rearr_masks_hi$[rbp], ymm0
	movsxd	rax, r9d
	mov	rbx, rcx
	vmovdqu	YMMWORD PTR shufmasks$[rbp+32], ymm2
	vmovdqu	YMMWORD PTR rearr_masks_lo$[rbp+32], ymm2
	vmovdqu	YMMWORD PTR rearr_masks_hi$[rbp+32], ymm2
	mov	r10d, DWORD PTR [rdx+rax*4]
	mov	edx, DWORD PTR width$[rsp]

; 152  :   return _mm256_inserti128_si256(v, hi, 1);

	vmovups	xmm0, XMMWORD PTR [rcx+r10*4]

; 162  :   const size_t row_offsets[4] = {

	lea	r9d, DWORD PTR [r10+rdx]

; 152  :   return _mm256_inserti128_si256(v, hi, 1);

	vinserti128 ymm5, ymm0, XMMWORD PTR [rcx+r9*4], 1

; 162  :   const size_t row_offsets[4] = {

	lea	eax, DWORD PTR [r10+rdx*2]

; 188  :     _mm256_setr_epi32(6, 3, 0, 1, 7, 2, 4, 5),
; 189  :     _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7),
; 190  :     _mm256_setr_epi32(2, 6, 0, 4, 3, 7, 1, 5),
; 191  :   };
; 192  : 
; 193  :   __m128i coeffs[4] = {
; 194  :     _mm_loadu_si128((__m128i *)(quant_coeff + row_offsets[0])),
; 195  :     _mm_loadu_si128((__m128i *)(quant_coeff + row_offsets[1])),
; 196  :     _mm_loadu_si128((__m128i *)(quant_coeff + row_offsets[2])),
; 197  :     _mm_loadu_si128((__m128i *)(quant_coeff + row_offsets[3])),
; 198  :   };
; 199  : 
; 200  :   __m256i coeffs_upper = concatenate_2x128i(coeffs[0], coeffs[1]);
; 201  :   __m256i coeffs_lower = concatenate_2x128i(coeffs[2], coeffs[3]);
; 202  : 
; 203  :   __m256i lower_shuffled = _mm256_permutevar8x32_epi32(coeffs_lower, shufmasks[scan_mode]);

	movsx	rcx, r8b
	lea	r11d, DWORD PTR [r10+rdx*2]
	add	edx, eax

; 152  :   return _mm256_inserti128_si256(v, hi, 1);

	vmovups	xmm0, XMMWORD PTR [rbx+r11*4]

; 210  : 
; 211  :   v_quant_coeffs[0] = result_lo;

	mov	rax, QWORD PTR v_quant_coeffs$[rsp]
	shl	rcx, 5

; 152  :   return _mm256_inserti128_si256(v, hi, 1);

	vinserti128 ymm2, ymm0, XMMWORD PTR [rbx+rdx*4], 1

; 188  :     _mm256_setr_epi32(6, 3, 0, 1, 7, 2, 4, 5),
; 189  :     _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7),
; 190  :     _mm256_setr_epi32(2, 6, 0, 4, 3, 7, 1, 5),
; 191  :   };
; 192  : 
; 193  :   __m128i coeffs[4] = {
; 194  :     _mm_loadu_si128((__m128i *)(quant_coeff + row_offsets[0])),
; 195  :     _mm_loadu_si128((__m128i *)(quant_coeff + row_offsets[1])),
; 196  :     _mm_loadu_si128((__m128i *)(quant_coeff + row_offsets[2])),
; 197  :     _mm_loadu_si128((__m128i *)(quant_coeff + row_offsets[3])),
; 198  :   };
; 199  : 
; 200  :   __m256i coeffs_upper = concatenate_2x128i(coeffs[0], coeffs[1]);
; 201  :   __m256i coeffs_lower = concatenate_2x128i(coeffs[2], coeffs[3]);
; 202  : 
; 203  :   __m256i lower_shuffled = _mm256_permutevar8x32_epi32(coeffs_lower, shufmasks[scan_mode]);

	vmovdqu	ymm1, YMMWORD PTR shufmasks$[rbp+rcx]

; 204  : 
; 205  :   __m256i upper_blended  = _mm256_blendv_epi8(coeffs_upper,   lower_shuffled, blend_masks[scan_mode]);

	vmovdqu	ymm3, YMMWORD PTR blend_masks$[rbp+rcx]

; 206  :   __m256i lower_blended  = _mm256_blendv_epi8(lower_shuffled, coeffs_upper,   blend_masks[scan_mode]);
; 207  : 
; 208  :   __m256i result_lo      = _mm256_permutevar8x32_epi32(upper_blended, rearr_masks_lo[scan_mode]);

	vmovdqu	ymm0, YMMWORD PTR rearr_masks_lo$[rbp+rcx]
	vpermd	ymm4, ymm1, ymm2
	vpblendvb ymm1, ymm5, ymm4, ymm3
	vpermd	ymm2, ymm0, ymm1

; 209  :   __m256i result_hi      = _mm256_permutevar8x32_epi32(lower_blended, rearr_masks_hi[scan_mode]);

	vmovdqu	ymm0, YMMWORD PTR rearr_masks_hi$[rbp+rcx]
	vpblendvb ymm1, ymm4, ymm5, ymm3
	vpermd	ymm1, ymm0, ymm1

; 212  :   v_quant_coeffs[1] = result_hi;

	vmovdqu	YMMWORD PTR [rax+32], ymm1
	vmovdqu	YMMWORD PTR [rax], ymm2
	vzeroupper

; 213  : }

	mov	rbx, QWORD PTR [rsp+432]
	add	rsp, 416				; 000001a0H
	pop	rbp
	ret	0
scanord_read_vector_32 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
coefs$ = 176
q_coefs$ = 184
deltas_h$ = 192
deltas_l$ = 200
q_coef$ = 208
scan$ = 216
subpos$ = 224
last_cg$ = 232
hide_block_sign PROC

; 220  : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rsi
	push	rdi
	sub	rsp, 160				; 000000a0H
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 164  :   __m256i zeros = _mm256_cmpeq_epi16(ints, zero);

	vmovdqu	ymm4, YMMWORD PTR [rdx]
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 75   :   __m128i b = _mm256_extracti128_si256(src, 1);

	vextracti128 xmm0, ymm4, 1

; 77   :   __m256i d = _mm256_cvtepi16_epi32(b);

	vpmovsxwd ymm1, xmm0
	vmovaps	XMMWORD PTR [rsp+80], xmm10
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 167  :   *last = (31 - (int32_t)_lzcnt_u32(nonzero_bytes)) >> 1;

	mov	edx, 31
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 76   :   __m256i c = _mm256_cvtepi16_epi32(a);

	vpmovsxwd ymm2, xmm4

; 78   : 
; 79   :   c = _mm256_add_epi32(c, d);

	vpaddd	ymm3, ymm1, ymm2

; 60   :   __m128i b = _mm256_extracti128_si256(src, 1);

	vextracti128 xmm0, ymm3, 1

; 61   : 
; 62   :   a = _mm_add_epi32(a, b);

	vpaddd	xmm1, xmm0, xmm3

; 63   :   b = _mm_shuffle_epi32(a, _MM_SHUFFLE(0, 1, 2, 3));

	vpshufd	xmm0, xmm1, 27

; 64   : 
; 65   :   a = _mm_add_epi32(a, b);

	vpaddd	xmm2, xmm0, xmm1
	vmovaps	XMMWORD PTR [rsp+16], xmm14

; 220  : {

	mov	rdi, r8

; 221  :   assert(SCAN_SET_SIZE == 16);
; 222  : 
; 223  :   int32_t first_nz_pos_in_cg, last_nz_pos_in_cg;
; 224  :   int32_t abssum = 0;
; 225  : 
; 226  :   // Find first and last nonzero coeffs
; 227  :   get_first_last_nz_int16(q_coefs, &first_nz_pos_in_cg, &last_nz_pos_in_cg);
; 228  : 
; 229  :   // Sum all kvz_quant coeffs between first and last
; 230  :   abssum = hsum32_16x16i(q_coefs);
; 231  : 
; 232  :   if (last_nz_pos_in_cg >= 0 && last_cg == -1) {

	mov	r8d, DWORD PTR last_cg$[rsp]
	mov	rbx, r9
	vpxor	xmm14, xmm14, xmm14
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 164  :   __m256i zeros = _mm256_cmpeq_epi16(ints, zero);

	vpcmpeqw ymm10, ymm4, ymm14

; 165  :   uint32_t nonzero_bytes = ~((uint32_t)_mm256_movemask_epi8(zeros));

	vpmovmskb eax, ymm10
	not	eax
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 220  : {

	mov	rsi, rcx
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 166  :   *first = (    (int32_t)_tzcnt_u32(nonzero_bytes)) >> 1;

	tzcnt	r10d, eax

; 167  :   *last = (31 - (int32_t)_lzcnt_u32(nonzero_bytes)) >> 1;

	lzcnt	eax, eax
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 66   :   b = _mm_shuffle_epi32(a, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	xmm1, xmm2, 177				; 000000b1H
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 166  :   *first = (    (int32_t)_tzcnt_u32(nonzero_bytes)) >> 1;

	sar	r10d, 1

; 167  :   *last = (31 - (int32_t)_lzcnt_u32(nonzero_bytes)) >> 1;

	sub	edx, eax
	sar	edx, 1
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 68   :   a = _mm_add_epi32(a, b);

	vpaddd	xmm0, xmm1, xmm2

; 69   :   return _mm_cvtsi128_si32(a);

	vmovd	r11d, xmm0

; 221  :   assert(SCAN_SET_SIZE == 16);
; 222  : 
; 223  :   int32_t first_nz_pos_in_cg, last_nz_pos_in_cg;
; 224  :   int32_t abssum = 0;
; 225  : 
; 226  :   // Find first and last nonzero coeffs
; 227  :   get_first_last_nz_int16(q_coefs, &first_nz_pos_in_cg, &last_nz_pos_in_cg);
; 228  : 
; 229  :   // Sum all kvz_quant coeffs between first and last
; 230  :   abssum = hsum32_16x16i(q_coefs);
; 231  : 
; 232  :   if (last_nz_pos_in_cg >= 0 && last_cg == -1) {

	js	SHORT $LN2@hide_block
	cmp	r8d, -1
	mov	eax, 1
	cmove	r8d, eax
$LN2@hide_block:

; 233  :     last_cg = 1;
; 234  :   }
; 235  : 
; 236  :   if (last_nz_pos_in_cg - first_nz_pos_in_cg >= 4) {

	mov	eax, edx
	sub	eax, r10d
	cmp	eax, 4
	jl	$LN4@hide_block

; 237  : 
; 238  :     uint32_t q_coef_signbits = _mm256_movemask_epi8(q_coefs);

	vpmovmskb ecx, ymm4

; 239  :     int32_t signbit = (q_coef_signbits >> (2 * first_nz_pos_in_cg + 1)) & 0x1;

	lea	eax, DWORD PTR [r10*2+1]

; 240  : 
; 241  :     if (signbit != (abssum & 0x1)) { // compare signbit with sum_parity

	and	r11d, 1
	shrx	r9d, ecx, eax
	and	r9d, 1
	cmp	r9d, r11d
	je	$LN4@hide_block

; 242  :       int32_t min_pos;
; 243  :       int16_t final_change;
; 244  :       int16_t cheapest_q;
; 245  : 
; 246  :       const int32_t mask_max = (last_cg == 1) ? last_nz_pos_in_cg : SCAN_SET_SIZE - 1;
; 247  : 
; 248  :       const __m256i zero = _mm256_setzero_si256();
; 249  :       const __m256i ones = _mm256_set1_epi16(1);

	vmovaps	XMMWORD PTR [rsp+144], xmm6

; 255  : 
; 256  :       __m256i block_signbit = _mm256_set1_epi16(((int16_t)signbit) * -1);

	neg	r9w
	vmovaps	XMMWORD PTR [rsp+128], xmm7
	cmp	r8d, 1
	vmovaps	XMMWORD PTR [rsp+112], xmm8

; 88   :   const __m256i perm8x32mask = _mm256_setr_epi32(0, 2, 4, 6, 1, 3, 5, 7);

	vmovdqu	ymm8, YMMWORD PTR __ymm@0000000700000005000000030000000100000006000000040000000200000000

; 242  :       int32_t min_pos;
; 243  :       int16_t final_change;
; 244  :       int16_t cheapest_q;
; 245  : 
; 246  :       const int32_t mask_max = (last_cg == 1) ? last_nz_pos_in_cg : SCAN_SET_SIZE - 1;
; 247  : 
; 248  :       const __m256i zero = _mm256_setzero_si256();
; 249  :       const __m256i ones = _mm256_set1_epi16(1);

	vmovaps	XMMWORD PTR [rsp+96], xmm9

; 250  :       const __m256i maxiters = _mm256_set1_epi16(mask_max);
; 251  :       const __m256i ff = _mm256_set1_epi8(0xff);
; 252  : 
; 253  :       const __m256i fnpics = _mm256_set1_epi16((int16_t)first_nz_pos_in_cg);
; 254  :       const __m256i ns = _mm256_setr_epi16(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);

	vmovdqu	ymm9, YMMWORD PTR __ymm@000f000e000d000c000b000a0009000800070006000500040003000200010000
	vmovaps	XMMWORD PTR [rsp+64], xmm11
	vmovaps	XMMWORD PTR [rsp+48], xmm12

; 257  :       __m256i coef_signbits = _mm256_cmpgt_epi16(zero, coefs);
; 258  :       __m256i signbits_equal_block = _mm256_cmpeq_epi16(coef_signbits, block_signbit);
; 259  : 
; 260  :       __m256i q_coefs_zero = _mm256_cmpeq_epi16(q_coefs, zero);
; 261  : 
; 262  :       __m256i dus_packed = _mm256_packs_epi32(deltas_l, deltas_h);

	vmovdqu	ymm12, YMMWORD PTR [rdi]
	vmovaps	XMMWORD PTR [rsp+32], xmm13
	vmovdqu	ymm13, YMMWORD PTR [rbx]
	vpackssdw ymm0, ymm13, ymm12

; 263  :       __m256i dus_ordered = _mm256_permute4x64_epi64(dus_packed, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm1, ymm0, 216				; 000000d8H

; 264  :       __m256i dus_positive = _mm256_cmpgt_epi16(dus_ordered, zero);

	vpcmpgtw ymm6, ymm1, ymm14

; 265  : 
; 266  :       __m256i q_coef_abss = _mm256_abs_epi16(q_coefs);

	vpabsw	ymm0, ymm4

; 267  :       __m256i q_coefs_plusminus_one = _mm256_cmpeq_epi16(q_coef_abss, ones);

	vpcmpeqw ymm1, ymm0, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	movsx	eax, r10w
	vmovd	xmm3, eax
	vpbroadcastw ymm3, xmm3

; 268  : 
; 269  :       __m256i eq_fnpics = _mm256_cmpeq_epi16(fnpics, ns);

	vpcmpeqw ymm2, ymm9, ymm3

; 270  :       __m256i lt_fnpics = _mm256_cmpgt_epi16(fnpics, ns);
; 271  : 
; 272  :       __m256i maxcost_subcond1s = _mm256_and_si256(eq_fnpics, q_coefs_plusminus_one);

	vpand	ymm5, ymm1, ymm2

; 273  :       __m256i maxcost_subcond2s = _mm256_andnot_si256(signbits_equal_block, lt_fnpics);
; 274  :       __m256i elsecond1s_inv = _mm256_or_si256(dus_positive, maxcost_subcond1s);

	vpor	ymm2, ymm5, ymm6
	movsx	eax, r9w
	vmovd	xmm0, eax
	vpbroadcastw ymm0, xmm0
	mov	eax, 15
	vmovaps	XMMWORD PTR [rsp], xmm15
	vpcmpgtw ymm15, ymm14, YMMWORD PTR [rsi]
	vpcmpeqw ymm1, ymm0, ymm15
	vpcmpgtw ymm0, ymm3, ymm9
	vpandn	ymm4, ymm1, ymm0

; 275  :       __m256i elsecond1s = _mm256_andnot_si256(elsecond1s_inv, ff);
; 276  : 
; 277  :       __m256i outside_maxiters = _mm256_cmpgt_epi16(ns, maxiters);
; 278  : 
; 279  :       __m256i negdelta_cond1s = _mm256_andnot_si256(q_coefs_zero, dus_positive);

	vpandn	ymm1, ymm10, ymm6
	cmove	ax, dx
	cwde
	vmovd	xmm0, eax
	vpbroadcastw ymm0, xmm0
	vpcmpgtw ymm3, ymm9, ymm0

; 280  :       __m256i negdelta_cond2s = _mm256_andnot_si256(maxcost_subcond2s, q_coefs_zero);

	vpandn	ymm0, ymm4, ymm10

; 281  :       __m256i negdelta_mask16s_part1 = _mm256_or_si256(negdelta_cond1s, negdelta_cond2s);

	vpor	ymm1, ymm0, ymm1

; 282  :       __m256i negdelta_mask16s = _mm256_andnot_si256(outside_maxiters, negdelta_mask16s_part1);

	vpandn	ymm7, ymm3, ymm1
	vpandn	ymm1, ymm2, YMMWORD PTR __ymm@ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff

; 283  : 
; 284  :       __m256i posdelta_mask16s_part1 = _mm256_andnot_si256(q_coefs_zero, elsecond1s);

	vpandn	ymm2, ymm10, ymm1

; 285  :       __m256i posdelta_mask16s = _mm256_andnot_si256(outside_maxiters, posdelta_mask16s_part1);

	vpandn	ymm11, ymm3, ymm2

; 286  : 
; 287  :       __m256i maxcost_cond1_parts = _mm256_andnot_si256(dus_positive, maxcost_subcond1s);
; 288  :       __m256i maxcost_cond1s = _mm256_andnot_si256(q_coefs_zero, maxcost_cond1_parts);
; 289  :       __m256i maxcost_cond2s = _mm256_and_si256(q_coefs_zero, maxcost_subcond2s);

	vpand	ymm2, ymm4, ymm10
	vpandn	ymm0, ymm6, ymm5
	vpandn	ymm1, ymm10, ymm0

; 290  :       __m256i maxcost_mask16s_parts = _mm256_or_si256(maxcost_cond1s, maxcost_cond2s);

	vpor	ymm2, ymm1, ymm2

; 291  :       __m256i maxcost_mask16s = _mm256_or_si256(maxcost_mask16s_parts, outside_maxiters);

	vpor	ymm6, ymm2, ymm3

; 315  :       // costs_posdelta_l and _h: deltas_l and _h
; 316  :       __m256i costs_max_lh = _mm256_set1_epi32(0x7fffffff);
; 317  : 
; 318  :       __m256i change_neg = _mm256_and_si256(negdelta_mask16s, ones);

	vpand	ymm2, ymm7, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 319  :       __m256i change_pos = _mm256_and_si256(posdelta_mask16s, ff);
; 320  :       __m256i change_max = _mm256_and_si256(maxcost_mask16s, zero);

	vpand	ymm0, ymm6, ymm14

; 328  : 
; 329  :       __m256i changes = _mm256_or_si256(change_neg, _mm256_or_si256(change_pos, change_max));

	vpor	ymm1, ymm0, ymm11
	vpor	ymm3, ymm1, ymm2

; 104  :   __m256i tmp1hi = _mm256_unpackhi_epi16(ns, changes);

	vpunpckhwd ymm10, ymm9, ymm3

; 105  :   __m256i tmp1lo = _mm256_unpacklo_epi16(ns, changes);

	vpunpcklwd ymm9, ymm9, ymm3

; 297  :       __m256i negdelta_mask32s_h = _mm256_cvtepi16_epi32(tmp_h);
; 298  : 
; 299  :       tmp_l = _mm256_extracti128_si256(posdelta_mask16s, 0);
; 300  :       tmp_h = _mm256_extracti128_si256(posdelta_mask16s, 1);
; 301  :       __m256i posdelta_mask32s_l = _mm256_cvtepi16_epi32(tmp_l);
; 302  :       __m256i posdelta_mask32s_h = _mm256_cvtepi16_epi32(tmp_h);
; 303  : 
; 304  :       tmp_l = _mm256_extracti128_si256(maxcost_mask16s, 0);
; 305  :       tmp_h = _mm256_extracti128_si256(maxcost_mask16s, 1);
; 306  :       __m256i maxcost_mask32s_l = _mm256_cvtepi16_epi32(tmp_l);
; 307  :       __m256i maxcost_mask32s_h = _mm256_cvtepi16_epi32(tmp_h);
; 308  : 
; 309  :       // Output value generation
; 310  :       // cur_change_max: zero
; 311  :       // cur_change_negdelta: ff
; 312  :       // cur_change_posdelta: ones
; 313  :       __m256i costs_negdelta_h = _mm256_sub_epi32(zero, deltas_h);

	vpsubd	ymm2, ymm14, ymm12
	vextracti128 xmm0, ymm7, 1
	vpmovsxwd ymm1, xmm0

; 323  :       __m256i cost_neg_h = _mm256_and_si256(negdelta_mask32s_h, costs_negdelta_h);

	vpand	ymm4, ymm1, ymm2
	vextracti128 xmm2, ymm6, 1
	vpmovsxwd ymm0, xmm2

; 327  :       __m256i cost_max_h = _mm256_and_si256(maxcost_mask32s_h, costs_max_lh);

	vpand	ymm3, ymm0, YMMWORD PTR __ymm@7fffffff7fffffff7fffffff7fffffff7fffffff7fffffff7fffffff7fffffff
	vextracti128 xmm1, ymm11, 1
	vpmovsxwd ymm2, xmm1
	vpand	ymm0, ymm2, ymm12

; 331  :       __m256i costs_h = _mm256_or_si256(cost_neg_h, _mm256_or_si256(cost_pos_h, cost_max_h));

	vpor	ymm3, ymm0, ymm3
	vpor	ymm1, ymm3, ymm4

; 90   :   __m256i tmphi = _mm256_permutevar8x32_epi32(*hi, perm8x32mask);

	vpermd	ymm4, ymm8, ymm1

; 292  : 
; 293  :       __m128i tmp_l, tmp_h;
; 294  :       tmp_l = _mm256_extracti128_si256(negdelta_mask16s, 0);
; 295  :       tmp_h = _mm256_extracti128_si256(negdelta_mask16s, 1);
; 296  :       __m256i negdelta_mask32s_l = _mm256_cvtepi16_epi32(tmp_l);

	vpmovsxwd ymm1, xmm7

; 314  :       __m256i costs_negdelta_l = _mm256_sub_epi32(zero, deltas_l);

	vpsubd	ymm0, ymm14, ymm13

; 321  : 
; 322  :       __m256i cost_neg_l = _mm256_and_si256(negdelta_mask32s_l, costs_negdelta_l);

	vpand	ymm3, ymm0, ymm1
	vpmovsxwd ymm0, xmm6

; 324  :       __m256i cost_pos_l = _mm256_and_si256(posdelta_mask32s_l, deltas_l);
; 325  :       __m256i cost_pos_h = _mm256_and_si256(posdelta_mask32s_h, deltas_h);
; 326  :       __m256i cost_max_l = _mm256_and_si256(maxcost_mask32s_l, costs_max_lh);

	vpand	ymm2, ymm0, YMMWORD PTR __ymm@7fffffff7fffffff7fffffff7fffffff7fffffff7fffffff7fffffff7fffffff
	vpmovsxwd ymm0, xmm11
	vpand	ymm1, ymm0, ymm13

; 330  :       __m256i costs_l = _mm256_or_si256(cost_neg_l, _mm256_or_si256(cost_pos_l, cost_max_l));

	vpor	ymm2, ymm1, ymm2
	vpor	ymm3, ymm2, ymm3

; 91   :   __m256i tmplo = _mm256_permutevar8x32_epi32(*lo, perm8x32mask);

	vpermd	ymm0, ymm8, ymm3

; 92   : 
; 93   :   *hi = _mm256_permute2x128_si256(tmplo, tmphi, 0x31);

	vperm2i128 ymm5, ymm0, ymm4, 49			; 00000031H

; 94   :   *lo = _mm256_permute2x128_si256(tmplo, tmphi, 0x20);

	vperm2i128 ymm4, ymm0, ymm4, 32			; 00000020H

; 337  :       if (cheapest_q == 32767 || cheapest_q == -32768)

	mov	ecx, DWORD PTR subpos$[rsp]
	vmovaps	xmm13, XMMWORD PTR [rsp+32]
	vmovaps	xmm12, XMMWORD PTR [rsp+48]
	vmovaps	xmm11, XMMWORD PTR [rsp+64]
	vmovaps	xmm7, XMMWORD PTR [rsp+128]
	vmovaps	xmm6, XMMWORD PTR [rsp+144]

; 116  :   __m256i cmpmask1 = _mm256_cmpgt_epi32(costs_hi, costs_lo);

	vpcmpgtd ymm2, ymm5, ymm4

; 117  :   __m256i cost1    = _mm256_blendv_epi8(costs_hi, costs_lo, cmpmask1);

	vpblendvb ymm4, ymm5, ymm4, ymm2
	vperm2i128 ymm0, ymm9, ymm10, 49		; 00000031H

; 90   :   __m256i tmphi = _mm256_permutevar8x32_epi32(*hi, perm8x32mask);

	vpermd	ymm3, ymm8, ymm0

; 108  :   __m256i pl1lo = _mm256_permute2x128_si256(tmp1lo, tmp1hi, 0x20);

	vperm2i128 ymm1, ymm9, ymm10, 32		; 00000020H

; 337  :       if (cheapest_q == 32767 || cheapest_q == -32768)

	vmovaps	xmm9, XMMWORD PTR [rsp+96]

; 91   :   __m256i tmplo = _mm256_permutevar8x32_epi32(*lo, perm8x32mask);

	vpermd	ymm0, ymm8, ymm1

; 337  :       if (cheapest_q == 32767 || cheapest_q == -32768)

	vmovaps	xmm8, XMMWORD PTR [rsp+112]

; 94   :   *lo = _mm256_permute2x128_si256(tmplo, tmphi, 0x20);

	vperm2i128 ymm1, ymm0, ymm3, 32			; 00000020H
	vperm2i128 ymm0, ymm0, ymm3, 49			; 00000031H

; 118  :   __m256i pl1_1    = _mm256_blendv_epi8(pl1hi,    pl1lo,    cmpmask1);

	vpblendvb ymm3, ymm0, ymm1, ymm2

; 119  : 
; 120  :   __m256i cost2    = _mm256_shuffle_epi32(cost1, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm2, ymm4, 177				; 000000b1H

; 121  :   __m256i pl1_2    = _mm256_shuffle_epi32(pl1_1, _MM_SHUFFLE(2, 3, 0, 1));
; 122  : 
; 123  :   __m256i cmpmask2 = _mm256_cmpgt_epi32(cost2, cost1);

	vpcmpgtd ymm1, ymm2, ymm4

; 124  :   __m256i cost3    = _mm256_blendv_epi8(cost2, cost1, cmpmask2);

	vpblendvb ymm4, ymm2, ymm4, ymm1
	vpshufd	ymm0, ymm3, 177				; 000000b1H

; 125  :   __m256i pl1_3    = _mm256_blendv_epi8(pl1_2, pl1_1, cmpmask2);

	vpblendvb ymm5, ymm0, ymm3, ymm1

; 126  : 
; 127  :   __m256i cost4    = _mm256_shuffle_epi32(cost3, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	ymm1, ymm4, 78				; 0000004eH

; 128  :   __m256i pl1_4    = _mm256_shuffle_epi32(pl1_3, _MM_SHUFFLE(1, 0, 3, 2));
; 129  : 
; 130  :   __m256i cmpmask3 = _mm256_cmpgt_epi32(cost4, cost3);

	vpcmpgtd ymm2, ymm1, ymm4

; 131  :   __m256i cost5    = _mm256_blendv_epi8(cost4, cost3, cmpmask3);

	vpblendvb ymm4, ymm1, ymm4, ymm2
	vpshufd	ymm0, ymm5, 78				; 0000004eH

; 132  :   __m256i pl1_5    = _mm256_blendv_epi8(pl1_4, pl1_3, cmpmask3);

	vpblendvb ymm3, ymm0, ymm5, ymm2

; 133  : 
; 134  :   __m256i cost6    = _mm256_permute4x64_epi64(cost5, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm1, ymm4, 78				; 0000004eH

; 135  :   __m256i pl1_6    = _mm256_permute4x64_epi64(pl1_5, _MM_SHUFFLE(1, 0, 3, 2));
; 136  : 
; 137  :   __m256i cmpmask4 = _mm256_cmpgt_epi32(cost6, cost5);

	vpcmpgtd ymm2, ymm1, ymm4
	vpermq	ymm0, ymm3, 78				; 0000004eH

; 138  :   __m256i pl1_7    = _mm256_blendv_epi8(pl1_6, pl1_5, cmpmask4);

	vpblendvb ymm3, ymm0, ymm3, ymm2

; 139  : 
; 140  :   __m128i res1_128 = _mm256_castsi256_si128(pl1_7);
; 141  :   uint32_t tmp1 = (uint32_t)_mm_extract_epi32(res1_128, 0);

	vpextrd	eax, xmm3, 0

; 144  : 
; 145  :   *final_change = (int16_t)chng;
; 146  :   *min_pos = (int32_t)n;

	movzx	r11d, ax
	mov	r10d, eax

; 332  : 
; 333  :       get_cheapest_alternative(costs_h, costs_l, ns, changes, &final_change, &min_pos);
; 334  :       const int32_t best_id = scan[min_pos + subpos];
; 335  : 
; 336  :       cheapest_q = q_coef[best_id];

	mov	rax, QWORD PTR scan$[rsp]
	add	ecx, r11d
	movsxd	rdx, ecx

; 143  :   uint16_t chng = (uint16_t)(tmp1 >> 16);

	shr	r10d, 16

; 332  : 
; 333  :       get_cheapest_alternative(costs_h, costs_l, ns, changes, &final_change, &min_pos);
; 334  :       const int32_t best_id = scan[min_pos + subpos];
; 335  : 
; 336  :       cheapest_q = q_coef[best_id];

	movsxd	rcx, DWORD PTR [rax+rdx*4]
	mov	rax, QWORD PTR q_coef$[rsp]
	movzx	r9d, WORD PTR [rax+rcx*2]
	lea	rbx, QWORD PTR [rax+rcx*2]

; 337  :       if (cheapest_q == 32767 || cheapest_q == -32768)

	mov	ecx, 32767				; 00007fffH
	movzx	eax, r9w
	sub	ax, cx
	mov	ecx, 65534				; 0000fffeH
	test	ax, cx
	jne	SHORT $LN5@hide_block

; 338  :         final_change = -1;

	mov	r10d, -1
$LN5@hide_block:

; 339  : 
; 340  :       uint32_t coef_signs = _mm256_movemask_epi8(coef_signbits);
; 341  :       uint32_t cheapest_coef_sign_mask = (uint32_t)(1 << (2 * min_pos));
; 342  : 
; 343  :       if (!(coef_signs & cheapest_coef_sign_mask))
; 344  :         cheapest_q += final_change;
; 345  :       else
; 346  :         cheapest_q -= final_change;
; 347  : 
; 348  :       q_coef[best_id] = cheapest_q;

	movzx	ecx, r9w
	lea	eax, DWORD PTR [r11+r11]
	sub	cx, r10w
	add	r9w, r10w
	vpmovmskb edx, ymm15
	vmovaps	xmm15, XMMWORD PTR [rsp]
	bt	edx, eax
	cmovb	r9w, cx
	mov	WORD PTR [rbx], r9w
$LN4@hide_block:

; 349  :     } // Hide
; 350  :   }
; 351  :   if (last_cg == 1)
; 352  :     last_cg = 0;
; 353  : 
; 354  :   return last_cg;

	xor	eax, eax
	cmp	r8d, 1
	cmovne	eax, r8d
	vzeroupper

; 355  : }

	vmovaps	xmm14, XMMWORD PTR [rsp+16]
	lea	r11, QWORD PTR [rsp+160]
	mov	rbx, QWORD PTR [r11+16]
	mov	rsi, QWORD PTR [r11+24]
	vmovaps	xmm10, XMMWORD PTR [r11-80]
	mov	rsp, r11
	pop	rdi
	ret	0
hide_block_sign ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
scan$1$ = 0
tv3098 = 8
tv3097 = 16
tv3096 = 24
tv3095 = 32
tv3086 = 40
tv3153 = 64
invec_rearr_masks_upper$2 = 64
tv3108 = 128
rearr_masks_lo$3 = 128
blend_masks$4 = 128
tv3109 = 224
blend_masks$5 = 224
low128_shuffle_masks$6 = 224
shufmasks$7 = 320
invec_rearr_masks_lower$8 = 320
tv3130 = 416
rearr_masks_hi$9 = 448
state$ = 784
coef$ = 792
q_coef$ = 800
width$ = 808
height$ = 816
type$ = 824
scan_idx$ = 832
block_type$ = 840
kvz_quant_avx2 PROC

; 363  : {

$LN76:
	mov	rax, rsp
	mov	QWORD PTR [rax+8], rbx
	mov	QWORD PTR [rax+16], rsi
	mov	QWORD PTR [rax+24], rdi
	mov	DWORD PTR [rax+32], r9d
	push	rbp
	push	r12
	push	r13
	push	r14
	push	r15
	sub	rsp, 736				; 000002e0H
	vmovaps	XMMWORD PTR [rax-56], xmm6
	vmovaps	XMMWORD PTR [rax-72], xmm7
	vmovaps	XMMWORD PTR [rax-88], xmm8
	vmovaps	XMMWORD PTR [rax-104], xmm9
	vmovaps	XMMWORD PTR [rax-120], xmm10
	vmovaps	XMMWORD PTR [rax-136], xmm11
	vmovaps	XMMWORD PTR [rax-152], xmm12
	vmovaps	XMMWORD PTR [rax-168], xmm13
	vmovaps	XMMWORD PTR [rax-184], xmm14
	vmovaps	XMMWORD PTR [rax-200], xmm15
	lea	rbp, QWORD PTR [rsp+32]
	and	rbp, -32				; ffffffffffffffe0H

; 364  :   const encoder_control_t * const encoder = state->encoder_control;

	mov	rsi, QWORD PTR [rcx]
	mov	rbx, rcx
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c

; 144  :   if(type == 0) {

	movsx	r11, BYTE PTR type$[rsp]
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 365  :   const uint32_t log2_block_size = kvz_g_convert_to_bit[width] + 2;

	lea	rcx, OFFSET FLAT:__ImageBase
	movsxd	rdi, r9d
	mov	r12, rdx

; 366  :   const uint32_t * const  __restrict scan = kvz_g_sig_last_scan[scan_idx][log2_block_size - 1];

	movsx	rdx, BYTE PTR scan_idx$[rsp]
	mov	r14, r8

; 367  : 
; 368  :   int32_t qp_scaled = kvz_get_scaled_qp(type, state->qp, (encoder->bitdepth - 8) * 6);

	movsx	r15d, BYTE PTR [rsi+2592]
	movsx	r9d, BYTE PTR kvz_g_convert_to_bit[rdi+rcx]
	lea	rax, QWORD PTR [rdx+rdx*4]
	lea	r10d, DWORD PTR [r9+1]
	add	r10, rax
	mov	rax, QWORD PTR kvz_g_sig_last_scan[rcx+r10*8]
	lea	ecx, DWORD PTR [r15-8]
	mov	QWORD PTR scan$1$[rbp], rax
	movzx	eax, cl
	add	al, al
	add	cl, al
	movsx	eax, BYTE PTR [rbx+384]
	add	cl, cl
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c

; 144  :   if(type == 0) {

	test	r11b, r11b
	jne	SHORT $LN15@kvz_quant_

; 145  :     qp_scaled = qp + qp_offset;

	mov	r10d, eax
	movsx	eax, cl
	add	r10d, eax

; 146  :   } else {

	lea	rcx, OFFSET FLAT:__ImageBase
	jmp	SHORT $LN18@kvz_quant_
$LN15@kvz_quant_:

; 147  :     qp_scaled = CLIP(-qp_offset, 57, qp);

	mov	r8d, 57					; 00000039H
	cmp	al, 57					; 00000039H
	jg	SHORT $LN21@kvz_quant_
	mov	r8d, eax
$LN21@kvz_quant_:
	movsx	edx, cl
	mov	r10d, edx
	neg	r10d
	cmp	r10d, r8d
	jg	$LN23@kvz_quant_
	cmp	al, 57					; 00000039H
	jle	$LN22@kvz_quant_
	mov	r10d, 57				; 00000039H
$LN17@kvz_quant_:

; 151  :       qp_scaled = kvz_g_chroma_scale[qp_scaled] + qp_offset;

	mov	eax, r10d
	lea	rcx, OFFSET FLAT:__ImageBase
	movzx	r10d, BYTE PTR kvz_g_chroma_scale[rax+rcx]
$LN74@kvz_quant_:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 369  :   const uint32_t log2_tr_size = kvz_g_convert_to_bit[width] + 2;

	add	r10d, edx
$LN18@kvz_quant_:

; 370  :   const int32_t scalinglist_type = (block_type == CU_INTRA ? 0 : 3) + (int8_t)("\0\3\1\2"[type]);

	movsx	ecx, BYTE PTR $SG4294950264[r11+rcx]
	xor	r8d, r8d

; 371  :   const int32_t *quant_coeff = encoder->scaling_list.quant_coeff[log2_tr_size - 2][scalinglist_type][qp_scaled % 6];
; 372  :   const int32_t transform_shift = MAX_TR_DYNAMIC_RANGE - encoder->bitdepth - log2_tr_size; //!< Represents scaling through forward transform
; 373  :   const int32_t q_bits = QUANT_SHIFT + qp_scaled / 6 + transform_shift;
; 374  :   const int32_t add = ((state->frame->slicetype == KVZ_SLICE_I) ? 171 : 85) << (q_bits - 9);
; 375  :   const int32_t q_bits8 = q_bits - 8;
; 376  : 
; 377  :   uint32_t ac_sum = 0;
; 378  :   int32_t last_cg = -1;
; 379  : 
; 380  :   __m256i v_ac_sum = _mm256_setzero_si256();
; 381  : 
; 382  :   // Loading once is enough if scaling lists are not off
; 383  :   __m256i low_b = _mm256_setzero_si256(), high_b = _mm256_setzero_si256();
; 384  :   if (!(state->encoder_control->scaling_list.enable)) {

	movzx	r13d, BYTE PTR [rsi+2616]
	add	r9d, 2
	mov	eax, 715827883				; 2aaaaaabH
	imul	r10d
	vpxor	xmm13, xmm13, xmm13
	mov	eax, edx
	shr	eax, 31
	add	edx, eax
	mov	eax, 3
	cmp	BYTE PTR block_type$[rsp], 1
	vpxor	xmm7, xmm7, xmm7
	cmove	eax, r8d
	add	ecx, eax
	lea	eax, DWORD PTR [r9-2]
	movsxd	r8, ecx
	vpxor	xmm14, xmm14, xmm14
	lea	rax, QWORD PTR [rax+rax*2]
	lea	rcx, QWORD PTR [r8+rax*2]
	lea	eax, DWORD PTR [rdx+rdx*2]
	sub	edx, r15d
	add	eax, eax
	lea	r8, QWORD PTR [rcx+rcx*2]
	sub	r10d, eax
	sub	edx, r9d
	add	edx, 29
	movsxd	rax, r10d
	lea	rcx, QWORD PTR [rax+r8*2]
	mov	rax, QWORD PTR [rbx+40]
	mov	r11, QWORD PTR [rsi+rcx*8+2912]
	mov	r8d, 171				; 000000abH
	mov	ecx, 85					; 00000055H
	cmp	DWORD PTR [rax+88], 2
	lea	eax, DWORD PTR [rdx-9]
	cmove	ecx, r8d
	shlx	r15d, ecx, eax
	mov	ecx, -1
	test	r13b, r13b
	jne	SHORT $LN8@kvz_quant_

; 385  :     low_b  = _mm256_set1_epi32(quant_coeff[0]);

	vpbroadcastd ymm14, DWORD PTR [r11]
$LN8@kvz_quant_:

; 386  :     high_b = low_b;
; 387  :   }
; 388  : 
; 389  :   for (int32_t n = 0; n < width * height; n += VEC_WIDTH) {

	mov	ebx, edi
	imul	ebx, DWORD PTR height$[rsp]
	vmovdqu	ymm15, ymm14
	movsxd	r10, ebx
	test	ebx, ebx
	jle	$LN3@kvz_quant_

; 394  : 
; 395  :     if (state->encoder_control->scaling_list.enable) {

	vmovdqu	ymm9, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	xor	r8d, r8d
	mov	r9, r12
	vmovd	xmm6, r15d
	vpbroadcastd ymm6, xmm6
	sub	r9, r14
	mov	rax, r14
	npad	4
$LL4@kvz_quant_:

; 390  : 
; 391  :     __m256i v_level = _mm256_loadu_si256((__m256i *)(coef + n));
; 392  :     __m256i v_sign = _mm256_cmpgt_epi16(_mm256_setzero_si256(), v_level);

	vmovdqu	ymm2, YMMWORD PTR [r9+rax]
	vpcmpgtw ymm0, ymm13, ymm2

; 393  :     v_sign = _mm256_or_si256(v_sign, _mm256_set1_epi16(1));

	vpor	ymm8, ymm0, ymm9

; 394  : 
; 395  :     if (state->encoder_control->scaling_list.enable) {

	test	r13b, r13b
	je	SHORT $LN9@kvz_quant_

; 396  :       __m256i v_quant_coeff_lo = _mm256_loadu_si256(((__m256i *)(quant_coeff + n)) + 0);

	vmovdqu	ymm1, YMMWORD PTR [r11+r8*4]

; 397  :       __m256i v_quant_coeff_hi = _mm256_loadu_si256(((__m256i *)(quant_coeff + n)) + 1);
; 398  : 
; 399  :       low_b  = _mm256_permute2x128_si256(v_quant_coeff_lo,

	vperm2i128 ymm14, ymm1, YMMWORD PTR [r11+r8*4+32], 32 ; 00000020H

; 400  :                                          v_quant_coeff_hi,
; 401  :                                          0x20);
; 402  : 
; 403  :       high_b = _mm256_permute2x128_si256(v_quant_coeff_lo,

	vperm2i128 ymm15, ymm1, YMMWORD PTR [r11+r8*4+32], 49 ; 00000031H
$LN9@kvz_quant_:

; 404  :                                          v_quant_coeff_hi,
; 405  :                                          0x31);
; 406  :     }
; 407  : 
; 408  : // TODO: do we need to have this?
; 409  : // #define CHECK_QUANT_COEFFS
; 410  : #ifdef CHECK_QUANT_COEFFS
; 411  :       __m256i abs_vq_lo = _mm256_abs_epi32(v_quant_coeff_lo);
; 412  :       __m256i abs_vq_hi = _mm256_abs_epi32(v_quant_coeff_hi);
; 413  : 
; 414  :       __m256i vq_over_16b_lo = _mm256_cmpgt_epi32(abs_vq_lo, _mm256_set1_epi32(0x7fff));
; 415  :       __m256i vq_over_16b_hi = _mm256_cmpgt_epi32(abs_vq_hi, _mm256_set1_epi32(0x7fff));
; 416  : 
; 417  :       uint32_t over_16b_mask_lo = _mm256_movemask_epi8(vq_over_16b_lo);
; 418  :       uint32_t over_16b_mask_hi = _mm256_movemask_epi8(vq_over_16b_hi);
; 419  : 
; 420  :       assert(!(over_16b_mask_lo || over_16b_mask_hi));
; 421  : #endif
; 422  : 
; 423  :     v_level = _mm256_abs_epi16(v_level);

	vpabsw	ymm4, ymm2

; 424  :     __m256i low_a  = _mm256_unpacklo_epi16(v_level, _mm256_setzero_si256());

	vpunpcklwd ymm0, ymm4, ymm13

; 425  :     __m256i high_a = _mm256_unpackhi_epi16(v_level, _mm256_setzero_si256());
; 426  : 
; 427  :     __m256i v_level32_a = _mm256_mullo_epi32(low_a,  low_b);

	vpmulld	ymm1, ymm0, ymm14

; 428  :     __m256i v_level32_b = _mm256_mullo_epi32(high_a, high_b);
; 429  : 
; 430  :     v_level32_a = _mm256_add_epi32(v_level32_a, _mm256_set1_epi32(add));

	vpaddd	ymm3, ymm1, ymm6
	vpunpckhwd ymm0, ymm4, ymm13

; 431  :     v_level32_b = _mm256_add_epi32(v_level32_b, _mm256_set1_epi32(add));
; 432  : 
; 433  :     v_level32_a = _mm256_srai_epi32(v_level32_a, q_bits);

	vmovd	xmm2, edx
	vpsrad	ymm5, ymm3, xmm2
	vpmulld	ymm1, ymm0, ymm15
	vpaddd	ymm3, ymm1, ymm6

; 434  :     v_level32_b = _mm256_srai_epi32(v_level32_b, q_bits);

	vmovd	xmm2, edx
	vpsrad	ymm4, ymm3, xmm2

; 435  : 
; 436  :     v_level = _mm256_packs_epi32(v_level32_a, v_level32_b);

	vpackssdw ymm0, ymm5, ymm4

; 437  :     v_level = _mm256_sign_epi16(v_level, v_sign);

	vpsignw	ymm1, ymm0, ymm8

; 438  : 
; 439  :     _mm256_storeu_si256((__m256i *)(q_coef + n), v_level);

	vmovdqu	YMMWORD PTR [rax], ymm1
	add	rax, 32					; 00000020H
	add	r8, 16

; 440  : 
; 441  :     v_ac_sum = _mm256_add_epi32(v_ac_sum, v_level32_a);

	vpaddd	ymm2, ymm5, ymm7

; 442  :     v_ac_sum = _mm256_add_epi32(v_ac_sum, v_level32_b);

	vpaddd	ymm7, ymm2, ymm4
	cmp	r8, r10
	jl	SHORT $LL4@kvz_quant_
$LN3@kvz_quant_:

; 449  : 
; 450  :   if (!encoder->cfg.signhide_enable || ac_sum < 2)

	cmp	DWORD PTR [rsi+52], 0
	vextracti128 xmm0, ymm7, 1
	vpaddd	xmm1, xmm0, xmm7
	vpshufd	xmm0, xmm1, 78				; 0000004eH
	vpaddd	xmm2, xmm0, xmm1
	je	$LN11@kvz_quant_

; 443  :   }
; 444  : 
; 445  :   __m128i temp = _mm_add_epi32(_mm256_castsi256_si128(v_ac_sum), _mm256_extracti128_si256(v_ac_sum, 1));
; 446  :   temp = _mm_add_epi32(temp, _mm_shuffle_epi32(temp, _MM_SHUFFLE(1, 0, 3, 2)));
; 447  :   temp = _mm_add_epi32(temp, _mm_shuffle_epi32(temp, _MM_SHUFFLE(0, 1, 0, 1)));

	vpshufd	xmm0, xmm2, 17
	vpaddd	xmm1, xmm0, xmm2

; 448  :   ac_sum += _mm_cvtsi128_si32(temp);

	vmovd	eax, xmm1

; 449  : 
; 450  :   if (!encoder->cfg.signhide_enable || ac_sum < 2)

	cmp	eax, 2
	jb	$LN11@kvz_quant_

; 451  :     return;
; 452  : 
; 453  :   assert(VEC_WIDTH == SCAN_SET_SIZE);
; 454  :   for (int32_t subpos = (width * height - 1) & (~(VEC_WIDTH - 1)); subpos >= 0; subpos -= VEC_WIDTH) {

	dec	ebx
	and	ebx, -16
	jl	$LN11@kvz_quant_
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 137  :     coeffs_lower   = _mm_shuffle_epi8(coeffs_lower, low128_shuffle_masks[scan_mode]);

	movsx	rsi, BYTE PTR scan_idx$[rsp]
	lea	r8, QWORD PTR low128_shuffle_masks$6[rbp]
	vmovdqu	xmm1, XMMWORD PTR __xmm@030209080f0e070601000d0c05040b0a
	vmovdqu	xmm0, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	vmovdqu	xmm2, XMMWORD PTR __xmm@0b0a09080f0e0d0c0302010007060504
	vmovdqu	xmm3, XMMWORD PTR __xmm@ffffffff00000000ffff000000000000
	vmovdqu	xmm5, XMMWORD PTR __xmm@ffffffff00000000ffffffff00000000
	vmovdqu	xmm6, XMMWORD PTR __xmm@0f0e0d0c05040b0a0706030209080100
	vmovdqu	xmm7, XMMWORD PTR __xmm@0f0e07060b0a03020d0c050409080100
	vmovdqu	xmm8, XMMWORD PTR __xmm@0b0a090805040f0e0302010007060d0c
	vmovdqu	xmm12, XMMWORD PTR __xmm@0b0a03020f0e0706090801000d0c0504
	vmovd	xmm11, r15d
	mov	rax, rsi
	lea	r10d, DWORD PTR [rdi+rdi]
	shl	rax, 4
	add	r8, rax
	mov	QWORD PTR tv3098[rbp], r8

; 138  : 
; 139  :     coeffs_rearr1_upper = _mm_blendv_epi8(coeffs_upper, coeffs_lower, blend_masks[scan_mode]);
; 140  :     coeffs_rearr1_lower = _mm_blendv_epi8(coeffs_lower, coeffs_upper, blend_masks[scan_mode]);

	lea	r8, QWORD PTR blend_masks$4[rbp]
	add	r8, rax
	mov	QWORD PTR tv3097[rbp], r8

; 141  : 
; 142  :     coeffs_rearr2_upper = _mm_shuffle_epi8(coeffs_rearr1_upper, invec_rearr_masks_upper[scan_mode]);
; 143  :     coeffs_rearr2_lower = _mm_shuffle_epi8(coeffs_rearr1_lower, invec_rearr_masks_lower[scan_mode]);

	lea	r8, QWORD PTR invec_rearr_masks_lower$8[rbp]
	add	r8, rax
	mov	QWORD PTR tv3096[rbp], r8
	lea	r8, QWORD PTR invec_rearr_masks_upper$2[rbp]
	add	rax, r8
	mov	r8, QWORD PTR scan$1$[rbp]
	mov	QWORD PTR tv3095[rbp], rax
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 467  :     if (state->encoder_control->scaling_list.enable) {

	movsxd	rax, ebx
	vpbroadcastd ymm11, xmm11
	vpxor	xmm4, xmm4, xmm4
	vxorpd	xmm9, xmm9, xmm9
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 137  :     coeffs_lower   = _mm_shuffle_epi8(coeffs_lower, low128_shuffle_masks[scan_mode]);

	lea	r15, QWORD PTR [r8+rax*4]
	vmovdqu	YMMWORD PTR tv3130[rbp], ymm11
	vpxor	xmm10, xmm10, xmm10
	npad	9
$LL7@kvz_quant_:

; 97   :   const size_t row_offsets[4] = {

	mov	r8d, DWORD PTR [r15]

; 137  :     coeffs_lower   = _mm_shuffle_epi8(coeffs_lower, low128_shuffle_masks[scan_mode]);

	mov	rax, QWORD PTR tv3098[rbp]
	add	r10d, r8d
	vmovdqu	XMMWORD PTR blend_masks$4[rbp], xmm3
	vmovdqu	XMMWORD PTR low128_shuffle_masks$6[rbp], xmm1
	vmovddup xmm3, QWORD PTR [r12+r8*2]
	vmovdqu	XMMWORD PTR low128_shuffle_masks$6[rbp+32], xmm2
	lea	r9d, DWORD PTR [r8+rdi]
	vmovhpd	xmm3, xmm3, QWORD PTR [r12+r9*2]
	vmovdqu	XMMWORD PTR invec_rearr_masks_upper$2[rbp+16], xmm0
	vmovdqu	XMMWORD PTR low128_shuffle_masks$6[rbp+16], xmm0
	vmovdqu	XMMWORD PTR blend_masks$4[rbp+16], xmm4
	lea	edi, DWORD PTR [rdi+rdi*2]
	add	edi, r8d
	vmovdqu	XMMWORD PTR invec_rearr_masks_lower$8[rbp+16], xmm0
	vmovddup xmm0, QWORD PTR [r12+r10*2]
	vmovdqu	XMMWORD PTR invec_rearr_masks_upper$2[rbp], xmm6
	vmovhpd	xmm0, xmm0, QWORD PTR [r12+rdi*2]
	vmovdqu	XMMWORD PTR invec_rearr_masks_lower$8[rbp], xmm8
	vmovdqu	XMMWORD PTR blend_masks$4[rbp+32], xmm5
	vmovdqu	xmm5, XMMWORD PTR [rax]

; 138  : 
; 139  :     coeffs_rearr1_upper = _mm_blendv_epi8(coeffs_upper, coeffs_lower, blend_masks[scan_mode]);
; 140  :     coeffs_rearr1_lower = _mm_blendv_epi8(coeffs_lower, coeffs_upper, blend_masks[scan_mode]);

	mov	rax, QWORD PTR tv3097[rbp]
	vmovdqu	XMMWORD PTR invec_rearr_masks_upper$2[rbp+32], xmm7
	vmovdqu	XMMWORD PTR invec_rearr_masks_lower$8[rbp+32], xmm12
	vpshufb	xmm2, xmm0, xmm5
	vmovdqu	xmm7, XMMWORD PTR [rax]

; 141  : 
; 142  :     coeffs_rearr2_upper = _mm_shuffle_epi8(coeffs_rearr1_upper, invec_rearr_masks_upper[scan_mode]);
; 143  :     coeffs_rearr2_lower = _mm_shuffle_epi8(coeffs_rearr1_lower, invec_rearr_masks_lower[scan_mode]);

	mov	rax, QWORD PTR tv3096[rbp]
	vpblendvb xmm1, xmm2, xmm3, xmm7
	vpblendvb xmm2, xmm3, xmm2, xmm7
	vmovdqu	xmm6, XMMWORD PTR [rax]
	mov	rax, QWORD PTR tv3095[rbp]
	vpshufb	xmm4, xmm1, xmm6
	vmovddup xmm1, QWORD PTR [r14+r10*2]
	vmovhpd	xmm1, xmm1, QWORD PTR [r14+rdi*2]
	vmovdqu	xmm8, XMMWORD PTR [rax]
	vpshufb	xmm3, xmm2, xmm8

; 144  : 
; 145  :     // The Intel Intrinsics Guide talks about _mm256_setr_m128i but my headers
; 146  :     // lack such an instruction. What it does is essentially this anyway.
; 147  :     result_vecs[i] = _mm256_inserti128_si256(_mm256_castsi128_si256(coeffs_rearr2_upper),

	vinserti128 ymm12, ymm3, xmm4, 1
	vmovddup xmm3, QWORD PTR [r14+r8*2]
	vmovhpd	xmm3, xmm3, QWORD PTR [r14+r9*2]
	vpshufb	xmm2, xmm1, xmm5
	vpblendvb xmm1, xmm2, xmm3, xmm7
	vpblendvb xmm2, xmm3, xmm2, xmm7
	vpshufb	xmm3, xmm2, xmm8
	vpshufb	xmm4, xmm1, xmm6
	vinserti128 ymm8, ymm3, xmm4, 1
	vmovdqu	YMMWORD PTR tv3153[rbp], ymm12
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 467  :     if (state->encoder_control->scaling_list.enable) {

	test	r13b, r13b
	je	$LN69@kvz_quant_

; 169  :   const __m256i shufmasks[3] = {

	vmovdqu	ymm2, YMMWORD PTR __ymm@0000000700000006000000050000000400000003000000020000000100000000
	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000100000004000000070000000300000000000000060000000200000005

; 170  :     _mm256_setr_epi32(5, 2, 6, 0, 3, 7, 4, 1),
; 171  :     _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7),
; 172  :     _mm256_setr_epi32(2, 3, 0, 1, 6, 7, 4, 5),
; 173  :   };
; 174  : 
; 175  :   const __m256i blend_masks[3] = {

	vmovdqu	ymm1, YMMWORD PTR __ymm@ffffffffffffffff0000000000000000ffffffff000000000000000000000000
	vmovdqu	YMMWORD PTR shufmasks$7[rbp], ymm0
	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000500000004000000070000000600000001000000000000000300000002
	vmovdqu	YMMWORD PTR shufmasks$7[rbp+64], ymm0
	vpxor	xmm0, xmm0, xmm0
	vmovdqu	YMMWORD PTR blend_masks$5[rbp+32], ymm0

; 176  :     _mm256_setr_epi32( 0,  0,  0, -1,  0,  0, -1, -1),
; 177  :     _mm256_setr_epi32( 0,  0,  0,  0,  0,  0,  0,  0),
; 178  :     _mm256_setr_epi32( 0,  0, -1, -1,  0,  0, -1, -1),
; 179  :   };
; 180  : 
; 181  :   const __m256i rearr_masks_lo[3] = {

	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000700000006000000020000000500000003000000010000000400000000
	vmovdqu	YMMWORD PTR blend_masks$5[rbp], ymm1
	vmovdqu	ymm1, YMMWORD PTR __ymm@ffffffffffffffff0000000000000000ffffffffffffffff0000000000000000
	vmovdqu	YMMWORD PTR blend_masks$5[rbp+64], ymm1
	vmovdqu	ymm1, YMMWORD PTR __ymm@0000000700000003000000050000000100000006000000020000000400000000
	vmovdqu	YMMWORD PTR rearr_masks_lo$3[rbp], ymm0

; 182  :     _mm256_setr_epi32(0, 4, 1, 3, 5, 2, 6, 7),
; 183  :     _mm256_setr_epi32(0, 1, 2, 3, 4, 5, 6, 7),
; 184  :     _mm256_setr_epi32(0, 4, 2, 6, 1, 5, 3, 7),
; 185  :   };
; 186  : 
; 187  :   const __m256i rearr_masks_hi[3] = {

	vmovdqu	ymm0, YMMWORD PTR __ymm@0000000500000004000000020000000700000001000000000000000300000006
	vmovdqu	YMMWORD PTR rearr_masks_lo$3[rbp+64], ymm1
	vmovdqu	ymm1, YMMWORD PTR __ymm@0000000500000001000000070000000300000004000000000000000600000002
	vmovdqu	YMMWORD PTR rearr_masks_hi$9[rbp], ymm0

; 152  :   return _mm256_inserti128_si256(v, hi, 1);

	vmovups	xmm0, XMMWORD PTR [r11+r8*4]
	vinserti128 ymm6, ymm0, XMMWORD PTR [r11+r9*4], 1
	vmovups	xmm0, XMMWORD PTR [r11+r10*4]

; 187  :   const __m256i rearr_masks_hi[3] = {

	vmovdqu	YMMWORD PTR rearr_masks_hi$9[rbp+64], ymm1
	vmovdqu	YMMWORD PTR shufmasks$7[rbp+32], ymm2
	vmovdqu	YMMWORD PTR rearr_masks_lo$3[rbp+32], ymm2
	vmovdqu	YMMWORD PTR rearr_masks_hi$9[rbp+32], ymm2

; 152  :   return _mm256_inserti128_si256(v, hi, 1);

	vinserti128 ymm2, ymm0, XMMWORD PTR [r11+rdi*4], 1

; 203  :   __m256i lower_shuffled = _mm256_permutevar8x32_epi32(coeffs_lower, shufmasks[scan_mode]);

	mov	rax, rsi
	shl	rax, 5
	vmovdqu	ymm1, YMMWORD PTR shufmasks$7[rbp+rax]

; 204  : 
; 205  :   __m256i upper_blended  = _mm256_blendv_epi8(coeffs_upper,   lower_shuffled, blend_masks[scan_mode]);

	vmovdqu	ymm3, YMMWORD PTR blend_masks$5[rbp+rax]

; 206  :   __m256i lower_blended  = _mm256_blendv_epi8(lower_shuffled, coeffs_upper,   blend_masks[scan_mode]);
; 207  : 
; 208  :   __m256i result_lo      = _mm256_permutevar8x32_epi32(upper_blended, rearr_masks_lo[scan_mode]);

	vmovdqu	ymm0, YMMWORD PTR rearr_masks_lo$3[rbp+rax]
	vpermd	ymm4, ymm1, ymm2
	vpblendvb ymm1, ymm6, ymm4, ymm3
	vpermd	ymm5, ymm0, ymm1

; 209  :   __m256i result_hi      = _mm256_permutevar8x32_epi32(lower_blended, rearr_masks_hi[scan_mode]);

	vmovdqu	ymm0, YMMWORD PTR rearr_masks_hi$9[rbp+rax]
	vpblendvb ymm2, ymm4, ymm6, ymm3
	vpermd	ymm1, ymm0, ymm2

; 468  :       scanord_read_vector_32(quant_coeff, scan, scan_idx, subpos, width, v_quant_coeffs);
; 469  : 
; 470  :       v_quant_coeff_lo = v_quant_coeffs[0];
; 471  :       v_quant_coeff_hi = v_quant_coeffs[1];
; 472  : 
; 473  :       low_b  = _mm256_permute2x128_si256(v_quant_coeff_lo,

	vperm2i128 ymm14, ymm5, ymm1, 32		; 00000020H

; 474  :                                          v_quant_coeff_hi,
; 475  :                                          0x20);
; 476  : 
; 477  :       high_b = _mm256_permute2x128_si256(v_quant_coeff_lo,

	vperm2i128 ymm15, ymm5, ymm1, 49		; 00000031H
$LN69@kvz_quant_:

; 478  :                                          v_quant_coeff_hi,
; 479  :                                          0x31);
; 480  :     }
; 481  : 
; 482  :     __m256i v_level = _mm256_abs_epi16(v_coef);

	vpabsw	ymm6, ymm12

; 483  :     __m256i low_a  = _mm256_unpacklo_epi16(v_level, _mm256_setzero_si256());
; 484  :     __m256i high_a = _mm256_unpackhi_epi16(v_level, _mm256_setzero_si256());

	vpunpckhwd ymm0, ymm6, ymm13

; 485  : 
; 486  :     __m256i v_quant_coeff_a = _mm256_or_si256(low_b,  _mm256_setzero_si256());
; 487  :     __m256i v_quant_coeff_b = _mm256_or_si256(high_b, _mm256_setzero_si256());
; 488  : 
; 489  :     __m256i v_level32_a = _mm256_mullo_epi32(low_a,  low_b);
; 490  :     __m256i v_level32_b = _mm256_mullo_epi32(high_a, high_b);

	vpmulld	ymm1, ymm0, ymm15

; 491  : 
; 492  :     v_level32_a = _mm256_add_epi32(v_level32_a, _mm256_set1_epi32(add));
; 493  :     v_level32_b = _mm256_add_epi32(v_level32_b, _mm256_set1_epi32(add));

	vpaddd	ymm3, ymm1, ymm11
	vpunpcklwd ymm0, ymm6, ymm13
	vpmulld	ymm1, ymm0, ymm14

; 496  :     v_level32_b = _mm256_srai_epi32(v_level32_b, q_bits);

	vmovd	xmm2, edx
	vpsrad	ymm4, ymm3, xmm2
	vpaddd	ymm3, ymm1, ymm11
	vpor	ymm1, ymm13, ymm14

; 507  :     v_coef_b = _mm256_sub_epi32(v_coef_b, _mm256_slli_epi32(_mm256_unpackhi_epi16(v_level, _mm256_set1_epi16(0)), q_bits) );
; 508  :     v_coef_a = _mm256_srai_epi32(v_coef_a, q_bits8);

	lea	eax, DWORD PTR [rdx-8]
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 167  :   *last = (31 - (int32_t)_lzcnt_u32(nonzero_bytes)) >> 1;

	mov	r10d, 31
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 495  :     v_level32_a = _mm256_srai_epi32(v_level32_a, q_bits);

	vmovd	xmm2, edx
	vpsrad	ymm0, ymm3, xmm2

; 497  : 
; 498  :     v_level = _mm256_packs_epi32(v_level32_a, v_level32_b);

	vpackssdw ymm5, ymm0, ymm4

; 499  : 
; 500  :     __m256i v_coef_a = _mm256_unpacklo_epi16(_mm256_abs_epi16(v_coef), _mm256_set1_epi16(0));
; 501  :     __m256i v_coef_b = _mm256_unpackhi_epi16(_mm256_abs_epi16(v_coef), _mm256_set1_epi16(0));
; 502  : 
; 503  :     v_coef_a = _mm256_mullo_epi32(v_coef_a, v_quant_coeff_a);
; 504  :     v_coef_b = _mm256_mullo_epi32(v_coef_b, v_quant_coeff_b);
; 505  : 
; 506  :     v_coef_a = _mm256_sub_epi32(v_coef_a, _mm256_slli_epi32(_mm256_unpacklo_epi16(v_level, _mm256_set1_epi16(0)), q_bits) );

	vpunpcklwd ymm2, ymm5, ymm10
	vpunpcklwd ymm0, ymm6, ymm10
	vpmulld	ymm3, ymm0, ymm1
	vmovd	xmm1, edx
	vpslld	ymm0, ymm2, xmm1
	vpsubd	ymm3, ymm3, ymm0

; 507  :     v_coef_b = _mm256_sub_epi32(v_coef_b, _mm256_slli_epi32(_mm256_unpackhi_epi16(v_level, _mm256_set1_epi16(0)), q_bits) );
; 508  :     v_coef_a = _mm256_srai_epi32(v_coef_a, q_bits8);

	vmovd	xmm2, eax
	vpsrad	ymm4, ymm3, xmm2
	vpunpckhwd ymm2, ymm5, ymm10
	vpunpckhwd ymm0, ymm6, ymm10
	vpor	ymm1, ymm13, ymm15
	vpmulld	ymm3, ymm0, ymm1
	vmovd	xmm1, edx
	vpslld	ymm0, ymm2, xmm1
	vpsubd	ymm3, ymm3, ymm0

; 509  :     v_coef_b = _mm256_srai_epi32(v_coef_b, q_bits8);

	vmovd	xmm2, eax
	vpsrad	ymm1, ymm3, xmm2

; 510  :     
; 511  :     __m256i deltas_h = _mm256_permute2x128_si256(v_coef_a, v_coef_b, 0x31);
; 512  :     __m256i deltas_l = _mm256_permute2x128_si256(v_coef_a, v_coef_b, 0x20);

	vperm2i128 ymm5, ymm4, ymm1, 32			; 00000020H
	vperm2i128 ymm6, ymm4, ymm1, 49			; 00000031H
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 164  :   __m256i zeros = _mm256_cmpeq_epi16(ints, zero);

	vpcmpeqw ymm0, ymm8, ymm13

; 165  :   uint32_t nonzero_bytes = ~((uint32_t)_mm256_movemask_epi8(zeros));

	vpmovmskb eax, ymm0
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 75   :   __m128i b = _mm256_extracti128_si256(src, 1);

	vextracti128 xmm0, ymm8, 1

; 76   :   __m256i c = _mm256_cvtepi16_epi32(a);
; 77   :   __m256i d = _mm256_cvtepi16_epi32(b);

	vpmovsxwd ymm1, xmm0
	vpmovsxwd ymm2, xmm8

; 78   : 
; 79   :   c = _mm256_add_epi32(c, d);

	vpaddd	ymm3, ymm1, ymm2

; 60   :   __m128i b = _mm256_extracti128_si256(src, 1);

	vextracti128 xmm0, ymm3, 1

; 61   : 
; 62   :   a = _mm_add_epi32(a, b);

	vpaddd	xmm1, xmm0, xmm3

; 63   :   b = _mm_shuffle_epi32(a, _MM_SHUFFLE(0, 1, 2, 3));

	vpshufd	xmm0, xmm1, 27
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 165  :   uint32_t nonzero_bytes = ~((uint32_t)_mm256_movemask_epi8(zeros));

	not	eax
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 514  :     last_cg = hide_block_sign(v_coef, q_coefs, deltas_h, deltas_l, q_coef, scan, subpos, last_cg);

	mov	r9d, ecx
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 166  :   *first = (    (int32_t)_tzcnt_u32(nonzero_bytes)) >> 1;

	tzcnt	edi, eax
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 65   :   a = _mm_add_epi32(a, b);

	vpaddd	xmm2, xmm0, xmm1
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 167  :   *last = (31 - (int32_t)_lzcnt_u32(nonzero_bytes)) >> 1;

	lzcnt	eax, eax
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 66   :   b = _mm_shuffle_epi32(a, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	xmm1, xmm2, 177				; 000000b1H
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\avx2_common_functions.h

; 166  :   *first = (    (int32_t)_tzcnt_u32(nonzero_bytes)) >> 1;

	sar	edi, 1

; 167  :   *last = (31 - (int32_t)_lzcnt_u32(nonzero_bytes)) >> 1;

	sub	r10d, eax
	sar	r10d, 1
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 68   :   a = _mm_add_epi32(a, b);

	vpaddd	xmm0, xmm1, xmm2

; 69   :   return _mm_cvtsi128_si32(a);

	vmovd	esi, xmm0

; 510  :     
; 511  :     __m256i deltas_h = _mm256_permute2x128_si256(v_coef_a, v_coef_b, 0x31);
; 512  :     __m256i deltas_l = _mm256_permute2x128_si256(v_coef_a, v_coef_b, 0x20);

	vmovdqu	YMMWORD PTR tv3109[rbp], ymm5
	vmovdqu	YMMWORD PTR tv3108[rbp], ymm6

; 232  :   if (last_nz_pos_in_cg >= 0 && last_cg == -1) {

	js	SHORT $LN38@kvz_quant_
	cmp	ecx, -1
	mov	eax, 1
	cmove	r9d, eax
$LN38@kvz_quant_:

; 233  :     last_cg = 1;
; 234  :   }
; 235  : 
; 236  :   if (last_nz_pos_in_cg - first_nz_pos_in_cg >= 4) {

	mov	eax, r10d
	sub	eax, edi
	cmp	eax, 4
	jl	$LN40@kvz_quant_

; 237  : 
; 238  :     uint32_t q_coef_signbits = _mm256_movemask_epi8(q_coefs);

	vpmovmskb ecx, ymm8

; 239  :     int32_t signbit = (q_coef_signbits >> (2 * first_nz_pos_in_cg + 1)) & 0x1;

	lea	eax, DWORD PTR [rdi*2+1]

; 240  : 
; 241  :     if (signbit != (abssum & 0x1)) { // compare signbit with sum_parity

	and	esi, 1
	shrx	r8d, ecx, eax
	and	r8d, 1
	cmp	r8d, esi
	je	$LN40@kvz_quant_

; 242  :       int32_t min_pos;
; 243  :       int16_t final_change;
; 244  :       int16_t cheapest_q;
; 245  : 
; 246  :       const int32_t mask_max = (last_cg == 1) ? last_nz_pos_in_cg : SCAN_SET_SIZE - 1;
; 247  : 
; 248  :       const __m256i zero = _mm256_setzero_si256();
; 249  :       const __m256i ones = _mm256_set1_epi16(1);
; 250  :       const __m256i maxiters = _mm256_set1_epi16(mask_max);
; 251  :       const __m256i ff = _mm256_set1_epi8(0xff);
; 252  : 
; 253  :       const __m256i fnpics = _mm256_set1_epi16((int16_t)first_nz_pos_in_cg);
; 254  :       const __m256i ns = _mm256_setr_epi16(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);

	vmovdqu	ymm12, YMMWORD PTR __ymm@000f000e000d000c000b000a0009000800070006000500040003000200010000

; 255  : 
; 256  :       __m256i block_signbit = _mm256_set1_epi16(((int16_t)signbit) * -1);
; 257  :       __m256i coef_signbits = _mm256_cmpgt_epi16(zero, coefs);

	vpcmpgtw ymm4, ymm13, YMMWORD PTR tv3153[rbp]

; 258  :       __m256i signbits_equal_block = _mm256_cmpeq_epi16(coef_signbits, block_signbit);
; 259  : 
; 260  :       __m256i q_coefs_zero = _mm256_cmpeq_epi16(q_coefs, zero);

	vpcmpeqw ymm7, ymm8, ymm13

; 261  : 
; 262  :       __m256i dus_packed = _mm256_packs_epi32(deltas_l, deltas_h);

	vpackssdw ymm0, ymm5, ymm6

; 263  :       __m256i dus_ordered = _mm256_permute4x64_epi64(dus_packed, _MM_SHUFFLE(3, 1, 2, 0));

	vpermq	ymm1, ymm0, 216				; 000000d8H

; 264  :       __m256i dus_positive = _mm256_cmpgt_epi16(dus_ordered, zero);

	vpcmpgtw ymm6, ymm1, ymm13

; 265  : 
; 266  :       __m256i q_coef_abss = _mm256_abs_epi16(q_coefs);

	vpabsw	ymm0, ymm8

; 267  :       __m256i q_coefs_plusminus_one = _mm256_cmpeq_epi16(q_coef_abss, ones);

	vpcmpeqw ymm1, ymm0, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001
	movsx	eax, di
	neg	r8w
	vmovd	xmm3, eax
	vpbroadcastw ymm3, xmm3

; 268  : 
; 269  :       __m256i eq_fnpics = _mm256_cmpeq_epi16(fnpics, ns);

	vpcmpeqw ymm2, ymm12, ymm3

; 270  :       __m256i lt_fnpics = _mm256_cmpgt_epi16(fnpics, ns);
; 271  : 
; 272  :       __m256i maxcost_subcond1s = _mm256_and_si256(eq_fnpics, q_coefs_plusminus_one);

	vpand	ymm5, ymm1, ymm2

; 273  :       __m256i maxcost_subcond2s = _mm256_andnot_si256(signbits_equal_block, lt_fnpics);
; 274  :       __m256i elsecond1s_inv = _mm256_or_si256(dus_positive, maxcost_subcond1s);

	vpor	ymm2, ymm5, ymm6
	movsx	eax, r8w
	cmp	r9d, 1
	vmovd	xmm0, eax
	vpbroadcastw ymm0, xmm0
	vpcmpeqw ymm1, ymm0, ymm4
	vpcmpgtw ymm0, ymm3, ymm12
	vpandn	ymm4, ymm1, ymm0

; 275  :       __m256i elsecond1s = _mm256_andnot_si256(elsecond1s_inv, ff);
; 276  : 
; 277  :       __m256i outside_maxiters = _mm256_cmpgt_epi16(ns, maxiters);
; 278  : 
; 279  :       __m256i negdelta_cond1s = _mm256_andnot_si256(q_coefs_zero, dus_positive);

	vpandn	ymm1, ymm7, ymm6
	mov	eax, 15
	cmove	ax, r10w
	cwde
	vmovd	xmm0, eax
	vpbroadcastw ymm0, xmm0
	vpcmpgtw ymm3, ymm12, ymm0

; 280  :       __m256i negdelta_cond2s = _mm256_andnot_si256(maxcost_subcond2s, q_coefs_zero);

	vpandn	ymm0, ymm4, ymm7

; 281  :       __m256i negdelta_mask16s_part1 = _mm256_or_si256(negdelta_cond1s, negdelta_cond2s);

	vpor	ymm1, ymm0, ymm1

; 282  :       __m256i negdelta_mask16s = _mm256_andnot_si256(outside_maxiters, negdelta_mask16s_part1);

	vpandn	ymm8, ymm3, ymm1
	vpandn	ymm1, ymm2, YMMWORD PTR __ymm@ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff

; 283  : 
; 284  :       __m256i posdelta_mask16s_part1 = _mm256_andnot_si256(q_coefs_zero, elsecond1s);

	vpandn	ymm2, ymm7, ymm1

; 285  :       __m256i posdelta_mask16s = _mm256_andnot_si256(outside_maxiters, posdelta_mask16s_part1);

	vpandn	ymm9, ymm3, ymm2

; 286  : 
; 287  :       __m256i maxcost_cond1_parts = _mm256_andnot_si256(dus_positive, maxcost_subcond1s);
; 288  :       __m256i maxcost_cond1s = _mm256_andnot_si256(q_coefs_zero, maxcost_cond1_parts);
; 289  :       __m256i maxcost_cond2s = _mm256_and_si256(q_coefs_zero, maxcost_subcond2s);

	vpand	ymm2, ymm4, ymm7
	vpandn	ymm0, ymm6, ymm5
	vpandn	ymm1, ymm7, ymm0

; 88   :   const __m256i perm8x32mask = _mm256_setr_epi32(0, 2, 4, 6, 1, 3, 5, 7);

	vmovdqu	ymm7, YMMWORD PTR __ymm@0000000700000005000000030000000100000006000000040000000200000000

; 290  :       __m256i maxcost_mask16s_parts = _mm256_or_si256(maxcost_cond1s, maxcost_cond2s);

	vpor	ymm2, ymm1, ymm2

; 291  :       __m256i maxcost_mask16s = _mm256_or_si256(maxcost_mask16s_parts, outside_maxiters);

	vpor	ymm6, ymm2, ymm3

; 298  : 
; 299  :       tmp_l = _mm256_extracti128_si256(posdelta_mask16s, 0);
; 300  :       tmp_h = _mm256_extracti128_si256(posdelta_mask16s, 1);
; 301  :       __m256i posdelta_mask32s_l = _mm256_cvtepi16_epi32(tmp_l);
; 302  :       __m256i posdelta_mask32s_h = _mm256_cvtepi16_epi32(tmp_h);
; 303  : 
; 304  :       tmp_l = _mm256_extracti128_si256(maxcost_mask16s, 0);
; 305  :       tmp_h = _mm256_extracti128_si256(maxcost_mask16s, 1);
; 306  :       __m256i maxcost_mask32s_l = _mm256_cvtepi16_epi32(tmp_l);
; 307  :       __m256i maxcost_mask32s_h = _mm256_cvtepi16_epi32(tmp_h);
; 308  : 
; 309  :       // Output value generation
; 310  :       // cur_change_max: zero
; 311  :       // cur_change_negdelta: ff
; 312  :       // cur_change_posdelta: ones
; 313  :       __m256i costs_negdelta_h = _mm256_sub_epi32(zero, deltas_h);
; 314  :       __m256i costs_negdelta_l = _mm256_sub_epi32(zero, deltas_l);
; 315  :       // costs_posdelta_l and _h: deltas_l and _h
; 316  :       __m256i costs_max_lh = _mm256_set1_epi32(0x7fffffff);
; 317  : 
; 318  :       __m256i change_neg = _mm256_and_si256(negdelta_mask16s, ones);

	vpand	ymm2, ymm8, YMMWORD PTR __ymm@0001000100010001000100010001000100010001000100010001000100010001

; 319  :       __m256i change_pos = _mm256_and_si256(posdelta_mask16s, ff);
; 320  :       __m256i change_max = _mm256_and_si256(maxcost_mask16s, zero);

	vpand	ymm0, ymm6, ymm13

; 328  : 
; 329  :       __m256i changes = _mm256_or_si256(change_neg, _mm256_or_si256(change_pos, change_max));

	vpor	ymm1, ymm0, ymm9
	vpor	ymm3, ymm1, ymm2
	vpsubd	ymm2, ymm13, YMMWORD PTR tv3108[rbp]

; 104  :   __m256i tmp1hi = _mm256_unpackhi_epi16(ns, changes);

	vpunpckhwd ymm11, ymm12, ymm3

; 105  :   __m256i tmp1lo = _mm256_unpacklo_epi16(ns, changes);

	vpunpcklwd ymm10, ymm12, ymm3

; 295  :       tmp_h = _mm256_extracti128_si256(negdelta_mask16s, 1);

	vextracti128 xmm0, ymm8, 1

; 297  :       __m256i negdelta_mask32s_h = _mm256_cvtepi16_epi32(tmp_h);

	vpmovsxwd ymm1, xmm0

; 323  :       __m256i cost_neg_h = _mm256_and_si256(negdelta_mask32s_h, costs_negdelta_h);

	vpand	ymm4, ymm1, ymm2
	vextracti128 xmm2, ymm9, 1
	vpmovsxwd ymm0, xmm2

; 325  :       __m256i cost_pos_h = _mm256_and_si256(posdelta_mask32s_h, deltas_h);

	vpand	ymm3, ymm0, YMMWORD PTR tv3108[rbp]
	vextracti128 xmm1, ymm6, 1
	vpmovsxwd ymm2, xmm1

; 327  :       __m256i cost_max_h = _mm256_and_si256(maxcost_mask32s_h, costs_max_lh);

	vpand	ymm0, ymm2, YMMWORD PTR __ymm@7fffffff7fffffff7fffffff7fffffff7fffffff7fffffff7fffffff7fffffff

; 331  :       __m256i costs_h = _mm256_or_si256(cost_neg_h, _mm256_or_si256(cost_pos_h, cost_max_h));

	vpor	ymm3, ymm0, ymm3
	vpsubd	ymm0, ymm13, YMMWORD PTR tv3109[rbp]
	vpor	ymm1, ymm3, ymm4

; 90   :   __m256i tmphi = _mm256_permutevar8x32_epi32(*hi, perm8x32mask);

	vpermd	ymm4, ymm7, ymm1

; 296  :       __m256i negdelta_mask32s_l = _mm256_cvtepi16_epi32(tmp_l);

	vpmovsxwd ymm1, xmm8

; 321  : 
; 322  :       __m256i cost_neg_l = _mm256_and_si256(negdelta_mask32s_l, costs_negdelta_l);

	vpand	ymm3, ymm0, ymm1
	vpmovsxwd ymm0, xmm9

; 324  :       __m256i cost_pos_l = _mm256_and_si256(posdelta_mask32s_l, deltas_l);

	vpand	ymm2, ymm0, YMMWORD PTR tv3109[rbp]
	vpmovsxwd ymm0, xmm6

; 326  :       __m256i cost_max_l = _mm256_and_si256(maxcost_mask32s_l, costs_max_lh);

	vpand	ymm1, ymm0, YMMWORD PTR __ymm@7fffffff7fffffff7fffffff7fffffff7fffffff7fffffff7fffffff7fffffff

; 330  :       __m256i costs_l = _mm256_or_si256(cost_neg_l, _mm256_or_si256(cost_pos_l, cost_max_l));

	vpor	ymm2, ymm1, ymm2
	vpor	ymm3, ymm2, ymm3

; 91   :   __m256i tmplo = _mm256_permutevar8x32_epi32(*lo, perm8x32mask);

	vpermd	ymm0, ymm7, ymm3

; 92   : 
; 93   :   *hi = _mm256_permute2x128_si256(tmplo, tmphi, 0x31);

	vperm2i128 ymm5, ymm0, ymm4, 49			; 00000031H

; 94   :   *lo = _mm256_permute2x128_si256(tmplo, tmphi, 0x20);

	vperm2i128 ymm4, ymm0, ymm4, 32			; 00000020H

; 107  :   __m256i pl1hi = _mm256_permute2x128_si256(tmp1lo, tmp1hi, 0x31);

	vperm2i128 ymm0, ymm10, ymm11, 49		; 00000031H

; 90   :   __m256i tmphi = _mm256_permutevar8x32_epi32(*hi, perm8x32mask);

	vpermd	ymm3, ymm7, ymm0

; 108  :   __m256i pl1lo = _mm256_permute2x128_si256(tmp1lo, tmp1hi, 0x20);

	vperm2i128 ymm1, ymm10, ymm11, 32		; 00000020H

; 91   :   __m256i tmplo = _mm256_permutevar8x32_epi32(*lo, perm8x32mask);

	vpermd	ymm0, ymm7, ymm1

; 94   :   *lo = _mm256_permute2x128_si256(tmplo, tmphi, 0x20);

	vperm2i128 ymm1, ymm0, ymm3, 32			; 00000020H

; 116  :   __m256i cmpmask1 = _mm256_cmpgt_epi32(costs_hi, costs_lo);

	vpcmpgtd ymm2, ymm5, ymm4

; 93   :   *hi = _mm256_permute2x128_si256(tmplo, tmphi, 0x31);

	vperm2i128 ymm0, ymm0, ymm3, 49			; 00000031H

; 118  :   __m256i pl1_1    = _mm256_blendv_epi8(pl1hi,    pl1lo,    cmpmask1);

	vpblendvb ymm3, ymm0, ymm1, ymm2
	vpblendvb ymm4, ymm5, ymm4, ymm2

; 119  : 
; 120  :   __m256i cost2    = _mm256_shuffle_epi32(cost1, _MM_SHUFFLE(2, 3, 0, 1));

	vpshufd	ymm2, ymm4, 177				; 000000b1H

; 121  :   __m256i pl1_2    = _mm256_shuffle_epi32(pl1_1, _MM_SHUFFLE(2, 3, 0, 1));
; 122  : 
; 123  :   __m256i cmpmask2 = _mm256_cmpgt_epi32(cost2, cost1);

	vpcmpgtd ymm1, ymm2, ymm4

; 124  :   __m256i cost3    = _mm256_blendv_epi8(cost2, cost1, cmpmask2);

	vpblendvb ymm4, ymm2, ymm4, ymm1
	vpshufd	ymm0, ymm3, 177				; 000000b1H

; 125  :   __m256i pl1_3    = _mm256_blendv_epi8(pl1_2, pl1_1, cmpmask2);

	vpblendvb ymm5, ymm0, ymm3, ymm1

; 126  : 
; 127  :   __m256i cost4    = _mm256_shuffle_epi32(cost3, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	ymm1, ymm4, 78				; 0000004eH

; 128  :   __m256i pl1_4    = _mm256_shuffle_epi32(pl1_3, _MM_SHUFFLE(1, 0, 3, 2));
; 129  : 
; 130  :   __m256i cmpmask3 = _mm256_cmpgt_epi32(cost4, cost3);

	vpcmpgtd ymm2, ymm1, ymm4

; 131  :   __m256i cost5    = _mm256_blendv_epi8(cost4, cost3, cmpmask3);

	vpblendvb ymm4, ymm1, ymm4, ymm2
	vpshufd	ymm0, ymm5, 78				; 0000004eH

; 132  :   __m256i pl1_5    = _mm256_blendv_epi8(pl1_4, pl1_3, cmpmask3);

	vpblendvb ymm3, ymm0, ymm5, ymm2

; 133  : 
; 134  :   __m256i cost6    = _mm256_permute4x64_epi64(cost5, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm1, ymm4, 78				; 0000004eH

; 135  :   __m256i pl1_6    = _mm256_permute4x64_epi64(pl1_5, _MM_SHUFFLE(1, 0, 3, 2));
; 136  : 
; 137  :   __m256i cmpmask4 = _mm256_cmpgt_epi32(cost6, cost5);

	vpcmpgtd ymm2, ymm1, ymm4
	vpermq	ymm0, ymm3, 78				; 0000004eH

; 138  :   __m256i pl1_7    = _mm256_blendv_epi8(pl1_6, pl1_5, cmpmask4);

	vpblendvb ymm3, ymm0, ymm3, ymm2

; 139  : 
; 140  :   __m128i res1_128 = _mm256_castsi256_si128(pl1_7);
; 141  :   uint32_t tmp1 = (uint32_t)_mm_extract_epi32(res1_128, 0);

	vpextrd	eax, xmm3, 0

; 142  :   uint16_t n = (uint16_t)(tmp1 & 0xffff);
; 143  :   uint16_t chng = (uint16_t)(tmp1 >> 16);

	mov	edi, eax

; 144  : 
; 145  :   *final_change = (int16_t)chng;
; 146  :   *min_pos = (int32_t)n;

	movzx	esi, ax
	shr	edi, 16

; 334  :       const int32_t best_id = scan[min_pos + subpos];

	lea	eax, DWORD PTR [rsi+rbx]
	movsxd	rcx, eax

; 335  : 
; 336  :       cheapest_q = q_coef[best_id];

	mov	rax, QWORD PTR scan$1$[rbp]
	movsxd	rax, DWORD PTR [rax+rcx*4]

; 337  :       if (cheapest_q == 32767 || cheapest_q == -32768)

	mov	ecx, 32767				; 00007fffH
	movzx	r10d, WORD PTR [r14+rax*2]
	lea	rax, QWORD PTR [r14+rax*2]
	mov	QWORD PTR tv3086[rbp], rax
	movzx	eax, r10w
	sub	ax, cx
	mov	ecx, 65534				; 0000fffeH
	test	ax, cx
	jne	SHORT $LN41@kvz_quant_

; 338  :         final_change = -1;

	mov	edi, -1
$LN41@kvz_quant_:

; 339  : 
; 340  :       uint32_t coef_signs = _mm256_movemask_epi8(coef_signbits);

	vpcmpgtw ymm0, ymm13, YMMWORD PTR tv3153[rbp]
	vmovdqu	ymm11, YMMWORD PTR tv3130[rbp]

; 341  :       uint32_t cheapest_coef_sign_mask = (uint32_t)(1 << (2 * min_pos));
; 342  : 
; 343  :       if (!(coef_signs & cheapest_coef_sign_mask))
; 344  :         cheapest_q += final_change;
; 345  :       else
; 346  :         cheapest_q -= final_change;
; 347  : 
; 348  :       q_coef[best_id] = cheapest_q;

	movzx	ecx, r10w
	lea	eax, DWORD PTR [rsi+rsi]
	sub	cx, di
	add	r10w, di
	vpmovmskb r8d, ymm0
	bt	r8d, eax
	mov	rax, QWORD PTR tv3086[rbp]
	vxorpd	xmm9, xmm9, xmm9
	cmovb	r10w, cx
	vpxor	xmm10, xmm10, xmm10
	mov	WORD PTR [rax], r10w
$LN40@kvz_quant_:

; 451  :     return;
; 452  : 
; 453  :   assert(VEC_WIDTH == SCAN_SET_SIZE);
; 454  :   for (int32_t subpos = (width * height - 1) & (~(VEC_WIDTH - 1)); subpos >= 0; subpos -= VEC_WIDTH) {

	mov	edi, DWORD PTR width$[rsp]

; 514  :     last_cg = hide_block_sign(v_coef, q_coefs, deltas_h, deltas_l, q_coef, scan, subpos, last_cg);

	xor	ecx, ecx
	vmovdqu	xmm0, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	vmovdqu	xmm1, XMMWORD PTR __xmm@030209080f0e070601000d0c05040b0a
	vmovdqu	xmm2, XMMWORD PTR __xmm@0b0a09080f0e0d0c0302010007060504
	vmovdqu	xmm3, XMMWORD PTR __xmm@ffffffff00000000ffff000000000000
	vmovdqu	xmm5, XMMWORD PTR __xmm@ffffffff00000000ffffffff00000000
	vmovdqu	xmm6, XMMWORD PTR __xmm@0f0e0d0c05040b0a0706030209080100
	vmovdqu	xmm7, XMMWORD PTR __xmm@0f0e07060b0a03020d0c050409080100
	vmovdqu	xmm8, XMMWORD PTR __xmm@0b0a090805040f0e0302010007060d0c
	vmovdqu	xmm12, XMMWORD PTR __xmm@0b0a03020f0e0706090801000d0c0504
	movsx	rsi, BYTE PTR scan_idx$[rsp]
	lea	r10d, DWORD PTR [rdi+rdi]
	cmp	r9d, 1
	vpxor	xmm4, xmm4, xmm4
	cmovne	ecx, r9d
	sub	r15, 64					; 00000040H
	sub	ebx, 16
	jns	$LL7@kvz_quant_
$LN11@kvz_quant_:
	vzeroupper

; 520  : }

	lea	r11, QWORD PTR [rsp+736]
	mov	rbx, QWORD PTR [r11+48]
	mov	rsi, QWORD PTR [r11+56]
	mov	rdi, QWORD PTR [r11+64]
	vmovaps	xmm6, XMMWORD PTR [r11-16]
	vmovaps	xmm7, XMMWORD PTR [r11-32]
	vmovaps	xmm8, XMMWORD PTR [r11-48]
	vmovaps	xmm9, XMMWORD PTR [r11-64]
	vmovaps	xmm10, XMMWORD PTR [r11-80]
	vmovaps	xmm11, XMMWORD PTR [r11-96]
	vmovaps	xmm12, XMMWORD PTR [r11-112]
	vmovaps	xmm13, XMMWORD PTR [r11-128]
	vmovaps	xmm14, XMMWORD PTR [r11-144]
	vmovaps	xmm15, XMMWORD PTR [r11-160]
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r13
	pop	r12
	pop	rbp
	ret	0
$LN22@kvz_quant_:
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c

; 147  :     qp_scaled = CLIP(-qp_offset, 57, qp);

	mov	r10d, eax
$LN23@kvz_quant_:

; 148  :     if(qp_scaled < 0) {

	test	r10d, r10d
	jns	$LN17@kvz_quant_

; 149  :       qp_scaled = qp_scaled + qp_offset;
; 150  :     } else {

	lea	rcx, OFFSET FLAT:__ImageBase
	jmp	$LN74@kvz_quant_
kvz_quant_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
a_in$ = 8
b_in$ = 16
get_residual_4x1_avx2 PROC

; 525  :   __m128i a = _mm_cvtsi32_si128(*(int32_t*)a_in);

	vmovd	xmm0, DWORD PTR [rcx]

; 526  :   __m128i b = _mm_cvtsi32_si128(*(int32_t*)b_in);

	vmovd	xmm1, DWORD PTR [rdx]

; 527  :   __m128i diff = _mm_sub_epi16(_mm_cvtepu8_epi16(a), _mm_cvtepu8_epi16(b) );

	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1
	vpsubw	xmm0, xmm2, xmm0

; 528  :   return diff;
; 529  : }

	ret	0
get_residual_4x1_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
a_in$ = 8
b_in$ = 16
get_residual_8x1_avx2 PROC

; 532  :   __m128i a = _mm_cvtsi64_si128(*(int64_t*)a_in);

	vmovq	xmm0, QWORD PTR [rcx]

; 533  :   __m128i b = _mm_cvtsi64_si128(*(int64_t*)b_in);

	vmovq	xmm1, QWORD PTR [rdx]

; 534  :   __m128i diff = _mm_sub_epi16(_mm_cvtepu8_epi16(a), _mm_cvtepu8_epi16(b) );

	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1
	vpsubw	xmm0, xmm2, xmm0

; 535  :   return diff;
; 536  : }

	ret	0
get_residual_8x1_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
residual$ = 8
pred_in$ = 16
get_quantized_recon_4x1_avx2 PROC

; 539  :   __m128i res = _mm_loadl_epi64((__m128i*)residual);
; 540  :   __m128i pred = _mm_cvtsi32_si128(*(int32_t*)pred_in);

	vmovd	xmm0, DWORD PTR [rdx]
	vmovq	xmm1, QWORD PTR [rcx]

; 541  :   __m128i rec = _mm_add_epi16(res, _mm_cvtepu8_epi16(pred));

	vpmovzxbw xmm2, xmm0
	vpaddw	xmm0, xmm2, xmm1

; 542  :   return _mm_cvtsi128_si32(_mm_packus_epi16(rec, rec));

	vpackuswb xmm0, xmm0, xmm0
	vmovd	eax, xmm0

; 543  : }

	ret	0
get_quantized_recon_4x1_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
residual$ = 8
pred_in$ = 16
get_quantized_recon_8x1_avx2 PROC

; 546  :   __m128i res = _mm_loadu_si128((__m128i*)residual);
; 547  :   __m128i pred = _mm_cvtsi64_si128(*(int64_t*)pred_in);

	vmovq	xmm0, QWORD PTR [rdx]

; 548  :   __m128i rec = _mm_add_epi16(res, _mm_cvtepu8_epi16(pred));

	vpmovzxbw xmm0, xmm0
	vpaddw	xmm1, xmm0, XMMWORD PTR [rcx]

; 549  :   return _mm_cvtsi128_si64(_mm_packus_epi16(rec, rec));

	vpackuswb xmm2, xmm1, xmm1
	vmovq	rax, xmm2

; 550  : }

	ret	0
get_quantized_recon_8x1_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
ref_in$ = 16
pred_in$ = 24
residual$ = 32
width$ = 40
in_stride$ = 48
get_residual_avx2 PROC

; 552  : static void get_residual_avx2(const uint8_t *ref_in, const uint8_t *pred_in, int16_t *residual, int width, int in_stride){

	mov	QWORD PTR [rsp+32], rsi
	push	rdi
	mov	eax, r9d
	mov	r10, r8
	mov	rdi, rdx
	mov	rsi, rcx

; 553  : 
; 554  :   __m128i diff = _mm_setzero_si128();
; 555  :   switch (width) {

	cmp	r9d, 4
	je	$LN10@get_residu
	cmp	eax, 8
	je	$LN11@get_residu

; 583  :     break;
; 584  :     default:
; 585  :       for (int y = 0; y < width; ++y) {

	test	eax, eax
	jle	$LN5@get_residu
	mov	QWORD PTR [rsp+16], rbx
	lea	r9, QWORD PTR [rdx+8]
	mov	QWORD PTR [rsp+24], rbp
	mov	ebx, eax
	movsxd	rbp, DWORD PTR in_stride$[rsp]
	movsxd	r11, eax
	mov	QWORD PTR [rsp+32], r14
	lea	r14, QWORD PTR [r11+r11]
	npad	2
$LL6@get_residu:

; 586  :         for (int x = 0; x < width; x+=16) {

	test	r11, r11
	je	SHORT $LN4@get_residu

; 587  :           diff = get_residual_8x1_avx2(&ref_in[x + y * in_stride], &pred_in[x + y * in_stride]);
; 588  :           _mm_storeu_si128((__m128i*)&residual[x + y * width], diff);

	mov	r8, rsi
	lea	rdx, QWORD PTR [r11-1]
	sub	r8, rdi
	shr	rdx, 4
	inc	rdx
	mov	rax, r9
	mov	rcx, r10
	npad	4
$LL9@get_residu:

; 586  :         for (int x = 0; x < width; x+=16) {

	lea	rcx, QWORD PTR [rcx+32]

; 532  :   __m128i a = _mm_cvtsi64_si128(*(int64_t*)a_in);

	vmovq	xmm0, QWORD PTR [r8+rax-8]

; 533  :   __m128i b = _mm_cvtsi64_si128(*(int64_t*)b_in);

	vmovq	xmm1, QWORD PTR [rax-8]

; 586  :         for (int x = 0; x < width; x+=16) {

	lea	rax, QWORD PTR [rax+16]

; 534  :   __m128i diff = _mm_sub_epi16(_mm_cvtepu8_epi16(a), _mm_cvtepu8_epi16(b) );

	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1
	vpsubw	xmm1, xmm2, xmm0

; 587  :           diff = get_residual_8x1_avx2(&ref_in[x + y * in_stride], &pred_in[x + y * in_stride]);
; 588  :           _mm_storeu_si128((__m128i*)&residual[x + y * width], diff);

	vmovdqu	XMMWORD PTR [rcx-32], xmm1

; 532  :   __m128i a = _mm_cvtsi64_si128(*(int64_t*)a_in);

	vmovq	xmm0, QWORD PTR [r8+rax-16]

; 533  :   __m128i b = _mm_cvtsi64_si128(*(int64_t*)b_in);

	vmovq	xmm1, QWORD PTR [rax-16]

; 534  :   __m128i diff = _mm_sub_epi16(_mm_cvtepu8_epi16(a), _mm_cvtepu8_epi16(b) );

	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1
	vpsubw	xmm1, xmm2, xmm0

; 589  :           diff = get_residual_8x1_avx2(&ref_in[(x+8) + y * in_stride], &pred_in[(x+8) + y * in_stride]);
; 590  :           _mm_storeu_si128((__m128i*)&residual[(x+8) + y * width], diff);

	vmovdqu	XMMWORD PTR [rcx-16], xmm1
	sub	rdx, 1
	jne	SHORT $LL9@get_residu
$LN4@get_residu:

; 583  :     break;
; 584  :     default:
; 585  :       for (int y = 0; y < width; ++y) {

	add	r9, rbp
	add	r10, r14
	sub	rbx, 1
	jne	SHORT $LL6@get_residu

; 586  :         for (int x = 0; x < width; x+=16) {

	mov	r14, QWORD PTR [rsp+32]
	mov	rbp, QWORD PTR [rsp+24]
	mov	rbx, QWORD PTR [rsp+16]

; 591  :         }
; 592  :       }
; 593  :     break;
; 594  :   }
; 595  : }

	mov	rsi, QWORD PTR [rsp+40]
	pop	rdi
	ret	0
$LN11@get_residu:

; 532  :   __m128i a = _mm_cvtsi64_si128(*(int64_t*)a_in);

	vmovq	xmm0, QWORD PTR [rcx]

; 533  :   __m128i b = _mm_cvtsi64_si128(*(int64_t*)b_in);

	vmovq	xmm1, QWORD PTR [rdx]

; 569  :       diff = get_residual_8x1_avx2(&ref_in[1 * in_stride], &pred_in[1 * in_stride]);

	movsxd	rdx, DWORD PTR in_stride$[rsp]

; 534  :   __m128i diff = _mm_sub_epi16(_mm_cvtepu8_epi16(a), _mm_cvtepu8_epi16(b) );

	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1
	vpsubw	xmm1, xmm2, xmm0

; 565  :     break;
; 566  :     case 8:
; 567  :       diff = get_residual_8x1_avx2(&ref_in[0 * in_stride], &pred_in[0 * in_stride]);
; 568  :       _mm_storeu_si128((__m128i*)&(residual[0]), diff);

	vmovdqu	XMMWORD PTR [r8], xmm1

; 532  :   __m128i a = _mm_cvtsi64_si128(*(int64_t*)a_in);

	vmovq	xmm0, QWORD PTR [rdx+rcx]

; 533  :   __m128i b = _mm_cvtsi64_si128(*(int64_t*)b_in);

	vmovq	xmm1, QWORD PTR [rdx+rdi]

; 534  :   __m128i diff = _mm_sub_epi16(_mm_cvtepu8_epi16(a), _mm_cvtepu8_epi16(b) );

	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1
	vpsubw	xmm1, xmm2, xmm0

; 570  :       _mm_storeu_si128((__m128i*)&(residual[8]), diff);

	vmovdqu	XMMWORD PTR [r8+16], xmm1

; 571  :       diff = get_residual_8x1_avx2(&ref_in[2 * in_stride], &pred_in[2 * in_stride]);

	lea	eax, DWORD PTR [rdx+rdx]
	movsxd	rcx, eax

; 573  :       diff = get_residual_8x1_avx2(&ref_in[3 * in_stride], &pred_in[3 * in_stride]);

	lea	eax, DWORD PTR [rdx+rdx*2]

; 532  :   __m128i a = _mm_cvtsi64_si128(*(int64_t*)a_in);

	vmovq	xmm0, QWORD PTR [rcx+rsi]

; 533  :   __m128i b = _mm_cvtsi64_si128(*(int64_t*)b_in);

	vmovq	xmm1, QWORD PTR [rcx+rdi]

; 534  :   __m128i diff = _mm_sub_epi16(_mm_cvtepu8_epi16(a), _mm_cvtepu8_epi16(b) );

	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1
	vpsubw	xmm1, xmm2, xmm0

; 572  :       _mm_storeu_si128((__m128i*)&(residual[16]), diff);

	vmovdqu	XMMWORD PTR [r8+32], xmm1

; 573  :       diff = get_residual_8x1_avx2(&ref_in[3 * in_stride], &pred_in[3 * in_stride]);

	movsxd	rcx, eax

; 575  :       diff = get_residual_8x1_avx2(&ref_in[4 * in_stride], &pred_in[4 * in_stride]);

	lea	eax, DWORD PTR [rdx*4]

; 532  :   __m128i a = _mm_cvtsi64_si128(*(int64_t*)a_in);

	vmovq	xmm0, QWORD PTR [rcx+rsi]

; 533  :   __m128i b = _mm_cvtsi64_si128(*(int64_t*)b_in);

	vmovq	xmm1, QWORD PTR [rcx+rdi]

; 534  :   __m128i diff = _mm_sub_epi16(_mm_cvtepu8_epi16(a), _mm_cvtepu8_epi16(b) );

	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1
	vpsubw	xmm1, xmm2, xmm0

; 574  :       _mm_storeu_si128((__m128i*)&(residual[24]), diff);

	vmovdqu	XMMWORD PTR [r8+48], xmm1

; 575  :       diff = get_residual_8x1_avx2(&ref_in[4 * in_stride], &pred_in[4 * in_stride]);

	movsxd	rcx, eax

; 577  :       diff = get_residual_8x1_avx2(&ref_in[5 * in_stride], &pred_in[5 * in_stride]);

	lea	eax, DWORD PTR [rdx+rdx*4]

; 532  :   __m128i a = _mm_cvtsi64_si128(*(int64_t*)a_in);

	vmovq	xmm0, QWORD PTR [rcx+rsi]

; 533  :   __m128i b = _mm_cvtsi64_si128(*(int64_t*)b_in);

	vmovq	xmm1, QWORD PTR [rcx+rdi]

; 534  :   __m128i diff = _mm_sub_epi16(_mm_cvtepu8_epi16(a), _mm_cvtepu8_epi16(b) );

	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1
	vpsubw	xmm1, xmm2, xmm0

; 576  :       _mm_storeu_si128((__m128i*)&(residual[32]), diff);

	vmovdqu	XMMWORD PTR [r8+64], xmm1

; 577  :       diff = get_residual_8x1_avx2(&ref_in[5 * in_stride], &pred_in[5 * in_stride]);

	movsxd	rcx, eax

; 579  :       diff = get_residual_8x1_avx2(&ref_in[6 * in_stride], &pred_in[6 * in_stride]);

	lea	eax, DWORD PTR [rdx+rdx*2]
	add	eax, eax

; 532  :   __m128i a = _mm_cvtsi64_si128(*(int64_t*)a_in);

	vmovq	xmm0, QWORD PTR [rcx+rsi]

; 533  :   __m128i b = _mm_cvtsi64_si128(*(int64_t*)b_in);

	vmovq	xmm1, QWORD PTR [rcx+rdi]

; 534  :   __m128i diff = _mm_sub_epi16(_mm_cvtepu8_epi16(a), _mm_cvtepu8_epi16(b) );

	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1
	vpsubw	xmm1, xmm2, xmm0

; 578  :       _mm_storeu_si128((__m128i*)&(residual[40]), diff);

	vmovdqu	XMMWORD PTR [r8+80], xmm1

; 579  :       diff = get_residual_8x1_avx2(&ref_in[6 * in_stride], &pred_in[6 * in_stride]);

	movsxd	rcx, eax

; 581  :       diff = get_residual_8x1_avx2(&ref_in[7 * in_stride], &pred_in[7 * in_stride]);

	imul	eax, edx, 7

; 532  :   __m128i a = _mm_cvtsi64_si128(*(int64_t*)a_in);

	vmovq	xmm0, QWORD PTR [rcx+rsi]

; 533  :   __m128i b = _mm_cvtsi64_si128(*(int64_t*)b_in);

	vmovq	xmm1, QWORD PTR [rcx+rdi]

; 534  :   __m128i diff = _mm_sub_epi16(_mm_cvtepu8_epi16(a), _mm_cvtepu8_epi16(b) );

	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1
	vpsubw	xmm1, xmm2, xmm0

; 580  :       _mm_storeu_si128((__m128i*)&(residual[48]), diff);

	vmovdqu	XMMWORD PTR [r8+96], xmm1

; 581  :       diff = get_residual_8x1_avx2(&ref_in[7 * in_stride], &pred_in[7 * in_stride]);

	movsxd	rcx, eax

; 532  :   __m128i a = _mm_cvtsi64_si128(*(int64_t*)a_in);

	vmovq	xmm0, QWORD PTR [rcx+rsi]

; 533  :   __m128i b = _mm_cvtsi64_si128(*(int64_t*)b_in);

	vmovq	xmm1, QWORD PTR [rcx+rdi]

; 534  :   __m128i diff = _mm_sub_epi16(_mm_cvtepu8_epi16(a), _mm_cvtepu8_epi16(b) );

	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1
	vpsubw	xmm1, xmm2, xmm0

; 582  :       _mm_storeu_si128((__m128i*)&(residual[56]), diff);

	vmovdqu	XMMWORD PTR [r8+112], xmm1

; 591  :         }
; 592  :       }
; 593  :     break;
; 594  :   }
; 595  : }

	mov	rsi, QWORD PTR [rsp+40]
	pop	rdi
	ret	0
$LN10@get_residu:

; 525  :   __m128i a = _mm_cvtsi32_si128(*(int32_t*)a_in);

	vmovd	xmm0, DWORD PTR [rcx]

; 526  :   __m128i b = _mm_cvtsi32_si128(*(int32_t*)b_in);

	vmovd	xmm1, DWORD PTR [rdx]

; 559  :       diff = get_residual_4x1_avx2(ref_in + 1 * in_stride, pred_in + 1 * in_stride);

	movsxd	rdx, DWORD PTR in_stride$[rsp]

; 527  :   __m128i diff = _mm_sub_epi16(_mm_cvtepu8_epi16(a), _mm_cvtepu8_epi16(b) );

	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1
	vpsubw	xmm2, xmm2, xmm0

; 556  :     case 4:
; 557  :       diff = get_residual_4x1_avx2(ref_in + 0 * in_stride, pred_in + 0 * in_stride);
; 558  :       _mm_storel_epi64((__m128i*)&(residual[0]), diff);

	vmovq	QWORD PTR [r8], xmm2

; 525  :   __m128i a = _mm_cvtsi32_si128(*(int32_t*)a_in);

	vmovd	xmm0, DWORD PTR [rdx+rcx]

; 526  :   __m128i b = _mm_cvtsi32_si128(*(int32_t*)b_in);

	vmovd	xmm1, DWORD PTR [rdx+rdi]

; 527  :   __m128i diff = _mm_sub_epi16(_mm_cvtepu8_epi16(a), _mm_cvtepu8_epi16(b) );

	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1
	vpsubw	xmm2, xmm2, xmm0

; 560  :       _mm_storel_epi64((__m128i*)&(residual[4]), diff);

	vmovq	QWORD PTR [r8+8], xmm2

; 561  :       diff = get_residual_4x1_avx2(ref_in + 2 * in_stride, pred_in + 2 * in_stride);

	lea	eax, DWORD PTR [rdx+rdx]
	movsxd	rcx, eax

; 563  :       diff = get_residual_4x1_avx2(ref_in + 3 * in_stride, pred_in + 3 * in_stride);

	lea	eax, DWORD PTR [rdx+rdx*2]

; 525  :   __m128i a = _mm_cvtsi32_si128(*(int32_t*)a_in);

	vmovd	xmm0, DWORD PTR [rcx+rsi]

; 526  :   __m128i b = _mm_cvtsi32_si128(*(int32_t*)b_in);

	vmovd	xmm1, DWORD PTR [rcx+rdi]

; 527  :   __m128i diff = _mm_sub_epi16(_mm_cvtepu8_epi16(a), _mm_cvtepu8_epi16(b) );

	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1
	vpsubw	xmm2, xmm2, xmm0

; 562  :       _mm_storel_epi64((__m128i*)&(residual[8]), diff);

	vmovq	QWORD PTR [r8+16], xmm2

; 563  :       diff = get_residual_4x1_avx2(ref_in + 3 * in_stride, pred_in + 3 * in_stride);

	movsxd	rcx, eax

; 525  :   __m128i a = _mm_cvtsi32_si128(*(int32_t*)a_in);

	vmovd	xmm0, DWORD PTR [rcx+rsi]

; 526  :   __m128i b = _mm_cvtsi32_si128(*(int32_t*)b_in);

	vmovd	xmm1, DWORD PTR [rcx+rdi]

; 527  :   __m128i diff = _mm_sub_epi16(_mm_cvtepu8_epi16(a), _mm_cvtepu8_epi16(b) );

	vpmovzxbw xmm2, xmm0
	vpmovzxbw xmm0, xmm1
	vpsubw	xmm2, xmm2, xmm0

; 564  :       _mm_storel_epi64((__m128i*)&(residual[12]), diff);

	vmovq	QWORD PTR [r8+24], xmm2
$LN5@get_residu:

; 591  :         }
; 592  :       }
; 593  :     break;
; 594  :   }
; 595  : }

	mov	rsi, QWORD PTR [rsp+40]
	pop	rdi
	ret	0
get_residual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
residual$ = 48
pred_in$ = 56
in_stride$ = 64
rec_out$ = 72
out_stride$ = 80
width$ = 88
get_quantized_recon_avx2 PROC

; 597  : static void get_quantized_recon_avx2(int16_t *residual, const uint8_t *pred_in, int in_stride, uint8_t *rec_out, int out_stride, int width){

	push	rbp
	push	r14
	sub	rsp, 24

; 598  : 
; 599  :   switch (width) {

	mov	eax, DWORD PTR width$[rsp]
	mov	r14, r9
	movsxd	r9, r8d
	mov	rbp, rdx
	mov	r11, rcx
	cmp	eax, 4
	je	$LN10@get_quanti
	cmp	eax, 8
	je	$LN11@get_quanti

; 615  :       break;
; 616  :     default:
; 617  :       for (int y = 0; y < width; ++y) {

	test	eax, eax
	jle	$LN5@get_quanti

; 618  :         for (int x = 0; x < width; x += 16) {

	mov	QWORD PTR [rsp+48], rbx
	xor	ebx, ebx
	mov	QWORD PTR [rsp+56], rsi
	mov	r10d, ebx
	mov	QWORD PTR [rsp+64], rdi
	mov	esi, eax
	mov	QWORD PTR [rsp+16], r12
	movsxd	r12, DWORD PTR out_stride$[rsp]
	mov	QWORD PTR [rsp+8], r13
	movsxd	rdi, eax
	mov	QWORD PTR [rsp], r15
	mov	r15, r9
	lea	r13, QWORD PTR [rdi+rdi]
$LL6@get_quanti:
	test	rdi, rdi
	je	SHORT $LN4@get_quanti

; 619  :           *(int64_t*)&(rec_out[x + y * out_stride]) = get_quantized_recon_8x1_avx2(residual + x + y * width, pred_in + x + y  * in_stride);

	mov	r9, rbx
	lea	rax, QWORD PTR [r14+8]
	sub	r9, r10
	lea	r8, QWORD PTR [rdi-1]
	sub	r9, r14
	shr	r8, 4
	add	r9, rbp
	add	rax, r10
	inc	r8
	mov	rdx, r11
	npad	5
$LL9@get_quanti:

; 547  :   __m128i pred = _mm_cvtsi64_si128(*(int64_t*)pred_in);

	vmovq	xmm0, QWORD PTR [r9+rax-8]

; 618  :         for (int x = 0; x < width; x += 16) {

	lea	rax, QWORD PTR [rax+16]
	lea	rdx, QWORD PTR [rdx+32]

; 548  :   __m128i rec = _mm_add_epi16(res, _mm_cvtepu8_epi16(pred));

	vpmovzxbw xmm0, xmm0
	vpaddw	xmm1, xmm0, XMMWORD PTR [rdx-32]

; 549  :   return _mm_cvtsi128_si64(_mm_packus_epi16(rec, rec));

	vpackuswb xmm2, xmm1, xmm1

; 619  :           *(int64_t*)&(rec_out[x + y * out_stride]) = get_quantized_recon_8x1_avx2(residual + x + y * width, pred_in + x + y  * in_stride);

	vmovq	QWORD PTR [rax-24], xmm2

; 547  :   __m128i pred = _mm_cvtsi64_si128(*(int64_t*)pred_in);

	vmovq	xmm0, QWORD PTR [r9+rax-16]

; 548  :   __m128i rec = _mm_add_epi16(res, _mm_cvtepu8_epi16(pred));

	vpmovzxbw xmm0, xmm0
	vpaddw	xmm1, xmm0, XMMWORD PTR [rdx-16]

; 549  :   return _mm_cvtsi128_si64(_mm_packus_epi16(rec, rec));

	vpackuswb xmm2, xmm1, xmm1

; 620  :           *(int64_t*)&(rec_out[(x + 8) + y * out_stride]) = get_quantized_recon_8x1_avx2(residual + (x + 8) + y * width, pred_in + (x + 8) + y  * in_stride);

	vmovq	QWORD PTR [rax-16], xmm2
	sub	r8, 1
	jne	SHORT $LL9@get_quanti
$LN4@get_quanti:

; 615  :       break;
; 616  :     default:
; 617  :       for (int y = 0; y < width; ++y) {

	add	r11, r13
	add	r10, r12
	add	rbx, r15
	sub	rsi, 1
	jne	SHORT $LL6@get_quanti

; 618  :         for (int x = 0; x < width; x += 16) {

	mov	r15, QWORD PTR [rsp]
	mov	r13, QWORD PTR [rsp+8]
	mov	r12, QWORD PTR [rsp+16]
	mov	rdi, QWORD PTR [rsp+64]
	mov	rsi, QWORD PTR [rsp+56]
	mov	rbx, QWORD PTR [rsp+48]

; 621  :         }
; 622  :       }
; 623  :       break;
; 624  :   }
; 625  : }

	add	rsp, 24
	pop	r14
	pop	rbp
	ret	0
$LN11@get_quanti:

; 547  :   __m128i pred = _mm_cvtsi64_si128(*(int64_t*)pred_in);

	vmovq	xmm0, QWORD PTR [rdx]

; 608  :       *(int64_t*)&(rec_out[1 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 1 * width, pred_in + 1 * in_stride);

	movsxd	r8, DWORD PTR out_stride$[rsp]

; 609  :       *(int64_t*)&(rec_out[2 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 2 * width, pred_in + 2 * in_stride);

	lea	eax, DWORD PTR [r9+r9]

; 548  :   __m128i rec = _mm_add_epi16(res, _mm_cvtepu8_epi16(pred));

	vpmovzxbw xmm0, xmm0
	vpaddw	xmm1, xmm0, XMMWORD PTR [rcx]

; 549  :   return _mm_cvtsi128_si64(_mm_packus_epi16(rec, rec));

	vpackuswb xmm2, xmm1, xmm1

; 605  :       break;
; 606  :     case 8:
; 607  :       *(int64_t*)&(rec_out[0 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 0 * width, pred_in + 0 * in_stride);

	vmovq	QWORD PTR [r14], xmm2

; 547  :   __m128i pred = _mm_cvtsi64_si128(*(int64_t*)pred_in);

	vmovq	xmm0, QWORD PTR [r9+rdx]

; 548  :   __m128i rec = _mm_add_epi16(res, _mm_cvtepu8_epi16(pred));

	vpmovzxbw xmm0, xmm0
	vpaddw	xmm1, xmm0, XMMWORD PTR [rcx+16]

; 609  :       *(int64_t*)&(rec_out[2 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 2 * width, pred_in + 2 * in_stride);

	movsxd	rcx, eax
	lea	eax, DWORD PTR [r8+r8]

; 549  :   return _mm_cvtsi128_si64(_mm_packus_epi16(rec, rec));

	vpackuswb xmm2, xmm1, xmm1

; 608  :       *(int64_t*)&(rec_out[1 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 1 * width, pred_in + 1 * in_stride);

	vmovq	QWORD PTR [r8+r14], xmm2

; 547  :   __m128i pred = _mm_cvtsi64_si128(*(int64_t*)pred_in);

	vmovq	xmm0, QWORD PTR [rcx+rdx]

; 609  :       *(int64_t*)&(rec_out[2 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 2 * width, pred_in + 2 * in_stride);

	movsxd	rcx, eax

; 610  :       *(int64_t*)&(rec_out[3 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 3 * width, pred_in + 3 * in_stride);

	lea	eax, DWORD PTR [r9+r9*2]

; 548  :   __m128i rec = _mm_add_epi16(res, _mm_cvtepu8_epi16(pred));

	vpmovzxbw xmm0, xmm0
	vpaddw	xmm1, xmm0, XMMWORD PTR [r11+32]

; 549  :   return _mm_cvtsi128_si64(_mm_packus_epi16(rec, rec));

	vpackuswb xmm2, xmm1, xmm1

; 609  :       *(int64_t*)&(rec_out[2 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 2 * width, pred_in + 2 * in_stride);

	vmovq	QWORD PTR [rcx+r14], xmm2

; 610  :       *(int64_t*)&(rec_out[3 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 3 * width, pred_in + 3 * in_stride);

	movsxd	rcx, eax
	lea	eax, DWORD PTR [r8+r8*2]

; 547  :   __m128i pred = _mm_cvtsi64_si128(*(int64_t*)pred_in);

	vmovq	xmm0, QWORD PTR [rcx+rdx]

; 610  :       *(int64_t*)&(rec_out[3 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 3 * width, pred_in + 3 * in_stride);

	movsxd	rcx, eax

; 611  :       *(int64_t*)&(rec_out[4 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 4 * width, pred_in + 4 * in_stride);

	lea	eax, DWORD PTR [r9*4]

; 548  :   __m128i rec = _mm_add_epi16(res, _mm_cvtepu8_epi16(pred));

	vpmovzxbw xmm0, xmm0
	vpaddw	xmm1, xmm0, XMMWORD PTR [r11+48]

; 549  :   return _mm_cvtsi128_si64(_mm_packus_epi16(rec, rec));

	vpackuswb xmm2, xmm1, xmm1

; 610  :       *(int64_t*)&(rec_out[3 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 3 * width, pred_in + 3 * in_stride);

	vmovq	QWORD PTR [rcx+r14], xmm2

; 611  :       *(int64_t*)&(rec_out[4 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 4 * width, pred_in + 4 * in_stride);

	movsxd	rcx, eax
	lea	eax, DWORD PTR [r8*4]

; 547  :   __m128i pred = _mm_cvtsi64_si128(*(int64_t*)pred_in);

	vmovq	xmm0, QWORD PTR [rcx+rdx]

; 611  :       *(int64_t*)&(rec_out[4 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 4 * width, pred_in + 4 * in_stride);

	movsxd	rcx, eax

; 612  :       *(int64_t*)&(rec_out[5 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 5 * width, pred_in + 5 * in_stride);

	lea	eax, DWORD PTR [r9+r9*4]

; 548  :   __m128i rec = _mm_add_epi16(res, _mm_cvtepu8_epi16(pred));

	vpmovzxbw xmm0, xmm0
	vpaddw	xmm1, xmm0, XMMWORD PTR [r11+64]

; 549  :   return _mm_cvtsi128_si64(_mm_packus_epi16(rec, rec));

	vpackuswb xmm2, xmm1, xmm1

; 611  :       *(int64_t*)&(rec_out[4 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 4 * width, pred_in + 4 * in_stride);

	vmovq	QWORD PTR [rcx+r14], xmm2

; 612  :       *(int64_t*)&(rec_out[5 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 5 * width, pred_in + 5 * in_stride);

	movsxd	rcx, eax
	lea	eax, DWORD PTR [r8+r8*4]

; 547  :   __m128i pred = _mm_cvtsi64_si128(*(int64_t*)pred_in);

	vmovq	xmm0, QWORD PTR [rcx+rdx]

; 612  :       *(int64_t*)&(rec_out[5 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 5 * width, pred_in + 5 * in_stride);

	movsxd	rcx, eax

; 613  :       *(int64_t*)&(rec_out[6 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 6 * width, pred_in + 6 * in_stride);

	lea	eax, DWORD PTR [r9+r9*2]
	add	eax, eax
	cdqe

; 548  :   __m128i rec = _mm_add_epi16(res, _mm_cvtepu8_epi16(pred));

	vpmovzxbw xmm0, xmm0
	vpaddw	xmm1, xmm0, XMMWORD PTR [r11+80]

; 549  :   return _mm_cvtsi128_si64(_mm_packus_epi16(rec, rec));

	vpackuswb xmm2, xmm1, xmm1

; 612  :       *(int64_t*)&(rec_out[5 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 5 * width, pred_in + 5 * in_stride);

	vmovq	QWORD PTR [rcx+r14], xmm2

; 547  :   __m128i pred = _mm_cvtsi64_si128(*(int64_t*)pred_in);

	vmovq	xmm0, QWORD PTR [rax+rdx]

; 548  :   __m128i rec = _mm_add_epi16(res, _mm_cvtepu8_epi16(pred));

	vpmovzxbw xmm0, xmm0
	vpaddw	xmm1, xmm0, XMMWORD PTR [r11+96]

; 549  :   return _mm_cvtsi128_si64(_mm_packus_epi16(rec, rec));

	vpackuswb xmm2, xmm1, xmm1

; 613  :       *(int64_t*)&(rec_out[6 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 6 * width, pred_in + 6 * in_stride);

	lea	eax, DWORD PTR [r8+r8*2]
	add	eax, eax
	cdqe
	vmovq	QWORD PTR [rax+r14], xmm2

; 614  :       *(int64_t*)&(rec_out[7 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 7 * width, pred_in + 7 * in_stride);

	imul	eax, r9d, 7
	movsxd	rcx, eax
	imul	eax, r8d, 7

; 547  :   __m128i pred = _mm_cvtsi64_si128(*(int64_t*)pred_in);

	vmovq	xmm0, QWORD PTR [rcx+rdx]

; 548  :   __m128i rec = _mm_add_epi16(res, _mm_cvtepu8_epi16(pred));

	vpmovzxbw xmm0, xmm0
	vpaddw	xmm1, xmm0, XMMWORD PTR [r11+112]

; 614  :       *(int64_t*)&(rec_out[7 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 7 * width, pred_in + 7 * in_stride);

	movsxd	rcx, eax

; 549  :   return _mm_cvtsi128_si64(_mm_packus_epi16(rec, rec));

	vpackuswb xmm2, xmm1, xmm1

; 614  :       *(int64_t*)&(rec_out[7 * out_stride]) = get_quantized_recon_8x1_avx2(residual + 7 * width, pred_in + 7 * in_stride);

	vmovq	QWORD PTR [rcx+r14], xmm2

; 621  :         }
; 622  :       }
; 623  :       break;
; 624  :   }
; 625  : }

	add	rsp, 24
	pop	r14
	pop	rbp
	ret	0
$LN10@get_quanti:

; 539  :   __m128i res = _mm_loadl_epi64((__m128i*)residual);

	vmovq	xmm1, QWORD PTR [rcx]

; 540  :   __m128i pred = _mm_cvtsi32_si128(*(int32_t*)pred_in);

	vmovd	xmm0, DWORD PTR [rdx]

; 602  :       *(int32_t*)&(rec_out[1 * out_stride]) = get_quantized_recon_4x1_avx2(residual + 1 * width, pred_in + 1 * in_stride);

	movsxd	r8, DWORD PTR out_stride$[rsp]

; 603  :       *(int32_t*)&(rec_out[2 * out_stride]) = get_quantized_recon_4x1_avx2(residual + 2 * width, pred_in + 2 * in_stride);

	lea	eax, DWORD PTR [r9+r9]

; 541  :   __m128i rec = _mm_add_epi16(res, _mm_cvtepu8_epi16(pred));

	vpmovzxbw xmm2, xmm0
	vpaddw	xmm0, xmm2, xmm1

; 542  :   return _mm_cvtsi128_si32(_mm_packus_epi16(rec, rec));

	vpackuswb xmm0, xmm0, xmm0

; 600  :     case 4:
; 601  :       *(int32_t*)&(rec_out[0 * out_stride]) = get_quantized_recon_4x1_avx2(residual + 0 * width, pred_in + 0 * in_stride);

	vmovd	DWORD PTR [r14], xmm0

; 539  :   __m128i res = _mm_loadl_epi64((__m128i*)residual);

	vmovq	xmm1, QWORD PTR [rcx+8]

; 540  :   __m128i pred = _mm_cvtsi32_si128(*(int32_t*)pred_in);

	vmovd	xmm0, DWORD PTR [r9+rdx]

; 541  :   __m128i rec = _mm_add_epi16(res, _mm_cvtepu8_epi16(pred));

	vpmovzxbw xmm2, xmm0
	vpaddw	xmm0, xmm2, xmm1

; 542  :   return _mm_cvtsi128_si32(_mm_packus_epi16(rec, rec));

	vpackuswb xmm0, xmm0, xmm0

; 602  :       *(int32_t*)&(rec_out[1 * out_stride]) = get_quantized_recon_4x1_avx2(residual + 1 * width, pred_in + 1 * in_stride);

	vmovd	DWORD PTR [r8+r14], xmm0

; 539  :   __m128i res = _mm_loadl_epi64((__m128i*)residual);

	vmovq	xmm1, QWORD PTR [r11+16]

; 603  :       *(int32_t*)&(rec_out[2 * out_stride]) = get_quantized_recon_4x1_avx2(residual + 2 * width, pred_in + 2 * in_stride);

	movsxd	rcx, eax
	lea	eax, DWORD PTR [r8+r8]

; 540  :   __m128i pred = _mm_cvtsi32_si128(*(int32_t*)pred_in);

	vmovd	xmm0, DWORD PTR [rcx+rdx]

; 541  :   __m128i rec = _mm_add_epi16(res, _mm_cvtepu8_epi16(pred));

	vpmovzxbw xmm2, xmm0

; 603  :       *(int32_t*)&(rec_out[2 * out_stride]) = get_quantized_recon_4x1_avx2(residual + 2 * width, pred_in + 2 * in_stride);

	movsxd	rcx, eax

; 604  :       *(int32_t*)&(rec_out[3 * out_stride]) = get_quantized_recon_4x1_avx2(residual + 3 * width, pred_in + 3 * in_stride);

	lea	eax, DWORD PTR [r9+r9*2]

; 541  :   __m128i rec = _mm_add_epi16(res, _mm_cvtepu8_epi16(pred));

	vpaddw	xmm0, xmm2, xmm1

; 542  :   return _mm_cvtsi128_si32(_mm_packus_epi16(rec, rec));

	vpackuswb xmm0, xmm0, xmm0

; 603  :       *(int32_t*)&(rec_out[2 * out_stride]) = get_quantized_recon_4x1_avx2(residual + 2 * width, pred_in + 2 * in_stride);

	vmovd	DWORD PTR [rcx+r14], xmm0

; 539  :   __m128i res = _mm_loadl_epi64((__m128i*)residual);

	vmovq	xmm1, QWORD PTR [r11+24]

; 604  :       *(int32_t*)&(rec_out[3 * out_stride]) = get_quantized_recon_4x1_avx2(residual + 3 * width, pred_in + 3 * in_stride);

	movsxd	rcx, eax
	lea	eax, DWORD PTR [r8+r8*2]

; 540  :   __m128i pred = _mm_cvtsi32_si128(*(int32_t*)pred_in);

	vmovd	xmm0, DWORD PTR [rcx+rdx]

; 541  :   __m128i rec = _mm_add_epi16(res, _mm_cvtepu8_epi16(pred));

	vpmovzxbw xmm2, xmm0

; 604  :       *(int32_t*)&(rec_out[3 * out_stride]) = get_quantized_recon_4x1_avx2(residual + 3 * width, pred_in + 3 * in_stride);

	movsxd	rcx, eax

; 541  :   __m128i rec = _mm_add_epi16(res, _mm_cvtepu8_epi16(pred));

	vpaddw	xmm0, xmm2, xmm1

; 542  :   return _mm_cvtsi128_si32(_mm_packus_epi16(rec, rec));

	vpackuswb xmm0, xmm0, xmm0

; 604  :       *(int32_t*)&(rec_out[3 * out_stride]) = get_quantized_recon_4x1_avx2(residual + 3 * width, pred_in + 3 * in_stride);

	vmovd	DWORD PTR [rcx+r14], xmm0
$LN5@get_quanti:

; 621  :         }
; 622  :       }
; 623  :       break;
; 624  :   }
; 625  : }

	add	rsp, 24
	pop	r14
	pop	rbp
	ret	0
get_quantized_recon_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\strategies-dct.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\strategies-dct.c
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\strategies-dct.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\strategies-dct.c
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
has_coeffs$1$ = 0
coeff$ = 64
residual$ = 2112
state$ = 4352
cur_cu$ = 4360
width$ = 4368
color$ = 4376
scan_order$ = 4384
use_trskip$ = 4392
in_stride$ = 4400
out_stride$ = 4408
ref_in$ = 4416
pred_in$ = 4424
rec_out$ = 4432
coeff_out$ = 4440
early_skip$ = 4448
kvz_quantize_residual_avx2 PROC

; 650  : {

$LN107:
	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+24], rsi
	mov	QWORD PTR [rsp+32], rdi
	mov	QWORD PTR [rsp+16], rdx
	push	rbp
	push	r12
	push	r13
	push	r14
	push	r15
	mov	eax, 4304				; 000010d0H
	call	__chkstk
	sub	rsp, rax
	lea	rbp, QWORD PTR [rsp+144]
	and	rbp, -64				; ffffffffffffffc0H

; 651  :   // Temporary arrays to pass data to and from kvz_quant and transform functions.
; 652  :   ALIGNED(64) int16_t residual[TR_MAX_WIDTH * TR_MAX_WIDTH];
; 653  :   ALIGNED(64) coeff_t coeff[TR_MAX_WIDTH * TR_MAX_WIDTH];
; 654  : 
; 655  :   int has_coeffs = 0;

	xor	ebx, ebx
	mov	esi, r8d
	mov	DWORD PTR has_coeffs$1$[rbp], ebx
	mov	r15d, r9d
	mov	r14, rdx
	mov	r13, rcx
	mov	r12d, ebx

; 656  : 
; 657  :   assert(width <= TR_MAX_WIDTH);

	cmp	r8d, 32					; 00000020H
	jle	SHORT $LN23@kvz_quanti
	mov	r8d, 657				; 00000291H
	lea	rdx, OFFSET FLAT:$SG4294950263
	lea	rcx, OFFSET FLAT:$SG4294950262

; 672  :   if (state->encoder_control->cfg.rdoq_enable &&

	jmp	SHORT $LN105@kvz_quanti
$LN23@kvz_quanti:

; 658  :   assert(width >= TR_MIN_WIDTH);

	cmp	esi, 4
	jge	SHORT $LN24@kvz_quanti
	mov	r8d, 658				; 00000292H
	lea	rdx, OFFSET FLAT:$SG4294950261
	lea	rcx, OFFSET FLAT:$SG4294950260
$LN105@kvz_quanti:

; 659  : 
; 660  :   // Get residual. (ref_in - pred_in -> residual)
; 661  :   get_residual_avx2(ref_in, pred_in, residual, width, in_stride);

	call	QWORD PTR __imp__wassert
$LN24@kvz_quanti:
	mov	eax, DWORD PTR in_stride$[rsp]
	lea	r8, QWORD PTR residual$[rbp]
	mov	rdx, QWORD PTR pred_in$[rsp]
	mov	r9d, esi
	mov	rcx, QWORD PTR ref_in$[rsp]
	mov	DWORD PTR [rsp+32], eax
	call	get_residual_avx2

; 662  : 
; 663  :   // Transform residual. (residual -> coeff)
; 664  :   if (use_trskip) {

	lea	rcx, OFFSET FLAT:kvz_g_convert_to_bit
	mov	r10, QWORD PTR [r13]
	mov	edi, 13
	cmp	DWORD PTR use_trskip$[rsp], ebx
	je	SHORT $LN11@kvz_quanti
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c

; 165  :   uint32_t log2_tr_size =  kvz_g_convert_to_bit[block_size] + 2;

	movsx	rax, sil

; 166  :   int32_t  shift = MAX_TR_DYNAMIC_RANGE - encoder->bitdepth - log2_tr_size;

	mov	r9d, edi

; 167  :   int32_t  j,k;
; 168  :   for (j = 0; j < block_size; j++) {

	movsx	r11d, sil
	movsx	ecx, BYTE PTR [rax+rcx]
	movsx	eax, BYTE PTR [r10+2592]
	sub	r9d, ecx
	sub	r9d, eax
	mov	r10d, ebx
	test	sil, sil
	jle	$LN35@kvz_quanti
	npad	15
$LL30@kvz_quanti:

; 169  :     for(k = 0; k < block_size; k ++) {

	mov	eax, r11d
	mov	r8d, r11d
	imul	eax, r10d
	movsxd	rdx, eax
	add	rdx, rdx
$LL33@kvz_quanti:

; 170  :       // Casting back and forth to make UBSan not trigger due to left-shifting negatives
; 171  :       coeff[j * block_size + k] = (int16_t)((uint16_t)(block[j * block_size + k]) << shift);

	movzx	eax, WORD PTR residual$[rbp+rdx]
	lea	rdx, QWORD PTR [rdx+2]
	mov	ecx, r9d
	shl	ax, cl
	mov	WORD PTR coeff$[rbp+rdx-2], ax
	sub	r8, 1
	jne	SHORT $LL33@kvz_quanti

; 167  :   int32_t  j,k;
; 168  :   for (j = 0; j < block_size; j++) {

	inc	r10d
	cmp	r10d, r11d
	jl	SHORT $LL30@kvz_quanti
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 666  :   }

	jmp	SHORT $LN35@kvz_quanti
$LN11@kvz_quanti:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\strategies-dct.c

; 80   :   switch (width) {

	movzx	ecx, sil
	cmp	sil, 4
	je	SHORT $LN40@kvz_quanti
	cmp	cl, 8
	je	SHORT $LN42@kvz_quanti
	cmp	cl, 16
	je	SHORT $LN44@kvz_quanti
	mov	rax, QWORD PTR kvz_dct_32x32
	cmp	cl, 32					; 00000020H
	cmovne	rax, rbx

; 91   :   case 32:
; 92   :     return kvz_dct_32x32;

	jmp	SHORT $LN37@kvz_quanti
$LN44@kvz_quanti:

; 89   :   case 16:
; 90   :     return kvz_dct_16x16;

	mov	rax, QWORD PTR kvz_dct_16x16
	jmp	SHORT $LN37@kvz_quanti
$LN42@kvz_quanti:

; 86   :     }
; 87   :   case 8:
; 88   :     return kvz_dct_8x8;

	mov	rax, QWORD PTR kvz_dct_8x8
	jmp	SHORT $LN37@kvz_quanti
$LN40@kvz_quanti:

; 82   :     if (color == COLOR_Y && type == CU_INTRA) {

	test	r15d, r15d
	jne	SHORT $LN41@kvz_quanti
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 668  :     kvz_transform2d(state->encoder_control, residual, coeff, width, color, cur_cu->type);

	movzx	eax, BYTE PTR [r14]
	and	al, 3
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\strategies-dct.c

; 82   :     if (color == COLOR_Y && type == CU_INTRA) {

	cmp	al, 1

; 83   :       return kvz_fast_forward_dst_4x4;

	mov	rax, QWORD PTR kvz_fast_forward_dst_4x4
	je	SHORT $LN37@kvz_quanti
$LN41@kvz_quanti:

; 84   :     } else {
; 85   :       return kvz_dct_4x4;

	mov	rax, QWORD PTR kvz_dct_4x4
$LN37@kvz_quanti:
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c

; 210  :   dct_func(encoder->bitdepth, block, coeff);

	movzx	ecx, BYTE PTR [r10+2592]
	lea	r8, QWORD PTR coeff$[rbp]
	lea	rdx, QWORD PTR residual$[rbp]
	call	rax
$LN35@kvz_quanti:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 672  :   if (state->encoder_control->cfg.rdoq_enable &&

	mov	rax, QWORD PTR [r13]
	cmp	DWORD PTR [rax+48], ebx
	je	SHORT $LN102@kvz_quanti
	cmp	esi, 4
	jg	SHORT $LN93@kvz_quanti
	cmp	DWORD PTR [rax+2400], ebx
	jne	SHORT $LN102@kvz_quanti
$LN93@kvz_quanti:

; 673  :       (width > 4 || !state->encoder_control->cfg.rdoq_skip))
; 674  :   {
; 675  :     int8_t tr_depth = cur_cu->tr_depth - cur_cu->depth;
; 676  :     tr_depth += (cur_cu->part_size == SIZE_NxN ? 1 : 0);

	movzx	r8d, BYTE PTR [r14]

; 677  :     kvz_rdoq(state, coeff, coeff_out, width, width, (color == COLOR_Y ? 0 : 2),

	mov	r9d, esi
	movzx	eax, r8b
	and	al, 224					; 000000e0H
	cmp	al, 96					; 00000060H
	movzx	eax, r8b
	sete	dl
	shr	al, 2
	and	al, 7
	and	r8b, 3
	sub	dl, al
	movzx	eax, BYTE PTR [r14+1]
	mov	r14, QWORD PTR coeff_out$[rsp]
	and	al, 7
	add	dl, al
	mov	eax, r15d
	mov	BYTE PTR [rsp+64], dl
	neg	eax
	movzx	eax, BYTE PTR scan_order$[rsp]
	lea	rdx, QWORD PTR coeff$[rbp]
	mov	BYTE PTR [rsp+56], r8b
	sbb	cl, cl
	and	cl, 2
	mov	BYTE PTR [rsp+48], al
	mov	BYTE PTR [rsp+40], cl
	mov	r8, r14
	mov	rcx, r13
	mov	DWORD PTR [rsp+32], esi
	call	kvz_rdoq

; 678  :       scan_order, cur_cu->type, tr_depth);
; 679  :   } else {

	jmp	SHORT $LN14@kvz_quanti
$LN102@kvz_quanti:

; 680  :     kvz_quant(state, coeff, coeff_out, width, width, (color == COLOR_Y ? 0 : 2),

	movzx	edx, BYTE PTR [r14]
	mov	eax, r15d
	mov	r14, QWORD PTR coeff_out$[rsp]
	and	dl, 3
	mov	BYTE PTR [rsp+56], dl
	neg	eax
	movzx	eax, BYTE PTR scan_order$[rsp]
	lea	rdx, QWORD PTR coeff$[rbp]
	sbb	cl, cl
	mov	BYTE PTR [rsp+48], al
	and	cl, 2
	mov	r9d, esi
	mov	BYTE PTR [rsp+40], cl
	mov	r8, r14
	mov	rcx, r13
	mov	DWORD PTR [rsp+32], esi
	call	QWORD PTR kvz_quant
$LN14@kvz_quanti:

; 681  :       scan_order, cur_cu->type);
; 682  :   }
; 683  : 
; 684  :   // Check if there are any non-zero coefficients.
; 685  :   for (int i = 0; i < width * width; i += 8) {

	mov	eax, esi
	imul	eax, esi
	movsxd	rcx, eax
	test	eax, eax
	jle	SHORT $LN17@kvz_quanti
	vmovdqu	xmm0, XMMWORD PTR __xmm@ffffffffffffffffffffffffffffffff
	mov	rax, rbx
	npad	6
$LL4@kvz_quanti:

; 686  :     __m128i v_quant_coeff = _mm_loadu_si128((__m128i*)&(coeff_out[i]));
; 687  :     has_coeffs = !_mm_testz_si128(_mm_set1_epi8(0xFF), v_quant_coeff);

	vptest	xmm0, XMMWORD PTR [r14+rax*2]
	mov	r12d, ebx
	setne	r12b
	mov	DWORD PTR has_coeffs$1$[rbp], r12d

; 688  :     if(has_coeffs) break;

	test	r12d, r12d
	jne	$LN101@kvz_quanti

; 681  :       scan_order, cur_cu->type);
; 682  :   }
; 683  : 
; 684  :   // Check if there are any non-zero coefficients.
; 685  :   for (int i = 0; i < width * width; i += 8) {

	add	rax, 8
	cmp	rax, rcx
	jl	SHORT $LL4@kvz_quanti
$LN17@kvz_quanti:

; 706  :   }
; 707  :   else if (rec_out != pred_in) {

	mov	r11, QWORD PTR rec_out$[rsp]
	cmp	r11, QWORD PTR pred_in$[rsp]
	je	$LN103@kvz_quanti

; 708  :     // With no coeffs and rec_out == pred_int we skip copying the coefficients
; 709  :     // because the reconstruction is just the prediction.
; 710  :     int y, x;
; 711  : 
; 712  :     for (y = 0; y < width; ++y) {

	test	esi, esi
	jle	$LN103@kvz_quanti
	movsxd	r14, DWORD PTR in_stride$[rsp]
	mov	r9, rbx
	movsxd	r15, DWORD PTR out_stride$[rsp]
	mov	r10, rsi
	mov	r12, QWORD PTR pred_in$[rsp]
	npad	1
$LL7@kvz_quanti:

; 713  :       for (x = 0; x < width; ++x) {

	mov	r8, rbx
	lea	rcx, QWORD PTR [r9+r11]
	sub	r8, r9
	mov	rdx, rsi
	sub	r8, r11
	add	r8, r12
	npad	13
$LL84@kvz_quanti:

; 714  :         rec_out[x + y * out_stride] = pred_in[x + y * in_stride];

	movzx	eax, BYTE PTR [r8+rcx]
	mov	BYTE PTR [rcx], al
	lea	rcx, QWORD PTR [rcx+1]
	sub	rdx, 1
	jne	SHORT $LL84@kvz_quanti

; 708  :     // With no coeffs and rec_out == pred_int we skip copying the coefficients
; 709  :     // because the reconstruction is just the prediction.
; 710  :     int y, x;
; 711  : 
; 712  :     for (y = 0; y < width; ++y) {

	add	r9, r15
	add	rbx, r14
	sub	r10, 1
	jne	SHORT $LL7@kvz_quanti

; 715  :       }
; 716  :     }
; 717  :   }
; 718  : 
; 719  :   return has_coeffs;

	mov	eax, DWORD PTR has_coeffs$1$[rbp]
	jmp	$LN6@kvz_quanti
$LN101@kvz_quanti:

; 689  :   }
; 690  : 
; 691  :   // Do the inverse quantization and transformation and the reconstruction to
; 692  :   // rec_out.
; 693  :   if (has_coeffs && !early_skip) {

	cmp	BYTE PTR early_skip$[rsp], bl
	jne	$LN17@kvz_quanti

; 694  : 
; 695  :     // Get quantized residual. (coeff_out -> coeff -> residual)
; 696  :     kvz_dequant(state, coeff_out, coeff, width, width, (color == COLOR_Y ? 0 : (color == COLOR_U ? 2 : 3)), cur_cu->type);

	mov	ecx, ebx
	test	r15d, r15d
	je	SHORT $LN26@kvz_quanti
	cmp	r15d, 1
	setne	cl
	add	ecx, 2
$LN26@kvz_quanti:
	mov	rax, QWORD PTR cur_cu$[rsp]
	lea	r8, QWORD PTR coeff$[rbp]
	mov	r9d, esi
	mov	rdx, r14
	movzx	eax, BYTE PTR [rax]
	and	al, 3
	mov	BYTE PTR [rsp+48], al
	mov	BYTE PTR [rsp+40], cl
	mov	rcx, r13
	mov	DWORD PTR [rsp+32], esi
	call	QWORD PTR kvz_dequant
	mov	r9, QWORD PTR [r13]

; 697  :     if (use_trskip) {

	cmp	DWORD PTR use_trskip$[rsp], ebx
	je	SHORT $LN19@kvz_quanti
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c

; 184  :   uint32_t log2_tr_size =  kvz_g_convert_to_bit[block_size] + 2;

	movsx	rax, sil
	lea	rcx, OFFSET FLAT:kvz_g_convert_to_bit
	movsx	ecx, BYTE PTR [rax+rcx]

; 185  :   int32_t  shift = MAX_TR_DYNAMIC_RANGE - encoder->bitdepth - log2_tr_size;

	movsx	eax, BYTE PTR [r9+2592]
	sub	edi, ecx
	sub	edi, eax

; 186  :   int32_t  j,k;
; 187  :   int32_t offset;
; 188  :   offset = (1 << (shift -1)); // For rounding
; 189  :   for ( j = 0; j < block_size; j++ ) {

	movsx	r9d, sil
	mov	ecx, 1
	lea	eax, DWORD PTR [rdi-1]
	shlx	r8d, ecx, eax
	test	sil, sil
	jle	$LN56@kvz_quanti
$LL51@kvz_quanti:

; 190  :     for(k = 0; k < block_size; k ++) {

	mov	eax, r9d
	mov	edx, r9d
	imul	eax, ebx
	cdqe
	add	rax, rax
	npad	1
$LL54@kvz_quanti:

; 191  :       block[j * block_size + k] =  (coeff[j * block_size + k] + offset) >> shift;

	movsx	ecx, WORD PTR coeff$[rbp+rax]
	lea	rax, QWORD PTR [rax+2]
	add	ecx, r8d
	sarx	ecx, ecx, edi
	mov	WORD PTR residual$[rbp+rax-2], cx
	sub	rdx, 1
	jne	SHORT $LL54@kvz_quanti

; 186  :   int32_t  j,k;
; 187  :   int32_t offset;
; 188  :   offset = (1 << (shift -1)); // For rounding
; 189  :   for ( j = 0; j < block_size; j++ ) {

	inc	ebx
	cmp	ebx, r9d
	jl	SHORT $LL51@kvz_quanti
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 699  :     }

	jmp	SHORT $LN56@kvz_quanti
$LN19@kvz_quanti:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\strategies-dct.c

; 109  :   switch (width) {

	movzx	ecx, sil
	cmp	sil, 4
	je	SHORT $LN61@kvz_quanti
	cmp	cl, 8
	je	SHORT $LN63@kvz_quanti
	cmp	cl, 16
	je	SHORT $LN65@kvz_quanti
	mov	rax, QWORD PTR kvz_idct_32x32
	cmp	cl, 32					; 00000020H
	cmovne	rax, rbx

; 120  :   case 32:
; 121  :     return kvz_idct_32x32;

	jmp	SHORT $LN58@kvz_quanti
$LN65@kvz_quanti:

; 118  :   case 16:
; 119  :     return kvz_idct_16x16;

	mov	rax, QWORD PTR kvz_idct_16x16
	jmp	SHORT $LN58@kvz_quanti
$LN63@kvz_quanti:

; 115  :     }
; 116  :   case 8:
; 117  :     return kvz_idct_8x8;

	mov	rax, QWORD PTR kvz_idct_8x8
	jmp	SHORT $LN58@kvz_quanti
$LN61@kvz_quanti:

; 111  :     if (color == COLOR_Y && type == CU_INTRA) {

	test	r15d, r15d
	jne	SHORT $LN62@kvz_quanti
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 701  :       kvz_itransform2d(state->encoder_control, residual, coeff, width, color, cur_cu->type);

	mov	rax, QWORD PTR cur_cu$[rsp]
	movzx	eax, BYTE PTR [rax]
	and	al, 3
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\strategies-dct.c

; 111  :     if (color == COLOR_Y && type == CU_INTRA) {

	cmp	al, 1

; 112  :       return kvz_fast_inverse_dst_4x4;

	mov	rax, QWORD PTR kvz_fast_inverse_dst_4x4
	je	SHORT $LN58@kvz_quanti
$LN62@kvz_quanti:

; 113  :     } else {
; 114  :       return kvz_idct_4x4;

	mov	rax, QWORD PTR kvz_idct_4x4
$LN58@kvz_quanti:
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c

; 221  :   idct_func(encoder->bitdepth, coeff, block);

	movzx	ecx, BYTE PTR [r9+2592]
	lea	r8, QWORD PTR residual$[rbp]
	lea	rdx, QWORD PTR coeff$[rbp]
	call	rax
$LN56@kvz_quanti:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 705  :     get_quantized_recon_avx2(residual, pred_in, in_stride, rec_out, out_stride, width);

	mov	ecx, DWORD PTR out_stride$[rsp]
	mov	r9, QWORD PTR rec_out$[rsp]
	mov	r8d, DWORD PTR in_stride$[rsp]
	mov	rdx, QWORD PTR pred_in$[rsp]
	mov	DWORD PTR [rsp+40], esi
	mov	DWORD PTR [rsp+32], ecx
	lea	rcx, QWORD PTR residual$[rbp]
	call	get_quantized_recon_avx2
$LN103@kvz_quanti:

; 720  : }

	mov	eax, r12d
$LN6@kvz_quanti:
	lea	r11, QWORD PTR [rsp+4304]
	mov	rbx, QWORD PTR [r11+48]
	mov	rsi, QWORD PTR [r11+64]
	mov	rdi, QWORD PTR [r11+72]
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r13
	pop	r12
	pop	rbp
	ret	0
kvz_quantize_residual_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
state$ = 80
q_coef$ = 88
coef$ = 96
width$ = 104
height$ = 112
type$ = 120
block_type$ = 128
kvz_dequant_avx2 PROC

; 727  : {

$LN78:
	mov	QWORD PTR [rsp+16], rbx
	mov	QWORD PTR [rsp+24], rsi
	mov	QWORD PTR [rsp+32], rdi
	push	r13
	push	r14
	push	r15
	sub	rsp, 48					; 00000030H

; 728  :   const encoder_control_t * const encoder = state->encoder_control;

	mov	r14, QWORD PTR [rcx]

; 729  :   int32_t shift,add,coeff_q;
; 730  :   int32_t n;
; 731  :   int32_t transform_shift = 15 - encoder->bitdepth - (kvz_g_convert_to_bit[ width ] + 2);

	lea	rbx, OFFSET FLAT:__ImageBase
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c

; 144  :   if(type == 0) {

	movzx	r10d, BYTE PTR type$[rsp]
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 727  : {

	mov	r11, rdx
	mov	r15d, DWORD PTR height$[rsp]

; 729  :   int32_t shift,add,coeff_q;
; 730  :   int32_t n;
; 731  :   int32_t transform_shift = 15 - encoder->bitdepth - (kvz_g_convert_to_bit[ width ] + 2);

	mov	esi, 13
	mov	QWORD PTR [rsp+80], r12
	movsx	edx, BYTE PTR [r14+2592]
	mov	edi, edx
	movsxd	r12, r9d

; 732  : 
; 733  :   int32_t qp_scaled = kvz_get_scaled_qp(type, state->qp, (encoder->bitdepth-8)*6);

	sub	dl, 8
	movzx	eax, dl
	add	al, al
	movsx	r13d, BYTE PTR kvz_g_convert_to_bit[r12+rbx]
	add	dl, al
	movsx	eax, BYTE PTR [rcx+384]
	add	dl, dl
	sub	esi, r13d
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c

; 144  :   if(type == 0) {

	test	r10b, r10b
	jne	SHORT $LN31@kvz_dequan

; 145  :     qp_scaled = qp + qp_offset;

	mov	ecx, eax
	movsx	eax, dl
	add	ecx, eax

; 146  :   } else {

	jmp	SHORT $LN34@kvz_dequan
$LN31@kvz_dequan:

; 147  :     qp_scaled = CLIP(-qp_offset, 57, qp);

	mov	r10d, 57				; 00000039H
	cmp	al, 57					; 00000039H
	jg	SHORT $LN37@kvz_dequan
	mov	r10d, eax
$LN37@kvz_dequan:
	movsx	r9d, dl
	mov	ecx, r9d
	neg	ecx
	cmp	ecx, r10d
	jg	$LN39@kvz_dequan
	cmp	al, 57					; 00000039H
	jle	$LN38@kvz_dequan
	mov	ecx, 57					; 00000039H
$LN33@kvz_dequan:

; 151  :       qp_scaled = kvz_g_chroma_scale[qp_scaled] + qp_offset;

	mov	eax, ecx
	lea	rbx, OFFSET FLAT:__ImageBase
	movzx	ecx, BYTE PTR kvz_g_chroma_scale[rax+rbx]
$LN76@kvz_dequan:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 735  :   shift = 20 - QUANT_SHIFT - transform_shift;

	movzx	r10d, BYTE PTR type$[rsp]
	add	ecx, r9d
$LN34@kvz_dequan:
	sub	edi, esi

; 736  : 
; 737  :   if (encoder->scaling_list.enable)

	imul	r15d, r12d
	mov	r12, QWORD PTR [rsp+80]
	mov	eax, 715827883				; 2aaaaaabH
	imul	ecx
	add	edi, 6
	mov	esi, edx
	shr	esi, 31
	add	esi, edx
	lea	eax, DWORD PTR [rsi+rsi*2]
	add	eax, eax
	sub	ecx, eax
	cmp	BYTE PTR [r14+2616], 0
	movsxd	r9, ecx
	je	$LN11@kvz_dequan

; 738  :   {
; 739  :     uint32_t log2_tr_size = kvz_g_convert_to_bit[ width ] + 2;
; 740  :     int32_t scalinglist_type = (block_type == CU_INTRA ? 0 : 3) + (int8_t)("\0\3\1\2"[type]);

	movsx	rax, r10b
	movsx	ecx, BYTE PTR $SG4294950259[rax+rbx]
	xor	ebx, ebx
	cmp	BYTE PTR block_type$[rsp], 1
	mov	eax, 3
	cmove	eax, ebx

; 741  : 
; 742  :     const int32_t *dequant_coef = encoder->scaling_list.de_quant_coeff[log2_tr_size-2][scalinglist_type][qp_scaled%6];
; 743  :     shift += 4;

	add	edi, 4
	add	ecx, eax
	movsxd	rdx, ecx
	lea	rcx, QWORD PTR [r13*2]
	add	rcx, r13
	lea	rax, QWORD PTR [rdx+rcx*2]
	lea	rcx, QWORD PTR [rax+rax*2]
	lea	rax, QWORD PTR [r9+rcx*2]
	mov	r10, QWORD PTR [r14+rax*8+4064]

; 744  : 
; 745  :     if (shift >qp_scaled / 6) {

	cmp	edi, esi
	jle	$LN13@kvz_dequan

; 746  :       add = 1 << (shift - qp_scaled/6 - 1);

	sub	edi, esi
	mov	ecx, 1

; 747  : 
; 748  :       for (n = 0; n < width * height; n++) {

	movsxd	rsi, r15d
	lea	eax, DWORD PTR [rdi-1]
	shlx	r14d, ecx, eax
	test	r15d, r15d
	jle	$LN9@kvz_dequan

; 749  :         coeff_q = ((q_coef[n] * dequant_coef[n]) + add ) >> (shift -  qp_scaled/6);

	sub	r11, r8
	mov	edx, 32767				; 00007fffH
	mov	r9d, -32768				; ffffffffffff8000H
	npad	11
$LL48@kvz_dequan:
	movsx	eax, WORD PTR [r11+r8]
	lea	r8, QWORD PTR [r8+2]
	mov	ecx, DWORD PTR [r10+rbx*4]
	imul	ecx, eax
	add	ecx, r14d
	sarx	eax, ecx, edi

; 750  :         coef[n] = (coeff_t)CLIP(-32768,32767,coeff_q);

	cmp	eax, edx
	cmovg	eax, edx
	cmp	eax, r9d
	cmovl	eax, r9d
	inc	rbx
	mov	WORD PTR [r8-2], ax
	cmp	rbx, rsi
	jl	SHORT $LL48@kvz_dequan

; 751  :       }
; 752  :     } else {

	jmp	$LN9@kvz_dequan
$LN38@kvz_dequan:
; File F:\open_codec_learn_2021\kvazaar-master\src\transform.c

; 147  :     qp_scaled = CLIP(-qp_offset, 57, qp);

	mov	ecx, eax
$LN39@kvz_dequan:

; 148  :     if(qp_scaled < 0) {

	test	ecx, ecx
	jns	$LN33@kvz_dequan

; 149  :       qp_scaled = qp_scaled + qp_offset;
; 150  :     } else {

	lea	rbx, OFFSET FLAT:__ImageBase
	jmp	$LN76@kvz_dequan
$LN13@kvz_dequan:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c

; 753  :       for (n = 0; n < width * height; n++) {

	test	r15d, r15d
	jle	$LN9@kvz_dequan

; 756  :         coef[n] = (coeff_t)CLIP(-32768, 32767, coeff_q << (qp_scaled/6 - shift));

	sub	esi, edi
	mov	ebx, r15d
	sub	r11, r8
	mov	edx, 32767				; 00007fffH
	mov	r9d, -32768				; ffffffffffff8000H
	npad	8
$LL52@kvz_dequan:

; 754  :         // Clip to avoid possible overflow in following shift left operation
; 755  :         coeff_q   = CLIP(-32768, 32767, q_coef[n] * dequant_coef[n]);

	movsx	eax, WORD PTR [r8+r11]
	imul	eax, DWORD PTR [r10]
	cmp	eax, edx
	jg	SHORT $LN72@kvz_dequan
	cmp	eax, r9d
	cmovl	eax, r9d
	jmp	SHORT $LN57@kvz_dequan
$LN72@kvz_dequan:
	mov	eax, edx
$LN57@kvz_dequan:

; 756  :         coef[n] = (coeff_t)CLIP(-32768, 32767, coeff_q << (qp_scaled/6 - shift));

	shlx	ecx, eax, esi
	cmp	ecx, edx
	jg	SHORT $LN73@kvz_dequan
	cmp	ecx, r9d
	cmovl	ecx, r9d
	jmp	SHORT $LN63@kvz_dequan
$LN73@kvz_dequan:
	mov	ecx, edx
$LN63@kvz_dequan:
	mov	WORD PTR [r8], cx
	add	r10, 4
	add	r8, 2
	sub	rbx, 1
	jne	SHORT $LL52@kvz_dequan

; 757  :       }
; 758  :     }
; 759  :   } else {

	jmp	$LN9@kvz_dequan
$LN11@kvz_dequan:

; 760  :     int32_t scale = kvz_g_inv_quant_scales[qp_scaled%6] << (qp_scaled/6);

	movsx	eax, WORD PTR kvz_g_inv_quant_scales[rbx+r9*2]
	vmovaps	XMMWORD PTR [rsp+16], xmm7
	vmovaps	XMMWORD PTR [rsp], xmm8
	shlx	ecx, eax, esi

; 761  :     add = 1 << (shift-1);

	lea	eax, DWORD PTR [rdi-1]

; 762  : 
; 763  :     __m256i v_scale = _mm256_set1_epi32(scale);

	vmovd	xmm7, ecx
	mov	ecx, 1
	shlx	eax, ecx, eax

; 764  :     __m256i v_add = _mm256_set1_epi32(add);
; 765  : 
; 766  :     for (n = 0; n < width*height; n+=16) {

	movsxd	rcx, r15d
	vmovd	xmm8, eax
	vpbroadcastd ymm8, xmm8
	vpbroadcastd ymm7, xmm7
	test	r15d, r15d
	jle	$LN75@kvz_dequan
	sub	r11, r8
	vmovaps	XMMWORD PTR [rsp+32], xmm6
	dec	rcx
	lea	rax, QWORD PTR [r8+16]
	shr	rcx, 4
	inc	rcx
	npad	2
$LL10@kvz_dequan:

; 767  :       __m128i temp0 = _mm_loadu_si128((__m128i*)&(q_coef[n]));

	vmovdqu	xmm6, XMMWORD PTR [r11+rax-16]

; 768  :       __m128i temp1 = _mm_loadu_si128((__m128i*)&(q_coef[n + 8]));
; 769  :       __m256i v_coeff_q_lo = _mm256_cvtepi16_epi32(_mm_unpacklo_epi64(temp0, temp1));
; 770  :       __m256i v_coeff_q_hi = _mm256_cvtepi16_epi32(_mm_unpackhi_epi64(temp0, temp1));

	vpunpckhqdq xmm0, xmm6, XMMWORD PTR [rax+r11]
	lea	rax, QWORD PTR [rax+32]
	vpmovsxwd ymm1, xmm0

; 771  :       v_coeff_q_lo = _mm256_mullo_epi32(v_coeff_q_lo, v_scale);
; 772  :       v_coeff_q_hi = _mm256_mullo_epi32(v_coeff_q_hi, v_scale);

	vpmulld	ymm2, ymm1, ymm7
	vpunpcklqdq xmm1, xmm6, XMMWORD PTR [rax+r11-32]

; 773  :       v_coeff_q_lo = _mm256_add_epi32(v_coeff_q_lo, v_add);
; 774  :       v_coeff_q_hi = _mm256_add_epi32(v_coeff_q_hi, v_add);

	vpaddd	ymm3, ymm2, ymm8
	vpmovsxwd ymm2, xmm1

; 775  :       v_coeff_q_lo = _mm256_srai_epi32(v_coeff_q_lo, shift);
; 776  :       v_coeff_q_hi = _mm256_srai_epi32(v_coeff_q_hi, shift);

	vmovd	xmm0, edi
	vpsrad	ymm5, ymm3, xmm0
	vpmulld	ymm0, ymm2, ymm7
	vpaddd	ymm3, ymm0, ymm8
	vmovd	xmm1, edi
	vpsrad	ymm2, ymm3, xmm1

; 777  :       v_coeff_q_lo = _mm256_packs_epi32(v_coeff_q_lo, v_coeff_q_hi);

	vpackssdw ymm0, ymm2, ymm5

; 778  :       _mm_storeu_si128((__m128i*)&(coef[n]), _mm256_castsi256_si128(v_coeff_q_lo) );

	vmovdqu	XMMWORD PTR [rax-48], xmm0

; 779  :       _mm_storeu_si128((__m128i*)&(coef[n + 8]), _mm256_extracti128_si256(v_coeff_q_lo, 1) );

	vextracti128 XMMWORD PTR [rax-32], ymm0, 1
	sub	rcx, 1
	jne	SHORT $LL10@kvz_dequan

; 764  :     __m256i v_add = _mm256_set1_epi32(add);
; 765  : 
; 766  :     for (n = 0; n < width*height; n+=16) {

	vmovaps	xmm6, XMMWORD PTR [rsp+32]
$LN75@kvz_dequan:
	vmovaps	xmm7, XMMWORD PTR [rsp+16]
	vmovaps	xmm8, XMMWORD PTR [rsp]
$LN9@kvz_dequan:
	vzeroupper

; 780  :     }
; 781  :   }
; 782  : }

	mov	rbx, QWORD PTR [rsp+88]
	mov	rsi, QWORD PTR [rsp+96]
	mov	rdi, QWORD PTR [rsp+104]
	add	rsp, 48					; 00000030H
	pop	r15
	pop	r14
	pop	r13
	ret	0
kvz_dequant_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
coeffs$ = 48
length$ = 56
coeff_abs_sum_avx2 PROC

; 787  : {

	mov	QWORD PTR [rsp+8], rbx
	push	rdi
	sub	rsp, 32					; 00000020H
	mov	rbx, rdx
	mov	rdi, rcx

; 788  :   assert(length % 8 == 0);

	test	dl, 7
	je	SHORT $LN6@coeff_abs_
	mov	r8d, 788				; 00000314H
	lea	rdx, OFFSET FLAT:$SG4294950258
	lea	rcx, OFFSET FLAT:$SG4294950257
	call	QWORD PTR __imp__wassert
$LN6@coeff_abs_:

; 789  : 
; 790  :   __m256i total = _mm256_abs_epi32(_mm256_cvtepi16_epi32(_mm_loadu_si128((__m128i*) coeffs)));

	vpmovsxwd ymm0, XMMWORD PTR [rdi]
	mov	edx, 8
	vpabsd	ymm2, ymm0

; 791  : 
; 792  :   for (int i = 8; i < length; i += 8) {

	cmp	rbx, rdx
	jbe	SHORT $LN3@coeff_abs_
	lea	rax, QWORD PTR [rdi+16]
	npad	9
$LL4@coeff_abs_:

; 793  :     __m256i temp = _mm256_abs_epi32(_mm256_cvtepi16_epi32(_mm_loadu_si128((__m128i*) &coeffs[i])));

	vpmovsxwd ymm0, XMMWORD PTR [rax]
	add	edx, 8
	lea	rax, QWORD PTR [rax+16]
	movsxd	rcx, edx
	vpabsd	ymm1, ymm0

; 794  :     total = _mm256_add_epi32(total, temp);

	vpaddd	ymm2, ymm1, ymm2
	cmp	rcx, rbx
	jb	SHORT $LL4@coeff_abs_
$LN3@coeff_abs_:

; 795  :   }
; 796  : 
; 797  :   __m128i result128 = _mm_add_epi32(

	vextractf128 xmm0, ymm2, 1
	vpaddd	xmm1, xmm0, xmm2

; 798  :     _mm256_castsi256_si128(total),
; 799  :     _mm256_extractf128_si256(total, 1)
; 800  :   );
; 801  : 
; 802  :   uint32_t parts[4];
; 803  :   _mm_storeu_si128((__m128i*) parts, result128);
; 804  : 
; 805  :   return parts[0] + parts[1] + parts[2] + parts[3];

	vmovd	ecx, xmm1
	vpextrd	eax, xmm1, 1
	add	eax, ecx
	vpextrd	ecx, xmm1, 2
	add	eax, ecx
	vpextrd	ecx, xmm1, 3
	add	eax, ecx
	vzeroupper

; 806  : }

	mov	rbx, QWORD PTR [rsp+48]
	add	rsp, 32					; 00000020H
	pop	rdi
	ret	0
coeff_abs_sum_avx2 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\avx2\quant-avx2.c
_TEXT	SEGMENT
$T1 = 0
$T2 = 0
coeff$ = 112
width$ = 120
weights$ = 128
fast_coeff_cost_avx2 PROC

; 809  : {

	mov	rax, rsp
	mov	QWORD PTR [rax+24], r8
	sub	rsp, 104				; 00000068H

; 810  :   const __m256i zero           = _mm256_setzero_si256();
; 811  :   const __m256i threes         = _mm256_set1_epi16(3);
; 812  :   const __m256i negate_hibytes = _mm256_set1_epi16(0xff00);
; 813  :   const __m128i wt_extract_los = _mm_cvtsi32_si128(0x06040200);
; 814  :   const __m128i wt_extract_his = _mm_cvtsi32_si128(0x07050301);
; 815  : 
; 816  :   __m256i lo_sum     = _mm256_setzero_si256();
; 817  :   __m256i hi_sum     = _mm256_setzero_si256();
; 818  : 
; 819  :   __m128i wts_128    = _mm_loadl_epi64 ((const __m128i *)&weights);

	vmovq	xmm2, QWORD PTR [rax+24]
	vmovdqu	ymm3, YMMWORD PTR __ymm@0003000300030003000300030003000300030003000300030003000300030003
	vmovaps	XMMWORD PTR [rax-24], xmm6
	vmovaps	XMMWORD PTR [rax-40], xmm7
	vmovaps	XMMWORD PTR [rax-56], xmm8
	vmovdqu	ymm8, YMMWORD PTR __ymm@ff00ff00ff00ff00ff00ff00ff00ff00ff00ff00ff00ff00ff00ff00ff00ff00
	vmovaps	XMMWORD PTR [rax-72], xmm9
	vmovaps	XMMWORD PTR [rax-88], xmm10
	mov	eax, 100925952				; 06040200H
	vmovd	xmm0, eax

; 820  :   __m128i wts_lo_128 = _mm_shuffle_epi8(wts_128, wt_extract_los);

	vpshufb	xmm1, xmm2, xmm0

; 821  :   __m128i wts_hi_128 = _mm_shuffle_epi8(wts_128, wt_extract_his);
; 822  : 
; 823  :   __m256i wts_lo     = _mm256_broadcastsi128_si256(wts_lo_128);

	vmovdqu	XMMWORD PTR $T1[rsp], xmm1
	vbroadcasti128 ymm9, XMMWORD PTR $T1[rsp]
	mov	eax, 117768961				; 07050301H
	vmovd	xmm0, eax

; 824  :   __m256i wts_hi     = _mm256_broadcastsi128_si256(wts_hi_128);
; 825  : 
; 826  :   for (int i = 0; i < width * width; i += 32) {

	mov	eax, edx
	imul	eax, edx
	vpshufb	xmm1, xmm2, xmm0
	vmovdqu	XMMWORD PTR $T2[rsp], xmm1
	vbroadcasti128 ymm10, XMMWORD PTR $T2[rsp]
	vpxor	xmm7, xmm7, xmm7
	movsxd	r8, eax
	vpxor	xmm5, xmm5, xmm5
	vpxor	xmm6, xmm6, xmm6
	test	eax, eax
	jle	SHORT $LN3@fast_coeff
	xor	eax, eax
	npad	11
$LL4@fast_coeff:

; 827  :     __m256i curr_lo      = _mm256_loadu_si256 ((const __m256i *)(coeff + i));
; 828  :     __m256i curr_abs_lo  = _mm256_abs_epi16   (curr_lo);

	vpabsw	ymm0, YMMWORD PTR [rcx+rax*2]

; 829  :     __m256i curr_max3_lo = _mm256_min_epu16   (curr_abs_lo, threes);

	vpminuw	ymm4, ymm0, ymm3

; 830  : 
; 831  :     // 4x4 blocks only have 16 coeffs, so handle them separately
; 832  :     __m256i curr_max3_hi;
; 833  :     if (width >= 8) {

	cmp	edx, 8
	jl	SHORT $LN5@fast_coeff

; 834  :       __m256i curr_hi      = _mm256_loadu_si256 ((const __m256i *)(coeff + i + 16));
; 835  :       __m256i curr_abs_hi  = _mm256_abs_epi16   (curr_hi);

	vpabsw	ymm0, YMMWORD PTR [rcx+rax*2+32]

; 836  :               curr_max3_hi = _mm256_min_epu16   (curr_abs_hi, threes);

	vpminuw	ymm1, ymm0, ymm3

; 837  :               curr_max3_hi = _mm256_slli_epi16  (curr_max3_hi, 8);

	vpsllw	ymm2, ymm1, 8

; 838  :     } else {

	jmp	SHORT $LN6@fast_coeff
$LN5@fast_coeff:

; 839  :       // Set MSBs for high bytes if they're meaningless, so shuffles will
; 840  :       // return zeros for them
; 841  :       curr_max3_hi = negate_hibytes;

	vmovdqu	ymm2, ymm8
$LN6@fast_coeff:

; 842  :     }
; 843  :     __m256i curr_max3    = _mm256_or_si256    (curr_max3_lo, curr_max3_hi);

	vpor	ymm2, ymm2, ymm4

; 844  :     __m256i curr_wts_lo  = _mm256_shuffle_epi8(wts_lo, curr_max3);

	vpshufb	ymm0, ymm9, ymm2

; 845  :     __m256i curr_wts_hi  = _mm256_shuffle_epi8(wts_hi, curr_max3);
; 846  : 
; 847  :     __m256i curr_sum_lo  = _mm256_sad_epu8    (curr_wts_lo, zero);

	vpsadbw	ymm1, ymm0, ymm7
	vpshufb	ymm0, ymm10, ymm2

; 848  :     __m256i curr_sum_hi  = _mm256_sad_epu8    (curr_wts_hi, zero);
; 849  : 
; 850  :             lo_sum       = _mm256_add_epi64   (lo_sum, curr_sum_lo);

	vpaddq	ymm5, ymm1, ymm5
	add	rax, 32					; 00000020H
	vpsadbw	ymm1, ymm0, ymm7

; 851  :             hi_sum       = _mm256_add_epi64   (hi_sum, curr_sum_hi);

	vpaddq	ymm6, ymm1, ymm6
	cmp	rax, r8
	jl	SHORT $LL4@fast_coeff
$LN3@fast_coeff:

; 852  :   }
; 853  :           hi_sum = _mm256_slli_epi64(hi_sum, 8);

	vpsllq	ymm0, ymm6, 8

; 854  :   __m256i sum0   = _mm256_add_epi64(lo_sum, hi_sum);

	vpaddq	ymm2, ymm0, ymm5

; 855  : 
; 856  :   __m256i sum1   = _mm256_permute4x64_epi64(sum0, _MM_SHUFFLE(1, 0, 3, 2));

	vpermq	ymm1, ymm2, 78				; 0000004eH

; 857  :   __m256i sum2   = _mm256_add_epi64        (sum0, sum1);

	vpaddq	ymm3, ymm1, ymm2

; 858  :   __m256i sum3   = _mm256_shuffle_epi32    (sum2, _MM_SHUFFLE(1, 0, 3, 2));

	vpshufd	ymm0, ymm3, 78				; 0000004eH

; 859  :   __m256i sum4   = _mm256_add_epi64        (sum2, sum3);

	vpaddq	ymm1, ymm0, ymm3

; 860  : 
; 861  :   __m128i sum128 = _mm256_castsi256_si128  (sum4);
; 862  :   return (_mm_cvtsi128_si32(sum128) + (1 << 7)) >> 8;

	vmovd	eax, xmm1
	sub	eax, -128				; ffffffffffffff80H
	sar	eax, 8
	vzeroupper

; 863  : }

	vmovaps	xmm6, XMMWORD PTR [rsp+80]
	vmovaps	xmm7, XMMWORD PTR [rsp+64]
	vmovaps	xmm8, XMMWORD PTR [rsp+48]
	vmovaps	xmm9, XMMWORD PTR [rsp+32]
	vmovaps	xmm10, XMMWORD PTR [rsp+16]
	add	rsp, 104				; 00000068H
	ret	0
fast_coeff_cost_avx2 ENDP
_TEXT	ENDS
END
