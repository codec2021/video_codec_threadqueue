; Listing generated by Microsoft (R) Optimizing Compiler Version 19.26.28806.0 

include listing.inc

INCLUDELIB OLDNAMES

cbf_masks DW	01fH
	DW	0fH
	DW	07H
	DW	03H
	DW	01H
	ORG $+6
g_sig_last_scan_16x16 DD 00H
	DD	04H
	DD	01H
	DD	08H
	DD	05H
	DD	02H
	DD	0cH
	DD	09H
	DD	06H
	DD	03H
	DD	0dH
	DD	0aH
	DD	07H
	DD	0eH
	DD	0bH
	DD	0fH
g_group_idx DB	00H
	DB	01H
	DB	02H
	DB	03H
	DB	04H
	DB	04H
	DB	05H
	DB	05H
	DB	06H
	DB	06H
	DB	06H
	DB	06H
	DB	07H
	DB	07H
	DB	07H
	DB	07H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	08H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
	DB	09H
strategies_to_select DQ FLAT:$SG4294951993
	DQ	FLAT:kvz_array_checksum
	DQ	FLAT:$SG4294951992
	DQ	FLAT:kvz_array_md5
	DQ	FLAT:$SG4294951991
	DQ	FLAT:kvz_reg_sad
	DQ	FLAT:$SG4294951990
	DQ	FLAT:kvz_sad_4x4
	DQ	FLAT:$SG4294951989
	DQ	FLAT:kvz_sad_8x8
	DQ	FLAT:$SG4294951988
	DQ	FLAT:kvz_sad_16x16
	DQ	FLAT:$SG4294951987
	DQ	FLAT:kvz_sad_32x32
	DQ	FLAT:$SG4294951986
	DQ	FLAT:kvz_sad_64x64
	DQ	FLAT:$SG4294951985
	DQ	FLAT:kvz_satd_4x4
	DQ	FLAT:$SG4294951984
	DQ	FLAT:kvz_satd_8x8
	DQ	FLAT:$SG4294951983
	DQ	FLAT:kvz_satd_16x16
	DQ	FLAT:$SG4294951982
	DQ	FLAT:kvz_satd_32x32
	DQ	FLAT:$SG4294951981
	DQ	FLAT:kvz_satd_64x64
	DQ	FLAT:$SG4294951980
	DQ	FLAT:kvz_satd_any_size
	DQ	FLAT:$SG4294951979
	DQ	FLAT:kvz_sad_4x4_dual
	DQ	FLAT:$SG4294951978
	DQ	FLAT:kvz_sad_8x8_dual
	DQ	FLAT:$SG4294951977
	DQ	FLAT:kvz_sad_16x16_dual
	DQ	FLAT:$SG4294951976
	DQ	FLAT:kvz_sad_32x32_dual
	DQ	FLAT:$SG4294951975
	DQ	FLAT:kvz_sad_64x64_dual
	DQ	FLAT:$SG4294951974
	DQ	FLAT:kvz_satd_4x4_dual
	DQ	FLAT:$SG4294951973
	DQ	FLAT:kvz_satd_8x8_dual
	DQ	FLAT:$SG4294951972
	DQ	FLAT:kvz_satd_16x16_dual
	DQ	FLAT:$SG4294951971
	DQ	FLAT:kvz_satd_32x32_dual
	DQ	FLAT:$SG4294951970
	DQ	FLAT:kvz_satd_64x64_dual
	DQ	FLAT:$SG4294951969
	DQ	FLAT:kvz_satd_any_size_quad
	DQ	FLAT:$SG4294951968
	DQ	FLAT:kvz_pixels_calc_ssd
	DQ	FLAT:$SG4294951967
	DQ	FLAT:kvz_bipred_average
	DQ	FLAT:$SG4294951966
	DQ	FLAT:kvz_get_optimized_sad
	DQ	FLAT:$SG4294951965
	DQ	FLAT:kvz_ver_sad
	DQ	FLAT:$SG4294951964
	DQ	FLAT:kvz_hor_sad
	DQ	FLAT:$SG4294951963
	DQ	FLAT:kvz_pixel_var
	DQ	FLAT:$SG4294951962
	DQ	FLAT:kvz_fast_forward_dst_4x4
	DQ	FLAT:$SG4294951961
	DQ	FLAT:kvz_dct_4x4
	DQ	FLAT:$SG4294951960
	DQ	FLAT:kvz_dct_8x8
	DQ	FLAT:$SG4294951959
	DQ	FLAT:kvz_dct_16x16
	DQ	FLAT:$SG4294951958
	DQ	FLAT:kvz_dct_32x32
	DQ	FLAT:$SG4294951957
	DQ	FLAT:kvz_fast_inverse_dst_4x4
	DQ	FLAT:$SG4294951956
	DQ	FLAT:kvz_idct_4x4
	DQ	FLAT:$SG4294951955
	DQ	FLAT:kvz_idct_8x8
	DQ	FLAT:$SG4294951954
	DQ	FLAT:kvz_idct_16x16
	DQ	FLAT:$SG4294951953
	DQ	FLAT:kvz_idct_32x32
	DQ	FLAT:$SG4294951952
	DQ	FLAT:kvz_filter_hpel_blocks_hor_ver_luma
	DQ	FLAT:$SG4294951951
	DQ	FLAT:kvz_filter_hpel_blocks_diag_luma
	DQ	FLAT:$SG4294951950
	DQ	FLAT:kvz_filter_qpel_blocks_hor_ver_luma
	DQ	FLAT:$SG4294951949
	DQ	FLAT:kvz_filter_qpel_blocks_diag_luma
	DQ	FLAT:$SG4294951948
	DQ	FLAT:kvz_sample_quarterpel_luma
	DQ	FLAT:$SG4294951947
	DQ	FLAT:kvz_sample_octpel_chroma
	DQ	FLAT:$SG4294951946
	DQ	FLAT:kvz_sample_quarterpel_luma_hi
	DQ	FLAT:$SG4294951945
	DQ	FLAT:kvz_sample_octpel_chroma_hi
	DQ	FLAT:$SG4294951944
	DQ	FLAT:kvz_get_extended_block
	DQ	FLAT:$SG4294951943
	DQ	FLAT:kvz_quant
	DQ	FLAT:$SG4294951942
	DQ	FLAT:kvz_quantize_residual
	DQ	FLAT:$SG4294951941
	DQ	FLAT:kvz_dequant
	DQ	FLAT:$SG4294951940
	DQ	FLAT:kvz_coeff_abs_sum
	DQ	FLAT:$SG4294951939
	DQ	FLAT:kvz_fast_coeff_cost
	DQ	FLAT:$SG4294951938
	DQ	FLAT:kvz_angular_pred
	DQ	FLAT:$SG4294951937
	DQ	FLAT:kvz_intra_pred_planar
	DQ	FLAT:$SG4294951936
	DQ	FLAT:kvz_intra_pred_filtered_dc
	DQ	FLAT:$SG4294951935
	DQ	FLAT:kvz_sao_edge_ddistortion
	DQ	FLAT:$SG4294951934
	DQ	FLAT:kvz_calc_sao_edge_dir
	DQ	FLAT:$SG4294951933
	DQ	FLAT:kvz_sao_reconstruct_color
	DQ	FLAT:$SG4294951932
	DQ	FLAT:kvz_sao_band_ddistortion
	DQ	FLAT:$SG4294951931
	DQ	FLAT:kvz_encode_coeff_nxn
	DQ	0000000000000000H
	DQ	0000000000000000H
g_sig_last_scan_32x32 DD 00H
	DD	08H
	DD	01H
	DD	010H
	DD	09H
	DD	02H
	DD	018H
	DD	011H
	DD	0aH
	DD	03H
	DD	020H
	DD	019H
	DD	012H
	DD	0bH
	DD	04H
	DD	028H
	DD	021H
	DD	01aH
	DD	013H
	DD	0cH
	DD	05H
	DD	030H
	DD	029H
	DD	022H
	DD	01bH
	DD	014H
	DD	0dH
	DD	06H
	DD	038H
	DD	031H
	DD	02aH
	DD	023H
	DD	01cH
	DD	015H
	DD	0eH
	DD	07H
	DD	039H
	DD	032H
	DD	02bH
	DD	024H
	DD	01dH
	DD	016H
	DD	0fH
	DD	03aH
	DD	033H
	DD	02cH
	DD	025H
	DD	01eH
	DD	017H
	DD	03bH
	DD	034H
	DD	02dH
	DD	026H
	DD	01fH
	DD	03cH
	DD	035H
	DD	02eH
	DD	027H
	DD	03dH
	DD	036H
	DD	02fH
	DD	03eH
	DD	037H
	DD	03fH
g_min_in_group DB 00H
	DB	01H
	DB	02H
	DB	03H
	DB	04H
	DB	06H
	DB	08H
	DB	0cH
	DB	010H
	DB	018H
	ORG $+6
default_fast_coeff_cost_wts DD 03e282e88r	; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e282e88r			; 0.16424
	DD	040852b41r			; 4.16153
	DD	0406093ffr			; 3.50903
	DD	040ddb290r			; 6.92805
	DD	03e26c094r			; 0.162844
	DD	04081ca43r			; 4.05594
	DD	04064203ar			; 3.56447
	DD	040db915ar			; 6.86149
	DD	03e03d189r			; 0.128729
	DD	04089fbafr			; 4.31197
	DD	0407c5771r			; 3.94284
	DD	040ddeed2r			; 6.9354
	DD	03de33ce6r			; 0.110956
	DD	0408ddcb1r			; 4.43319
	DD	0407c8738r			; 3.94575
	DD	040dc1618r			; 6.8777
	DD	03dc29cfer			; 0.095026
	DD	0408f7938r			; 4.48355
	DD	0408636aar			; 4.19417
	DD	040d90260r			; 6.78154
	DD	03d99b1b8r			; 0.075046
	DD	04094474cr			; 4.6337
	DD	04082b1b6r			; 4.08419
	DD	040d65aeer			; 6.6986
	DD	03d56bca5r			; 0.052426
	DD	0409ef37er			; 4.96722
	DD	04080dee8r			; 4.02721
	DD	040d19306r			; 6.5492
	DD	03d24bcaer			; 0.040219
	DD	040a489car			; 5.14182
	DD	0407ee3bdr			; 3.98265
	DD	040cec513r			; 6.46156
	DD	03d0fba88r			; 0.03509
	DD	040a628e7r			; 5.19249
	DD	040752e49r			; 3.83095
	DD	040cd642ar			; 6.41848
	DD	03cf47d80r			; 0.029845
	DD	040a6c5d0r			; 5.21165
	DD	040743073r			; 3.81546
	DD	040cb0dd8r			; 6.34544
	DD	03cc0b136r			; 0.023522
	DD	040aa4f92r			; 5.32221
	DD	040744224r			; 3.81654
	DD	040cb8aaar			; 6.36068
	DD	03cae87d3r			; 0.021305
	DD	040a73ac3r			; 5.22592
	DD	04075eeccr			; 3.8427
	DD	040ca6cd9r			; 6.32579
	DD	03c821294r			; 0.015878
	DD	040a5dbe0r			; 5.18309
	DD	0407d2f27r			; 3.956
	DD	040ca8cbdr			; 6.32968
	DD	03c2ae297r			; 0.01043
	DD	040a32ce4r			; 5.09923
	DD	04085a85fr			; 4.1768
	DD	040c9c5d6r			; 6.3054
	DD	03c0a2a91r			; 0.008433
	DD	040a0f7der			; 5.03026
	DD	040879a50r			; 4.23759
	DD	040c8a4eer			; 6.27013
	DD	03bd4fdf4r			; 0.0065
	DD	0409f0412r			; 4.96925
	DD	0408adc57r			; 4.3394
	DD	040c6f870r			; 6.21783
	DD	03ba18373r			; 0.004929
	DD	0409d8d50r			; 4.9235
	DD	0408e283fr			; 4.44241
	DD	040c5df6cr			; 6.18352
	DD	03b73775cr			; 0.003715
	DD	0409d4c75r			; 4.91558
	DD	0408dbb1br			; 4.42909
	DD	040c4029fr			; 6.12532
	DD	03b4a70d2r			; 0.003089
	DD	0409c48f7r			; 4.88391
	DD	040920260r			; 4.56279
	DD	040c5019dr			; 6.15645
	DD	03b219c9dr			; 0.002466
	DD	0409c31abr			; 4.88106
	DD	040942800r			; 4.62988
	DD	040c49088r			; 6.14264
	DD	03b0e25c8r			; 0.002169
	DD	0409c3d62r			; 4.88249
	DD	04094ae99r			; 4.64631
	DD	040c415d1r			; 6.12766
	DD	03b26dacbr			; 0.002546
	DD	040996304r			; 4.79334
	DD	0409acc16r			; 4.83741
	DD	040c6606br			; 6.19927
	DD	03aac3a86r			; 0.001314
	DD	04099e220r			; 4.80885
	DD	0409a81bdr			; 4.82834
	DD	040c7ca3cr			; 6.24344
	DD	03a9741d1r			; 0.001154
	DD	0409b9a72r			; 4.8626
	DD	0409b19aar			; 4.84688
	DD	040c693a5r			; 6.20552
	DD	03a80f990r			; 0.000984
	DD	0409bb993r			; 4.8664
	DD	0409b7fa2r			; 4.85933
	DD	040c7b565r			; 6.24089
	DD	03a551f82r			; 0.000813
	DD	0409b698ar			; 4.85663
	DD	0409d95bar			; 4.92453
	DD	040c963a4r			; 6.29341
	DD	03a91c087r			; 0.001112
	DD	04099419er			; 4.78926
	DD	040a050f0r			; 5.00988
	DD	040cddf8fr			; 6.43354
	DD	03a10b418r			; 0.000552
	DD	04098580ar			; 4.76075
	DD	040a2e4f1r			; 5.09045
	DD	040d32e1fr			; 6.59938
	DD	039ccff22r			; 0.000391
	DD	0409ec42cr			; 4.96145
	DD	040a38d95r			; 5.11103
	DD	040d8342fr			; 6.75637
	DD	039ae1049r			; 0.000332
	DD	0409f63f8r			; 4.98095
	DD	040a46b89r			; 5.13813
	DD	040dbc1e8r			; 6.86742
	DD	03952c387r			; 0.000201
	DD	040a5d297r			; 5.18196
	DD	04097af64r			; 4.74016
	DD	040cec07dr			; 6.461
	DD	0397ba882r			; 0.00024
	DD	040a5eeb7r			; 5.18539
	DD	0409bfeb0r			; 4.87484
	DD	040da3603r			; 6.81909
	DD	03908509cr			; 0.00013
	DD	040a8a6b5r			; 5.27035
	DD	040977eacr			; 4.73421
	DD	040da708fr			; 6.82624
	DD	038da1a93r			; 0.000104
	DD	040abe6e8r			; 5.37194
	DD	040930af4r			; 4.59509
	DD	040d5189ar			; 6.65925
	DD	038ae1049r			; 8.3e-05
	DD	040ab9581r			; 5.362
	DD	04093c250r			; 4.61747
	DD	040dacf03r			; 6.83777
	DD	03890b418r			; 6.9e-05
	DD	040a926e3r			; 5.286
	DD	0409828e7r			; 4.75499
	DD	040e516e1r			; 7.15904
	DD	0384d8559r			; 4.9e-05
	DD	040afa18cr			; 5.48847
	DD	0408cace9r			; 4.39611
	DD	040d74682r			; 6.72736
	DD	038734507r			; 5.8e-05
	DD	0409eafa3r			; 4.95894
	DD	040929321r			; 4.58046
	DD	040cf49a5r			; 6.47774
	DD	037eae18br			; 2.8e-05
	DD	040b0ae1br			; 5.52125
	DD	0408e1885r			; 4.44049
	DD	040e68f80r			; 7.20502
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	000000000r			; 0
	DD	0379f6230r			; 1.9e-05
	DD	040b9f5d8r			; 5.81126
	DD	0408cc582r			; 4.39911
	DD	040eac30dr			; 7.33631
g_sao_edge_offsets DD 0ffffffffH
	DD	00H
	DD	01H
	DD	00H
	DD	00H
	DD	0ffffffffH
	DD	00H
	DD	01H
	DD	0ffffffffH
	DD	0ffffffffH
	DD	01H
	DD	01H
	DD	01H
	DD	0ffffffffH
	DD	0ffffffffH
	DD	01H
g_sig_last_scan_cg DQ FLAT:g_sig_last_scan_8x8
	DQ	FLAT:g_sig_last_scan_8x8+16
	DQ	FLAT:g_sig_last_scan_8x8+32
	DQ	FLAT:g_sig_last_scan_8x8
	DQ	FLAT:g_sig_last_scan_8x8+16
	DQ	FLAT:g_sig_last_scan_8x8+32
	DQ	FLAT:g_sig_last_scan_16x16
	DQ	0000000000000000H
	DQ	0000000000000000H
	DQ	FLAT:g_sig_last_scan_32x32
	DQ	0000000000000000H
	DQ	0000000000000000H
g_sig_last_scan_8x8 DD 00H
	DD	02H
	DD	01H
	DD	03H
	DD	00H
	DD	01H
	DD	02H
	DD	03H
	DD	00H
	DD	02H
	DD	01H
	DD	03H
	ORG $+3
$SG4294951943 DB 'quant', 00H
	ORG $+7
$SG4294951993 DB 'array_checksum', 00H
	ORG $+1
$SG4294951992 DB 'array_md5', 00H
	ORG $+6
$SG4294951991 DB 'reg_sad', 00H
$SG4294951990 DB 'sad_4x4', 00H
$SG4294951989 DB 'sad_8x8', 00H
$SG4294951988 DB 'sad_16x16', 00H
	ORG $+6
$SG4294951987 DB 'sad_32x32', 00H
	ORG $+6
$SG4294951986 DB 'sad_64x64', 00H
	ORG $+6
$SG4294951985 DB 'satd_4x4', 00H
	ORG $+7
$SG4294951984 DB 'satd_8x8', 00H
	ORG $+7
$SG4294951983 DB 'satd_16x16', 00H
	ORG $+5
$SG4294951982 DB 'satd_32x32', 00H
	ORG $+5
$SG4294951981 DB 'satd_64x64', 00H
	ORG $+5
$SG4294951980 DB 'satd_any_size', 00H
	ORG $+2
$SG4294951979 DB 'sad_4x4_dual', 00H
	ORG $+3
$SG4294951978 DB 'sad_8x8_dual', 00H
	ORG $+3
$SG4294951977 DB 'sad_16x16_dual', 00H
	ORG $+1
$SG4294951976 DB 'sad_32x32_dual', 00H
	ORG $+1
$SG4294951975 DB 'sad_64x64_dual', 00H
	ORG $+1
$SG4294951974 DB 'satd_4x4_dual', 00H
	ORG $+2
$SG4294951973 DB 'satd_8x8_dual', 00H
	ORG $+2
$SG4294951972 DB 'satd_16x16_dual', 00H
$SG4294951971 DB 'satd_32x32_dual', 00H
$SG4294951970 DB 'satd_64x64_dual', 00H
$SG4294951969 DB 'satd_any_size_quad', 00H
	ORG $+5
$SG4294951968 DB 'pixels_calc_ssd', 00H
$SG4294951967 DB 'bipred_average', 00H
	ORG $+1
$SG4294951966 DB 'get_optimized_sad', 00H
	ORG $+6
$SG4294951965 DB 'ver_sad', 00H
$SG4294951964 DB 'hor_sad', 00H
$SG4294951963 DB 'pixel_var', 00H
	ORG $+6
$SG4294951962 DB 'fast_forward_dst_4x4', 00H
	ORG $+3
$SG4294951961 DB 'dct_4x4', 00H
$SG4294951960 DB 'dct_8x8', 00H
$SG4294951959 DB 'dct_16x16', 00H
	ORG $+6
$SG4294951958 DB 'dct_32x32', 00H
	ORG $+6
$SG4294951957 DB 'fast_inverse_dst_4x4', 00H
	ORG $+3
$SG4294951956 DB 'idct_4x4', 00H
	ORG $+7
$SG4294951955 DB 'idct_8x8', 00H
	ORG $+7
$SG4294951954 DB 'idct_16x16', 00H
	ORG $+5
$SG4294951953 DB 'idct_32x32', 00H
	ORG $+5
$SG4294951952 DB 'filter_hpel_blocks_hor_ver_luma', 00H
$SG4294951951 DB 'filter_hpel_blocks_diag_luma', 00H
	ORG $+3
$SG4294951950 DB 'filter_qpel_blocks_hor_ver_luma', 00H
$SG4294951949 DB 'filter_qpel_blocks_diag_luma', 00H
	ORG $+3
$SG4294951948 DB 'sample_quarterpel_luma', 00H
	ORG $+1
$SG4294951947 DB 'sample_octpel_chroma', 00H
	ORG $+3
$SG4294951946 DB 'sample_quarterpel_luma_hi', 00H
	ORG $+6
$SG4294951945 DB 'sample_octpel_chroma_hi', 00H
$SG4294951944 DB 'get_extended_block', 00H
	ORG $+5
$SG4294951942 DB 'quantize_residual', 00H
	ORG $+6
$SG4294951941 DB 'dequant', 00H
$SG4294951940 DB 'coeff_abs_sum', 00H
	ORG $+2
$SG4294951939 DB 'fast_coeff_cost', 00H
$SG4294951938 DB 'angular_pred', 00H
	ORG $+3
$SG4294951937 DB 'intra_pred_planar', 00H
	ORG $+6
$SG4294951936 DB 'intra_pred_filtered_dc', 00H
	ORG $+1
$SG4294951935 DB 'sao_edge_ddistortion', 00H
	ORG $+3
$SG4294951934 DB 'calc_sao_edge_dir', 00H
	ORG $+6
$SG4294951933 DB 'sao_reconstruct_color', 00H
	ORG $+2
$SG4294951932 DB 'sao_band_ddistortion', 00H
	ORG $+3
$SG4294951931 DB 'encode_coeff_nxn', 00H
PUBLIC	kvz_reg_sad_sse41
PUBLIC	kvz_strategy_register_picture_sse41
pdata	SEGMENT
$pdata$hor_sad_sse41 DD imagerel hor_sad_sse41
	DD	imagerel hor_sad_sse41+304
	DD	imagerel $unwind$hor_sad_sse41
$pdata$11$hor_sad_sse41 DD imagerel hor_sad_sse41+304
	DD	imagerel hor_sad_sse41+643
	DD	imagerel $chain$11$hor_sad_sse41
$pdata$13$hor_sad_sse41 DD imagerel hor_sad_sse41+643
	DD	imagerel hor_sad_sse41+835
	DD	imagerel $chain$13$hor_sad_sse41
$pdata$16$hor_sad_sse41 DD imagerel hor_sad_sse41+835
	DD	imagerel hor_sad_sse41+2279
	DD	imagerel $chain$16$hor_sad_sse41
$pdata$17$hor_sad_sse41 DD imagerel hor_sad_sse41+2279
	DD	imagerel hor_sad_sse41+2881
	DD	imagerel $chain$17$hor_sad_sse41
$pdata$hor_sad_sse41_w32 DD imagerel hor_sad_sse41_w32
	DD	imagerel hor_sad_sse41_w32+722
	DD	imagerel $unwind$hor_sad_sse41_w32
$pdata$ver_sad_sse41 DD imagerel ver_sad_sse41
	DD	imagerel ver_sad_sse41+22
	DD	imagerel $unwind$ver_sad_sse41
$pdata$1$ver_sad_sse41 DD imagerel ver_sad_sse41+22
	DD	imagerel ver_sad_sse41+55
	DD	imagerel $chain$1$ver_sad_sse41
$pdata$3$ver_sad_sse41 DD imagerel ver_sad_sse41+55
	DD	imagerel ver_sad_sse41+192
	DD	imagerel $chain$3$ver_sad_sse41
$pdata$4$ver_sad_sse41 DD imagerel ver_sad_sse41+192
	DD	imagerel ver_sad_sse41+266
	DD	imagerel $chain$4$ver_sad_sse41
$pdata$6$ver_sad_sse41 DD imagerel ver_sad_sse41+266
	DD	imagerel ver_sad_sse41+491
	DD	imagerel $chain$6$ver_sad_sse41
$pdata$8$ver_sad_sse41 DD imagerel ver_sad_sse41+491
	DD	imagerel ver_sad_sse41+600
	DD	imagerel $chain$8$ver_sad_sse41
$pdata$10$ver_sad_sse41 DD imagerel ver_sad_sse41+600
	DD	imagerel ver_sad_sse41+810
	DD	imagerel $chain$10$ver_sad_sse41
$pdata$12$ver_sad_sse41 DD imagerel ver_sad_sse41+810
	DD	imagerel ver_sad_sse41+829
	DD	imagerel $chain$12$ver_sad_sse41
$pdata$kvz_reg_sad_sse41 DD imagerel $LN127
	DD	imagerel $LN127+1314
	DD	imagerel $unwind$kvz_reg_sad_sse41
$pdata$6$kvz_reg_sad_sse41 DD imagerel $LN127+1314
	DD	imagerel $LN127+1775
	DD	imagerel $chain$6$kvz_reg_sad_sse41
$pdata$7$kvz_reg_sad_sse41 DD imagerel $LN127+1775
	DD	imagerel $LN127+2016
	DD	imagerel $chain$7$kvz_reg_sad_sse41
$pdata$8$kvz_reg_sad_sse41 DD imagerel $LN127+2016
	DD	imagerel $LN127+2112
	DD	imagerel $chain$8$kvz_reg_sad_sse41
$pdata$9$kvz_reg_sad_sse41 DD imagerel $LN127+2112
	DD	imagerel $LN127+2158
	DD	imagerel $chain$9$kvz_reg_sad_sse41
$pdata$hor_sad_sse41_arbitrary DD imagerel hor_sad_sse41_arbitrary
	DD	imagerel hor_sad_sse41_arbitrary+333
	DD	imagerel $unwind$hor_sad_sse41_arbitrary
$pdata$1$hor_sad_sse41_arbitrary DD imagerel hor_sad_sse41_arbitrary+333
	DD	imagerel hor_sad_sse41_arbitrary+524
	DD	imagerel $chain$1$hor_sad_sse41_arbitrary
$pdata$4$hor_sad_sse41_arbitrary DD imagerel hor_sad_sse41_arbitrary+524
	DD	imagerel hor_sad_sse41_arbitrary+1891
	DD	imagerel $chain$4$hor_sad_sse41_arbitrary
$pdata$5$hor_sad_sse41_arbitrary DD imagerel hor_sad_sse41_arbitrary+1891
	DD	imagerel hor_sad_sse41_arbitrary+2440
	DD	imagerel $chain$5$hor_sad_sse41_arbitrary
$pdata$hor_sad_sse41_w16 DD imagerel hor_sad_sse41_w16
	DD	imagerel hor_sad_sse41_w16+55
	DD	imagerel $unwind$hor_sad_sse41_w16
$pdata$1$hor_sad_sse41_w16 DD imagerel hor_sad_sse41_w16+55
	DD	imagerel hor_sad_sse41_w16+160
	DD	imagerel $chain$1$hor_sad_sse41_w16
$pdata$2$hor_sad_sse41_w16 DD imagerel hor_sad_sse41_w16+160
	DD	imagerel hor_sad_sse41_w16+363
	DD	imagerel $chain$2$hor_sad_sse41_w16
$pdata$3$hor_sad_sse41_w16 DD imagerel hor_sad_sse41_w16+363
	DD	imagerel hor_sad_sse41_w16+378
	DD	imagerel $chain$3$hor_sad_sse41_w16
$pdata$4$hor_sad_sse41_w16 DD imagerel hor_sad_sse41_w16+378
	DD	imagerel hor_sad_sse41_w16+455
	DD	imagerel $chain$4$hor_sad_sse41_w16
$pdata$hor_sad_sse41_w8 DD imagerel hor_sad_sse41_w8
	DD	imagerel hor_sad_sse41_w8+41
	DD	imagerel $unwind$hor_sad_sse41_w8
$pdata$1$hor_sad_sse41_w8 DD imagerel hor_sad_sse41_w8+41
	DD	imagerel hor_sad_sse41_w8+164
	DD	imagerel $chain$1$hor_sad_sse41_w8
$pdata$4$hor_sad_sse41_w8 DD imagerel hor_sad_sse41_w8+164
	DD	imagerel hor_sad_sse41_w8+375
	DD	imagerel $chain$4$hor_sad_sse41_w8
$pdata$5$hor_sad_sse41_w8 DD imagerel hor_sad_sse41_w8+375
	DD	imagerel hor_sad_sse41_w8+390
	DD	imagerel $chain$5$hor_sad_sse41_w8
$pdata$6$hor_sad_sse41_w8 DD imagerel hor_sad_sse41_w8+390
	DD	imagerel hor_sad_sse41_w8+470
	DD	imagerel $chain$6$hor_sad_sse41_w8
$pdata$hor_sad_sse41_w4 DD imagerel hor_sad_sse41_w4
	DD	imagerel hor_sad_sse41_w4+61
	DD	imagerel $unwind$hor_sad_sse41_w4
$pdata$0$hor_sad_sse41_w4 DD imagerel hor_sad_sse41_w4+61
	DD	imagerel hor_sad_sse41_w4+176
	DD	imagerel $chain$0$hor_sad_sse41_w4
$pdata$1$hor_sad_sse41_w4 DD imagerel hor_sad_sse41_w4+176
	DD	imagerel hor_sad_sse41_w4+356
	DD	imagerel $chain$1$hor_sad_sse41_w4
$pdata$2$hor_sad_sse41_w4 DD imagerel hor_sad_sse41_w4+356
	DD	imagerel hor_sad_sse41_w4+368
	DD	imagerel $chain$2$hor_sad_sse41_w4
$pdata$3$hor_sad_sse41_w4 DD imagerel hor_sad_sse41_w4+368
	DD	imagerel hor_sad_sse41_w4+462
	DD	imagerel $chain$3$hor_sad_sse41_w4
$pdata$ver_sad_arbitrary DD imagerel ver_sad_arbitrary
	DD	imagerel ver_sad_arbitrary+688
	DD	imagerel $unwind$ver_sad_arbitrary
$pdata$ver_sad_w16 DD imagerel ver_sad_w16
	DD	imagerel ver_sad_w16+210
	DD	imagerel $unwind$ver_sad_w16
$pdata$ver_sad_w8 DD imagerel ver_sad_w8
	DD	imagerel ver_sad_w8+215
	DD	imagerel $unwind$ver_sad_w8
$pdata$ver_sad_w4 DD imagerel ver_sad_w4
	DD	imagerel ver_sad_w4+8
	DD	imagerel $unwind$ver_sad_w4
$pdata$1$ver_sad_w4 DD imagerel ver_sad_w4+8
	DD	imagerel ver_sad_w4+51
	DD	imagerel $chain$1$ver_sad_w4
$pdata$2$ver_sad_w4 DD imagerel ver_sad_w4+51
	DD	imagerel ver_sad_w4+149
	DD	imagerel $chain$2$ver_sad_w4
$pdata$3$ver_sad_w4 DD imagerel ver_sad_w4+149
	DD	imagerel ver_sad_w4+163
	DD	imagerel $chain$3$ver_sad_w4
$pdata$4$ver_sad_w4 DD imagerel ver_sad_w4+163
	DD	imagerel ver_sad_w4+221
	DD	imagerel $chain$4$ver_sad_w4
$pdata$reg_sad_arbitrary DD imagerel reg_sad_arbitrary
	DD	imagerel reg_sad_arbitrary+492
	DD	imagerel $unwind$reg_sad_arbitrary
$pdata$0$reg_sad_arbitrary DD imagerel reg_sad_arbitrary+492
	DD	imagerel reg_sad_arbitrary+715
	DD	imagerel $chain$0$reg_sad_arbitrary
$pdata$1$reg_sad_arbitrary DD imagerel reg_sad_arbitrary+715
	DD	imagerel reg_sad_arbitrary+846
	DD	imagerel $chain$1$reg_sad_arbitrary
$pdata$reg_sad_w24 DD imagerel reg_sad_w24
	DD	imagerel reg_sad_w24+279
	DD	imagerel $unwind$reg_sad_w24
$pdata$reg_sad_w16 DD imagerel reg_sad_w16
	DD	imagerel reg_sad_w16+14
	DD	imagerel $unwind$reg_sad_w16
$pdata$1$reg_sad_w16 DD imagerel reg_sad_w16+14
	DD	imagerel reg_sad_w16+59
	DD	imagerel $chain$1$reg_sad_w16
$pdata$2$reg_sad_w16 DD imagerel reg_sad_w16+59
	DD	imagerel reg_sad_w16+234
	DD	imagerel $chain$2$reg_sad_w16
$pdata$3$reg_sad_w16 DD imagerel reg_sad_w16+234
	DD	imagerel reg_sad_w16+249
	DD	imagerel $chain$3$reg_sad_w16
$pdata$4$reg_sad_w16 DD imagerel reg_sad_w16+249
	DD	imagerel reg_sad_w16+320
	DD	imagerel $chain$4$reg_sad_w16
$pdata$reg_sad_w8 DD imagerel reg_sad_w8
	DD	imagerel reg_sad_w8+12
	DD	imagerel $unwind$reg_sad_w8
$pdata$1$reg_sad_w8 DD imagerel reg_sad_w8+12
	DD	imagerel reg_sad_w8+52
	DD	imagerel $chain$1$reg_sad_w8
$pdata$2$reg_sad_w8 DD imagerel reg_sad_w8+52
	DD	imagerel reg_sad_w8+225
	DD	imagerel $chain$2$reg_sad_w8
$pdata$3$reg_sad_w8 DD imagerel reg_sad_w8+225
	DD	imagerel reg_sad_w8+240
	DD	imagerel $chain$3$reg_sad_w8
$pdata$4$reg_sad_w8 DD imagerel reg_sad_w8+240
	DD	imagerel reg_sad_w8+316
	DD	imagerel $chain$4$reg_sad_w8
$pdata$reg_sad_w4 DD imagerel reg_sad_w4
	DD	imagerel reg_sad_w4+17
	DD	imagerel $unwind$reg_sad_w4
$pdata$1$reg_sad_w4 DD imagerel reg_sad_w4+17
	DD	imagerel reg_sad_w4+65
	DD	imagerel $chain$1$reg_sad_w4
$pdata$2$reg_sad_w4 DD imagerel reg_sad_w4+65
	DD	imagerel reg_sad_w4+224
	DD	imagerel $chain$2$reg_sad_w4
$pdata$3$reg_sad_w4 DD imagerel reg_sad_w4+224
	DD	imagerel reg_sad_w4+239
	DD	imagerel $chain$3$reg_sad_w4
$pdata$4$reg_sad_w4 DD imagerel reg_sad_w4+239
	DD	imagerel reg_sad_w4+319
	DD	imagerel $chain$4$reg_sad_w4
$pdata$kvz_strategy_register_picture_sse41 DD imagerel $LN5
	DD	imagerel $LN5+206
	DD	imagerel $unwind$kvz_strategy_register_picture_sse41
xdata	SEGMENT
$unwind$hor_sad_sse41 DD 031201H
	DD	0400112H
	DD	0300bH
$chain$11$hor_sad_sse41 DD 0188f21H
	DD	014e88fH
	DD	015d882H
	DD	016c879H
	DD	017b85fH
	DD	018a849H
	DD	03af440H
	DD	03be438H
	DD	03cd428H
	DD	03dc420H
	DD	03e7418H
	DD	03f6410H
	DD	0425408H
	DD	imagerel hor_sad_sse41
	DD	imagerel hor_sad_sse41+304
	DD	imagerel $unwind$hor_sad_sse41
$chain$13$hor_sad_sse41 DD 048a21H
	DD	01b788aH
	DD	01c6808H
	DD	imagerel hor_sad_sse41+304
	DD	imagerel hor_sad_sse41+643
	DD	imagerel $chain$11$hor_sad_sse41
$chain$16$hor_sad_sse41 DD 062721H
	DD	013f827H
	DD	0199815H
	DD	01a8809H
	DD	imagerel hor_sad_sse41+643
	DD	imagerel hor_sad_sse41+835
	DD	imagerel $chain$13$hor_sad_sse41
$chain$17$hor_sad_sse41 DD 021H
	DD	imagerel hor_sad_sse41+643
	DD	imagerel hor_sad_sse41+835
	DD	imagerel $chain$13$hor_sad_sse41
$unwind$hor_sad_sse41_w32 DD 0168901H
	DD	0b889H
	DD	01a868H
	DD	029853H
	DD	03884bH
	DD	047844H
	DD	056833H
	DD	0117419H
	DD	0106419H
	DD	0f5419H
	DD	0e3419H
	DD	0e015b219H
$unwind$ver_sad_sse41 DD 010401H
	DD	06204H
$chain$1$ver_sad_sse41 DD 040a21H
	DD	06740aH
	DD	083405H
	DD	imagerel ver_sad_sse41
	DD	imagerel ver_sad_sse41+22
	DD	imagerel $unwind$ver_sad_sse41
$chain$3$ver_sad_sse41 DD 040d21H
	DD	0a640dH
	DD	095405H
	DD	imagerel ver_sad_sse41+22
	DD	imagerel ver_sad_sse41+55
	DD	imagerel $chain$1$ver_sad_sse41
$chain$4$ver_sad_sse41 DD 021H
	DD	imagerel ver_sad_sse41+22
	DD	imagerel ver_sad_sse41+55
	DD	imagerel $chain$1$ver_sad_sse41
$chain$6$ver_sad_sse41 DD 040021H
	DD	067400H
	DD	083400H
	DD	imagerel ver_sad_sse41
	DD	imagerel ver_sad_sse41+22
	DD	imagerel $unwind$ver_sad_sse41
$chain$8$ver_sad_sse41 DD 040021H
	DD	067400H
	DD	083400H
	DD	imagerel ver_sad_sse41
	DD	imagerel ver_sad_sse41+22
	DD	imagerel $unwind$ver_sad_sse41
$chain$10$ver_sad_sse41 DD 040021H
	DD	067400H
	DD	083400H
	DD	imagerel ver_sad_sse41
	DD	imagerel ver_sad_sse41+22
	DD	imagerel $unwind$ver_sad_sse41
$chain$12$ver_sad_sse41 DD 040021H
	DD	067400H
	DD	083400H
	DD	imagerel ver_sad_sse41
	DD	imagerel ver_sad_sse41+22
	DD	imagerel $unwind$ver_sad_sse41
$unwind$kvz_reg_sad_sse41 DD 0e4201H
	DD	0af442H
	DD	0be43dH
	DD	013d438H
	DD	011c430H
	DD	0106428H
	DD	07004b208H
	DD	030025003H
$chain$6$kvz_reg_sad_sse41 DD 040f21H
	DD	02880fH
	DD	037805H
	DD	imagerel $LN127
	DD	imagerel $LN127+1314
	DD	imagerel $unwind$kvz_reg_sad_sse41
$chain$7$kvz_reg_sad_sse41 DD 020521H
	DD	046805H
	DD	imagerel $LN127+1314
	DD	imagerel $LN127+1775
	DD	imagerel $chain$6$kvz_reg_sad_sse41
$chain$8$kvz_reg_sad_sse41 DD 021H
	DD	imagerel $LN127+1314
	DD	imagerel $LN127+1775
	DD	imagerel $chain$6$kvz_reg_sad_sse41
$chain$9$kvz_reg_sad_sse41 DD 021H
	DD	imagerel $LN127
	DD	imagerel $LN127+1314
	DD	imagerel $unwind$kvz_reg_sad_sse41
$unwind$hor_sad_sse41_arbitrary DD 0157701H
	DD	010e877H
	DD	011d86bH
	DD	012c863H
	DD	013b852H
	DD	014a846H
	DD	03a3421H
	DD	0320121H
	DD	0e018f01aH
	DD	0c014d016H
	DD	060117012H
	DD	05010H
$chain$1$hor_sad_sse41_arbitrary DD 048921H
	DD	0177889H
	DD	0186808H
	DD	imagerel hor_sad_sse41_arbitrary
	DD	imagerel hor_sad_sse41_arbitrary+333
	DD	imagerel $unwind$hor_sad_sse41_arbitrary
$chain$4$hor_sad_sse41_arbitrary DD 062721H
	DD	0ff827H
	DD	0159815H
	DD	0168809H
	DD	imagerel hor_sad_sse41_arbitrary+333
	DD	imagerel hor_sad_sse41_arbitrary+524
	DD	imagerel $chain$1$hor_sad_sse41_arbitrary
$chain$5$hor_sad_sse41_arbitrary DD 021H
	DD	imagerel hor_sad_sse41_arbitrary+333
	DD	imagerel hor_sad_sse41_arbitrary+524
	DD	imagerel $chain$1$hor_sad_sse41_arbitrary
$unwind$hor_sad_sse41_w16 DD 060a01H
	DD	08340aH
	DD	07008e00aH
	DD	050066007H
$chain$1$hor_sad_sse41_w16 DD 040d21H
	DD	06d40dH
	DD	05c405H
	DD	imagerel hor_sad_sse41_w16
	DD	imagerel hor_sad_sse41_w16+55
	DD	imagerel $unwind$hor_sad_sse41_w16
$chain$2$hor_sad_sse41_w16 DD 020521H
	DD	07f405H
	DD	imagerel hor_sad_sse41_w16+55
	DD	imagerel hor_sad_sse41_w16+160
	DD	imagerel $chain$1$hor_sad_sse41_w16
$chain$3$hor_sad_sse41_w16 DD 021H
	DD	imagerel hor_sad_sse41_w16+55
	DD	imagerel hor_sad_sse41_w16+160
	DD	imagerel $chain$1$hor_sad_sse41_w16
$chain$4$hor_sad_sse41_w16 DD 021H
	DD	imagerel hor_sad_sse41_w16
	DD	imagerel hor_sad_sse41_w16+55
	DD	imagerel $unwind$hor_sad_sse41_w16
$unwind$hor_sad_sse41_w8 DD 040801H
	DD	070041208H
	DD	030026003H
$chain$1$hor_sad_sse41_w8 DD 041121H
	DD	08e411H
	DD	07c405H
	DD	imagerel hor_sad_sse41_w8
	DD	imagerel hor_sad_sse41_w8+41
	DD	imagerel $unwind$hor_sad_sse41_w8
$chain$4$hor_sad_sse41_w8 DD 061321H
	DD	06813H
	DD	09f40fH
	DD	065405H
	DD	imagerel hor_sad_sse41_w8+41
	DD	imagerel hor_sad_sse41_w8+164
	DD	imagerel $chain$1$hor_sad_sse41_w8
$chain$5$hor_sad_sse41_w8 DD 021H
	DD	imagerel hor_sad_sse41_w8+41
	DD	imagerel hor_sad_sse41_w8+164
	DD	imagerel $chain$1$hor_sad_sse41_w8
$chain$6$hor_sad_sse41_w8 DD 021H
	DD	imagerel hor_sad_sse41_w8
	DD	imagerel hor_sad_sse41_w8+41
	DD	imagerel $unwind$hor_sad_sse41_w8
$unwind$hor_sad_sse41_w4 DD 081001H
	DD	085410H
	DD	073410H
	DD	0e00ef010H
	DD	0600b700cH
$chain$0$hor_sad_sse41_w4 DD 020521H
	DD	05c405H
	DD	imagerel hor_sad_sse41_w4
	DD	imagerel hor_sad_sse41_w4+61
	DD	imagerel $unwind$hor_sad_sse41_w4
$chain$1$hor_sad_sse41_w4 DD 020521H
	DD	06d405H
	DD	imagerel hor_sad_sse41_w4+61
	DD	imagerel hor_sad_sse41_w4+176
	DD	imagerel $chain$0$hor_sad_sse41_w4
$chain$2$hor_sad_sse41_w4 DD 021H
	DD	imagerel hor_sad_sse41_w4+61
	DD	imagerel hor_sad_sse41_w4+176
	DD	imagerel $chain$0$hor_sad_sse41_w4
$chain$3$hor_sad_sse41_w4 DD 021H
	DD	imagerel hor_sad_sse41_w4
	DD	imagerel hor_sad_sse41_w4+61
	DD	imagerel $unwind$hor_sad_sse41_w4
$unwind$ver_sad_arbitrary DD 0e2001H
	DD	06820H
	DD	0b641cH
	DD	09541cH
	DD	08341cH
	DD	0f018121cH
	DD	0d014e016H
	DD	07010c012H
$unwind$ver_sad_w16 DD 020501H
	DD	013405H
$unwind$ver_sad_w8 DD 040a01H
	DD	02740aH
	DD	013405H
$unwind$ver_sad_w4 DD 010201H
	DD	03002H
$chain$1$ver_sad_w4 DD 040d21H
	DD	03640dH
	DD	025405H
	DD	imagerel ver_sad_w4
	DD	imagerel ver_sad_w4+8
	DD	imagerel $unwind$ver_sad_w4
$chain$2$ver_sad_w4 DD 020521H
	DD	047405H
	DD	imagerel ver_sad_w4+8
	DD	imagerel ver_sad_w4+51
	DD	imagerel $chain$1$ver_sad_w4
$chain$3$ver_sad_w4 DD 021H
	DD	imagerel ver_sad_w4+8
	DD	imagerel ver_sad_w4+51
	DD	imagerel $chain$1$ver_sad_w4
$chain$4$ver_sad_w4 DD 021H
	DD	imagerel ver_sad_w4
	DD	imagerel ver_sad_w4+8
	DD	imagerel $unwind$ver_sad_w4
$unwind$reg_sad_arbitrary DD 0e3601H
	DD	018836H
	DD	027830H
	DD	0103414H
	DD	0f0107214H
	DD	0d00ce00eH
	DD	07008c00aH
	DD	050066007H
$chain$0$reg_sad_arbitrary DD 020521H
	DD	036805H
	DD	imagerel reg_sad_arbitrary
	DD	imagerel reg_sad_arbitrary+492
	DD	imagerel $unwind$reg_sad_arbitrary
$chain$1$reg_sad_arbitrary DD 021H
	DD	imagerel reg_sad_arbitrary
	DD	imagerel reg_sad_arbitrary+492
	DD	imagerel $unwind$reg_sad_arbitrary
$unwind$reg_sad_w24 DD 091601H
	DD	057416H
	DD	046416H
	DD	035416H
	DD	023416H
	DD	0e016H
$unwind$reg_sad_w16 DD 050801H
	DD	073408H
	DD	060077008H
	DD	05006H
$chain$1$reg_sad_w16 DD 040d21H
	DD	06f40dH
	DD	05e405H
	DD	imagerel reg_sad_w16
	DD	imagerel reg_sad_w16+14
	DD	imagerel $unwind$reg_sad_w16
$chain$2$reg_sad_w16 DD 020521H
	DD	04d405H
	DD	imagerel reg_sad_w16+14
	DD	imagerel reg_sad_w16+59
	DD	imagerel $chain$1$reg_sad_w16
$chain$3$reg_sad_w16 DD 021H
	DD	imagerel reg_sad_w16+14
	DD	imagerel reg_sad_w16+59
	DD	imagerel $chain$1$reg_sad_w16
$chain$4$reg_sad_w16 DD 021H
	DD	imagerel reg_sad_w16
	DD	imagerel reg_sad_w16+14
	DD	imagerel $unwind$reg_sad_w16
$unwind$reg_sad_w8 DD 030601H
	DD	053406H
	DD	07006H
$chain$1$reg_sad_w8 DD 040d21H
	DD	04e40dH
	DD	036405H
	DD	imagerel reg_sad_w8
	DD	imagerel reg_sad_w8+12
	DD	imagerel $unwind$reg_sad_w8
$chain$2$reg_sad_w8 DD 020521H
	DD	025405H
	DD	imagerel reg_sad_w8+12
	DD	imagerel reg_sad_w8+52
	DD	imagerel $chain$1$reg_sad_w8
$chain$3$reg_sad_w8 DD 021H
	DD	imagerel reg_sad_w8+12
	DD	imagerel reg_sad_w8+52
	DD	imagerel $chain$1$reg_sad_w8
$chain$4$reg_sad_w8 DD 021H
	DD	imagerel reg_sad_w8
	DD	imagerel reg_sad_w8+12
	DD	imagerel $unwind$reg_sad_w8
$unwind$reg_sad_w4 DD 060a01H
	DD	08340aH
	DD	07008e00aH
	DD	050066007H
$chain$1$reg_sad_w4 DD 040d21H
	DD	07f40dH
	DD	05c405H
	DD	imagerel reg_sad_w4
	DD	imagerel reg_sad_w4+17
	DD	imagerel $unwind$reg_sad_w4
$chain$2$reg_sad_w4 DD 020521H
	DD	06d405H
	DD	imagerel reg_sad_w4+17
	DD	imagerel reg_sad_w4+65
	DD	imagerel $chain$1$reg_sad_w4
$chain$3$reg_sad_w4 DD 021H
	DD	imagerel reg_sad_w4+17
	DD	imagerel reg_sad_w4+65
	DD	imagerel $chain$1$reg_sad_w4
$chain$4$reg_sad_w4 DD 021H
	DD	imagerel reg_sad_w4
	DD	imagerel reg_sad_w4+17
	DD	imagerel $unwind$reg_sad_w4
$unwind$kvz_strategy_register_picture_sse41 DD 060f01H
	DD	09640fH
	DD	08340fH
	DD	0700b520fH
	ORG $+3
$SG4294951923 DB 'hor_sad', 00H
$SG4294951924 DB 'sse41', 00H
	ORG $+2
$SG4294951925 DB 'ver_sad', 00H
$SG4294951926 DB 'sse41', 00H
	ORG $+2
$SG4294951927 DB 'get_optimized_sad', 00H
	ORG $+2
$SG4294951928 DB 'sse41', 00H
	ORG $+6
$SG4294951929 DB 'reg_sad', 00H
$SG4294951930 DB 'sse41', 00H
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
_TEXT	SEGMENT
opaque$ = 64
bitdepth$dead$ = 72
kvz_strategy_register_picture_sse41 PROC

; 235  : int kvz_strategy_register_picture_sse41(void* opaque, uint8_t bitdepth) {

$LN5:
	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rsi
	push	rdi
	sub	rsp, 48					; 00000030H

; 236  :   bool success = true;
; 237  : #if COMPILE_INTEL_SSE41
; 238  : #if KVZ_BIT_DEPTH == 8
; 239  :   if (bitdepth == 8){
; 240  :     success &= kvz_strategyselector_register(opaque, "reg_sad", "sse41", 20, &kvz_reg_sad_sse41);

	lea	rax, OFFSET FLAT:kvz_reg_sad_sse41
	mov	r9d, 20
	lea	r8, OFFSET FLAT:$SG4294951930
	mov	QWORD PTR [rsp+32], rax
	lea	rdx, OFFSET FLAT:$SG4294951929
	mov	rsi, rcx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 241  :     success &= kvz_strategyselector_register(opaque, "get_optimized_sad", "sse41", 20, &get_optimized_sad_sse41);

	lea	r8, OFFSET FLAT:$SG4294951928
	lea	rax, OFFSET FLAT:get_optimized_sad_sse41
	mov	r9d, 20
	lea	rdx, OFFSET FLAT:$SG4294951927
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	ebx, 1
	call	kvz_strategyselector_register
	mov	edi, eax

; 242  :     success &= kvz_strategyselector_register(opaque, "ver_sad", "sse41", 20, &ver_sad_sse41);

	lea	r8, OFFSET FLAT:$SG4294951926
	lea	rax, OFFSET FLAT:ver_sad_sse41
	mov	r9d, 20
	lea	rdx, OFFSET FLAT:$SG4294951925
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	edi, ebx
	call	kvz_strategyselector_register
	mov	ebx, eax

; 243  :     success &= kvz_strategyselector_register(opaque, "hor_sad", "sse41", 20, &hor_sad_sse41);

	lea	r8, OFFSET FLAT:$SG4294951924
	lea	rax, OFFSET FLAT:hor_sad_sse41
	mov	r9d, 20
	lea	rdx, OFFSET FLAT:$SG4294951923
	mov	QWORD PTR [rsp+32], rax
	mov	rcx, rsi
	and	ebx, edi
	call	kvz_strategyselector_register

; 244  :   }
; 245  : #endif // KVZ_BIT_DEPTH == 8
; 246  : #endif
; 247  :   return success;
; 248  : }

	mov	rsi, QWORD PTR [rsp+72]
	and	eax, ebx
	mov	rbx, QWORD PTR [rsp+64]
	add	rsp, 48					; 00000030H
	pop	rdi
	ret	0
kvz_strategy_register_picture_sse41 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
data1$ = 8
data2$ = 16
height$ = 24
stride1$ = 32
stride2$ = 40
reg_sad_w0 PROC

; 47   :   return 0;

	xor	eax, eax

; 48   : }

	ret	0
reg_sad_w0 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
data1$ = 40
data2$ = 48
height$ = 56
stride1$ = 64
stride2$ = 72
reg_sad_w4 PROC

; 53   : {

	mov	QWORD PTR [rsp+32], rbx
	push	rbp
	push	rsi
	push	rdi
	push	r14

; 55   :   int32_t y;
; 56   : 
; 57   :   const int32_t height_fourline_groups = height & ~3;
; 58   :   const int32_t height_residual_lines  = height &  3;
; 59   : 
; 60   :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r14d, DWORD PTR stride2$[rsp]
	xor	eax, eax
	mov	QWORD PTR [rsp+40], r12
	mov	edi, r9d
	mov	QWORD PTR [rsp+56], r15
	mov	r12d, r8d
	mov	r15d, r8d
	and	r12d, 3
	and	r15d, -4
	mov	ebx, r8d
	mov	rsi, rdx
	mov	rbp, rcx
	xorps	xmm2, xmm2
	test	r15d, r15d
	jle	$LN3@reg_sad_w4

; 54   :   __m128i sse_inc = _mm_setzero_si128();

	mov	QWORD PTR [rsp+48], r13
	mov	r13d, 2
	npad	4
$LL4@reg_sad_w4:

; 61   :     __m128i a = _mm_cvtsi32_si128(*(uint32_t *)(data1 + y * stride1));
; 62   :     __m128i b = _mm_cvtsi32_si128(*(uint32_t *)(data2 + y * stride2));
; 63   : 
; 64   :     a = _mm_insert_epi32(a, *(const uint32_t *)(data1 + (y + 1) * stride1), 1);

	lea	r11d, DWORD PTR [r13-1]
	mov	ecx, eax
	imul	ecx, edi

; 65   :     b = _mm_insert_epi32(b, *(const uint32_t *)(data2 + (y + 1) * stride2), 1);
; 66   :     a = _mm_insert_epi32(a, *(const uint32_t *)(data1 + (y + 2) * stride1), 2);
; 67   :     b = _mm_insert_epi32(b, *(const uint32_t *)(data2 + (y + 2) * stride2), 2);
; 68   :     a = _mm_insert_epi32(a, *(const uint32_t *)(data1 + (y + 3) * stride1), 3);

	lea	r10d, DWORD PTR [r13+1]
	mov	r8d, r13d
	mov	edx, r11d
	imul	r8d, edi
	mov	r9d, r10d
	imul	edx, edi
	imul	r11d, r14d
	imul	r9d, edi

; 69   :     b = _mm_insert_epi32(b, *(const uint32_t *)(data2 + (y + 3) * stride2), 3);

	imul	r10d, r14d
	movd	xmm1, DWORD PTR [rcx+rbp]
	mov	ecx, eax
	imul	ecx, r14d
	add	eax, 4
	pinsrd	xmm1, DWORD PTR [rdx+rbp], 1
	pinsrd	xmm1, DWORD PTR [r8+rbp], 2
	pinsrd	xmm1, DWORD PTR [r9+rbp], 3
	mov	r8d, r13d
	add	r13d, 4
	imul	r8d, r14d
	movd	xmm0, DWORD PTR [rcx+rsi]
	pinsrd	xmm0, DWORD PTR [r11+rsi], 1
	pinsrd	xmm0, DWORD PTR [r8+rsi], 2
	pinsrd	xmm0, DWORD PTR [r10+rsi], 3

; 70   : 
; 71   :     __m128i curr_sads = _mm_sad_epu8(a, b);

	psadbw	xmm1, xmm0

; 72   :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm1, xmm2
	movdqa	xmm2, xmm1
	cmp	eax, r15d
	jl	$LL4@reg_sad_w4

; 55   :   int32_t y;
; 56   : 
; 57   :   const int32_t height_fourline_groups = height & ~3;
; 58   :   const int32_t height_residual_lines  = height &  3;
; 59   : 
; 60   :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r13, QWORD PTR [rsp+48]
$LN3@reg_sad_w4:
	mov	r15, QWORD PTR [rsp+56]

; 73   :   }
; 74   :   if (height_residual_lines) {

	test	r12d, r12d
	mov	r12, QWORD PTR [rsp+40]
	je	SHORT $LN6@reg_sad_w4

; 75   :     for (; y < height; y++) {

	cmp	eax, ebx
	jge	SHORT $LN6@reg_sad_w4
	npad	13
$LL7@reg_sad_w4:

; 76   :       __m128i a = _mm_cvtsi32_si128(*(const uint32_t *)(data1 + y * stride1));

	mov	ecx, eax
	imul	ecx, edi
	movd	xmm1, DWORD PTR [rcx+rbp]

; 77   :       __m128i b = _mm_cvtsi32_si128(*(const uint32_t *)(data2 + y * stride2));

	mov	ecx, eax
	imul	ecx, r14d
	inc	eax
	movd	xmm0, DWORD PTR [rcx+rsi]

; 78   : 
; 79   :       __m128i curr_sads = _mm_sad_epu8(a, b);

	psadbw	xmm1, xmm0

; 80   :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm1, xmm2
	movdqa	xmm2, xmm1
	cmp	eax, ebx
	jl	SHORT $LL7@reg_sad_w4
$LN6@reg_sad_w4:

; 81   :     }
; 82   :   }
; 83   :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 84   :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 85   : 
; 86   :   return _mm_cvtsi128_si32(sad);
; 87   : }

	mov	rbx, QWORD PTR [rsp+64]
	pshufd	xmm0, xmm2, 78				; 0000004eH
	paddq	xmm0, xmm2
	movd	eax, xmm0
	pop	r14
	pop	rdi
	pop	rsi
	pop	rbp
	ret	0
reg_sad_w4 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
data1$ = 16
data2$ = 24
height$ = 32
stride1$ = 40
stride2$ = 48
reg_sad_w8 PROC

; 92   : {

	mov	QWORD PTR [rsp+32], rbx
	push	rdi

; 94   :   int32_t y;
; 95   : 
; 96   :   const int32_t height_fourline_groups = height & ~3;
; 97   :   const int32_t height_residual_lines  = height &  3;
; 98   : 
; 99   :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	edi, DWORD PTR stride2$[rsp]
	xor	eax, eax
	mov	QWORD PTR [rsp+24], rsi
	mov	r10d, r8d
	mov	QWORD PTR [rsp+32], r14
	mov	esi, r8d
	mov	r14d, r8d
	and	esi, -4
	and	r14d, 3
	mov	rbx, rcx
	xorps	xmm4, xmm4
	test	esi, esi
	jle	$LN3@reg_sad_w8

; 93   :   __m128i sse_inc = _mm_setzero_si128();

	mov	QWORD PTR [rsp+16], rbp
	xorps	xmm5, xmm5
	mov	ebp, 2
	npad	15
$LL4@reg_sad_w8:

; 100  :     __m128d a_d = _mm_setzero_pd();
; 101  :     __m128d b_d = _mm_setzero_pd();
; 102  :     __m128d c_d = _mm_setzero_pd();
; 103  :     __m128d d_d = _mm_setzero_pd();
; 104  : 
; 105  :     a_d = _mm_loadl_pd(a_d, (const double *)(data1 + (y + 0) * stride1));

	mov	ecx, eax

; 106  :     b_d = _mm_loadl_pd(b_d, (const double *)(data2 + (y + 0) * stride2));
; 107  :     a_d = _mm_loadh_pd(a_d, (const double *)(data1 + (y + 1) * stride1));

	lea	r8d, DWORD PTR [rbp-1]
	imul	ecx, r9d
	movaps	xmm3, xmm5
	movaps	xmm2, xmm5

; 108  :     b_d = _mm_loadh_pd(b_d, (const double *)(data2 + (y + 1) * stride2));
; 109  : 
; 110  :     c_d = _mm_loadl_pd(c_d, (const double *)(data1 + (y + 2) * stride1));

	movaps	xmm1, xmm5

; 111  :     d_d = _mm_loadl_pd(d_d, (const double *)(data2 + (y + 2) * stride2));

	movaps	xmm0, xmm5
	movlpd	xmm3, QWORD PTR [rcx+rbx]
	mov	ecx, r8d
	imul	ecx, r9d
	imul	r8d, edi
	movhpd	xmm3, QWORD PTR [rcx+rbx]
	mov	ecx, eax
	movhpd	xmm2, QWORD PTR [r8+rdx]
	add	eax, 4
	imul	ecx, edi

; 112  :     c_d = _mm_loadh_pd(c_d, (const double *)(data1 + (y + 3) * stride1));

	lea	r8d, DWORD PTR [rbp+1]
	movlpd	xmm2, QWORD PTR [rcx+rdx]
	mov	ecx, ebp
	imul	ecx, r9d

; 113  :     d_d = _mm_loadh_pd(d_d, (const double *)(data2 + (y + 3) * stride2));
; 114  : 
; 115  :     __m128i a = _mm_castpd_si128(a_d);
; 116  :     __m128i b = _mm_castpd_si128(b_d);
; 117  :     __m128i c = _mm_castpd_si128(c_d);
; 118  :     __m128i d = _mm_castpd_si128(d_d);
; 119  : 
; 120  :     __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	psadbw	xmm3, xmm2

; 121  :     __m128i curr_sads_cd = _mm_sad_epu8(c, d);
; 122  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm3, xmm4
	movdqa	xmm4, xmm3
	movlpd	xmm1, QWORD PTR [rcx+rbx]
	mov	ecx, r8d
	imul	ecx, r9d
	imul	r8d, edi
	movhpd	xmm1, QWORD PTR [rcx+rbx]
	mov	ecx, ebp
	movhpd	xmm0, QWORD PTR [r8+rdx]
	add	ebp, 4
	imul	ecx, edi
	movlpd	xmm0, QWORD PTR [rcx+rdx]
	psadbw	xmm1, xmm0

; 123  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	paddq	xmm4, xmm1
	cmp	eax, esi
	jl	$LL4@reg_sad_w8

; 94   :   int32_t y;
; 95   : 
; 96   :   const int32_t height_fourline_groups = height & ~3;
; 97   :   const int32_t height_residual_lines  = height &  3;
; 98   : 
; 99   :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	rbp, QWORD PTR [rsp+16]
$LN3@reg_sad_w8:
	mov	rsi, QWORD PTR [rsp+24]

; 124  :   }
; 125  :   if (height_residual_lines) {

	test	r14d, r14d
	mov	r14, QWORD PTR [rsp+32]
	je	SHORT $LN6@reg_sad_w8

; 126  :     for (; y < height; y++) {

	cmp	eax, r10d
	jge	SHORT $LN6@reg_sad_w8
	npad	11
$LL7@reg_sad_w8:

; 127  :       __m128i a = _mm_loadl_epi64((__m128i *)(data1 + y * stride1));

	mov	ecx, eax
	imul	ecx, r9d
	movq	xmm1, QWORD PTR [rcx+rbx]

; 128  :       __m128i b = _mm_loadl_epi64((__m128i *)(data2 + y * stride2));

	mov	ecx, eax
	imul	ecx, edi
	inc	eax
	movq	xmm0, QWORD PTR [rcx+rdx]

; 129  : 
; 130  :       __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	psadbw	xmm1, xmm0

; 131  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm1, xmm4
	movdqa	xmm4, xmm1
	cmp	eax, r10d
	jl	SHORT $LL7@reg_sad_w8
$LN6@reg_sad_w8:

; 132  :     }
; 133  :   }
; 134  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 135  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 136  : 
; 137  :   return _mm_cvtsi128_si32(sad);
; 138  : }

	mov	rbx, QWORD PTR [rsp+40]
	pshufd	xmm0, xmm4, 78				; 0000004eH
	paddq	xmm0, xmm4
	movd	eax, xmm0
	pop	rdi
	ret	0
reg_sad_w8 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
data1$ = 8
data2$ = 16
height$ = 24
stride1$ = 32
stride2$ = 40
reg_sad_w12 PROC

; 144  :   __m128i sse_inc = _mm_setzero_si128();

	xor	r10d, r10d
	xorps	xmm3, xmm3

; 145  :   int32_t y;
; 146  :   for (y = 0; y < height; y++) {

	test	r8d, r8d
	jle	SHORT $LN3@reg_sad_w1
	mov	r11d, DWORD PTR stride2$[rsp]
$LL4@reg_sad_w1:

; 147  :     __m128i a = _mm_loadu_si128((const __m128i *)(data1 + y * stride1));

	mov	eax, r10d
	imul	eax, r9d
	movdqu	xmm2, XMMWORD PTR [rax+rcx]

; 148  :     __m128i b = _mm_loadu_si128((const __m128i *)(data2 + y * stride2));

	mov	eax, r10d
	inc	r10d
	imul	eax, r11d

; 149  : 
; 150  :     __m128i b_masked  = _mm_blend_epi16(a, b, 0x3f);

	movdqa	xmm1, xmm2
	movdqu	xmm0, XMMWORD PTR [rax+rdx]
	pblendw	xmm1, xmm0, 63				; 0000003fH

; 151  :     __m128i curr_sads = _mm_sad_epu8   (a, b_masked);

	psadbw	xmm2, xmm1

; 152  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm2, xmm3
	movdqa	xmm3, xmm2
	cmp	r10d, r8d
	jl	SHORT $LL4@reg_sad_w1
$LN3@reg_sad_w1:

; 153  :   }
; 154  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	pshufd	xmm0, xmm3, 78				; 0000004eH

; 155  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	paddq	xmm0, xmm3

; 156  :   return _mm_cvtsi128_si32(sad);

	movd	eax, xmm0

; 157  : }

	ret	0
reg_sad_w12 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
data1$ = 32
data2$ = 40
height$ = 48
stride1$ = 56
stride2$ = 64
reg_sad_w16 PROC

; 162  : {

	mov	QWORD PTR [rsp+32], rbx
	push	rbp
	push	rsi
	push	rdi

; 164  :   int32_t y;
; 165  : 
; 166  :   const int32_t height_fourline_groups = height & ~3;
; 167  :   const int32_t height_residual_lines  = height &  3;
; 168  : 
; 169  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	ebp, DWORD PTR stride2$[rsp]
	xor	eax, eax
	mov	QWORD PTR [rsp+40], r14
	mov	ebx, r9d
	mov	QWORD PTR [rsp+48], r15
	mov	r14d, r8d
	mov	r15d, r8d
	and	r14d, -4
	and	r15d, 3
	mov	r11d, r8d
	mov	rsi, rcx
	xorps	xmm2, xmm2
	test	r14d, r14d
	jle	$LN3@reg_sad_w1

; 163  :   __m128i sse_inc = _mm_setzero_si128();

	mov	QWORD PTR [rsp+32], r13
	mov	r13d, 2
	npad	10
$LL4@reg_sad_w1:

; 170  :     __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1));
; 171  :     __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2));

	mov	ecx, eax

; 172  :     __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1));

	lea	r9d, DWORD PTR [r13-1]
	imul	ecx, ebp

; 173  :     __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2));
; 174  :     __m128i e = _mm_loadu_si128((const __m128i *)(data1 + (y + 2) * stride1));
; 175  :     __m128i f = _mm_loadu_si128((const __m128i *)(data2 + (y + 2) * stride2));
; 176  :     __m128i g = _mm_loadu_si128((const __m128i *)(data1 + (y + 3) * stride1));

	lea	r10d, DWORD PTR [r13+1]
	mov	r8d, eax
	add	eax, 4
	imul	r8d, ebx

; 177  :     __m128i h = _mm_loadu_si128((const __m128i *)(data2 + (y + 3) * stride2));
; 178  : 
; 179  :     __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	movdqu	xmm0, XMMWORD PTR [rcx+rdx]
	mov	ecx, r9d
	imul	r9d, ebp
	movdqu	xmm1, XMMWORD PTR [r8+rsi]
	mov	r8d, r13d
	imul	ecx, ebx
	psadbw	xmm1, xmm0
	imul	r8d, ebx

; 180  :     __m128i curr_sads_cd = _mm_sad_epu8(c, d);

	movdqu	xmm0, XMMWORD PTR [r9+rdx]

; 181  :     __m128i curr_sads_ef = _mm_sad_epu8(e, f);
; 182  :     __m128i curr_sads_gh = _mm_sad_epu8(g, h);
; 183  : 
; 184  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm1, xmm2
	movdqa	xmm2, xmm1
	movdqu	xmm1, XMMWORD PTR [rcx+rsi]
	mov	ecx, r13d
	add	r13d, 4
	imul	ecx, ebp
	psadbw	xmm1, xmm0

; 185  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	paddq	xmm2, xmm1
	movdqu	xmm1, XMMWORD PTR [r8+rsi]
	movdqu	xmm0, XMMWORD PTR [rcx+rdx]
	mov	ecx, r10d
	imul	r10d, ebp
	psadbw	xmm1, xmm0
	imul	ecx, ebx

; 186  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	paddq	xmm2, xmm1
	movdqu	xmm0, XMMWORD PTR [r10+rdx]
	movdqu	xmm1, XMMWORD PTR [rcx+rsi]
	psadbw	xmm1, xmm0

; 187  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	paddq	xmm2, xmm1
	cmp	eax, r14d
	jl	$LL4@reg_sad_w1

; 164  :   int32_t y;
; 165  : 
; 166  :   const int32_t height_fourline_groups = height & ~3;
; 167  :   const int32_t height_residual_lines  = height &  3;
; 168  : 
; 169  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r13, QWORD PTR [rsp+32]
$LN3@reg_sad_w1:
	mov	r14, QWORD PTR [rsp+40]

; 188  :   }
; 189  :   if (height_residual_lines) {

	test	r15d, r15d
	mov	r15, QWORD PTR [rsp+48]
	je	SHORT $LN6@reg_sad_w1

; 190  :     for (; y < height; y++) {

	cmp	eax, r11d
	jge	SHORT $LN6@reg_sad_w1
	npad	2
$LL7@reg_sad_w1:

; 191  :       __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1));

	mov	r8d, eax

; 192  :       __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2));

	mov	ecx, eax
	imul	r8d, ebx
	inc	eax
	imul	ecx, ebp

; 193  : 
; 194  :       __m128i curr_sads = _mm_sad_epu8(a, b);

	movdqu	xmm1, XMMWORD PTR [r8+rsi]
	movdqu	xmm0, XMMWORD PTR [rcx+rdx]
	psadbw	xmm1, xmm0

; 195  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm1, xmm2
	movdqa	xmm2, xmm1
	cmp	eax, r11d
	jl	SHORT $LL7@reg_sad_w1
$LN6@reg_sad_w1:

; 196  :     }
; 197  :   }
; 198  : 
; 199  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 200  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 201  :   return _mm_cvtsi128_si32(sad);
; 202  : }

	mov	rbx, QWORD PTR [rsp+56]
	pshufd	xmm0, xmm2, 78				; 0000004eH
	paddq	xmm0, xmm2
	movd	eax, xmm0
	pop	rdi
	pop	rsi
	pop	rbp
	ret	0
reg_sad_w16 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
data1$ = 16
data2$ = 24
height$ = 32
stride1$ = 40
stride2$ = 48
reg_sad_w24 PROC

; 207  : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rbp
	mov	QWORD PTR [rsp+24], rsi
	mov	QWORD PTR [rsp+32], rdi
	push	r14

; 208  :   __m128i sse_inc = _mm_setzero_si128();
; 209  :   int32_t y;
; 210  : 
; 211  :   const int32_t height_doublelines = height & ~1;
; 212  :   const int32_t height_parity      = height &  1;
; 213  : 
; 214  :   for (y = 0; y < height_doublelines; y += 2) {

	mov	r14d, DWORD PTR stride2$[rsp]
	mov	r11d, r8d
	mov	ebx, r8d
	and	r11d, 1
	and	ebx, -2
	xor	r10d, r10d
	mov	edi, r9d
	mov	rsi, rdx
	xorps	xmm4, xmm4
	test	ebx, ebx
	jle	$LN3@reg_sad_w2
	xorps	xmm5, xmm5
	npad	1
$LL4@reg_sad_w2:

; 215  :     __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1));
; 216  :     __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2));
; 217  :     __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1));
; 218  :     __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2));
; 219  : 
; 220  :     __m128d e_d = _mm_setzero_pd();
; 221  :     __m128d f_d = _mm_setzero_pd();
; 222  : 
; 223  :     e_d = _mm_loadl_pd(e_d, (const double *)(data1 + (y + 0) * stride1 + 16));
; 224  :     f_d = _mm_loadl_pd(f_d, (const double *)(data2 + (y + 0) * stride2 + 16));
; 225  :     e_d = _mm_loadh_pd(e_d, (const double *)(data1 + (y + 1) * stride1 + 16));

	lea	edx, DWORD PTR [r10+1]
	mov	r8d, r10d
	mov	eax, edx
	imul	r8d, edi
	imul	eax, edi
	movaps	xmm3, xmm5

; 226  :     f_d = _mm_loadh_pd(f_d, (const double *)(data2 + (y + 1) * stride2 + 16));

	imul	edx, r14d
	movaps	xmm2, xmm5
	mov	r9d, eax

; 227  : 
; 228  :     __m128i e = _mm_castpd_si128(e_d);
; 229  :     __m128i f = _mm_castpd_si128(f_d);
; 230  : 
; 231  :     __m128i curr_sads_1 = _mm_sad_epu8(a, b);

	movdqu	xmm1, XMMWORD PTR [r8+rcx]
	movhpd	xmm3, QWORD PTR [rax+rcx+16]
	mov	eax, r10d
	movlpd	xmm3, QWORD PTR [r8+rcx+16]
	add	r10d, 2
	movhpd	xmm2, QWORD PTR [rdx+rsi+16]
	imul	eax, r14d
	movlpd	xmm2, QWORD PTR [rax+rsi+16]

; 232  :     __m128i curr_sads_2 = _mm_sad_epu8(c, d);
; 233  :     __m128i curr_sads_3 = _mm_sad_epu8(e, f);

	psadbw	xmm3, xmm2
	movdqu	xmm0, XMMWORD PTR [rax+rsi]
	psadbw	xmm1, xmm0
	movdqu	xmm0, XMMWORD PTR [rdx+rsi]

; 234  : 
; 235  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_1);

	paddq	xmm1, xmm4
	movdqa	xmm4, xmm1
	movdqu	xmm1, XMMWORD PTR [r9+rcx]
	psadbw	xmm1, xmm0

; 236  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_2);

	paddq	xmm4, xmm1

; 237  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_3);

	paddq	xmm4, xmm3
	cmp	r10d, ebx
	jl	SHORT $LL4@reg_sad_w2
$LN3@reg_sad_w2:

; 238  :   }
; 239  :   if (height_parity) {

	test	r11d, r11d
	je	SHORT $LN5@reg_sad_w2

; 240  :     __m128i a = _mm_loadu_si128   ((const __m128i *)(data1 + y * stride1));
; 241  :     __m128i b = _mm_loadu_si128   ((const __m128i *)(data2 + y * stride2));
; 242  :     __m128i c = _mm_loadl_epi64   ((const __m128i *)(data1 + y * stride1 + 16));

	mov	eax, r10d
	imul	eax, edi

; 243  :     __m128i d = _mm_loadl_epi64   ((const __m128i *)(data2 + y * stride2 + 16));

	imul	r10d, r14d
	movq	xmm2, QWORD PTR [rax+rcx+16]
	movq	xmm0, QWORD PTR [r10+rsi+16]

; 244  : 
; 245  :     __m128i curr_sads_1 = _mm_sad_epu8(a, b);

	movdqu	xmm1, XMMWORD PTR [rax+rcx]

; 246  :     __m128i curr_sads_2 = _mm_sad_epu8(c, d);

	psadbw	xmm2, xmm0
	movdqu	xmm0, XMMWORD PTR [r10+rsi]
	psadbw	xmm1, xmm0

; 247  : 
; 248  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_1);

	paddq	xmm1, xmm4

; 249  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_2);

	movdqa	xmm4, xmm2
	paddq	xmm4, xmm1
$LN5@reg_sad_w2:

; 250  :   }
; 251  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 252  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 253  :   return _mm_cvtsi128_si32(sad);
; 254  : }

	mov	rbx, QWORD PTR [rsp+16]
	mov	rbp, QWORD PTR [rsp+24]
	mov	rsi, QWORD PTR [rsp+32]
	mov	rdi, QWORD PTR [rsp+40]
	pshufd	xmm0, xmm4, 78				; 0000004eH
	paddq	xmm0, xmm4
	movd	eax, xmm0
	pop	r14
	ret	0
reg_sad_w24 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
tv985 = 0
tv1079 = 8
data1$ = 128
data2$ = 136
tv986 = 144
width$ = 144
tv987 = 152
height$ = 152
stride1$ = 160
stride2$ = 168
reg_sad_arbitrary PROC

; 259  : {

	mov	QWORD PTR [rsp+8], rbx
	push	rbp
	push	rsi
	push	rdi
	push	r12
	push	r13
	push	r14
	push	r15
	sub	rsp, 64					; 00000040H

; 260  :   int32_t y, x;
; 261  :   __m128i sse_inc = _mm_setzero_si128();
; 262  :   
; 263  :   // Bytes in block in 128-bit blocks per each scanline, and remainder
; 264  :   const int32_t width_xmms             = width  & ~15;

	mov	esi, DWORD PTR stride2$[rsp]

; 265  :   const int32_t width_residual_pixels  = width  &  15;
; 266  : 
; 267  :   const int32_t height_fourline_groups = height & ~3;

	mov	r13d, r9d
	mov	ebp, DWORD PTR stride1$[rsp]

; 268  :   const int32_t height_residual_lines  = height &  3;

	mov	r10d, r9d

; 269  : 
; 270  :   const __m128i rds    = _mm_set1_epi8 (width_residual_pixels);
; 271  :   const __m128i ns     = _mm_setr_epi8 (0,  1,  2,  3,  4,  5,  6,  7,
; 272  :                                         8,  9,  10, 11, 12, 13, 14, 15);
; 273  :   const __m128i rdmask = _mm_cmpgt_epi8(rds, ns);
; 274  : 
; 275  :   for (x = 0; x < width_xmms; x += 16) {

	xor	r15d, r15d
	movaps	XMMWORD PTR [rsp+32], xmm7
	movaps	XMMWORD PTR [rsp+16], xmm8
	mov	r12d, r9d
	and	r10d, 3
	movsxd	r9, r8d
	mov	eax, r8d
	mov	DWORD PTR tv986[rsp], r10d
	and	eax, 15
	lea	r14d, QWORD PTR [r15+2]
	mov	DWORD PTR tv987[rsp], eax
	and	r13d, -4
	movsx	eax, al
	and	r9, -16
	mov	rbx, rdx
	mov	QWORD PTR tv985[rsp], r9
	mov	rdi, rcx
	xorps	xmm7, xmm7
	movd	xmm8, eax
	mov	eax, r15d
	punpcklbw xmm8, xmm8
	punpcklwd xmm8, xmm8
	pshufd	xmm8, xmm8, 0
	pcmpgtb	xmm8, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	jle	$LN3@reg_sad_ar

; 295  :     }
; 296  :     if (height_residual_lines) {

	lea	rax, QWORD PTR [r9-1]
	mov	r11d, r15d
	shr	rax, 4
	inc	eax
	shl	eax, 4
	mov	QWORD PTR tv1079[rsp], rax
	npad	3
$LL4@reg_sad_ar:

; 276  :     for (y = 0; y < height_fourline_groups; y += 4) {

	mov	eax, r15d
	test	r13d, r13d
	jle	$LN6@reg_sad_ar
	npad	4
$LL7@reg_sad_ar:

; 277  :       __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1 + x));
; 278  :       __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2 + x));

	mov	edx, eax

; 279  :       __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1 + x));

	lea	r9d, DWORD PTR [r14-1]
	imul	edx, esi

; 280  :       __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2 + x));
; 281  :       __m128i e = _mm_loadu_si128((const __m128i *)(data1 + (y + 2) * stride1 + x));
; 282  :       __m128i f = _mm_loadu_si128((const __m128i *)(data2 + (y + 2) * stride2 + x));
; 283  :       __m128i g = _mm_loadu_si128((const __m128i *)(data1 + (y + 3) * stride1 + x));

	lea	r10d, DWORD PTR [r14+1]
	mov	r8d, eax
	add	eax, 4
	imul	r8d, ebp
	add	rdx, r11
	add	r8, r11

; 284  :       __m128i h = _mm_loadu_si128((const __m128i *)(data2 + (y + 3) * stride2 + x));
; 285  : 
; 286  :       __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	movdqu	xmm0, XMMWORD PTR [rdx+rbx]
	mov	edx, r9d
	imul	r9d, esi
	movdqu	xmm1, XMMWORD PTR [r8+rdi]
	mov	r8d, r14d
	imul	edx, ebp
	psadbw	xmm1, xmm0
	imul	r8d, ebp
	mov	ecx, r9d
	add	rcx, r11
	add	rdx, r11

; 287  :       __m128i curr_sads_cd = _mm_sad_epu8(c, d);
; 288  :       __m128i curr_sads_ef = _mm_sad_epu8(e, f);
; 289  :       __m128i curr_sads_gh = _mm_sad_epu8(g, h);
; 290  : 
; 291  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm1, xmm7
	movdqa	xmm7, xmm1
	add	r8, r11
	movdqu	xmm0, XMMWORD PTR [rcx+rbx]
	movdqu	xmm1, XMMWORD PTR [rdx+rdi]
	mov	edx, r14d
	add	r14d, 4
	imul	edx, esi
	psadbw	xmm1, xmm0
	add	rdx, r11

; 292  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	paddq	xmm7, xmm1
	movdqu	xmm1, XMMWORD PTR [r8+rdi]
	movdqu	xmm0, XMMWORD PTR [rdx+rbx]
	mov	edx, r10d
	imul	r10d, esi
	psadbw	xmm1, xmm0
	imul	edx, ebp
	mov	ecx, r10d

; 293  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	paddq	xmm7, xmm1
	add	rcx, r11
	add	rdx, r11
	movdqu	xmm0, XMMWORD PTR [rcx+rbx]
	movdqu	xmm1, XMMWORD PTR [rdx+rdi]
	psadbw	xmm1, xmm0

; 294  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	paddq	xmm7, xmm1
	cmp	eax, r13d
	jl	$LL7@reg_sad_ar
	mov	r9, QWORD PTR tv985[rsp]
	mov	r14d, 2
	mov	r10d, DWORD PTR tv986[rsp]
$LN6@reg_sad_ar:

; 295  :     }
; 296  :     if (height_residual_lines) {

	test	r10d, r10d
	je	SHORT $LN2@reg_sad_ar

; 297  :       for (; y < height; y++) {

	cmp	eax, r12d
	jge	SHORT $LN2@reg_sad_ar
	npad	3
$LL10@reg_sad_ar:

; 298  :         __m128i a = _mm_loadu_si128((const __m128i *)(data1 + y * stride1 + x));

	mov	r8d, eax

; 299  :         __m128i b = _mm_loadu_si128((const __m128i *)(data2 + y * stride2 + x));

	mov	edx, eax
	imul	r8d, ebp
	inc	eax
	imul	edx, esi
	add	r8, r11
	add	rdx, r11

; 300  : 
; 301  :         __m128i curr_sads = _mm_sad_epu8(a, b);

	movdqu	xmm1, XMMWORD PTR [r8+rdi]
	movdqu	xmm0, XMMWORD PTR [rdx+rbx]
	psadbw	xmm1, xmm0

; 302  : 
; 303  :         sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm1, xmm7
	movdqa	xmm7, xmm1
	cmp	eax, r12d
	jl	SHORT $LL10@reg_sad_ar
$LN2@reg_sad_ar:

; 269  : 
; 270  :   const __m128i rds    = _mm_set1_epi8 (width_residual_pixels);
; 271  :   const __m128i ns     = _mm_setr_epi8 (0,  1,  2,  3,  4,  5,  6,  7,
; 272  :                                         8,  9,  10, 11, 12, 13, 14, 15);
; 273  :   const __m128i rdmask = _mm_cmpgt_epi8(rds, ns);
; 274  : 
; 275  :   for (x = 0; x < width_xmms; x += 16) {

	add	r11, 16
	cmp	r11, r9
	jl	$LL4@reg_sad_ar
	mov	rax, QWORD PTR tv1079[rsp]
$LN3@reg_sad_ar:

; 304  :       }
; 305  :     }
; 306  :   }
; 307  : 
; 308  :   if (width_residual_pixels) {

	cmp	DWORD PTR tv987[rsp], r15d
	je	$LN15@reg_sad_ar

; 309  :     for (y = 0; y < height_fourline_groups; y += 4) {

	movsxd	r9, eax
	test	r13d, r13d
	jle	$LN12@reg_sad_ar
	movaps	XMMWORD PTR [rsp+48], xmm6
$LL13@reg_sad_ar:

; 310  :       __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1 + x));

	mov	ecx, r15d

; 311  :       __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2 + x));
; 312  :       __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1 + x));

	lea	edx, DWORD PTR [r14-1]
	imul	ecx, ebp

; 313  :       __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2 + x));
; 314  :       __m128i e = _mm_loadu_si128((const __m128i *)(data1 + (y + 2) * stride1 + x));
; 315  :       __m128i f = _mm_loadu_si128((const __m128i *)(data2 + (y + 2) * stride2 + x));
; 316  :       __m128i g = _mm_loadu_si128((const __m128i *)(data1 + (y + 3) * stride1 + x));

	lea	r8d, DWORD PTR [r14+1]

; 317  :       __m128i h = _mm_loadu_si128((const __m128i *)(data2 + (y + 3) * stride2 + x));
; 318  : 
; 319  :       __m128i b_masked     = _mm_blendv_epi8(a, b, rdmask);

	movdqa	xmm0, xmm8
	add	rcx, r9
	movdqu	xmm3, XMMWORD PTR [rcx+rdi]
	mov	ecx, edx
	imul	edx, esi
	imul	ecx, ebp
	movdqa	xmm2, xmm3
	mov	eax, edx
	add	rcx, r9
	add	rax, r9
	movdqu	xmm4, XMMWORD PTR [rcx+rdi]
	mov	ecx, r14d
	imul	ecx, ebp
	add	rcx, r9
	movdqu	xmm5, XMMWORD PTR [rcx+rdi]
	mov	ecx, r8d
	imul	ecx, ebp
	imul	r8d, esi
	add	rcx, r9
	movdqu	xmm6, XMMWORD PTR [rcx+rdi]
	mov	ecx, r15d
	add	r15d, 4
	imul	ecx, esi
	add	rcx, r9
	movdqu	xmm1, XMMWORD PTR [rcx+rbx]
	mov	ecx, r14d
	add	r14d, 4
	imul	ecx, esi
	pblendvb xmm2, xmm1, xmm0

; 320  :       __m128i d_masked     = _mm_blendv_epi8(c, d, rdmask);

	movdqu	xmm1, XMMWORD PTR [rax+rbx]
	mov	eax, r8d

; 321  :       __m128i f_masked     = _mm_blendv_epi8(e, f, rdmask);
; 322  :       __m128i h_masked     = _mm_blendv_epi8(g, h, rdmask);
; 323  : 
; 324  :       __m128i curr_sads_ab = _mm_sad_epu8   (a, b_masked);

	psadbw	xmm3, xmm2
	movdqa	xmm2, xmm4
	add	rcx, r9
	add	rax, r9
	pblendvb xmm2, xmm1, xmm0

; 325  :       __m128i curr_sads_cd = _mm_sad_epu8   (c, d_masked);

	psadbw	xmm4, xmm2
	movdqa	xmm2, xmm5

; 326  :       __m128i curr_sads_ef = _mm_sad_epu8   (e, f_masked);
; 327  :       __m128i curr_sads_gh = _mm_sad_epu8   (g, h_masked);
; 328  : 
; 329  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm3, xmm7
	movdqa	xmm7, xmm3
	movdqu	xmm1, XMMWORD PTR [rcx+rbx]

; 330  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	paddq	xmm7, xmm4
	pblendvb xmm2, xmm1, xmm0
	psadbw	xmm5, xmm2
	movdqa	xmm2, xmm6
	movdqu	xmm1, XMMWORD PTR [rax+rbx]
	pblendvb xmm2, xmm1, xmm0
	psadbw	xmm6, xmm2

; 331  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	paddq	xmm7, xmm5

; 332  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	paddq	xmm7, xmm6
	cmp	r15d, r13d
	jl	$LL13@reg_sad_ar

; 309  :     for (y = 0; y < height_fourline_groups; y += 4) {

	movaps	xmm6, XMMWORD PTR [rsp+48]
$LN12@reg_sad_ar:

; 333  :     }
; 334  :     if (height_residual_lines) {

	test	r10d, r10d
	je	SHORT $LN15@reg_sad_ar

; 335  :       for (; y < height; y++) {

	cmp	r15d, r12d
	jge	SHORT $LN15@reg_sad_ar
	npad	11
$LL16@reg_sad_ar:

; 336  :         __m128i a = _mm_loadu_si128((const __m128i *)(data1 + y * stride1 + x));

	mov	ecx, r15d

; 337  :         __m128i b = _mm_loadu_si128((const __m128i *)(data2 + y * stride2 + x));
; 338  : 
; 339  :         __m128i b_masked  = _mm_blendv_epi8(a, b, rdmask);

	movdqa	xmm0, xmm8
	imul	ecx, ebp
	add	rcx, r9
	movdqu	xmm3, XMMWORD PTR [rcx+rdi]
	mov	ecx, r15d
	inc	r15d
	imul	ecx, esi
	movdqa	xmm2, xmm3
	add	rcx, r9
	movdqu	xmm1, XMMWORD PTR [rcx+rbx]
	pblendvb xmm2, xmm1, xmm0

; 340  :         __m128i curr_sads = _mm_sad_epu8   (a, b_masked);

	psadbw	xmm3, xmm2

; 341  : 
; 342  :         sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm3, xmm7
	movdqa	xmm7, xmm3
	cmp	r15d, r12d
	jl	SHORT $LL16@reg_sad_ar
$LN15@reg_sad_ar:

; 343  :       }
; 344  :     }
; 345  :   }
; 346  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 347  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 348  : 
; 349  :   return _mm_cvtsi128_si32(sad);
; 350  : }

	mov	rbx, QWORD PTR [rsp+128]
	movaps	xmm8, XMMWORD PTR [rsp+16]
	pshufd	xmm0, xmm7, 78				; 0000004eH
	paddq	xmm0, xmm7
	movaps	xmm7, XMMWORD PTR [rsp+32]
	movd	eax, xmm0
	add	rsp, 64					; 00000040H
	pop	r15
	pop	r14
	pop	r13
	pop	r12
	pop	rdi
	pop	rsi
	pop	rbp
	ret	0
reg_sad_arbitrary ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
pic_data$ = 16
ref_data$ = 24
height$ = 32
stride$ = 40
ver_sad_w4 PROC

; 354  : {

	push	rbx

; 355  :   __m128i ref_row = _mm_set1_epi32(*(const uint32_t *)ref_data);

	movd	xmm2, DWORD PTR [rdx]

; 356  :   __m128i sse_inc = _mm_setzero_si128();
; 357  :   int32_t y;
; 358  : 
; 359  :   const int32_t height_fourline_groups = height & ~3;
; 360  :   const int32_t height_residual_lines  = height &  3;
; 361  : 
; 362  :   for (y = 0; y < height_fourline_groups; y += 4) {

	xor	eax, eax
	mov	QWORD PTR [rsp+16], rbp
	mov	r10d, r9d
	mov	QWORD PTR [rsp+24], rsi
	mov	ebp, r8d
	mov	esi, r8d
	pshufd	xmm2, xmm2, 0
	and	esi, -4
	and	ebp, 3
	mov	ebx, r8d
	mov	r11, rcx
	xorps	xmm1, xmm1
	test	esi, esi
	jle	SHORT $LN3@ver_sad_w4

; 355  :   __m128i ref_row = _mm_set1_epi32(*(const uint32_t *)ref_data);

	mov	QWORD PTR [rsp+32], rdi
	mov	edi, 2
	npad	3
$LL4@ver_sad_w4:

; 363  :     __m128i a = _mm_cvtsi32_si128(*(uint32_t *)(pic_data + y * stride));

	mov	ecx, eax

; 364  : 
; 365  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 1) * stride), 1);

	lea	edx, DWORD PTR [rdi-1]

; 366  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 2) * stride), 2);

	mov	r8d, edi
	imul	ecx, r10d

; 367  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 3) * stride), 3);

	lea	r9d, DWORD PTR [rdi+1]
	imul	edx, r10d
	imul	r8d, r10d
	add	eax, 4
	imul	r9d, r10d
	add	edi, 4
	movd	xmm0, DWORD PTR [rcx+r11]
	pinsrd	xmm0, DWORD PTR [rdx+r11], 1
	pinsrd	xmm0, DWORD PTR [r8+r11], 2
	pinsrd	xmm0, DWORD PTR [r9+r11], 3

; 368  : 
; 369  :     __m128i curr_sads = _mm_sad_epu8(a, ref_row);

	psadbw	xmm0, xmm2

; 370  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm0, xmm1
	movdqa	xmm1, xmm0
	cmp	eax, esi
	jl	SHORT $LL4@ver_sad_w4

; 356  :   __m128i sse_inc = _mm_setzero_si128();
; 357  :   int32_t y;
; 358  : 
; 359  :   const int32_t height_fourline_groups = height & ~3;
; 360  :   const int32_t height_residual_lines  = height &  3;
; 361  : 
; 362  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	rdi, QWORD PTR [rsp+32]
$LN3@ver_sad_w4:
	mov	rsi, QWORD PTR [rsp+24]

; 371  :   }
; 372  :   if (height_residual_lines) {

	test	ebp, ebp
	mov	rbp, QWORD PTR [rsp+16]
	je	SHORT $LN6@ver_sad_w4

; 373  :     // Only pick the last dword, because we're comparing single dwords (lines)
; 374  :     ref_row = _mm_bsrli_si128(ref_row, 12);

	psrldq	xmm2, 12

; 375  : 
; 376  :     for (; y < height; y++) {

	cmp	eax, ebx
	jge	SHORT $LN6@ver_sad_w4
	npad	4
$LL7@ver_sad_w4:

; 377  :       __m128i a = _mm_cvtsi32_si128(*(const uint32_t *)(pic_data + y * stride));

	mov	ecx, eax
	inc	eax
	imul	ecx, r10d
	movd	xmm0, DWORD PTR [rcx+r11]

; 378  : 
; 379  :       __m128i curr_sads = _mm_sad_epu8(a, ref_row);

	psadbw	xmm0, xmm2

; 380  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm0, xmm1
	movdqa	xmm1, xmm0
	cmp	eax, ebx
	jl	SHORT $LL7@ver_sad_w4
$LN6@ver_sad_w4:

; 381  :     }
; 382  :   }
; 383  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	pshufd	xmm0, xmm1, 78				; 0000004eH

; 384  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	paddq	xmm0, xmm1

; 385  : 
; 386  :   return _mm_cvtsi128_si32(sad);

	movd	eax, xmm0

; 387  : }

	pop	rbx
	ret	0
ver_sad_w4 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
pic_data$ = 8
ref_data$ = 16
height$ = 24
stride$ = 32
ver_sad_w8 PROC

; 391  : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rdi

; 392  :   const __m128i ref_row = _mm_set1_epi64x(*(const uint64_t *)ref_data);

	movq	xmm3, QWORD PTR [rdx]

; 393  :   __m128i sse_inc = _mm_setzero_si128();
; 394  :   int32_t y;
; 395  : 
; 396  :   const int32_t height_fourline_groups = height & ~3;

	mov	ebx, r8d

; 397  :   const int32_t height_residual_lines  = height &  3;

	mov	edi, r8d
	punpcklqdq xmm3, xmm3
	and	ebx, -4
	and	edi, 3

; 398  : 
; 399  :   for (y = 0; y < height_fourline_groups; y += 4) {

	xor	eax, eax
	mov	r10, rcx
	xorps	xmm2, xmm2
	test	ebx, ebx
	jle	SHORT $LN3@ver_sad_w8

; 392  :   const __m128i ref_row = _mm_set1_epi64x(*(const uint64_t *)ref_data);

	xorps	xmm4, xmm4
	lea	r11d, QWORD PTR [rax+2]
$LL4@ver_sad_w8:

; 400  :     __m128d a_d = _mm_setzero_pd();
; 401  :     __m128d c_d = _mm_setzero_pd();
; 402  : 
; 403  :     a_d = _mm_loadl_pd(a_d, (const double *)(pic_data + (y + 0) * stride));

	mov	ecx, eax
	movaps	xmm1, xmm4
	imul	ecx, r9d

; 404  :     a_d = _mm_loadh_pd(a_d, (const double *)(pic_data + (y + 1) * stride));
; 405  : 
; 406  :     c_d = _mm_loadl_pd(c_d, (const double *)(pic_data + (y + 2) * stride));

	movaps	xmm0, xmm4
	add	eax, 4
	movlpd	xmm1, QWORD PTR [rcx+r10]
	lea	ecx, DWORD PTR [r11-1]
	imul	ecx, r9d
	movhpd	xmm1, QWORD PTR [rcx+r10]
	mov	ecx, r11d
	imul	ecx, r9d

; 407  :     c_d = _mm_loadh_pd(c_d, (const double *)(pic_data + (y + 3) * stride));
; 408  : 
; 409  :     __m128i a = _mm_castpd_si128(a_d);
; 410  :     __m128i c = _mm_castpd_si128(c_d);
; 411  : 
; 412  :     __m128i curr_sads_ab = _mm_sad_epu8(a, ref_row);

	psadbw	xmm1, xmm3

; 413  :     __m128i curr_sads_cd = _mm_sad_epu8(c, ref_row);
; 414  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm1, xmm2
	movdqa	xmm2, xmm1
	movlpd	xmm0, QWORD PTR [rcx+r10]
	lea	ecx, DWORD PTR [r11+1]
	imul	ecx, r9d
	add	r11d, 4
	movhpd	xmm0, QWORD PTR [rcx+r10]
	psadbw	xmm0, xmm3

; 415  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	paddq	xmm2, xmm0
	cmp	eax, ebx
	jl	SHORT $LL4@ver_sad_w8
$LN3@ver_sad_w8:

; 416  :   }
; 417  :   if (height_residual_lines) {

	test	edi, edi
	je	SHORT $LN6@ver_sad_w8

; 418  :     __m128i b = _mm_move_epi64(ref_row);

	movq	xmm1, xmm3

; 419  : 
; 420  :     for (; y < height; y++) {

	cmp	eax, r8d
	jge	SHORT $LN6@ver_sad_w8
	npad	8
$LL7@ver_sad_w8:

; 421  :       __m128i a = _mm_loadl_epi64((__m128i *)(pic_data + y * stride));

	mov	ecx, eax
	inc	eax
	imul	ecx, r9d
	movq	xmm0, QWORD PTR [rcx+r10]

; 422  : 
; 423  :       __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	psadbw	xmm0, xmm1

; 424  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm0, xmm2
	movdqa	xmm2, xmm0
	cmp	eax, r8d
	jl	SHORT $LL7@ver_sad_w8
$LN6@ver_sad_w8:

; 425  :     }
; 426  :   }
; 427  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 428  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 429  : 
; 430  :   return _mm_cvtsi128_si32(sad);
; 431  : }

	mov	rbx, QWORD PTR [rsp+8]
	mov	rdi, QWORD PTR [rsp+16]
	pshufd	xmm0, xmm2, 78				; 0000004eH
	paddq	xmm0, xmm2
	movd	eax, xmm0
	ret	0
ver_sad_w8 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
pic_data$ = 8
ref_data$ = 16
height$ = 24
stride$ = 32
ver_sad_w12 PROC

; 436  :   const __m128i ref_row = _mm_loadu_si128((__m128i *)ref_data);

	movdqu	xmm4, XMMWORD PTR [rdx]
	xor	edx, edx
	xorps	xmm3, xmm3

; 437  :   __m128i sse_inc = _mm_setzero_si128();
; 438  :   int32_t y;
; 439  : 
; 440  :   for (y = 0; y < height; y++) {

	test	r8d, r8d
	jle	SHORT $LN3@ver_sad_w1
	npad	2
$LL4@ver_sad_w1:

; 441  :     __m128i a = _mm_loadu_si128((const __m128i *)(pic_data + y * stride));

	mov	eax, edx

; 442  : 
; 443  :     __m128i a_masked  = _mm_blend_epi16(ref_row, a, 0x3f);

	movdqa	xmm1, xmm4
	imul	eax, r9d

; 444  :     __m128i curr_sads = _mm_sad_epu8   (ref_row, a_masked);

	movdqa	xmm2, xmm4
	inc	edx
	movdqu	xmm0, XMMWORD PTR [rax+rcx]
	pblendw	xmm1, xmm0, 63				; 0000003fH
	psadbw	xmm2, xmm1

; 445  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm2, xmm3
	movdqa	xmm3, xmm2
	cmp	edx, r8d
	jl	SHORT $LL4@ver_sad_w1
$LN3@ver_sad_w1:

; 446  :   }
; 447  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	pshufd	xmm0, xmm3, 78				; 0000004eH

; 448  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	paddq	xmm0, xmm3

; 449  :   return _mm_cvtsi128_si32(sad);

	movd	eax, xmm0

; 450  : }

	ret	0
ver_sad_w12 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
pic_data$ = 8
ref_data$ = 16
height$ = 24
stride$ = 32
ver_sad_w16 PROC

; 454  : {

	mov	QWORD PTR [rsp+8], rbx

; 455  :   const __m128i ref_row = _mm_loadu_si128((__m128i *)ref_data);

	movdqu	xmm2, XMMWORD PTR [rdx]

; 456  :   __m128i sse_inc       = _mm_setzero_si128();
; 457  :   int32_t y;
; 458  : 
; 459  :   const int32_t height_fourline_groups = height & ~3;

	mov	edx, r8d

; 460  :   const int32_t height_residual_lines  = height &  3;

	mov	ebx, r8d
	and	edx, -4
	and	ebx, 3

; 461  : 
; 462  :   for (y = 0; y < height_fourline_groups; y += 4) {

	xor	eax, eax
	mov	r10, rcx
	xorps	xmm1, xmm1
	test	edx, edx
	jle	SHORT $LN3@ver_sad_w1

; 455  :   const __m128i ref_row = _mm_loadu_si128((__m128i *)ref_data);

	lea	r11d, QWORD PTR [rax+2]
	npad	11
$LL4@ver_sad_w1:

; 463  :     __m128i pic_row_1   = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * stride));

	mov	ecx, eax
	add	eax, 4
	imul	ecx, r9d

; 464  :     __m128i pic_row_2   = _mm_loadu_si128((__m128i *)(pic_data + (y + 1) * stride));
; 465  :     __m128i pic_row_3   = _mm_loadu_si128((__m128i *)(pic_data + (y + 2) * stride));
; 466  :     __m128i pic_row_4   = _mm_loadu_si128((__m128i *)(pic_data + (y + 3) * stride));
; 467  : 
; 468  :     __m128i curr_sads_1 = _mm_sad_epu8   (pic_row_1, ref_row);

	movdqu	xmm0, XMMWORD PTR [rcx+r10]
	lea	ecx, DWORD PTR [r11-1]
	imul	ecx, r9d
	psadbw	xmm0, xmm2

; 469  :     __m128i curr_sads_2 = _mm_sad_epu8   (pic_row_2, ref_row);
; 470  :     __m128i curr_sads_3 = _mm_sad_epu8   (pic_row_3, ref_row);
; 471  :     __m128i curr_sads_4 = _mm_sad_epu8   (pic_row_4, ref_row);
; 472  : 
; 473  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_1);

	paddq	xmm0, xmm1
	movdqa	xmm1, xmm0
	movdqu	xmm0, XMMWORD PTR [rcx+r10]
	mov	ecx, r11d
	imul	ecx, r9d
	psadbw	xmm0, xmm2

; 474  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_2);

	paddq	xmm1, xmm0
	movdqu	xmm0, XMMWORD PTR [rcx+r10]
	lea	ecx, DWORD PTR [r11+1]
	add	r11d, 4
	imul	ecx, r9d
	psadbw	xmm0, xmm2

; 475  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_3);

	paddq	xmm1, xmm0
	movdqu	xmm0, XMMWORD PTR [rcx+r10]
	psadbw	xmm0, xmm2

; 476  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_4);

	paddq	xmm1, xmm0
	cmp	eax, edx
	jl	SHORT $LL4@ver_sad_w1
$LN3@ver_sad_w1:

; 477  :   }
; 478  :   if (height_residual_lines) {

	test	ebx, ebx
	je	SHORT $LN6@ver_sad_w1

; 479  :     for (; y < height; y++) {

	cmp	eax, r8d
	jge	SHORT $LN6@ver_sad_w1
	npad	3
$LL7@ver_sad_w1:

; 480  :       __m128i pic_row   = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * stride));

	mov	ecx, eax
	inc	eax
	imul	ecx, r9d

; 481  :       __m128i curr_sads = _mm_sad_epu8   (pic_row, ref_row);

	movdqu	xmm0, XMMWORD PTR [rcx+r10]
	psadbw	xmm0, xmm2

; 482  : 
; 483  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm0, xmm1
	movdqa	xmm1, xmm0
	cmp	eax, r8d
	jl	SHORT $LL7@ver_sad_w1
$LN6@ver_sad_w1:

; 484  :     }
; 485  :   }
; 486  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 487  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 488  : 
; 489  :   return _mm_cvtsi128_si32(sad);
; 490  : }

	mov	rbx, QWORD PTR [rsp+8]
	pshufd	xmm0, xmm1, 78				; 0000004eH
	paddq	xmm0, xmm1
	movd	eax, xmm0
	ret	0
ver_sad_w16 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
pic_data$ = 64
ref_data$ = 72
tv799 = 80
width$ = 80
height$ = 88
stride$ = 96
ver_sad_arbitrary PROC

; 494  : {

	mov	QWORD PTR [rsp+8], rbx
	mov	QWORD PTR [rsp+16], rbp
	mov	QWORD PTR [rsp+32], rsi
	push	rdi
	push	r12
	push	r13
	push	r14
	push	r15
	sub	rsp, 16
	movaps	XMMWORD PTR [rsp], xmm6

; 495  :   int32_t y, x;
; 496  :   __m128i sse_inc = _mm_setzero_si128();
; 497  : 
; 498  :   // Bytes in block in 128-bit blocks per each scanline, and remainder
; 499  :   const int32_t width_xmms             = width  & ~15;
; 500  :   const int32_t width_residual_pixels  = width  &  15;
; 501  : 
; 502  :   const int32_t height_fourline_groups = height & ~3;
; 503  :   const int32_t height_residual_lines  = height &  3;
; 504  : 
; 505  :   const __m128i rds    = _mm_set1_epi8 (width_residual_pixels);
; 506  :   const __m128i ns     = _mm_setr_epi8 (0,  1,  2,  3,  4,  5,  6,  7,
; 507  :                                         8,  9,  10, 11, 12, 13, 14, 15);
; 508  :   const __m128i rdmask = _mm_cmpgt_epi8(rds, ns);
; 509  : 
; 510  :   for (x = 0; x < width_xmms; x += 16) {

	xor	edi, edi
	mov	esi, r9d
	movsxd	r14, r8d
	mov	r12d, r9d
	mov	eax, r8d
	and	eax, 15
	and	esi, -4
	mov	DWORD PTR tv799[rsp], eax
	lea	r15d, QWORD PTR [rdi+2]
	movsx	eax, al
	and	r12d, 3
	mov	ebx, r9d
	mov	r9d, DWORD PTR stride$[rsp]
	mov	r13, rdx
	mov	r10, rcx
	xorps	xmm4, xmm4
	mov	ebp, edi
	movd	xmm6, eax
	punpcklbw xmm6, xmm6
	punpcklwd xmm6, xmm6
	pshufd	xmm6, xmm6, 0
	pcmpgtb	xmm6, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	and	r14, -16
	jle	$LN3@ver_sad_ar

; 527  :     }
; 528  :     if (height_residual_lines) {

	lea	rbp, QWORD PTR [r14-1]
	mov	r8d, edi
	shr	rbp, 4
	inc	ebp
	shl	ebp, 4
	npad	7
$LL4@ver_sad_ar:

; 511  :     const __m128i ref_row = _mm_loadu_si128((__m128i *)(ref_data + x));

	mov	eax, edi
	movdqu	xmm2, XMMWORD PTR [r8+r13]

; 512  :     for (y = 0; y < height_fourline_groups; y += 4) {

	test	esi, esi
	jle	$LN6@ver_sad_ar

; 511  :     const __m128i ref_row = _mm_loadu_si128((__m128i *)(ref_data + x));

	mov	r11d, r15d
	npad	13
$LL7@ver_sad_ar:

; 513  :       __m128i a = _mm_loadu_si128((const __m128i *)(pic_data + (y + 0) * stride + x));

	mov	edx, eax

; 514  :       __m128i c = _mm_loadu_si128((const __m128i *)(pic_data + (y + 1) * stride + x));
; 515  :       __m128i e = _mm_loadu_si128((const __m128i *)(pic_data + (y + 2) * stride + x));
; 516  :       __m128i g = _mm_loadu_si128((const __m128i *)(pic_data + (y + 3) * stride + x));
; 517  : 
; 518  :       __m128i curr_sads_ab = _mm_sad_epu8(ref_row, a);

	movdqa	xmm1, xmm2
	imul	edx, r9d
	add	eax, 4
	add	rdx, r8
	movdqu	xmm0, XMMWORD PTR [rdx+rcx]
	lea	edx, DWORD PTR [r11-1]
	imul	edx, r9d
	psadbw	xmm1, xmm0
	add	rdx, r8

; 519  :       __m128i curr_sads_cd = _mm_sad_epu8(ref_row, c);
; 520  :       __m128i curr_sads_ef = _mm_sad_epu8(ref_row, e);
; 521  :       __m128i curr_sads_gh = _mm_sad_epu8(ref_row, g);
; 522  : 
; 523  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm1, xmm4
	movdqa	xmm4, xmm1
	movdqa	xmm1, xmm2
	movdqu	xmm0, XMMWORD PTR [rdx+rcx]
	mov	edx, r11d
	imul	edx, r9d
	psadbw	xmm1, xmm0
	add	rdx, r8

; 524  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	paddq	xmm4, xmm1
	movdqa	xmm1, xmm2
	movdqu	xmm0, XMMWORD PTR [rdx+rcx]
	lea	edx, DWORD PTR [r11+1]
	add	r11d, 4
	imul	edx, r9d
	psadbw	xmm1, xmm0
	add	rdx, r8

; 525  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	paddq	xmm4, xmm1
	movdqa	xmm1, xmm2
	movdqu	xmm0, XMMWORD PTR [rdx+rcx]
	psadbw	xmm1, xmm0

; 526  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	paddq	xmm4, xmm1
	cmp	eax, esi
	jl	SHORT $LL7@ver_sad_ar
$LN6@ver_sad_ar:

; 527  :     }
; 528  :     if (height_residual_lines) {

	test	r12d, r12d
	je	SHORT $LN2@ver_sad_ar

; 529  :       for (; y < height; y++) {

	cmp	eax, ebx
	jge	SHORT $LN2@ver_sad_ar
	npad	11
$LL10@ver_sad_ar:

; 530  :         __m128i a = _mm_loadu_si128((const __m128i *)(pic_data + y * stride + x));

	mov	edx, eax
	inc	eax
	imul	edx, r9d
	add	rdx, r8

; 531  : 
; 532  :         __m128i curr_sads = _mm_sad_epu8(a, ref_row);

	movdqu	xmm0, XMMWORD PTR [rdx+rcx]
	psadbw	xmm0, xmm2

; 533  : 
; 534  :         sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm0, xmm4
	movdqa	xmm4, xmm0
	cmp	eax, ebx
	jl	SHORT $LL10@ver_sad_ar
$LN2@ver_sad_ar:

; 495  :   int32_t y, x;
; 496  :   __m128i sse_inc = _mm_setzero_si128();
; 497  : 
; 498  :   // Bytes in block in 128-bit blocks per each scanline, and remainder
; 499  :   const int32_t width_xmms             = width  & ~15;
; 500  :   const int32_t width_residual_pixels  = width  &  15;
; 501  : 
; 502  :   const int32_t height_fourline_groups = height & ~3;
; 503  :   const int32_t height_residual_lines  = height &  3;
; 504  : 
; 505  :   const __m128i rds    = _mm_set1_epi8 (width_residual_pixels);
; 506  :   const __m128i ns     = _mm_setr_epi8 (0,  1,  2,  3,  4,  5,  6,  7,
; 507  :                                         8,  9,  10, 11, 12, 13, 14, 15);
; 508  :   const __m128i rdmask = _mm_cmpgt_epi8(rds, ns);
; 509  : 
; 510  :   for (x = 0; x < width_xmms; x += 16) {

	add	r8, 16
	cmp	r8, r14
	jl	$LL4@ver_sad_ar
$LN3@ver_sad_ar:

; 535  :       }
; 536  :     }
; 537  :   }
; 538  : 
; 539  :   if (width_residual_pixels) {

	cmp	DWORD PTR tv799[rsp], edi
	je	$LN15@ver_sad_ar

; 540  :     const __m128i ref_row = _mm_loadu_si128((__m128i *)(ref_data + x));

	movsxd	rdx, ebp
	movdqu	xmm5, XMMWORD PTR [rdx+r13]

; 541  :     for (y = 0; y < height_fourline_groups; y += 4) {

	test	esi, esi
	jle	$LN12@ver_sad_ar
	npad	8
$LL13@ver_sad_ar:

; 542  :       __m128i a = _mm_loadu_si128((const __m128i *)(pic_data + (y + 0) * stride + x));
; 543  :       __m128i c = _mm_loadu_si128((const __m128i *)(pic_data + (y + 1) * stride + x));
; 544  :       __m128i e = _mm_loadu_si128((const __m128i *)(pic_data + (y + 2) * stride + x));
; 545  :       __m128i g = _mm_loadu_si128((const __m128i *)(pic_data + (y + 3) * stride + x));
; 546  : 
; 547  :       __m128i a_masked     = _mm_blendv_epi8(ref_row, a, rdmask);

	movdqa	xmm0, xmm6
	mov	ecx, edi
	imul	ecx, r9d
	movdqa	xmm2, xmm5

; 548  :       __m128i c_masked     = _mm_blendv_epi8(ref_row, c, rdmask);
; 549  :       __m128i e_masked     = _mm_blendv_epi8(ref_row, e, rdmask);
; 550  :       __m128i g_masked     = _mm_blendv_epi8(ref_row, g, rdmask);
; 551  : 
; 552  :       __m128i curr_sads_ab = _mm_sad_epu8   (ref_row, a_masked);

	movdqa	xmm3, xmm5
	add	edi, 4
	add	rcx, rdx
	movdqu	xmm1, XMMWORD PTR [rcx+r10]
	lea	ecx, DWORD PTR [r15-1]
	imul	ecx, r9d
	pblendvb xmm2, xmm1, xmm0
	psadbw	xmm3, xmm2
	movdqa	xmm2, xmm5
	add	rcx, rdx

; 553  :       __m128i curr_sads_cd = _mm_sad_epu8   (ref_row, c_masked);
; 554  :       __m128i curr_sads_ef = _mm_sad_epu8   (ref_row, e_masked);
; 555  :       __m128i curr_sads_gh = _mm_sad_epu8   (ref_row, g_masked);
; 556  : 
; 557  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm3, xmm4
	movdqa	xmm4, xmm3
	movdqa	xmm3, xmm5
	movdqu	xmm1, XMMWORD PTR [rcx+r10]
	mov	ecx, r15d
	imul	ecx, r9d
	pblendvb xmm2, xmm1, xmm0
	psadbw	xmm3, xmm2
	movdqa	xmm2, xmm5
	add	rcx, rdx

; 558  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	paddq	xmm4, xmm3
	movdqa	xmm3, xmm5
	movdqu	xmm1, XMMWORD PTR [rcx+r10]
	lea	ecx, DWORD PTR [r15+1]
	add	r15d, 4
	imul	ecx, r9d
	pblendvb xmm2, xmm1, xmm0
	psadbw	xmm3, xmm2
	movdqa	xmm2, xmm5
	add	rcx, rdx

; 559  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	paddq	xmm4, xmm3
	movdqa	xmm3, xmm5
	movdqu	xmm1, XMMWORD PTR [rcx+r10]
	pblendvb xmm2, xmm1, xmm0
	psadbw	xmm3, xmm2

; 560  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	paddq	xmm4, xmm3
	cmp	edi, esi
	jl	$LL13@ver_sad_ar
$LN12@ver_sad_ar:

; 561  :     }
; 562  :     if (height_residual_lines) {

	test	r12d, r12d
	je	SHORT $LN15@ver_sad_ar

; 563  :       for (; y < height; y++) {

	cmp	edi, ebx
	jge	SHORT $LN15@ver_sad_ar
	npad	11
$LL16@ver_sad_ar:

; 564  :         __m128i a = _mm_loadu_si128((const __m128i *)(pic_data + y * stride + x));

	mov	ecx, edi

; 565  : 
; 566  :         __m128i a_masked  = _mm_blendv_epi8(ref_row, a, rdmask);

	movdqa	xmm0, xmm6
	imul	ecx, r9d
	movdqa	xmm2, xmm5

; 567  :         __m128i curr_sads = _mm_sad_epu8   (ref_row, a_masked);

	movdqa	xmm3, xmm5
	inc	edi
	add	rcx, rdx
	movdqu	xmm1, XMMWORD PTR [rcx+r10]
	pblendvb xmm2, xmm1, xmm0
	psadbw	xmm3, xmm2

; 568  : 
; 569  :         sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm3, xmm4
	movdqa	xmm4, xmm3
	cmp	edi, ebx
	jl	SHORT $LL16@ver_sad_ar
$LN15@ver_sad_ar:

; 570  :       }
; 571  :     }
; 572  :   }
; 573  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 574  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 575  : 
; 576  :   return _mm_cvtsi128_si32(sad);
; 577  : }

	mov	rbx, QWORD PTR [rsp+64]
	mov	rbp, QWORD PTR [rsp+72]
	mov	rsi, QWORD PTR [rsp+88]
	movaps	xmm6, XMMWORD PTR [rsp]
	pshufd	xmm0, xmm4, 78				; 0000004eH
	paddq	xmm0, xmm4
	movd	eax, xmm0
	add	rsp, 16
	pop	r15
	pop	r14
	pop	r13
	pop	r12
	pop	rdi
	ret	0
ver_sad_arbitrary ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
pic_data$ = 40
ref_data$ = 48
height$ = 56
pic_stride$ = 64
ref_stride$ = 72
left$ = 80
tv624 = 88
right$ = 88
hor_sad_sse41_w4 PROC

; 582  : {

	mov	QWORD PTR [rsp+24], rbx
	mov	QWORD PTR [rsp+32], rbp
	push	rsi
	push	rdi
	push	r14
	push	r15

; 584  :   const int32_t border_idx       = left ? left : right_border_idx;
; 585  : 
; 586  :   const __m128i ns               = _mm_setr_epi8(0,  1,  2,  3,  4,  5,  6,  7,

	movdqa	xmm3, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	mov	r10d, 3
	sub	r10d, DWORD PTR right$[rsp]
	mov	rbp, rcx

; 587  :                                                  8,  9,  10, 11, 12, 13, 14, 15);
; 588  : 
; 589  :   const int32_t border_idx_negative = border_idx >> 31;
; 590  :   const int32_t leftoff             = border_idx_negative | left;
; 591  : 
; 592  :   // Dualword (ie. line) base indexes, ie. the edges the lines read will be
; 593  :   // clamped towards
; 594  :   const __m128i dwbaseids   = _mm_setr_epi8(0, 0, 0, 0, 4, 4, 4, 4,
; 595  :                                             8, 8, 8, 8, 12, 12, 12, 12);
; 596  : 
; 597  :   __m128i right_border_idxs = _mm_set1_epi8((int8_t)right_border_idx);
; 598  :   __m128i left_128          = _mm_set1_epi8((int8_t)left);

	movsxd	rcx, DWORD PTR left$[rsp]
	mov	edi, r9d

; 599  : 
; 600  :   right_border_idxs         = _mm_add_epi8 (right_border_idxs, dwbaseids);
; 601  : 
; 602  :   __m128i mask_right        = _mm_min_epi8 (ns,         right_border_idxs);
; 603  :   __m128i mask1             = _mm_sub_epi8 (mask_right, left_128);
; 604  : 
; 605  :   const __m128i epol_mask   = _mm_max_epi8(mask1, dwbaseids);
; 606  : 
; 607  :   const int32_t height_fourline_groups = height & ~3;
; 608  :   const int32_t height_residual_lines  = height &  3;
; 609  : 
; 610  :   __m128i sse_inc = _mm_setzero_si128();
; 611  :   int32_t y;
; 612  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r14d, DWORD PTR ref_stride$[rsp]
	mov	r15d, r8d
	movsx	eax, r10b
	mov	rsi, rdx
	mov	QWORD PTR [rsp+40], r12
	xorps	xmm2, xmm2
	mov	r12d, r8d
	and	r12d, -4
	movd	xmm0, eax
	punpcklbw xmm0, xmm0
	punpcklwd xmm0, xmm0
	pshufd	xmm0, xmm0, 0
	paddb	xmm0, XMMWORD PTR __xmm@0c0c0c0c080808080404040400000000
	pminsb	xmm3, xmm0
	movsx	eax, cl
	movd	xmm0, eax
	mov	eax, r8d
	and	eax, 3
	punpcklbw xmm0, xmm0
	mov	DWORD PTR tv624[rsp], eax
	xor	eax, eax
	test	ecx, ecx
	punpcklwd xmm0, xmm0
	pshufd	xmm0, xmm0, 0
	cmovne	r10d, ecx
	psubb	xmm3, xmm0
	pmaxsb	xmm3, XMMWORD PTR __xmm@0c0c0c0c080808080404040400000000
	movsxd	rbx, r10d
	sar	rbx, 31
	or	rbx, rcx
	test	r12d, r12d
	jle	$LN3@hor_sad_ss

; 583  :   const int32_t right_border_idx = 3 - right;

	mov	QWORD PTR [rsp+48], r13
	mov	r13d, 2
	npad	5
$LL4@hor_sad_ss:

; 613  :     __m128i a = _mm_cvtsi32_si128(*(const uint32_t *)(pic_data + y * pic_stride));
; 614  :     __m128i b = _mm_cvtsi32_si128(*(const uint32_t *)(ref_data + y * ref_stride + leftoff));
; 615  : 
; 616  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 1) * pic_stride),           1);
; 617  :     b = _mm_insert_epi32(b, *(const uint32_t *)(ref_data + (y + 1) * ref_stride + leftoff), 1);
; 618  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 2) * pic_stride),           2);
; 619  :     b = _mm_insert_epi32(b, *(const uint32_t *)(ref_data + (y + 2) * ref_stride + leftoff), 2);
; 620  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 3) * pic_stride),           3);

	lea	r10d, DWORD PTR [r13+1]
	mov	r8d, r13d
	mov	r9d, r10d
	imul	r8d, edi
	imul	r9d, edi
	lea	r11d, DWORD PTR [r13-1]
	mov	edx, r11d

; 621  :     b = _mm_insert_epi32(b, *(const uint32_t *)(ref_data + (y + 3) * ref_stride + leftoff), 3);

	imul	r10d, r14d
	imul	edx, edi
	mov	ecx, eax
	imul	ecx, edi
	imul	r11d, r14d
	add	r10, rbx
	movd	xmm1, DWORD PTR [rcx+rbp]
	pinsrd	xmm1, DWORD PTR [rdx+rbp], 1
	pinsrd	xmm1, DWORD PTR [r8+rbp], 2
	pinsrd	xmm1, DWORD PTR [r9+rbp], 3
	mov	edx, eax
	mov	r8d, r11d
	mov	r9d, r13d
	imul	edx, r14d
	imul	r9d, r14d
	add	r8, rbx
	add	eax, 4
	add	r13d, 4
	add	rdx, rbx
	add	r9, rbx
	movd	xmm0, DWORD PTR [rdx+rsi]
	pinsrd	xmm0, DWORD PTR [r8+rsi], 1
	pinsrd	xmm0, DWORD PTR [r9+rsi], 2
	pinsrd	xmm0, DWORD PTR [r10+rsi], 3

; 622  : 
; 623  :     __m128i b_epol    = _mm_shuffle_epi8(b,       epol_mask);

	pshufb	xmm0, xmm3

; 624  :     __m128i curr_sads = _mm_sad_epu8    (a,       b_epol);

	psadbw	xmm1, xmm0

; 625  :             sse_inc   = _mm_add_epi64   (sse_inc, curr_sads);

	paddq	xmm1, xmm2
	movdqa	xmm2, xmm1
	cmp	eax, r12d
	jl	$LL4@hor_sad_ss

; 599  : 
; 600  :   right_border_idxs         = _mm_add_epi8 (right_border_idxs, dwbaseids);
; 601  : 
; 602  :   __m128i mask_right        = _mm_min_epi8 (ns,         right_border_idxs);
; 603  :   __m128i mask1             = _mm_sub_epi8 (mask_right, left_128);
; 604  : 
; 605  :   const __m128i epol_mask   = _mm_max_epi8(mask1, dwbaseids);
; 606  : 
; 607  :   const int32_t height_fourline_groups = height & ~3;
; 608  :   const int32_t height_residual_lines  = height &  3;
; 609  : 
; 610  :   __m128i sse_inc = _mm_setzero_si128();
; 611  :   int32_t y;
; 612  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r13, QWORD PTR [rsp+48]
$LN3@hor_sad_ss:

; 626  :   }
; 627  :   if (height_residual_lines) {

	cmp	DWORD PTR tv624[rsp], 0
	mov	r12, QWORD PTR [rsp+40]
	je	SHORT $LN6@hor_sad_ss

; 628  :     for (; y < height; y++) {

	cmp	eax, r15d
	jge	SHORT $LN6@hor_sad_ss
	npad	11
$LL7@hor_sad_ss:

; 629  :       __m128i a = _mm_cvtsi32_si128(*(const uint32_t *)(pic_data + y * pic_stride));
; 630  :       __m128i b = _mm_cvtsi32_si128(*(const uint32_t *)(ref_data + y * ref_stride + leftoff));

	mov	edx, eax
	mov	ecx, eax
	imul	edx, r14d
	inc	eax
	imul	ecx, edi
	add	rdx, rbx
	movd	xmm0, DWORD PTR [rdx+rsi]
	movd	xmm1, DWORD PTR [rcx+rbp]

; 631  : 
; 632  :       __m128i b_epol = _mm_shuffle_epi8(b, epol_mask);

	pshufb	xmm0, xmm3

; 633  :       __m128i curr_sads = _mm_sad_epu8 (a, b_epol);

	psadbw	xmm1, xmm0

; 634  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm1, xmm2
	movdqa	xmm2, xmm1
	cmp	eax, r15d
	jl	SHORT $LL7@hor_sad_ss
$LN6@hor_sad_ss:

; 635  :     }
; 636  :   }
; 637  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 638  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 639  : 
; 640  :   return _mm_cvtsi128_si32(sad);
; 641  : }

	mov	rbx, QWORD PTR [rsp+56]
	mov	rbp, QWORD PTR [rsp+64]
	pshufd	xmm0, xmm2, 78				; 0000004eH
	paddq	xmm0, xmm2
	movd	eax, xmm0
	pop	r15
	pop	r14
	pop	rdi
	pop	rsi
	ret	0
hor_sad_sse41_w4 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
pic_data$ = 48
ref_data$ = 56
height$ = 64
pic_stride$ = 72
ref_stride$ = 80
left$ = 88
right$ = 96
hor_sad_sse41_w8 PROC

; 646  : {

	push	rbx
	push	rsi
	push	rdi
	sub	rsp, 16

; 650  :   const int32_t border_idx       = left ? left : right_border_idx;

	mov	r10d, DWORD PTR left$[rsp]
	mov	eax, 7
	sub	eax, DWORD PTR right$[rsp]
	mov	r11d, r8d

; 651  : 
; 652  :   const __m128i ns               = _mm_setr_epi8(0,  1,  2,  3,  4,  5,  6,  7,

	movdqa	xmm5, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	mov	esi, eax

; 653  :                                                  8,  9,  10, 11, 12, 13, 14, 15);
; 654  : 
; 655  :   // Quadword (ie. line) base indexes, ie. the edges the lines read will be
; 656  :   // clamped towards; higher qword (lower line) bytes tend towards 8 and lower
; 657  :   // qword (higher line) bytes towards 0
; 658  :   const __m128i qwbaseids   = _mm_setr_epi8(0, 0, 0, 0, 0, 0, 0, 0,
; 659  :                                             8, 8, 8, 8, 8, 8, 8, 8);
; 660  : 
; 661  :   // Dirty hack alert! If right == block_width (ie. the entire vector is
; 662  :   // outside the frame), move the block offset one pixel to the left (so
; 663  :   // that the leftmost pixel in vector is actually the valid border pixel
; 664  :   // from which we want to extrapolate), and use an epol mask that will
; 665  :   // simply stretch the pixel all over the vector.
; 666  :   //
; 667  :   // To avoid a branch here:
; 668  :   // The mask will be -1 (0xffffffff) for border_idx -1 and 0 for >= 0
; 669  :   const int32_t border_idx_negative = border_idx >> 31;
; 670  :   const int32_t leftoff             = border_idx_negative | left;
; 671  : 
; 672  :   __m128i right_border_idxs = _mm_set1_epi8((int8_t)right_border_idx);

	movsx	eax, al
	test	r10d, r10d
	mov	QWORD PTR [rsp+56], r12
	mov	rbx, rdx
	cmovne	esi, r10d
	mov	QWORD PTR [rsp+64], r14
	sar	esi, 31

; 673  :   __m128i left_128          = _mm_set1_epi8((int8_t)left);
; 674  : 
; 675  :   right_border_idxs         = _mm_add_epi8 (right_border_idxs, qwbaseids);
; 676  : 
; 677  :   // If we're straddling the left border, right_border_idx is 7 and the first
; 678  :   // operation does nothing. If right border, left is 0 and the second
; 679  :   // operation does nothing.
; 680  :   __m128i mask_right        = _mm_min_epi8 (ns,         right_border_idxs);
; 681  :   __m128i mask1             = _mm_sub_epi8 (mask_right, left_128);
; 682  : 
; 683  :   // If right == 8 (we're completely outside the frame), right_border_idx is
; 684  :   // -1 and so is mask1. Clamp negative values to qwbaseid and as discussed
; 685  :   // earlier, adjust the load offset instead to load the "-1'st" pixels and
; 686  :   // using qwbaseids as the shuffle mask, broadcast it all over the rows.
; 687  :   const __m128i epol_mask = _mm_max_epi8(mask1, qwbaseids);
; 688  : 
; 689  :   const int32_t height_fourline_groups = height & ~3;

	mov	r14d, r8d
	movd	xmm0, eax
	or	esi, r10d
	punpcklbw xmm0, xmm0

; 690  :   const int32_t height_residual_lines  = height &  3;

	mov	r12d, r8d
	punpcklwd xmm0, xmm0
	and	r14d, -4
	pshufd	xmm0, xmm0, 0
	and	r12d, 3
	paddb	xmm0, XMMWORD PTR __xmm@08080808080808080000000000000000
	mov	rdi, rcx
	pminsb	xmm5, xmm0
	movsx	eax, r10b

; 691  : 
; 692  :   __m128i sse_inc = _mm_setzero_si128();
; 693  :   int32_t y;
; 694  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r10d, DWORD PTR ref_stride$[rsp]
	xorps	xmm4, xmm4
	movd	xmm0, eax
	xor	eax, eax
	punpcklbw xmm0, xmm0
	punpcklwd xmm0, xmm0
	pshufd	xmm0, xmm0, 0
	psubb	xmm5, xmm0
	pmaxsb	xmm5, XMMWORD PTR __xmm@08080808080808080000000000000000
	test	r14d, r14d
	jle	$LN16@hor_sad_ss

; 647  :   // right is the number of overhanging pixels in the vector, so it has to be
; 648  :   // handled this way to produce the index of last valid (border) pixel
; 649  :   const int32_t right_border_idx = 7 - right;

	mov	QWORD PTR [rsp+48], rbp
	mov	ebp, 2
	mov	QWORD PTR [rsp+72], r15
	movaps	XMMWORD PTR [rsp], xmm6
	xorps	xmm6, xmm6
	movsxd	r15, esi
	npad	3
$LL4@hor_sad_ss:

; 695  :     __m128d a_d = _mm_setzero_pd();
; 696  :     __m128d b_d = _mm_setzero_pd();
; 697  :     __m128d c_d = _mm_setzero_pd();
; 698  :     __m128d d_d = _mm_setzero_pd();
; 699  : 
; 700  :     a_d = _mm_loadl_pd(a_d, (const double *)(pic_data + (y + 0) * pic_stride));

	mov	ecx, eax

; 701  :     b_d = _mm_loadl_pd(b_d, (const double *)(ref_data + (y + 0) * ref_stride + leftoff));
; 702  :     a_d = _mm_loadh_pd(a_d, (const double *)(pic_data + (y + 1) * pic_stride));

	lea	r8d, DWORD PTR [rbp-1]
	imul	ecx, r9d
	movaps	xmm3, xmm6
	movaps	xmm2, xmm6
	mov	edx, eax
	imul	edx, r10d

; 703  :     b_d = _mm_loadh_pd(b_d, (const double *)(ref_data + (y + 1) * ref_stride + leftoff));
; 704  : 
; 705  :     c_d = _mm_loadl_pd(c_d, (const double *)(pic_data + (y + 2) * pic_stride));

	movaps	xmm1, xmm6

; 706  :     d_d = _mm_loadl_pd(d_d, (const double *)(ref_data + (y + 2) * ref_stride + leftoff));

	movaps	xmm0, xmm6
	add	eax, 4
	add	rdx, r15
	movlpd	xmm3, QWORD PTR [rcx+rdi]
	mov	ecx, r8d
	imul	ecx, r9d
	imul	r8d, r10d
	movlpd	xmm2, QWORD PTR [rdx+rbx]
	mov	edx, ebp
	imul	edx, r10d
	movhpd	xmm3, QWORD PTR [rcx+rdi]
	add	rdx, r15
	mov	ecx, r8d

; 707  :     c_d = _mm_loadh_pd(c_d, (const double *)(pic_data + (y + 3) * pic_stride));

	lea	r8d, DWORD PTR [rbp+1]
	add	rcx, r15
	movlpd	xmm0, QWORD PTR [rdx+rbx]
	movhpd	xmm2, QWORD PTR [rcx+rbx]
	mov	ecx, ebp
	imul	ecx, r9d
	add	ebp, 4

; 708  :     d_d = _mm_loadh_pd(d_d, (const double *)(ref_data + (y + 3) * ref_stride + leftoff));
; 709  : 
; 710  :     __m128i a = _mm_castpd_si128(a_d);
; 711  :     __m128i b = _mm_castpd_si128(b_d);
; 712  :     __m128i c = _mm_castpd_si128(c_d);
; 713  :     __m128i d = _mm_castpd_si128(d_d);
; 714  : 
; 715  :     __m128i b_epol = _mm_shuffle_epi8(b, epol_mask);

	pshufb	xmm2, xmm5

; 716  :     __m128i d_epol = _mm_shuffle_epi8(d, epol_mask);
; 717  : 
; 718  :     __m128i curr_sads_ab = _mm_sad_epu8(a, b_epol);

	psadbw	xmm3, xmm2
	movlpd	xmm1, QWORD PTR [rcx+rdi]
	mov	ecx, r8d
	imul	ecx, r9d
	imul	r8d, r10d

; 719  :     __m128i curr_sads_cd = _mm_sad_epu8(c, d_epol);
; 720  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm3, xmm4
	movdqa	xmm4, xmm3
	movhpd	xmm1, QWORD PTR [rcx+rdi]
	mov	ecx, r8d
	add	rcx, r15
	movhpd	xmm0, QWORD PTR [rcx+rbx]
	pshufb	xmm0, xmm5
	psadbw	xmm1, xmm0

; 721  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	paddq	xmm4, xmm1
	cmp	eax, r14d
	jl	$LL4@hor_sad_ss

; 691  : 
; 692  :   __m128i sse_inc = _mm_setzero_si128();
; 693  :   int32_t y;
; 694  :   for (y = 0; y < height_fourline_groups; y += 4) {

	movaps	xmm6, XMMWORD PTR [rsp]
	mov	r15, QWORD PTR [rsp+72]
	mov	rbp, QWORD PTR [rsp+48]
$LN16@hor_sad_ss:

; 722  :   }
; 723  :   if (height_residual_lines) {

	mov	r14, QWORD PTR [rsp+64]
	test	r12d, r12d
	mov	r12, QWORD PTR [rsp+56]
	je	SHORT $LN6@hor_sad_ss

; 724  :     for (; y < height; y++) {

	cmp	eax, r11d
	jge	SHORT $LN6@hor_sad_ss
	movsxd	r8, esi
	npad	2
$LL7@hor_sad_ss:

; 725  :       __m128i a = _mm_loadl_epi64((__m128i *)(pic_data + y * pic_stride));
; 726  :       __m128i b = _mm_loadl_epi64((__m128i *)(ref_data + y * ref_stride + leftoff));

	mov	edx, eax
	mov	ecx, eax
	imul	edx, r10d
	inc	eax
	imul	ecx, r9d
	add	rdx, r8
	movq	xmm0, QWORD PTR [rdx+rbx]
	movq	xmm1, QWORD PTR [rcx+rdi]

; 727  : 
; 728  :       __m128i b_epol = _mm_shuffle_epi8(b, epol_mask);

	pshufb	xmm0, xmm5

; 729  : 
; 730  :       __m128i curr_sads_ab = _mm_sad_epu8(a, b_epol);

	psadbw	xmm1, xmm0

; 731  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm1, xmm4
	movdqa	xmm4, xmm1
	cmp	eax, r11d
	jl	SHORT $LL7@hor_sad_ss
$LN6@hor_sad_ss:

; 732  :     }
; 733  :   }
; 734  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	pshufd	xmm0, xmm4, 78				; 0000004eH

; 735  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	paddq	xmm0, xmm4

; 736  :   return _mm_cvtsi128_si32(sad);

	movd	eax, xmm0

; 737  : }

	add	rsp, 16
	pop	rdi
	pop	rsi
	pop	rbx
	ret	0
hor_sad_sse41_w8 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
pic_data$ = 40
ref_data$ = 48
height$ = 56
pic_stride$ = 64
ref_stride$ = 72
left$ = 80
right$ = 88
hor_sad_sse41_w16 PROC

; 759  : {

	mov	QWORD PTR [rsp+32], rbx
	push	rbp
	push	rsi
	push	rdi
	push	r14

; 763  :   const int32_t border_idx       = left ? left : right_border_idx;
; 764  : 
; 765  :   const __m128i ns               = _mm_setr_epi8(0,  1,  2,  3,  4,  5,  6,  7,

	movdqa	xmm3, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	mov	r10d, 15
	sub	r10d, DWORD PTR right$[rsp]
	mov	rbp, rcx

; 766  :                                                  8,  9,  10, 11, 12, 13, 14, 15);
; 767  :   const __m128i zero             = _mm_setzero_si128();
; 768  : 
; 769  :   // Dirty hack alert! If right == block_width (ie. the entire vector is
; 770  :   // outside the frame), move the block offset one pixel to the left (so
; 771  :   // that the leftmost pixel in vector is actually the valid border pixel
; 772  :   // from which we want to extrapolate), and use an epol mask that will
; 773  :   // simply stretch the pixel all over the vector.
; 774  :   //
; 775  :   // To avoid a branch here:
; 776  :   // The mask will be -1 (0xffffffff) for border_idx -1 and 0 for >= 0
; 777  :   const int32_t border_idx_negative = border_idx >> 31;
; 778  :   const int32_t leftoff             = border_idx_negative | left;
; 779  : 
; 780  :   __m128i right_border_idxs = _mm_set1_epi8((int8_t)right_border_idx);
; 781  :   __m128i left_128          = _mm_set1_epi8((int8_t)left);

	movsxd	rcx, DWORD PTR left$[rsp]
	xorps	xmm2, xmm2

; 782  : 
; 783  :   // If we're straddling the left border, right_border_idx is 15 and the first
; 784  :   // operation does nothing. If right border, left is 0 and the second
; 785  :   // operation does nothing.
; 786  :   __m128i mask_right        = _mm_min_epi8 (ns,         right_border_idxs);
; 787  :   __m128i mask1             = _mm_sub_epi8 (mask_right, left_128);
; 788  : 
; 789  :   // If right == 16 (we're completely outside the frame), right_border_idx is
; 790  :   // -1 and so is mask1. Clamp negative values to zero and as discussed
; 791  :   // earlier, adjust the load offset instead to load the "-1'st" pixel and
; 792  :   // using an all-zero shuffle mask, broadcast it all over the vector.
; 793  :   const __m128i epol_mask = _mm_max_epi8(mask1, zero);
; 794  : 
; 795  :   const int32_t height_fourline_groups = height & ~3;
; 796  :   const int32_t height_residual_lines  = height &  3;
; 797  : 
; 798  :   __m128i sse_inc = _mm_setzero_si128();
; 799  :   int32_t y;
; 800  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r14d, DWORD PTR ref_stride$[rsp]
	mov	edi, r9d
	movsx	eax, r10b
	mov	ebx, r8d
	mov	QWORD PTR [rsp+40], r12
	mov	rsi, rdx
	mov	QWORD PTR [rsp+48], r13
	mov	r12d, r8d
	and	r12d, -4
	mov	r13d, r8d
	movd	xmm0, eax
	and	r13d, 3
	punpcklbw xmm0, xmm0
	punpcklwd xmm0, xmm0
	pshufd	xmm0, xmm0, 0
	pminsb	xmm3, xmm0
	movsx	eax, cl
	movd	xmm0, eax
	xor	eax, eax
	test	ecx, ecx
	punpcklbw xmm0, xmm0
	punpcklwd xmm0, xmm0
	cmovne	r10d, ecx
	pshufd	xmm0, xmm0, 0
	movsxd	r11, r10d
	psubb	xmm3, xmm0
	sar	r11, 31
	pmaxsb	xmm3, xmm2
	or	r11, rcx
	test	r12d, r12d
	jle	$LN3@hor_sad_ss

; 760  :   // right is the number of overhanging pixels in the vector, so it has to be
; 761  :   // handled this way to produce the index of last valid (border) pixel
; 762  :   const int32_t right_border_idx = 15 - right;

	mov	QWORD PTR [rsp+56], r15
	mov	r15d, 2
	npad	5
$LL4@hor_sad_ss:

; 801  :     __m128i a = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * pic_stride));

	mov	ecx, eax

; 802  :     __m128i b = _mm_loadu_si128((__m128i *)(ref_data + (y + 0) * ref_stride + leftoff));
; 803  :     __m128i c = _mm_loadu_si128((__m128i *)(pic_data + (y + 1) * pic_stride));

	lea	r9d, DWORD PTR [r15-1]
	imul	ecx, edi

; 804  :     __m128i d = _mm_loadu_si128((__m128i *)(ref_data + (y + 1) * ref_stride + leftoff));
; 805  :     __m128i e = _mm_loadu_si128((__m128i *)(pic_data + (y + 2) * pic_stride));
; 806  :     __m128i f = _mm_loadu_si128((__m128i *)(ref_data + (y + 2) * ref_stride + leftoff));
; 807  :     __m128i g = _mm_loadu_si128((__m128i *)(pic_data + (y + 3) * pic_stride));

	lea	r10d, DWORD PTR [r15+1]
	mov	edx, eax
	add	eax, 4
	imul	edx, r14d
	add	rdx, r11

; 808  :     __m128i h = _mm_loadu_si128((__m128i *)(ref_data + (y + 3) * ref_stride + leftoff));
; 809  : 
; 810  :     __m128i b_epol = _mm_shuffle_epi8(b, epol_mask);
; 811  :     __m128i d_epol = _mm_shuffle_epi8(d, epol_mask);
; 812  :     __m128i f_epol = _mm_shuffle_epi8(f, epol_mask);
; 813  :     __m128i h_epol = _mm_shuffle_epi8(h, epol_mask);
; 814  : 
; 815  :     __m128i curr_sads_ab = _mm_sad_epu8(a, b_epol);

	movdqu	xmm1, XMMWORD PTR [rcx+rbp]
	movdqu	xmm0, XMMWORD PTR [rdx+rsi]
	mov	edx, r9d
	imul	r9d, r14d
	pshufb	xmm0, xmm3
	imul	edx, edi
	psadbw	xmm1, xmm0
	mov	ecx, r9d
	add	rcx, r11

; 816  :     __m128i curr_sads_cd = _mm_sad_epu8(c, d_epol);
; 817  :     __m128i curr_sads_ef = _mm_sad_epu8(e, f_epol);
; 818  :     __m128i curr_sads_gh = _mm_sad_epu8(g, h_epol);
; 819  : 
; 820  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm1, xmm2
	movdqa	xmm2, xmm1
	movdqu	xmm0, XMMWORD PTR [rdx+rbp]
	mov	edx, r15d
	movdqu	xmm1, XMMWORD PTR [rcx+rsi]
	mov	ecx, r15d
	add	r15d, 4
	imul	edx, r14d
	pshufb	xmm1, xmm3
	imul	ecx, edi
	psadbw	xmm0, xmm1
	add	rdx, r11
	movdqu	xmm1, XMMWORD PTR [rdx+rsi]
	mov	edx, r10d

; 821  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	paddq	xmm2, xmm0
	imul	r10d, r14d
	movdqu	xmm0, XMMWORD PTR [rcx+rbp]
	pshufb	xmm1, xmm3
	mov	ecx, r10d
	add	rcx, r11
	imul	edx, edi
	psadbw	xmm0, xmm1
	movdqu	xmm1, XMMWORD PTR [rcx+rsi]

; 822  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	paddq	xmm2, xmm0
	pshufb	xmm1, xmm3
	movdqu	xmm0, XMMWORD PTR [rdx+rbp]
	psadbw	xmm0, xmm1

; 823  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	paddq	xmm2, xmm0
	cmp	eax, r12d
	jl	$LL4@hor_sad_ss

; 782  : 
; 783  :   // If we're straddling the left border, right_border_idx is 15 and the first
; 784  :   // operation does nothing. If right border, left is 0 and the second
; 785  :   // operation does nothing.
; 786  :   __m128i mask_right        = _mm_min_epi8 (ns,         right_border_idxs);
; 787  :   __m128i mask1             = _mm_sub_epi8 (mask_right, left_128);
; 788  : 
; 789  :   // If right == 16 (we're completely outside the frame), right_border_idx is
; 790  :   // -1 and so is mask1. Clamp negative values to zero and as discussed
; 791  :   // earlier, adjust the load offset instead to load the "-1'st" pixel and
; 792  :   // using an all-zero shuffle mask, broadcast it all over the vector.
; 793  :   const __m128i epol_mask = _mm_max_epi8(mask1, zero);
; 794  : 
; 795  :   const int32_t height_fourline_groups = height & ~3;
; 796  :   const int32_t height_residual_lines  = height &  3;
; 797  : 
; 798  :   __m128i sse_inc = _mm_setzero_si128();
; 799  :   int32_t y;
; 800  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r15, QWORD PTR [rsp+56]
$LN3@hor_sad_ss:
	mov	r12, QWORD PTR [rsp+40]

; 824  :   }
; 825  :   if (height_residual_lines) {

	test	r13d, r13d
	mov	r13, QWORD PTR [rsp+48]
	je	SHORT $LN6@hor_sad_ss

; 826  :     for (; y < height; y++) {

	cmp	eax, ebx
	jge	SHORT $LN6@hor_sad_ss
	npad	2
$LL7@hor_sad_ss:

; 827  :       __m128i a = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * pic_stride));
; 828  :       __m128i b = _mm_loadu_si128((__m128i *)(ref_data + (y + 0) * ref_stride + leftoff));

	mov	edx, eax
	mov	ecx, eax
	imul	edx, r14d
	inc	eax
	imul	ecx, edi
	add	rdx, r11

; 829  :       __m128i b_epol = _mm_shuffle_epi8(b, epol_mask);

	movdqu	xmm0, XMMWORD PTR [rdx+rsi]

; 830  :       __m128i curr_sads = _mm_sad_epu8(a, b_epol);

	movdqu	xmm1, XMMWORD PTR [rcx+rbp]
	pshufb	xmm0, xmm3
	psadbw	xmm1, xmm0

; 831  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm1, xmm2
	movdqa	xmm2, xmm1
	cmp	eax, ebx
	jl	SHORT $LL7@hor_sad_ss
$LN6@hor_sad_ss:

; 832  :     }
; 833  :   }
; 834  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 835  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 836  :   return _mm_cvtsi128_si32(sad);
; 837  : }

	mov	rbx, QWORD PTR [rsp+64]
	pshufd	xmm0, xmm2, 78				; 0000004eH
	paddq	xmm0, xmm2
	movd	eax, xmm0
	pop	r14
	pop	rdi
	pop	rsi
	pop	rbp
	ret	0
hor_sad_sse41_w16 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
_TEXT	SEGMENT
border_off$1$ = 0
a_off$1$ = 4
is_left_bm$1$ = 8
invec_lstart$1$ = 12
invec_linc$1$ = 16
x$2$ = 20
tv1828 = 20
invec_lend$1$ = 24
a_off$1$ = 28
outside_vecs$1$ = 32
y$1$ = 36
tv1809 = 40
height_fourline_groups$1$ = 44
tv1802 = 48
tv1829 = 52
tv1819 = 56
tv1800 = 56
tv1818 = 64
tv1801 = 64
tv1826 = 72
tv1807 = 72
tv1815 = 80
tv1799 = 80
old_d$1$ = 96
tv1814 = 96
tv1989 = 112
tv1823 = 120
move_old_to_b_imask$1$ = 128
shufmask1$1$ = 144
tv1806 = 160
tv1798 = 168
tv1797 = 176
tv1805 = 184
old_f$1$ = 192
old_h$1$ = 208
is_left$1$ = 224
pic_data$ = 464
ref_data$ = 472
width$ = 480
height$ = 488
pic_stride$ = 496
ref_stride$ = 504
left$ = 512
right$ = 520
hor_sad_sse41_arbitrary PROC

; 842  : {

	mov	rax, rsp
	mov	QWORD PTR [rax+8], rbx
	mov	DWORD PTR [rax+32], r9d
	mov	QWORD PTR [rax+16], rdx
	push	rbp
	push	rsi
	push	rdi
	push	r12
	push	r13
	push	r14
	push	r15
	sub	rsp, 400				; 00000190H

; 843  :   __m128i sse_inc = _mm_setzero_si128();
; 844  : 
; 845  :   const size_t vec_width = 16;
; 846  :   const size_t vecwid_bitmask = 15;
; 847  :   const size_t vec_width_log2 = 4;
; 848  : 
; 849  :   const int32_t height_fourline_groups = height & ~3;
; 850  :   const int32_t height_residual_lines  = height &  3;
; 851  : 
; 852  :   const __m128i rights     = _mm_set1_epi8((uint8_t)right);

	mov	ebx, DWORD PTR right$[rsp]
	mov	rsi, rdx

; 853  :   const __m128i blk_widths = _mm_set1_epi8((uint8_t)width);
; 854  :   const __m128i vec_widths = _mm_set1_epi8((uint8_t)vec_width);
; 855  :   const __m128i nslo       = _mm_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
; 856  : 
; 857  :   uint32_t outside_vecs,  inside_vecs,  left_offset, is_left_bm;
; 858  :   int32_t  outside_width, inside_width, border_off,  invec_lstart,
; 859  :            invec_lend,    invec_linc;
; 860  :   if (left) {

	mov	r13d, DWORD PTR left$[rsp]
	mov	r10, rcx
	movdqa	xmm4, XMMWORD PTR __xmm@10101010101010101010101010101010
	movaps	XMMWORD PTR [rax-136], xmm10
	xorps	xmm10, xmm10
	movaps	XMMWORD PTR [rax-152], xmm11
	movdqa	xmm11, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	movaps	XMMWORD PTR [rax-168], xmm12
	movaps	XMMWORD PTR [rax-184], xmm13
	xorps	xmm13, xmm13
	movaps	XMMWORD PTR [rax-200], xmm14
	mov	eax, r9d
	and	eax, -4
	movsxd	rdx, r8d
	mov	DWORD PTR height_fourline_groups$1$[rsp], eax
	add	rdx, 15
	mov	eax, r9d
	shr	rdx, 4
	and	eax, 3
	mov	DWORD PTR tv1829[rsp], eax
	movsx	eax, bl
	movd	xmm0, eax
	punpcklbw xmm0, xmm0
	movsx	eax, r8b
	punpcklwd xmm0, xmm0
	pshufd	xmm0, xmm0, 0
	movd	xmm14, eax
	punpcklbw xmm14, xmm14
	punpcklwd xmm14, xmm14
	pshufd	xmm14, xmm14, 0
	test	r13d, r13d
	je	SHORT $LN20@hor_sad_ss

; 861  :     outside_vecs  =    left                              >> vec_width_log2;

	mov	r11d, r13d

; 862  :     inside_vecs   = (( width           + vecwid_bitmask) >> vec_width_log2) - outside_vecs;
; 863  :     outside_width =    outside_vecs * vec_width;
; 864  :     inside_width  =    inside_vecs  * vec_width;
; 865  :     left_offset   =    left;
; 866  :     border_off    =    left;

	mov	DWORD PTR border_off$1$[rsp], r13d
	shr	r11d, 4
	mov	r14d, r13d
	sub	edx, r11d

; 867  :     invec_lstart  =    0;
; 868  :     invec_lend    =    inside_vecs;
; 869  :     invec_linc    =    1;

	mov	DWORD PTR invec_linc$1$[rsp], 1
	mov	ebp, edx
	xor	eax, eax
	mov	r15d, edx

; 870  :     is_left_bm    =    -1;

	mov	r12d, -1				; ffffffffH

; 871  :   } else {

	jmp	SHORT $LN54@hor_sad_ss
$LN20@hor_sad_ss:

; 872  :     inside_vecs   =  ((width - right) + vecwid_bitmask)  >> vec_width_log2;

	mov	eax, r8d

; 873  :     outside_vecs  = (( width          + vecwid_bitmask)  >> vec_width_log2) - inside_vecs;

	mov	r11d, edx
	sub	eax, ebx

; 874  :     outside_width =    outside_vecs * vec_width;
; 875  :     inside_width  =    inside_vecs  * vec_width;
; 876  :     left_offset   =    right - width;

	mov	r13d, ebx
	sub	r13d, r8d

; 877  :     border_off    =    width - 1 - right;
; 878  :     invec_lstart  =    inside_vecs - 1;
; 879  :     invec_lend    =    -1;

	mov	r15d, -1

; 880  :     invec_linc    =    -1;

	mov	DWORD PTR invec_linc$1$[rsp], r15d
	lea	rbp, QWORD PTR [rax+15]
	shr	rbp, 4
	lea	r14d, DWORD PTR [rax-1]
	sub	r11d, ebp
	mov	DWORD PTR border_off$1$[rsp], r14d

; 881  :     is_left_bm    =    0;

	xor	r12d, r12d
	lea	eax, DWORD PTR [rbp-1]
$LN54@hor_sad_ss:

; 884  : 
; 885  :   const __m128i left_offsets = _mm_set1_epi8 ((uint8_t)left_offset);
; 886  :   const __m128i is_left      = _mm_cmpeq_epi8(rights, _mm_setzero_si128());
; 887  :   const __m128i vw_for_left  = _mm_and_si128 (is_left, vec_widths);
; 888  : 
; 889  :   // -x == (x ^ 0xff) + 1 = (x ^ 0xff) - 0xff. Also x == (x ^ 0x00) - 0x00.
; 890  :   // in other words, calculate inverse of left_offsets if is_left is true.
; 891  :   const __m128i offs_neg            = _mm_xor_si128 (left_offsets, is_left);
; 892  :   const __m128i offs_for_sm1        = _mm_sub_epi8  (offs_neg,     is_left);
; 893  : 
; 894  :   const __m128i ns_for_sm1          = _mm_or_si128  (vw_for_left,  nslo);
; 895  :   const __m128i shufmask1           = _mm_add_epi8  (ns_for_sm1,   offs_for_sm1);
; 896  : 
; 897  :   const __m128i mo2bmask_l          = _mm_cmpgt_epi8(left_offsets, nslo);
; 898  :   const __m128i mo2bimask_l         = _mm_cmpeq_epi8(mo2bmask_l,   _mm_setzero_si128());
; 899  :   const __m128i mo2bimask_r         = _mm_cmpgt_epi8(vec_widths,   shufmask1);
; 900  :   const __m128i move_old_to_b_imask = _mm_blendv_epi8(mo2bimask_r, mo2bimask_l, is_left);
; 901  : 
; 902  :   const int32_t outvec_offset = (~is_left_bm) & inside_width;
; 903  :   int32_t x, y;
; 904  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	edx, DWORD PTR ref_stride$[rsp]
	and	r13d, 15
	mov	ebx, DWORD PTR pic_stride$[rsp]
	xor	r8d, r8d
	mov	DWORD PTR invec_lstart$1$[rsp], eax
	xorps	xmm12, xmm12
	pcmpeqb	xmm12, xmm0
	movsx	eax, r13b
	movdqa	xmm3, xmm12
	movaps	XMMWORD PTR [rsp+384], xmm6
	movdqa	xmm0, xmm12
	mov	DWORD PTR is_left_bm$1$[rsp], r12d
	pand	xmm0, xmm4
	mov	DWORD PTR invec_lend$1$[rsp], r15d
	movd	xmm1, eax
	por	xmm0, xmm11
	punpcklbw xmm1, xmm1
	mov	eax, ebp
	shl	eax, 4
	mov	edi, r11d
	movsxd	r9, eax
	mov	eax, r12d
	punpcklwd xmm1, xmm1
	not	eax
	pshufd	xmm1, xmm1, 0
	pxor	xmm3, xmm1
	shl	edi, 4
	psubb	xmm3, xmm12
	cdqe
	paddb	xmm3, xmm0
	mov	DWORD PTR outside_vecs$1$[rsp], r11d
	and	r9, rax
	mov	QWORD PTR tv1989[rsp], rbp
	pcmpgtb	xmm1, xmm11
	pcmpgtb	xmm4, xmm3
	pcmpeqb	xmm1, xmm10
	movdqa	xmm0, xmm12
	pblendvb xmm4, xmm1, xmm0
	movdqa	XMMWORD PTR move_old_to_b_imask$1$[rsp], xmm4
	movaps	XMMWORD PTR [rsp+368], xmm7
	mov	DWORD PTR a_off$1$[rsp], edi
	mov	DWORD PTR left$[rsp], r13d
	movdqa	XMMWORD PTR is_left$1$[rsp], xmm12
	movdqa	XMMWORD PTR shufmask1$1$[rsp], xmm3
	mov	DWORD PTR y$1$[rsp], r8d
	mov	QWORD PTR tv1823[rsp], r9
	cmp	DWORD PTR height_fourline_groups$1$[rsp], r8d
	jle	$LN3@hor_sad_ss

; 882  :   }
; 883  :   left_offset &= vecwid_bitmask;

	mov	eax, edi
	movaps	XMMWORD PTR [rsp+352], xmm8
	and	eax, r12d
	movaps	XMMWORD PTR [rsp+336], xmm9
	mov	edi, 2
	mov	DWORD PTR a_off$1$[rsp], eax
	movaps	XMMWORD PTR [rsp+240], xmm15
	mov	DWORD PTR tv1809[rsp], edi

; 884  : 
; 885  :   const __m128i left_offsets = _mm_set1_epi8 ((uint8_t)left_offset);
; 886  :   const __m128i is_left      = _mm_cmpeq_epi8(rights, _mm_setzero_si128());
; 887  :   const __m128i vw_for_left  = _mm_and_si128 (is_left, vec_widths);
; 888  : 
; 889  :   // -x == (x ^ 0xff) + 1 = (x ^ 0xff) - 0xff. Also x == (x ^ 0x00) - 0x00.
; 890  :   // in other words, calculate inverse of left_offsets if is_left is true.
; 891  :   const __m128i offs_neg            = _mm_xor_si128 (left_offsets, is_left);
; 892  :   const __m128i offs_for_sm1        = _mm_sub_epi8  (offs_neg,     is_left);
; 893  : 
; 894  :   const __m128i ns_for_sm1          = _mm_or_si128  (vw_for_left,  nslo);
; 895  :   const __m128i shufmask1           = _mm_add_epi8  (ns_for_sm1,   offs_for_sm1);
; 896  : 
; 897  :   const __m128i mo2bmask_l          = _mm_cmpgt_epi8(left_offsets, nslo);
; 898  :   const __m128i mo2bimask_l         = _mm_cmpeq_epi8(mo2bmask_l,   _mm_setzero_si128());
; 899  :   const __m128i mo2bimask_r         = _mm_cmpgt_epi8(vec_widths,   shufmask1);
; 900  :   const __m128i move_old_to_b_imask = _mm_blendv_epi8(mo2bimask_r, mo2bimask_l, is_left);
; 901  : 
; 902  :   const int32_t outvec_offset = (~is_left_bm) & inside_width;
; 903  :   int32_t x, y;
; 904  :   for (y = 0; y < height_fourline_groups; y += 4) {

	jmp	SHORT $LN4@hor_sad_ss
	npad	7
$LL53@hor_sad_ss:
	mov	r14d, DWORD PTR border_off$1$[rsp]
$LN4@hor_sad_ss:

; 905  :     __m128i borderpx_vec_b = _mm_set1_epi8(ref_data[(int32_t)((y + 0) * ref_stride + border_off)]);

	mov	r12d, r8d

; 906  :     __m128i borderpx_vec_d = _mm_set1_epi8(ref_data[(int32_t)((y + 1) * ref_stride + border_off)]);
; 907  :     __m128i borderpx_vec_f = _mm_set1_epi8(ref_data[(int32_t)((y + 2) * ref_stride + border_off)]);

	mov	r13d, edi
	imul	r12d, edx
	imul	r13d, edx
	mov	DWORD PTR tv1828[rsp], r12d
	lea	eax, DWORD PTR [r12+r14]
	movsxd	rcx, eax
	movsx	eax, BYTE PTR [rcx+rsi]
	movd	xmm15, eax
	lea	eax, DWORD PTR [rdi-1]
	imul	eax, edx
	punpcklbw xmm15, xmm15
	punpcklwd xmm15, xmm15
	pshufd	xmm15, xmm15, 0
	add	eax, r14d
	cdqe
	movsx	ecx, BYTE PTR [rax+rsi]
	lea	eax, DWORD PTR [r14+r13]

; 908  :     __m128i borderpx_vec_h = _mm_set1_epi8(ref_data[(int32_t)((y + 3) * ref_stride + border_off)]);

	lea	r14d, DWORD PTR [rdi+1]
	movd	xmm2, ecx
	movsxd	rcx, eax
	punpcklbw xmm2, xmm2
	punpcklwd xmm2, xmm2
	pshufd	xmm2, xmm2, 0
	movsx	eax, BYTE PTR [rcx+rsi]
	movdqa	XMMWORD PTR old_d$1$[rsp], xmm2
	movd	xmm8, eax
	mov	eax, r14d
	imul	eax, edx

; 909  : 
; 910  :     for (x = 0; x < outside_vecs; x++) {

	xor	edx, edx
	punpcklbw xmm8, xmm8
	punpcklwd xmm8, xmm8
	pshufd	xmm8, xmm8, 0
	movdqa	XMMWORD PTR old_f$1$[rsp], xmm8
	mov	DWORD PTR tv1802[rsp], eax
	add	eax, DWORD PTR border_off$1$[rsp]
	movsxd	rcx, eax
	movsx	eax, BYTE PTR [rcx+rsi]
	movd	xmm9, eax
	punpcklbw xmm9, xmm9
	punpcklwd xmm9, xmm9
	pshufd	xmm9, xmm9, 0
	movdqa	XMMWORD PTR old_h$1$[rsp], xmm9
	test	r11d, r11d
	je	$LN43@hor_sad_ss

; 911  :       __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + outvec_offset));

	mov	eax, r8d
	movdqa	xmm7, xmm2
	imul	eax, ebx
	mov	QWORD PTR tv1819[rsp], rax
	lea	eax, DWORD PTR [rdi-1]
	imul	eax, ebx
	mov	QWORD PTR tv1801[rsp], rax
	mov	eax, edi
	mov	rsi, QWORD PTR tv1801[rsp]
	imul	eax, ebx
	mov	QWORD PTR tv1807[rsp], rax
	mov	eax, r14d
	mov	r15, QWORD PTR tv1807[rsp]
	imul	eax, ebx
	mov	rbx, QWORD PTR tv1819[rsp]
	mov	QWORD PTR tv1799[rsp], rax
	mov	r12d, eax
	npad	6
$LL7@hor_sad_ss:
	movsxd	rcx, edx

; 912  :       __m128i c = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 1) * pic_stride + outvec_offset));
; 913  :       __m128i e = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 2) * pic_stride + outvec_offset));
; 914  :       __m128i g = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 3) * pic_stride + outvec_offset));
; 915  : 
; 916  :       __m128i startoffs  = _mm_set1_epi8  ((x + inside_vecs) << vec_width_log2);
; 917  :       __m128i ns         = _mm_add_epi8   (startoffs, nslo);
; 918  : 
; 919  :       // Unread imask is (is_left NOR unrd_imask_for_right), do the maths etc
; 920  :       __m128i unrd_imask = _mm_cmpgt_epi8 (blk_widths, ns);

	movdqa	xmm2, xmm14
	add	rcx, rcx

; 921  :               unrd_imask = _mm_or_si128   (unrd_imask, is_left);
; 922  :       __m128i unrd_mask  = _mm_cmpeq_epi8 (unrd_imask, _mm_setzero_si128());
; 923  : 
; 924  :       __m128i b_unread   = _mm_blendv_epi8(borderpx_vec_b, a, unrd_mask);

	movdqa	xmm1, xmm15
	lea	rax, QWORD PTR [rbx+rcx*8]
	add	rax, r9
	movdqu	xmm3, XMMWORD PTR [rax+r10]
	lea	rax, QWORD PTR [rsi+rcx*8]
	add	rax, r9
	movdqu	xmm4, XMMWORD PTR [rax+r10]
	lea	rax, QWORD PTR [r15+rcx*8]
	add	rax, r9
	movdqu	xmm5, XMMWORD PTR [rax+r10]
	lea	rax, QWORD PTR [r12+rcx*8]
	add	rax, r9
	movdqu	xmm6, XMMWORD PTR [rax+r10]
	lea	eax, DWORD PTR [rdx+rbp]
	inc	edx
	shl	al, 4
	movsx	eax, al
	movd	xmm0, eax
	punpcklbw xmm0, xmm0
	punpcklwd xmm0, xmm0
	pshufd	xmm0, xmm0, 0
	paddb	xmm0, xmm11
	pcmpgtb	xmm2, xmm0
	por	xmm2, xmm12
	pcmpeqb	xmm2, xmm10
	movdqa	xmm0, xmm2
	pblendvb xmm1, xmm3, xmm0

; 925  :       __m128i d_unread   = _mm_blendv_epi8(borderpx_vec_d, c, unrd_mask);
; 926  :       __m128i f_unread   = _mm_blendv_epi8(borderpx_vec_f, e, unrd_mask);
; 927  :       __m128i h_unread   = _mm_blendv_epi8(borderpx_vec_h, g, unrd_mask);
; 928  : 
; 929  :       __m128i sad_ab     = _mm_sad_epu8   (a, b_unread);

	psadbw	xmm3, xmm1
	movdqa	xmm1, xmm7
	pblendvb xmm1, xmm4, xmm0

; 930  :       __m128i sad_cd     = _mm_sad_epu8   (c, d_unread);

	psadbw	xmm4, xmm1
	movdqa	xmm1, xmm8
	pblendvb xmm1, xmm5, xmm0

; 931  :       __m128i sad_ef     = _mm_sad_epu8   (e, f_unread);

	psadbw	xmm5, xmm1
	movdqa	xmm1, xmm9

; 932  :       __m128i sad_gh     = _mm_sad_epu8   (g, h_unread);
; 933  : 
; 934  :       sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	paddq	xmm3, xmm13
	movdqa	xmm13, xmm3

; 935  :       sse_inc = _mm_add_epi64(sse_inc, sad_cd);

	paddq	xmm13, xmm4
	pblendvb xmm1, xmm6, xmm0
	psadbw	xmm6, xmm1

; 936  :       sse_inc = _mm_add_epi64(sse_inc, sad_ef);

	paddq	xmm13, xmm5

; 937  :       sse_inc = _mm_add_epi64(sse_inc, sad_gh);

	paddq	xmm13, xmm6
	cmp	edx, r11d
	jb	$LL7@hor_sad_ss
	movdqa	xmm2, XMMWORD PTR old_d$1$[rsp]
	movdqa	xmm3, XMMWORD PTR shufmask1$1$[rsp]
	movdqa	xmm4, XMMWORD PTR move_old_to_b_imask$1$[rsp]
	mov	ebx, DWORD PTR pic_stride$[rsp]
	mov	rsi, QWORD PTR ref_data$[rsp]
	mov	r15d, DWORD PTR invec_lend$1$[rsp]
	mov	r12d, DWORD PTR tv1828[rsp]
$LN43@hor_sad_ss:

; 941  : 
; 942  :     __m128i old_b = borderpx_vec_b;
; 943  :     __m128i old_d = borderpx_vec_d;
; 944  :     __m128i old_f = borderpx_vec_f;
; 945  :     __m128i old_h = borderpx_vec_h;
; 946  : 
; 947  :     for (x = invec_lstart; x != invec_lend; x += invec_linc) {

	mov	eax, DWORD PTR invec_lstart$1$[rsp]
	mov	DWORD PTR x$2$[rsp], eax
	cmp	eax, r15d
	je	$LN52@hor_sad_ss

; 938  :     }
; 939  :     int32_t a_off = outside_width & is_left_bm;
; 940  :     int32_t leftoff_with_sign_neg = (left_offset ^ is_left_bm) - is_left_bm;

	mov	eax, DWORD PTR is_left_bm$1$[rsp]
	xor	eax, DWORD PTR left$[rsp]
	sub	eax, DWORD PTR is_left_bm$1$[rsp]
	mov	r9d, DWORD PTR invec_linc$1$[rsp]

; 949  :       __m128i c = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 1) * pic_stride + a_off));
; 950  :       __m128i e = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 2) * pic_stride + a_off));
; 951  :       __m128i g = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 3) * pic_stride + a_off));
; 952  :       __m128i b = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 0) * ref_stride + a_off - leftoff_with_sign_neg));

	movsxd	rcx, eax
	mov	eax, r8d
	imul	eax, ebx
	imul	r14d, ebx
	mov	QWORD PTR tv1826[rsp], rcx
	lea	ecx, DWORD PTR [rdi-1]
	mov	QWORD PTR tv1818[rsp], rax
	movsxd	rax, DWORD PTR a_off$1$[rsp]
	mov	rdx, QWORD PTR tv1818[rsp]
	mov	QWORD PTR tv1815[rsp], rax
	mov	eax, ecx
	imul	eax, ebx
	mov	QWORD PTR tv1800[rsp], rax
	mov	eax, edi
	mov	rbp, QWORD PTR tv1800[rsp]
	imul	eax, ebx
	mov	ebx, DWORD PTR a_off$1$[rsp]
	mov	QWORD PTR tv1806[rsp], rax
	mov	r11, QWORD PTR tv1806[rsp]
	mov	eax, r14d
	mov	r14, QWORD PTR tv1815[rsp]
	mov	QWORD PTR tv1798[rsp], rax
	mov	r8, QWORD PTR tv1798[rsp]
	mov	eax, r12d
	mov	r12d, DWORD PTR x$2$[rsp]
	mov	QWORD PTR tv1814[rsp], rax
	mov	eax, DWORD PTR ref_stride$[rsp]
	mov	rdi, QWORD PTR tv1814[rsp]
	imul	eax, ecx
	mov	QWORD PTR tv1797[rsp], rax
	mov	eax, r13d
	mov	r13, QWORD PTR tv1826[rsp]
	mov	QWORD PTR tv1805[rsp], rax
	npad	2
$LL10@hor_sad_ss:

; 948  :       __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + a_off));

	movsxd	rcx, r12d
	lea	rax, QWORD PTR [r14+rdx]
	shl	rcx, 4

; 953  :       __m128i d = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 1) * ref_stride + a_off - leftoff_with_sign_neg));
; 954  :       __m128i f = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 2) * ref_stride + a_off - leftoff_with_sign_neg));
; 955  :       __m128i h = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 3) * ref_stride + a_off - leftoff_with_sign_neg));
; 956  : 
; 957  :       __m128i b_shifted    = _mm_shuffle_epi8(b,     shufmask1);
; 958  :       __m128i d_shifted    = _mm_shuffle_epi8(d,     shufmask1);
; 959  :       __m128i f_shifted    = _mm_shuffle_epi8(f,     shufmask1);
; 960  :       __m128i h_shifted    = _mm_shuffle_epi8(h,     shufmask1);
; 961  : 
; 962  :       __m128i b_with_old   = _mm_blendv_epi8 (old_b, b_shifted, move_old_to_b_imask);
; 963  :       __m128i d_with_old   = _mm_blendv_epi8 (old_d, d_shifted, move_old_to_b_imask);
; 964  :       __m128i f_with_old   = _mm_blendv_epi8 (old_f, f_shifted, move_old_to_b_imask);
; 965  :       __m128i h_with_old   = _mm_blendv_epi8 (old_h, h_shifted, move_old_to_b_imask);
; 966  : 
; 967  :       uint8_t startoff     = (x << vec_width_log2) + a_off;
; 968  :       __m128i startoffs    = _mm_set1_epi8   (startoff);
; 969  :       __m128i curr_ns      = _mm_add_epi8    (startoffs,    nslo);
; 970  :       __m128i unrd_imask   = _mm_cmpgt_epi8  (blk_widths,   curr_ns);

	movdqa	xmm1, xmm14
	add	rax, rcx
	movdqu	xmm9, XMMWORD PTR [rax+r10]
	lea	rax, QWORD PTR [r14+rbp]
	add	rax, rcx
	movdqu	xmm10, XMMWORD PTR [rax+r10]
	lea	rax, QWORD PTR [r11+r14]
	add	rax, rcx
	movdqu	xmm11, XMMWORD PTR [rax+r10]
	lea	rax, QWORD PTR [r8+r14]
	add	rax, rcx
	movdqu	xmm12, XMMWORD PTR [rax+r10]
	mov	rax, rdi
	sub	rax, r13
	add	rax, r14
	add	rax, rcx
	movdqu	xmm8, XMMWORD PTR [rax+rsi]
	mov	rax, QWORD PTR tv1797[rsp]
	sub	rax, r13
	add	rax, r14
	add	rax, rcx
	pshufb	xmm8, xmm3
	movdqu	xmm7, XMMWORD PTR [rax+rsi]
	mov	rax, QWORD PTR tv1805[rsp]
	sub	rax, r13
	add	rax, r14
	add	rax, rcx
	pshufb	xmm7, xmm3
	movdqu	xmm6, XMMWORD PTR [rax+rsi]
	mov	eax, DWORD PTR tv1802[rsp]
	sub	rax, r13

; 971  :       __m128i unrd_mask    = _mm_cmpeq_epi8  (unrd_imask,   _mm_setzero_si128());
; 972  : 
; 973  :       __m128i b_unread     = _mm_blendv_epi8 (b_with_old,   a, unrd_mask);
; 974  :       __m128i d_unread     = _mm_blendv_epi8 (d_with_old,   c, unrd_mask);
; 975  :       __m128i f_unread     = _mm_blendv_epi8 (f_with_old,   e, unrd_mask);
; 976  :       __m128i h_unread     = _mm_blendv_epi8 (h_with_old,   g, unrd_mask);
; 977  : 
; 978  :       old_b = b_shifted;
; 979  :       old_d = d_shifted;

	movdqa	XMMWORD PTR old_d$1$[rsp], xmm7
	add	rax, r14
	add	rax, rcx
	pshufb	xmm6, xmm3
	movdqu	xmm5, XMMWORD PTR [rax+rsi]
	movzx	eax, r12b
	shl	al, 4
	pshufb	xmm5, xmm3
	add	al, bl
	movsx	eax, al
	movd	xmm0, eax
	punpcklbw xmm0, xmm0
	punpcklwd xmm0, xmm0
	pshufd	xmm0, xmm0, 0
	paddb	xmm0, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	pcmpgtb	xmm1, xmm0
	xorps	xmm0, xmm0
	pcmpeqb	xmm1, xmm0
	movdqa	xmm0, xmm4
	pblendvb xmm15, xmm8, xmm0
	movdqa	xmm0, xmm1
	pblendvb xmm15, xmm9, xmm0
	movdqa	xmm4, xmm15
	movdqa	xmm15, XMMWORD PTR move_old_to_b_imask$1$[rsp]
	movdqa	xmm0, xmm15

; 980  :       old_f = f_shifted;
; 981  :       old_h = h_shifted;
; 982  : 
; 983  :       __m128i sad_ab     = _mm_sad_epu8(a, b_unread);

	psadbw	xmm9, xmm4
	pblendvb xmm2, xmm7, xmm0
	movdqa	xmm0, xmm1
	pblendvb xmm2, xmm10, xmm0
	movdqa	xmm0, xmm15
	movdqa	xmm3, xmm2
	movdqa	xmm2, XMMWORD PTR old_f$1$[rsp]

; 984  :       __m128i sad_cd     = _mm_sad_epu8(c, d_unread);
; 985  :       __m128i sad_ef     = _mm_sad_epu8(e, f_unread);
; 986  :       __m128i sad_gh     = _mm_sad_epu8(g, h_unread);
; 987  : 
; 988  :       sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	paddq	xmm9, xmm13
	pblendvb xmm2, xmm6, xmm0
	movdqa	xmm0, xmm1
	movdqa	XMMWORD PTR old_f$1$[rsp], xmm6
	pblendvb xmm2, xmm11, xmm0
	movdqa	xmm0, xmm15
	psadbw	xmm10, xmm3
	movdqa	xmm15, XMMWORD PTR old_h$1$[rsp]
	movdqa	xmm13, xmm9
	pblendvb xmm15, xmm5, xmm0
	movdqa	xmm0, xmm1
	movdqa	XMMWORD PTR old_h$1$[rsp], xmm5
	pblendvb xmm15, xmm12, xmm0
	movdqa	xmm1, xmm15
	movdqa	xmm15, xmm8

; 989  :       sse_inc = _mm_add_epi64(sse_inc, sad_cd);

	movdqa	xmm3, XMMWORD PTR shufmask1$1$[rsp]
	paddq	xmm13, xmm10
	movdqa	xmm4, XMMWORD PTR move_old_to_b_imask$1$[rsp]
	add	r12d, r9d
	psadbw	xmm11, xmm2
	movdqa	xmm2, xmm7
	psadbw	xmm12, xmm1

; 990  :       sse_inc = _mm_add_epi64(sse_inc, sad_ef);

	paddq	xmm13, xmm11

; 991  :       sse_inc = _mm_add_epi64(sse_inc, sad_gh);

	paddq	xmm13, xmm12
	cmp	r12d, r15d
	jne	$LL10@hor_sad_ss
	movdqa	xmm11, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	xorps	xmm10, xmm10
	movdqa	xmm12, XMMWORD PTR is_left$1$[rsp]
	mov	r8d, DWORD PTR y$1$[rsp]
	mov	r9, QWORD PTR tv1823[rsp]
	mov	r11d, DWORD PTR outside_vecs$1$[rsp]
	mov	ebx, DWORD PTR pic_stride$[rsp]
	mov	edi, DWORD PTR tv1809[rsp]
	mov	rbp, QWORD PTR tv1989[rsp]
$LN52@hor_sad_ss:

; 884  : 
; 885  :   const __m128i left_offsets = _mm_set1_epi8 ((uint8_t)left_offset);
; 886  :   const __m128i is_left      = _mm_cmpeq_epi8(rights, _mm_setzero_si128());
; 887  :   const __m128i vw_for_left  = _mm_and_si128 (is_left, vec_widths);
; 888  : 
; 889  :   // -x == (x ^ 0xff) + 1 = (x ^ 0xff) - 0xff. Also x == (x ^ 0x00) - 0x00.
; 890  :   // in other words, calculate inverse of left_offsets if is_left is true.
; 891  :   const __m128i offs_neg            = _mm_xor_si128 (left_offsets, is_left);
; 892  :   const __m128i offs_for_sm1        = _mm_sub_epi8  (offs_neg,     is_left);
; 893  : 
; 894  :   const __m128i ns_for_sm1          = _mm_or_si128  (vw_for_left,  nslo);
; 895  :   const __m128i shufmask1           = _mm_add_epi8  (ns_for_sm1,   offs_for_sm1);
; 896  : 
; 897  :   const __m128i mo2bmask_l          = _mm_cmpgt_epi8(left_offsets, nslo);
; 898  :   const __m128i mo2bimask_l         = _mm_cmpeq_epi8(mo2bmask_l,   _mm_setzero_si128());
; 899  :   const __m128i mo2bimask_r         = _mm_cmpgt_epi8(vec_widths,   shufmask1);
; 900  :   const __m128i move_old_to_b_imask = _mm_blendv_epi8(mo2bimask_r, mo2bimask_l, is_left);
; 901  : 
; 902  :   const int32_t outvec_offset = (~is_left_bm) & inside_width;
; 903  :   int32_t x, y;
; 904  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	edx, DWORD PTR ref_stride$[rsp]
	add	r8d, 4
	movdqa	xmm3, XMMWORD PTR shufmask1$1$[rsp]
	add	edi, 4
	movdqa	xmm4, XMMWORD PTR move_old_to_b_imask$1$[rsp]
	mov	DWORD PTR y$1$[rsp], r8d
	mov	DWORD PTR tv1809[rsp], edi
	cmp	r8d, DWORD PTR height_fourline_groups$1$[rsp]
	jl	$LL53@hor_sad_ss
	mov	edi, DWORD PTR a_off$1$[rsp]
	mov	r12d, DWORD PTR is_left_bm$1$[rsp]
	mov	r13d, DWORD PTR left$[rsp]
	movaps	xmm15, XMMWORD PTR [rsp+240]
	movaps	xmm9, XMMWORD PTR [rsp+336]
	movaps	xmm8, XMMWORD PTR [rsp+352]
$LN3@hor_sad_ss:

; 992  :     }
; 993  :   }
; 994  :   if (height_residual_lines) {

	cmp	DWORD PTR tv1829[rsp], 0
	je	$LN12@hor_sad_ss

; 995  :     for (; y < height; y++) {

	cmp	r8d, DWORD PTR height$[rsp]
	jge	$LN12@hor_sad_ss
	movdqa	xmm6, XMMWORD PTR shufmask1$1$[rsp]
	and	edi, r12d
	movdqa	xmm7, XMMWORD PTR move_old_to_b_imask$1$[rsp]
	mov	DWORD PTR a_off$1$[rsp], edi
	npad	11
$LL13@hor_sad_ss:

; 996  :       __m128i borderpx_vec = _mm_set1_epi8(ref_data[(int32_t)((y + 0) * ref_stride + border_off)]);

	mov	eax, DWORD PTR border_off$1$[rsp]
	mov	r14d, r8d
	imul	r14d, edx
	add	eax, r14d
	movsxd	rcx, eax
	movsx	eax, BYTE PTR [rcx+rsi]
	movd	xmm5, eax

; 997  :       for (x = 0; x < outside_vecs; x++) {

	xor	eax, eax
	punpcklbw xmm5, xmm5
	punpcklwd xmm5, xmm5
	pshufd	xmm5, xmm5, 0
	test	r11d, r11d
	je	SHORT $LN45@hor_sad_ss

; 998  :         __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + outvec_offset));

	mov	edx, r8d
	imul	edx, ebx
	npad	14
$LL16@hor_sad_ss:
	movsxd	rcx, eax

; 999  : 
; 1000 :         __m128i startoffs  = _mm_set1_epi8  ((x + inside_vecs) << vec_width_log2);
; 1001 :         __m128i ns         = _mm_add_epi8   (startoffs, nslo);
; 1002 : 
; 1003 :         // Unread imask is (is_left NOR unrd_imask_for_right), do the maths etc
; 1004 :         __m128i unrd_imask = _mm_cmpgt_epi8 (blk_widths, ns);

	movdqa	xmm0, xmm14
	shl	rcx, 4
	add	rcx, rdx
	add	rcx, r9
	movdqu	xmm2, XMMWORD PTR [rcx+r10]
	lea	ecx, DWORD PTR [rax+rbp]
	inc	eax
	shl	cl, 4
	movsx	ecx, cl
	movd	xmm1, ecx
	punpcklbw xmm1, xmm1
	punpcklwd xmm1, xmm1
	pshufd	xmm1, xmm1, 0
	paddb	xmm1, xmm11
	pcmpgtb	xmm0, xmm1

; 1005 :                 unrd_imask = _mm_or_si128   (unrd_imask, is_left);
; 1006 :         __m128i unrd_mask  = _mm_cmpeq_epi8 (unrd_imask, _mm_setzero_si128());
; 1007 :         __m128i b_unread   = _mm_blendv_epi8(borderpx_vec, a, unrd_mask);

	movdqa	xmm1, xmm5
	por	xmm0, xmm12
	pcmpeqb	xmm0, xmm10
	pblendvb xmm1, xmm2, xmm0

; 1008 : 
; 1009 :         __m128i sad_ab     = _mm_sad_epu8   (a, b_unread);

	psadbw	xmm2, xmm1

; 1010 :         sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	paddq	xmm2, xmm13
	movdqa	xmm13, xmm2
	cmp	eax, r11d
	jb	SHORT $LL16@hor_sad_ss
$LN45@hor_sad_ss:

; 1014 : 
; 1015 :       __m128i old_b = borderpx_vec;
; 1016 :       for (x = invec_lstart; x != invec_lend; x += invec_linc) {

	mov	edx, DWORD PTR invec_lstart$1$[rsp]
	cmp	edx, r15d
	je	$LN11@hor_sad_ss

; 1011 :       }
; 1012 :       int32_t a_off = outside_width & is_left_bm;
; 1013 :       int32_t leftoff_with_sign_neg = (left_offset ^ is_left_bm) - is_left_bm;

	mov	r9d, DWORD PTR invec_linc$1$[rsp]
	mov	eax, r12d
	xor	eax, r13d
	movsxd	rdi, edi
	sub	eax, r12d

; 1017 :         __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + a_off));

	mov	r13d, r8d
	imul	r13d, ebx
	mov	ebx, DWORD PTR a_off$1$[rsp]

; 1018 :         __m128i b = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 0) * ref_stride + a_off - leftoff_with_sign_neg));

	movsxd	r12, eax
	npad	1
$LL19@hor_sad_ss:

; 1017 :         __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + a_off));

	movsxd	rcx, edx
	lea	rax, QWORD PTR [rdi+r13]
	shl	rcx, 4

; 1019 : 
; 1020 :         __m128i b_shifted    = _mm_shuffle_epi8(b,     shufmask1);
; 1021 :         __m128i b_with_old   = _mm_blendv_epi8 (old_b, b_shifted, move_old_to_b_imask);

	movdqa	xmm0, xmm7
	add	rax, rcx

; 1022 : 
; 1023 :         uint8_t startoff     = (x << vec_width_log2) + a_off;
; 1024 :         __m128i startoffs    = _mm_set1_epi8   (startoff);
; 1025 :         __m128i curr_ns      = _mm_add_epi8    (startoffs,    nslo);
; 1026 :         __m128i unrd_imask   = _mm_cmpgt_epi8  (blk_widths,   curr_ns);

	movdqa	xmm2, xmm14
	movdqu	xmm4, XMMWORD PTR [rax+r10]
	mov	rax, rdi
	sub	rax, r12
	add	rax, r14
	add	rax, rcx
	movdqu	xmm3, XMMWORD PTR [rax+rsi]
	movzx	eax, dl
	add	edx, r9d
	shl	al, 4
	add	al, bl
	movsx	eax, al
	pshufb	xmm3, xmm6
	movd	xmm1, eax
	punpcklbw xmm1, xmm1
	punpcklwd xmm1, xmm1
	pshufd	xmm1, xmm1, 0
	paddb	xmm1, xmm11
	pcmpgtb	xmm2, xmm1

; 1027 :         __m128i unrd_mask    = _mm_cmpeq_epi8  (unrd_imask,   _mm_setzero_si128());

	pcmpeqb	xmm2, xmm10
	pblendvb xmm5, xmm3, xmm0

; 1028 :         __m128i b_unread     = _mm_blendv_epi8 (b_with_old,   a, unrd_mask);

	movdqa	xmm0, xmm2
	pblendvb xmm5, xmm4, xmm0
	movdqa	xmm1, xmm5

; 1029 : 
; 1030 :         old_b = b_shifted;

	movdqa	xmm5, xmm3

; 1031 : 
; 1032 :         __m128i sad_ab     = _mm_sad_epu8(a, b_unread);

	psadbw	xmm4, xmm1

; 1033 :         sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	paddq	xmm4, xmm13
	movdqa	xmm13, xmm4
	cmp	edx, r15d
	jne	$LL19@hor_sad_ss
	mov	r9, QWORD PTR tv1823[rsp]
	mov	ebx, DWORD PTR pic_stride$[rsp]
	mov	edi, DWORD PTR a_off$1$[rsp]
	mov	r12d, DWORD PTR is_left_bm$1$[rsp]
$LN11@hor_sad_ss:

; 995  :     for (; y < height; y++) {

	mov	edx, DWORD PTR ref_stride$[rsp]
	inc	r8d
	mov	r13d, DWORD PTR left$[rsp]
	cmp	r8d, DWORD PTR height$[rsp]
	jl	$LL13@hor_sad_ss
$LN12@hor_sad_ss:

; 1034 :       }
; 1035 :     }
; 1036 :   }
; 1037 :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 1038 :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 1039 :   return _mm_cvtsi128_si32(sad);

	movaps	xmm7, XMMWORD PTR [rsp+368]

; 1040 : }

	lea	r11, QWORD PTR [rsp+400]
	movaps	xmm6, XMMWORD PTR [rsp+384]
	mov	rbx, QWORD PTR [r11+64]
	movaps	xmm10, XMMWORD PTR [r11-80]
	movaps	xmm11, XMMWORD PTR [r11-96]
	movaps	xmm12, XMMWORD PTR [r11-112]
	movaps	xmm14, XMMWORD PTR [r11-144]
	pshufd	xmm0, xmm13, 78				; 0000004eH
	paddq	xmm0, xmm13
	movaps	xmm13, XMMWORD PTR [r11-128]
	movd	eax, xmm0
	mov	rsp, r11
	pop	r15
	pop	r14
	pop	r13
	pop	r12
	pop	rdi
	pop	rsi
	pop	rbp
	ret	0
hor_sad_sse41_arbitrary ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
_TEXT	SEGMENT
tv2411 = 0
tv2406 = 4
tv2405 = 8
tv2506 = 16
data1$ = 128
data2$ = 136
height_fourline_groups$1$ = 144
width$ = 144
height$ = 152
stride1$ = 160
stride2$ = 168
kvz_reg_sad_sse41 PROC

; 49   : {

$LN127:
	push	rbx
	push	rbp
	push	rdi
	sub	rsp, 96					; 00000060H
	mov	ebp, r9d
	mov	rbx, rdx
	mov	rdi, rcx

; 50   :   if (width == 0)

	test	r8d, r8d
	jne	SHORT $LN2@kvz_reg_sa

; 51   :     return 0;

	xor	eax, eax

; 64   : }

	add	rsp, 96					; 00000060H
	pop	rdi
	pop	rbp
	pop	rbx
	ret	0
$LN2@kvz_reg_sa:
	mov	QWORD PTR [rsp+128], rsi
	mov	QWORD PTR [rsp+136], r12
	mov	QWORD PTR [rsp+152], r13
	mov	QWORD PTR [rsp+88], r14
	mov	QWORD PTR [rsp+80], r15

; 52   :   if (width == 4)

	cmp	r8d, 4
	jne	$LN3@kvz_reg_sa
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 60   :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	esi, DWORD PTR stride2$[rsp]
	mov	r12d, ebp
	mov	r15d, DWORD PTR stride1$[rsp]
	mov	r13d, ebp
	and	r12d, -4
	and	r13d, 3
	xor	eax, eax
	xorps	xmm2, xmm2
	test	r12d, r12d
	jle	$LN12@kvz_reg_sa

; 54   :   __m128i sse_inc = _mm_setzero_si128();

	mov	r14d, 2
	npad	3
$LL13@kvz_reg_sa:

; 61   :     __m128i a = _mm_cvtsi32_si128(*(uint32_t *)(data1 + y * stride1));
; 62   :     __m128i b = _mm_cvtsi32_si128(*(uint32_t *)(data2 + y * stride2));
; 63   : 
; 64   :     a = _mm_insert_epi32(a, *(const uint32_t *)(data1 + (y + 1) * stride1), 1);

	lea	r11d, DWORD PTR [r14-1]
	mov	ecx, eax
	imul	ecx, r15d

; 65   :     b = _mm_insert_epi32(b, *(const uint32_t *)(data2 + (y + 1) * stride2), 1);
; 66   :     a = _mm_insert_epi32(a, *(const uint32_t *)(data1 + (y + 2) * stride1), 2);
; 67   :     b = _mm_insert_epi32(b, *(const uint32_t *)(data2 + (y + 2) * stride2), 2);
; 68   :     a = _mm_insert_epi32(a, *(const uint32_t *)(data1 + (y + 3) * stride1), 3);

	lea	r10d, DWORD PTR [r14+1]
	mov	r8d, r14d
	mov	edx, r11d
	imul	r8d, r15d
	mov	r9d, r10d
	imul	edx, r15d
	imul	r11d, esi
	imul	r9d, r15d

; 69   :     b = _mm_insert_epi32(b, *(const uint32_t *)(data2 + (y + 3) * stride2), 3);

	imul	r10d, esi
	movd	xmm1, DWORD PTR [rcx+rdi]
	mov	ecx, eax
	imul	ecx, esi
	add	eax, 4
	pinsrd	xmm1, DWORD PTR [rdx+rdi], 1
	pinsrd	xmm1, DWORD PTR [r8+rdi], 2
	pinsrd	xmm1, DWORD PTR [r9+rdi], 3
	mov	r8d, r14d
	add	r14d, 4
	imul	r8d, esi
	movd	xmm0, DWORD PTR [rcx+rbx]
	pinsrd	xmm0, DWORD PTR [r11+rbx], 1
	pinsrd	xmm0, DWORD PTR [r8+rbx], 2
	pinsrd	xmm0, DWORD PTR [r10+rbx], 3

; 70   : 
; 71   :     __m128i curr_sads = _mm_sad_epu8(a, b);

	psadbw	xmm1, xmm0

; 72   :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm1, xmm2
	movdqa	xmm2, xmm1
	cmp	eax, r12d
	jl	$LL13@kvz_reg_sa
$LN12@kvz_reg_sa:

; 73   :   }
; 74   :   if (height_residual_lines) {

	test	r13d, r13d
	je	SHORT $LN15@kvz_reg_sa

; 75   :     for (; y < height; y++) {

	cmp	eax, ebp
	jge	SHORT $LN15@kvz_reg_sa
	npad	11
$LL16@kvz_reg_sa:

; 76   :       __m128i a = _mm_cvtsi32_si128(*(const uint32_t *)(data1 + y * stride1));

	mov	ecx, eax
	imul	ecx, r15d
	movd	xmm1, DWORD PTR [rcx+rdi]

; 77   :       __m128i b = _mm_cvtsi32_si128(*(const uint32_t *)(data2 + y * stride2));

	mov	ecx, eax
	imul	ecx, esi
	inc	eax
	movd	xmm0, DWORD PTR [rcx+rbx]

; 78   : 
; 79   :       __m128i curr_sads = _mm_sad_epu8(a, b);

	psadbw	xmm1, xmm0

; 80   :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm1, xmm2
	movdqa	xmm2, xmm1
	cmp	eax, ebp
	jl	SHORT $LL16@kvz_reg_sa
$LN15@kvz_reg_sa:

; 81   :     }
; 82   :   }
; 83   :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	pshufd	xmm0, xmm2, 78				; 0000004eH

; 84   :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	paddq	xmm0, xmm2
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c

; 53   :     return reg_sad_w4(data1, data2, height, stride1, stride2);

	jmp	$LN125@kvz_reg_sa
$LN3@kvz_reg_sa:

; 54   :   if (width == 8)

	cmp	r8d, 8
	jne	$LN4@kvz_reg_sa
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 99   :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r9d, DWORD PTR stride2$[rsp]
	mov	r11d, ebp
	mov	r10d, DWORD PTR stride1$[rsp]
	mov	esi, ebp
	and	r11d, -4
	and	esi, 3
	xor	eax, eax
	xorps	xmm4, xmm4
	test	r11d, r11d
	jle	$LN21@kvz_reg_sa
	xorps	xmm5, xmm5

; 85   : 
; 86   :   return _mm_cvtsi128_si32(sad);
; 87   : }
; 88   : 
; 89   : static INLINE uint32_t reg_sad_w8(const uint8_t * const data1, const uint8_t * const data2,
; 90   :                            const int32_t height, const uint32_t stride1,
; 91   :                            const uint32_t stride2)
; 92   : {
; 93   :   __m128i sse_inc = _mm_setzero_si128();

	mov	r14d, 2
	npad	14
$LL22@kvz_reg_sa:

; 100  :     __m128d a_d = _mm_setzero_pd();
; 101  :     __m128d b_d = _mm_setzero_pd();
; 102  :     __m128d c_d = _mm_setzero_pd();
; 103  :     __m128d d_d = _mm_setzero_pd();
; 104  : 
; 105  :     a_d = _mm_loadl_pd(a_d, (const double *)(data1 + (y + 0) * stride1));

	mov	ecx, eax

; 106  :     b_d = _mm_loadl_pd(b_d, (const double *)(data2 + (y + 0) * stride2));
; 107  :     a_d = _mm_loadh_pd(a_d, (const double *)(data1 + (y + 1) * stride1));

	lea	r8d, DWORD PTR [r14-1]
	imul	ecx, r10d
	movaps	xmm3, xmm5
	movaps	xmm2, xmm5

; 108  :     b_d = _mm_loadh_pd(b_d, (const double *)(data2 + (y + 1) * stride2));
; 109  : 
; 110  :     c_d = _mm_loadl_pd(c_d, (const double *)(data1 + (y + 2) * stride1));

	movaps	xmm1, xmm5

; 111  :     d_d = _mm_loadl_pd(d_d, (const double *)(data2 + (y + 2) * stride2));

	movaps	xmm0, xmm5
	movlpd	xmm3, QWORD PTR [rcx+rdi]
	mov	ecx, r8d
	imul	ecx, r10d
	imul	r8d, r9d
	movhpd	xmm3, QWORD PTR [rcx+rdi]
	mov	ecx, eax
	movhpd	xmm2, QWORD PTR [r8+rdx]
	add	eax, 4
	imul	ecx, r9d

; 112  :     c_d = _mm_loadh_pd(c_d, (const double *)(data1 + (y + 3) * stride1));

	lea	r8d, DWORD PTR [r14+1]
	movlpd	xmm2, QWORD PTR [rcx+rdx]
	mov	ecx, r14d
	imul	ecx, r10d

; 113  :     d_d = _mm_loadh_pd(d_d, (const double *)(data2 + (y + 3) * stride2));
; 114  : 
; 115  :     __m128i a = _mm_castpd_si128(a_d);
; 116  :     __m128i b = _mm_castpd_si128(b_d);
; 117  :     __m128i c = _mm_castpd_si128(c_d);
; 118  :     __m128i d = _mm_castpd_si128(d_d);
; 119  : 
; 120  :     __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	psadbw	xmm3, xmm2

; 121  :     __m128i curr_sads_cd = _mm_sad_epu8(c, d);
; 122  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm3, xmm4
	movdqa	xmm4, xmm3
	movlpd	xmm1, QWORD PTR [rcx+rdi]
	mov	ecx, r8d
	imul	ecx, r10d
	imul	r8d, r9d
	movhpd	xmm1, QWORD PTR [rcx+rdi]
	mov	ecx, r14d
	movhpd	xmm0, QWORD PTR [r8+rdx]
	add	r14d, 4
	imul	ecx, r9d
	movlpd	xmm0, QWORD PTR [rcx+rdx]
	psadbw	xmm1, xmm0

; 123  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	paddq	xmm4, xmm1
	cmp	eax, r11d
	jl	$LL22@kvz_reg_sa
$LN21@kvz_reg_sa:

; 124  :   }
; 125  :   if (height_residual_lines) {

	test	esi, esi
	je	SHORT $LN24@kvz_reg_sa

; 126  :     for (; y < height; y++) {

	cmp	eax, ebp
	jge	SHORT $LN24@kvz_reg_sa
	npad	6
$LL25@kvz_reg_sa:

; 127  :       __m128i a = _mm_loadl_epi64((__m128i *)(data1 + y * stride1));

	mov	ecx, eax
	imul	ecx, r10d
	movq	xmm1, QWORD PTR [rcx+rdi]

; 128  :       __m128i b = _mm_loadl_epi64((__m128i *)(data2 + y * stride2));

	mov	ecx, eax
	imul	ecx, r9d
	inc	eax
	movq	xmm0, QWORD PTR [rcx+rdx]

; 129  : 
; 130  :       __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	psadbw	xmm1, xmm0

; 131  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm1, xmm4
	movdqa	xmm4, xmm1
	cmp	eax, ebp
	jl	SHORT $LL25@kvz_reg_sa
$LN24@kvz_reg_sa:

; 132  :     }
; 133  :   }
; 134  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	pshufd	xmm0, xmm4, 78				; 0000004eH

; 135  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	paddq	xmm0, xmm4
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c

; 55   :     return reg_sad_w8(data1, data2, height, stride1, stride2);

	jmp	$LN125@kvz_reg_sa
$LN4@kvz_reg_sa:

; 56   :   if (width == 12)

	cmp	r8d, 12
	jne	SHORT $LN5@kvz_reg_sa
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 144  :   __m128i sse_inc = _mm_setzero_si128();

	xor	eax, eax
	xorps	xmm3, xmm3

; 145  :   int32_t y;
; 146  :   for (y = 0; y < height; y++) {

	test	ebp, ebp
	jle	SHORT $LN30@kvz_reg_sa
	mov	edx, DWORD PTR stride2$[rsp]
	mov	r8d, DWORD PTR stride1$[rsp]
	npad	12
$LL31@kvz_reg_sa:

; 147  :     __m128i a = _mm_loadu_si128((const __m128i *)(data1 + y * stride1));

	mov	ecx, eax
	imul	ecx, r8d
	movdqu	xmm2, XMMWORD PTR [rcx+rdi]

; 148  :     __m128i b = _mm_loadu_si128((const __m128i *)(data2 + y * stride2));

	mov	ecx, eax
	inc	eax
	imul	ecx, edx

; 149  : 
; 150  :     __m128i b_masked  = _mm_blend_epi16(a, b, 0x3f);

	movdqa	xmm1, xmm2
	movdqu	xmm0, XMMWORD PTR [rcx+rbx]
	pblendw	xmm1, xmm0, 63				; 0000003fH

; 151  :     __m128i curr_sads = _mm_sad_epu8   (a, b_masked);

	psadbw	xmm2, xmm1

; 152  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm2, xmm3
	movdqa	xmm3, xmm2
	cmp	eax, ebp
	jl	SHORT $LL31@kvz_reg_sa
$LN30@kvz_reg_sa:

; 153  :   }
; 154  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	pshufd	xmm0, xmm3, 78				; 0000004eH

; 155  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	paddq	xmm0, xmm3
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c

; 57   :     return reg_sad_w12(data1, data2, height, stride1, stride2);

	jmp	$LN125@kvz_reg_sa
$LN5@kvz_reg_sa:

; 58   :   if (width == 16)

	cmp	r8d, 16
	jne	$LN6@kvz_reg_sa
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 169  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r11d, DWORD PTR stride2$[rsp]
	mov	r15d, ebp
	mov	esi, DWORD PTR stride1$[rsp]
	mov	r12d, ebp
	and	r15d, -4
	and	r12d, 3
	xor	eax, eax
	xorps	xmm2, xmm2
	test	r15d, r15d
	jle	$LN35@kvz_reg_sa

; 156  :   return _mm_cvtsi128_si32(sad);
; 157  : }
; 158  : 
; 159  : static INLINE uint32_t reg_sad_w16(const uint8_t * const data1, const uint8_t * const data2,
; 160  :                             const int32_t height, const uint32_t stride1,
; 161  :                             const uint32_t stride2)
; 162  : {
; 163  :   __m128i sse_inc = _mm_setzero_si128();

	mov	r14d, 2
	npad	6
$LL36@kvz_reg_sa:

; 170  :     __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1));
; 171  :     __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2));

	mov	ecx, eax

; 172  :     __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1));

	lea	r9d, DWORD PTR [r14-1]
	imul	ecx, r11d

; 173  :     __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2));
; 174  :     __m128i e = _mm_loadu_si128((const __m128i *)(data1 + (y + 2) * stride1));
; 175  :     __m128i f = _mm_loadu_si128((const __m128i *)(data2 + (y + 2) * stride2));
; 176  :     __m128i g = _mm_loadu_si128((const __m128i *)(data1 + (y + 3) * stride1));

	lea	r10d, DWORD PTR [r14+1]
	mov	r8d, eax
	add	eax, 4
	imul	r8d, esi

; 177  :     __m128i h = _mm_loadu_si128((const __m128i *)(data2 + (y + 3) * stride2));
; 178  : 
; 179  :     __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	movdqu	xmm0, XMMWORD PTR [rcx+rdx]
	mov	ecx, r9d
	imul	r9d, r11d
	movdqu	xmm1, XMMWORD PTR [r8+rdi]
	mov	r8d, r14d
	imul	ecx, esi
	psadbw	xmm1, xmm0
	imul	r8d, esi

; 180  :     __m128i curr_sads_cd = _mm_sad_epu8(c, d);

	movdqu	xmm0, XMMWORD PTR [r9+rdx]

; 181  :     __m128i curr_sads_ef = _mm_sad_epu8(e, f);
; 182  :     __m128i curr_sads_gh = _mm_sad_epu8(g, h);
; 183  : 
; 184  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm1, xmm2
	movdqa	xmm2, xmm1
	movdqu	xmm1, XMMWORD PTR [rcx+rdi]
	mov	ecx, r14d
	add	r14d, 4
	imul	ecx, r11d
	psadbw	xmm1, xmm0

; 185  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	paddq	xmm2, xmm1
	movdqu	xmm1, XMMWORD PTR [r8+rdi]
	movdqu	xmm0, XMMWORD PTR [rcx+rdx]
	mov	ecx, r10d
	imul	r10d, r11d
	psadbw	xmm1, xmm0
	imul	ecx, esi

; 186  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	paddq	xmm2, xmm1
	movdqu	xmm0, XMMWORD PTR [r10+rdx]
	movdqu	xmm1, XMMWORD PTR [rcx+rdi]
	psadbw	xmm1, xmm0

; 187  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	paddq	xmm2, xmm1
	cmp	eax, r15d
	jl	$LL36@kvz_reg_sa
$LN35@kvz_reg_sa:

; 188  :   }
; 189  :   if (height_residual_lines) {

	test	r12d, r12d
	je	$LN15@kvz_reg_sa

; 190  :     for (; y < height; y++) {

	cmp	eax, ebp
	jge	$LN15@kvz_reg_sa
	npad	8
$LL39@kvz_reg_sa:

; 191  :       __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1));

	mov	r8d, eax

; 192  :       __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2));

	mov	ecx, eax
	imul	r8d, esi
	inc	eax
	imul	ecx, r11d

; 193  : 
; 194  :       __m128i curr_sads = _mm_sad_epu8(a, b);

	movdqu	xmm1, XMMWORD PTR [r8+rdi]
	movdqu	xmm0, XMMWORD PTR [rcx+rdx]
	psadbw	xmm1, xmm0

; 195  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm1, xmm2
	movdqa	xmm2, xmm1
	cmp	eax, ebp
	jl	SHORT $LL39@kvz_reg_sa

; 196  :     }
; 197  :   }
; 198  : 
; 199  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	pshufd	xmm0, xmm1, 78				; 0000004eH

; 200  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	paddq	xmm0, xmm1
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c

; 59   :     return reg_sad_w16(data1, data2, height, stride1, stride2);

	jmp	$LN125@kvz_reg_sa
$LN6@kvz_reg_sa:

; 60   :   if (width == 24)

	mov	r11d, ebp
	cmp	r8d, 24
	jne	$LN7@kvz_reg_sa
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 214  :   for (y = 0; y < height_doublelines; y += 2) {

	mov	esi, DWORD PTR stride2$[rsp]
	and	r11d, -2
	mov	r14d, DWORD PTR stride1$[rsp]
	and	ebp, 1
	xor	eax, eax
	xorps	xmm4, xmm4
	test	r11d, r11d
	jle	$LN44@kvz_reg_sa
	xorps	xmm5, xmm5
	npad	4
$LL45@kvz_reg_sa:

; 215  :     __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1));
; 216  :     __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2));
; 217  :     __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1));
; 218  :     __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2));
; 219  : 
; 220  :     __m128d e_d = _mm_setzero_pd();
; 221  :     __m128d f_d = _mm_setzero_pd();
; 222  : 
; 223  :     e_d = _mm_loadl_pd(e_d, (const double *)(data1 + (y + 0) * stride1 + 16));
; 224  :     f_d = _mm_loadl_pd(f_d, (const double *)(data2 + (y + 0) * stride2 + 16));
; 225  :     e_d = _mm_loadh_pd(e_d, (const double *)(data1 + (y + 1) * stride1 + 16));

	lea	r8d, DWORD PTR [rax+1]
	mov	r9d, eax
	mov	ecx, r8d
	imul	r9d, r14d
	imul	ecx, r14d
	movaps	xmm3, xmm5

; 226  :     f_d = _mm_loadh_pd(f_d, (const double *)(data2 + (y + 1) * stride2 + 16));

	imul	r8d, esi
	movaps	xmm2, xmm5
	mov	r10d, ecx

; 227  : 
; 228  :     __m128i e = _mm_castpd_si128(e_d);
; 229  :     __m128i f = _mm_castpd_si128(f_d);
; 230  : 
; 231  :     __m128i curr_sads_1 = _mm_sad_epu8(a, b);

	movdqu	xmm1, XMMWORD PTR [r9+rdi]
	movhpd	xmm3, QWORD PTR [rcx+rdi+16]
	mov	ecx, eax
	movlpd	xmm3, QWORD PTR [r9+rdi+16]
	add	eax, 2
	movhpd	xmm2, QWORD PTR [r8+rdx+16]
	imul	ecx, esi
	movlpd	xmm2, QWORD PTR [rcx+rdx+16]

; 232  :     __m128i curr_sads_2 = _mm_sad_epu8(c, d);
; 233  :     __m128i curr_sads_3 = _mm_sad_epu8(e, f);

	psadbw	xmm3, xmm2
	movdqu	xmm0, XMMWORD PTR [rcx+rdx]
	psadbw	xmm1, xmm0
	movdqu	xmm0, XMMWORD PTR [r8+rdx]

; 234  : 
; 235  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_1);

	paddq	xmm1, xmm4
	movdqa	xmm4, xmm1
	movdqu	xmm1, XMMWORD PTR [r10+rdi]
	psadbw	xmm1, xmm0

; 236  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_2);

	paddq	xmm4, xmm1

; 237  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_3);

	paddq	xmm4, xmm3
	cmp	eax, r11d
	jl	SHORT $LL45@kvz_reg_sa
$LN44@kvz_reg_sa:

; 238  :   }
; 239  :   if (height_parity) {

	test	ebp, ebp
	je	SHORT $LN46@kvz_reg_sa

; 240  :     __m128i a = _mm_loadu_si128   ((const __m128i *)(data1 + y * stride1));
; 241  :     __m128i b = _mm_loadu_si128   ((const __m128i *)(data2 + y * stride2));
; 242  :     __m128i c = _mm_loadl_epi64   ((const __m128i *)(data1 + y * stride1 + 16));

	mov	ecx, eax
	imul	ecx, r14d

; 243  :     __m128i d = _mm_loadl_epi64   ((const __m128i *)(data2 + y * stride2 + 16));

	imul	eax, esi
	movq	xmm2, QWORD PTR [rcx+rdi+16]
	movq	xmm0, QWORD PTR [rax+rdx+16]

; 244  : 
; 245  :     __m128i curr_sads_1 = _mm_sad_epu8(a, b);

	movdqu	xmm1, XMMWORD PTR [rcx+rdi]

; 246  :     __m128i curr_sads_2 = _mm_sad_epu8(c, d);

	psadbw	xmm2, xmm0
	movdqu	xmm0, XMMWORD PTR [rax+rdx]
	psadbw	xmm1, xmm0

; 247  : 
; 248  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_1);

	paddq	xmm1, xmm4

; 249  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_2);

	movdqa	xmm4, xmm2
	paddq	xmm4, xmm1
$LN46@kvz_reg_sa:

; 250  :   }
; 251  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	pshufd	xmm0, xmm4, 78				; 0000004eH

; 252  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	paddq	xmm0, xmm4
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c

; 61   :     return reg_sad_w24(data1, data2, height, stride1, stride2);

	jmp	$LN125@kvz_reg_sa
$LN7@kvz_reg_sa:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 264  :   const int32_t width_xmms             = width  & ~15;

	mov	r15d, DWORD PTR stride2$[rsp]

; 265  :   const int32_t width_residual_pixels  = width  &  15;

	mov	eax, r8d
	mov	r12d, DWORD PTR stride1$[rsp]
	and	eax, 15
	mov	DWORD PTR tv2406[rsp], eax

; 268  :   const int32_t height_residual_lines  = height &  3;

	and	r11d, 3

; 269  : 
; 270  :   const __m128i rds    = _mm_set1_epi8 (width_residual_pixels);

	movsx	eax, al
	mov	r13d, ebp
	movaps	XMMWORD PTR [rsp+48], xmm7

; 266  : 
; 267  :   const int32_t height_fourline_groups = height & ~3;

	and	r13d, -4
	movaps	XMMWORD PTR [rsp+32], xmm8
	xorps	xmm7, xmm7

; 264  :   const int32_t width_xmms             = width  & ~15;

	movsxd	r9, r8d

; 269  : 
; 270  :   const __m128i rds    = _mm_set1_epi8 (width_residual_pixels);

	movd	xmm8, eax

; 271  :   const __m128i ns     = _mm_setr_epi8 (0,  1,  2,  3,  4,  5,  6,  7,
; 272  :                                         8,  9,  10, 11, 12, 13, 14, 15);
; 273  :   const __m128i rdmask = _mm_cmpgt_epi8(rds, ns);
; 274  : 
; 275  :   for (x = 0; x < width_xmms; x += 16) {

	xor	eax, eax
	punpcklbw xmm8, xmm8
	and	r9, -16
	punpcklwd xmm8, xmm8
	mov	ecx, eax
	pshufd	xmm8, xmm8, 0
	pcmpgtb	xmm8, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	lea	r14d, QWORD PTR [rax+2]
	mov	DWORD PTR height_fourline_groups$1$[rsp], r13d
	mov	DWORD PTR tv2411[rsp], r11d
	mov	QWORD PTR tv2405[rsp], r9
	jle	$LN50@kvz_reg_sa

; 295  :     }
; 296  :     if (height_residual_lines) {

	lea	rcx, QWORD PTR [r9-1]
	mov	esi, eax
	shr	rcx, 4
	inc	ecx
	shl	ecx, 4
	mov	QWORD PTR tv2506[rsp], rcx
	npad	4
$LL51@kvz_reg_sa:

; 276  :     for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r11d, eax
	test	r13d, r13d
	jle	$LN53@kvz_reg_sa
	mov	r13d, r14d
	mov	r14d, DWORD PTR height_fourline_groups$1$[rsp]
	npad	9
$LL54@kvz_reg_sa:

; 277  :       __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1 + x));
; 278  :       __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2 + x));

	mov	edx, r11d

; 279  :       __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1 + x));

	lea	r9d, DWORD PTR [r13-1]
	imul	edx, r15d

; 280  :       __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2 + x));
; 281  :       __m128i e = _mm_loadu_si128((const __m128i *)(data1 + (y + 2) * stride1 + x));
; 282  :       __m128i f = _mm_loadu_si128((const __m128i *)(data2 + (y + 2) * stride2 + x));
; 283  :       __m128i g = _mm_loadu_si128((const __m128i *)(data1 + (y + 3) * stride1 + x));

	lea	r10d, DWORD PTR [r13+1]
	mov	r8d, r11d
	add	r11d, 4
	imul	r8d, r12d
	add	rdx, rsi
	add	r8, rsi

; 284  :       __m128i h = _mm_loadu_si128((const __m128i *)(data2 + (y + 3) * stride2 + x));
; 285  : 
; 286  :       __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	movdqu	xmm0, XMMWORD PTR [rdx+rbx]
	mov	edx, r9d
	imul	r9d, r15d
	movdqu	xmm1, XMMWORD PTR [r8+rdi]
	mov	r8d, r13d
	imul	edx, r12d
	psadbw	xmm1, xmm0
	imul	r8d, r12d
	mov	ecx, r9d
	add	rcx, rsi
	add	rdx, rsi

; 287  :       __m128i curr_sads_cd = _mm_sad_epu8(c, d);
; 288  :       __m128i curr_sads_ef = _mm_sad_epu8(e, f);
; 289  :       __m128i curr_sads_gh = _mm_sad_epu8(g, h);
; 290  : 
; 291  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm1, xmm7
	movdqa	xmm7, xmm1
	add	r8, rsi
	movdqu	xmm0, XMMWORD PTR [rcx+rbx]
	movdqu	xmm1, XMMWORD PTR [rdx+rdi]
	mov	edx, r13d
	add	r13d, 4
	imul	edx, r15d
	psadbw	xmm1, xmm0
	add	rdx, rsi

; 292  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	paddq	xmm7, xmm1
	movdqu	xmm1, XMMWORD PTR [r8+rdi]
	movdqu	xmm0, XMMWORD PTR [rdx+rbx]
	mov	edx, r10d
	imul	r10d, r15d
	psadbw	xmm1, xmm0
	imul	edx, r12d
	mov	ecx, r10d

; 293  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	paddq	xmm7, xmm1
	add	rcx, rsi
	add	rdx, rsi
	movdqu	xmm0, XMMWORD PTR [rcx+rbx]
	movdqu	xmm1, XMMWORD PTR [rdx+rdi]
	psadbw	xmm1, xmm0

; 294  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	paddq	xmm7, xmm1
	cmp	r11d, r14d
	jl	$LL54@kvz_reg_sa
	mov	r13d, DWORD PTR height_fourline_groups$1$[rsp]
	mov	r14d, 2
	mov	r9, QWORD PTR tv2405[rsp]
$LN53@kvz_reg_sa:

; 295  :     }
; 296  :     if (height_residual_lines) {

	cmp	DWORD PTR tv2411[rsp], eax
	je	SHORT $LN49@kvz_reg_sa

; 297  :       for (; y < height; y++) {

	cmp	r11d, ebp
	jge	SHORT $LN49@kvz_reg_sa
	npad	12
$LL57@kvz_reg_sa:

; 298  :         __m128i a = _mm_loadu_si128((const __m128i *)(data1 + y * stride1 + x));

	mov	r8d, r11d

; 299  :         __m128i b = _mm_loadu_si128((const __m128i *)(data2 + y * stride2 + x));

	mov	edx, r11d
	imul	r8d, r12d
	inc	r11d
	imul	edx, r15d
	add	r8, rsi
	add	rdx, rsi

; 300  : 
; 301  :         __m128i curr_sads = _mm_sad_epu8(a, b);

	movdqu	xmm1, XMMWORD PTR [r8+rdi]
	movdqu	xmm0, XMMWORD PTR [rdx+rbx]
	psadbw	xmm1, xmm0

; 302  : 
; 303  :         sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm1, xmm7
	movdqa	xmm7, xmm1
	cmp	r11d, ebp
	jl	SHORT $LL57@kvz_reg_sa
$LN49@kvz_reg_sa:

; 271  :   const __m128i ns     = _mm_setr_epi8 (0,  1,  2,  3,  4,  5,  6,  7,
; 272  :                                         8,  9,  10, 11, 12, 13, 14, 15);
; 273  :   const __m128i rdmask = _mm_cmpgt_epi8(rds, ns);
; 274  : 
; 275  :   for (x = 0; x < width_xmms; x += 16) {

	add	rsi, 16
	cmp	rsi, r9
	jl	$LL51@kvz_reg_sa
	mov	rcx, QWORD PTR tv2506[rsp]
	mov	r11d, DWORD PTR tv2411[rsp]
$LN50@kvz_reg_sa:

; 304  :       }
; 305  :     }
; 306  :   }
; 307  : 
; 308  :   if (width_residual_pixels) {

	cmp	DWORD PTR tv2406[rsp], eax
	je	$LN62@kvz_reg_sa

; 309  :     for (y = 0; y < height_fourline_groups; y += 4) {

	movsxd	r10, ecx
	test	r13d, r13d
	jle	$LN59@kvz_reg_sa
	movaps	XMMWORD PTR [rsp+64], xmm6
	npad	12
$LL60@kvz_reg_sa:

; 310  :       __m128i a = _mm_loadu_si128((const __m128i *)(data1 + (y + 0) * stride1 + x));

	mov	edx, eax

; 311  :       __m128i b = _mm_loadu_si128((const __m128i *)(data2 + (y + 0) * stride2 + x));
; 312  :       __m128i c = _mm_loadu_si128((const __m128i *)(data1 + (y + 1) * stride1 + x));

	lea	r8d, DWORD PTR [r14-1]
	imul	edx, r12d

; 313  :       __m128i d = _mm_loadu_si128((const __m128i *)(data2 + (y + 1) * stride2 + x));
; 314  :       __m128i e = _mm_loadu_si128((const __m128i *)(data1 + (y + 2) * stride1 + x));
; 315  :       __m128i f = _mm_loadu_si128((const __m128i *)(data2 + (y + 2) * stride2 + x));
; 316  :       __m128i g = _mm_loadu_si128((const __m128i *)(data1 + (y + 3) * stride1 + x));

	lea	r9d, DWORD PTR [r14+1]

; 317  :       __m128i h = _mm_loadu_si128((const __m128i *)(data2 + (y + 3) * stride2 + x));
; 318  : 
; 319  :       __m128i b_masked     = _mm_blendv_epi8(a, b, rdmask);

	movdqa	xmm0, xmm8
	add	rdx, r10
	movdqu	xmm3, XMMWORD PTR [rdx+rdi]
	mov	edx, r8d
	imul	r8d, r15d
	imul	edx, r12d
	movdqa	xmm2, xmm3
	mov	ecx, r8d
	add	rdx, r10
	add	rcx, r10
	movdqu	xmm4, XMMWORD PTR [rdx+rdi]
	mov	edx, r14d
	imul	edx, r12d
	add	rdx, r10
	movdqu	xmm5, XMMWORD PTR [rdx+rdi]
	mov	edx, r9d
	imul	edx, r12d
	imul	r9d, r15d
	add	rdx, r10
	movdqu	xmm6, XMMWORD PTR [rdx+rdi]
	mov	edx, eax
	add	eax, 4
	imul	edx, r15d
	add	rdx, r10
	movdqu	xmm1, XMMWORD PTR [rdx+rbx]
	mov	edx, r14d
	add	r14d, 4
	imul	edx, r15d
	pblendvb xmm2, xmm1, xmm0

; 320  :       __m128i d_masked     = _mm_blendv_epi8(c, d, rdmask);

	movdqu	xmm1, XMMWORD PTR [rcx+rbx]
	mov	ecx, r9d

; 321  :       __m128i f_masked     = _mm_blendv_epi8(e, f, rdmask);
; 322  :       __m128i h_masked     = _mm_blendv_epi8(g, h, rdmask);
; 323  : 
; 324  :       __m128i curr_sads_ab = _mm_sad_epu8   (a, b_masked);

	psadbw	xmm3, xmm2
	movdqa	xmm2, xmm4
	add	rdx, r10
	add	rcx, r10
	pblendvb xmm2, xmm1, xmm0

; 325  :       __m128i curr_sads_cd = _mm_sad_epu8   (c, d_masked);

	psadbw	xmm4, xmm2
	movdqa	xmm2, xmm5

; 326  :       __m128i curr_sads_ef = _mm_sad_epu8   (e, f_masked);
; 327  :       __m128i curr_sads_gh = _mm_sad_epu8   (g, h_masked);
; 328  : 
; 329  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm3, xmm7
	movdqa	xmm7, xmm3
	movdqu	xmm1, XMMWORD PTR [rdx+rbx]

; 330  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	paddq	xmm7, xmm4
	pblendvb xmm2, xmm1, xmm0
	psadbw	xmm5, xmm2
	movdqa	xmm2, xmm6
	movdqu	xmm1, XMMWORD PTR [rcx+rbx]
	pblendvb xmm2, xmm1, xmm0
	psadbw	xmm6, xmm2

; 331  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	paddq	xmm7, xmm5

; 332  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	paddq	xmm7, xmm6
	cmp	eax, r13d
	jl	$LL60@kvz_reg_sa

; 309  :     for (y = 0; y < height_fourline_groups; y += 4) {

	movaps	xmm6, XMMWORD PTR [rsp+64]
$LN59@kvz_reg_sa:

; 333  :     }
; 334  :     if (height_residual_lines) {

	test	r11d, r11d
	je	SHORT $LN62@kvz_reg_sa

; 335  :       for (; y < height; y++) {

	cmp	eax, ebp
	jge	SHORT $LN62@kvz_reg_sa
	npad	7
$LL63@kvz_reg_sa:

; 336  :         __m128i a = _mm_loadu_si128((const __m128i *)(data1 + y * stride1 + x));

	mov	edx, eax

; 337  :         __m128i b = _mm_loadu_si128((const __m128i *)(data2 + y * stride2 + x));
; 338  : 
; 339  :         __m128i b_masked  = _mm_blendv_epi8(a, b, rdmask);

	movdqa	xmm0, xmm8
	imul	edx, r12d
	add	rdx, r10
	movdqu	xmm3, XMMWORD PTR [rdx+rdi]
	mov	edx, eax
	inc	eax
	imul	edx, r15d
	movdqa	xmm2, xmm3
	add	rdx, r10
	movdqu	xmm1, XMMWORD PTR [rdx+rbx]
	pblendvb xmm2, xmm1, xmm0

; 340  :         __m128i curr_sads = _mm_sad_epu8   (a, b_masked);

	psadbw	xmm3, xmm2

; 341  : 
; 342  :         sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm3, xmm7
	movdqa	xmm7, xmm3
	cmp	eax, ebp
	jl	SHORT $LL63@kvz_reg_sa
$LN62@kvz_reg_sa:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c

; 63   :     return reg_sad_arbitrary(data1, data2, width, height, stride1, stride2);

	movaps	xmm8, XMMWORD PTR [rsp+32]
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 346  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	pshufd	xmm0, xmm7, 78				; 0000004eH

; 347  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	paddq	xmm0, xmm7
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c

; 63   :     return reg_sad_arbitrary(data1, data2, width, height, stride1, stride2);

	movaps	xmm7, XMMWORD PTR [rsp+48]
$LN125@kvz_reg_sa:
	mov	r14, QWORD PTR [rsp+88]
	mov	r13, QWORD PTR [rsp+152]
	mov	r12, QWORD PTR [rsp+136]
	mov	rsi, QWORD PTR [rsp+128]
	mov	r15, QWORD PTR [rsp+80]
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 349  :   return _mm_cvtsi128_si32(sad);

	movd	eax, xmm0
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c

; 64   : }

	add	rsp, 96					; 00000060H
	pop	rdi
	pop	rbp
	pop	rbx
	ret	0
kvz_reg_sad_sse41 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
_TEXT	SEGMENT
width$ = 8
get_optimized_sad_sse41 PROC

; 68   :   if (width == 0)

	test	ecx, ecx
	jne	SHORT $LN2@get_optimi

; 69   :     return reg_sad_w0;

	lea	rax, OFFSET FLAT:reg_sad_w0

; 79   :     return reg_sad_w24;
; 80   :   else
; 81   :     return NULL;
; 82   : }

	ret	0
$LN2@get_optimi:

; 70   :   if (width == 4)

	cmp	ecx, 4
	jne	SHORT $LN3@get_optimi

; 71   :     return reg_sad_w4;

	lea	rax, OFFSET FLAT:reg_sad_w4

; 79   :     return reg_sad_w24;
; 80   :   else
; 81   :     return NULL;
; 82   : }

	ret	0
$LN3@get_optimi:

; 72   :   if (width == 8)

	cmp	ecx, 8
	jne	SHORT $LN4@get_optimi

; 73   :     return reg_sad_w8;

	lea	rax, OFFSET FLAT:reg_sad_w8

; 79   :     return reg_sad_w24;
; 80   :   else
; 81   :     return NULL;
; 82   : }

	ret	0
$LN4@get_optimi:

; 74   :   if (width == 12)

	cmp	ecx, 12
	jne	SHORT $LN5@get_optimi

; 75   :     return reg_sad_w12;

	lea	rax, OFFSET FLAT:reg_sad_w12

; 79   :     return reg_sad_w24;
; 80   :   else
; 81   :     return NULL;
; 82   : }

	ret	0
$LN5@get_optimi:

; 76   :   if (width == 16)

	cmp	ecx, 16
	jne	SHORT $LN6@get_optimi

; 77   :     return reg_sad_w16;

	lea	rax, OFFSET FLAT:reg_sad_w16

; 79   :     return reg_sad_w24;
; 80   :   else
; 81   :     return NULL;
; 82   : }

	ret	0
$LN6@get_optimi:

; 78   :   if (width == 24)

	xor	eax, eax
	lea	rdx, OFFSET FLAT:reg_sad_w24
	cmp	ecx, 24
	cmove	rax, rdx

; 79   :     return reg_sad_w24;
; 80   :   else
; 81   :     return NULL;
; 82   : }

	ret	0
get_optimized_sad_sse41 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
_TEXT	SEGMENT
pic_data$ = 64
ref_data$ = 72
width$ = 80
height$ = 88
stride$ = 96
ver_sad_sse41 PROC

; 86   : {

	sub	rsp, 56					; 00000038H
	mov	r11d, r9d
	mov	r10, rcx

; 87   :   if (width == 0)

	test	r8d, r8d
	jne	SHORT $LN2@ver_sad_ss

; 88   :     return 0;

	xor	eax, eax

; 99   : }

	add	rsp, 56					; 00000038H
	ret	0
$LN2@ver_sad_ss:
	mov	QWORD PTR [rsp+64], rbx
	mov	QWORD PTR [rsp+48], rdi

; 89   :   if (width == 4)

	cmp	r8d, 4
	jne	$LN3@ver_sad_ss
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 355  :   __m128i ref_row = _mm_set1_epi32(*(const uint32_t *)ref_data);

	movd	xmm2, DWORD PTR [rdx]

; 361  : 
; 362  :   for (y = 0; y < height_fourline_groups; y += 4) {

	xor	eax, eax
	mov	edi, DWORD PTR stride$[rsp]
	xorps	xmm1, xmm1

; 355  :   __m128i ref_row = _mm_set1_epi32(*(const uint32_t *)ref_data);

	mov	QWORD PTR [rsp+72], rbp

; 356  :   __m128i sse_inc = _mm_setzero_si128();
; 357  :   int32_t y;
; 358  : 
; 359  :   const int32_t height_fourline_groups = height & ~3;
; 360  :   const int32_t height_residual_lines  = height &  3;

	mov	ebp, r11d
	mov	QWORD PTR [rsp+80], rsi
	and	ebp, 3
	mov	esi, r11d
	pshufd	xmm2, xmm2, 0
	and	esi, -4

; 361  : 
; 362  :   for (y = 0; y < height_fourline_groups; y += 4) {

	test	esi, esi
	jle	SHORT $LN11@ver_sad_ss

; 355  :   __m128i ref_row = _mm_set1_epi32(*(const uint32_t *)ref_data);

	mov	ebx, 2
	npad	5
$LL12@ver_sad_ss:

; 363  :     __m128i a = _mm_cvtsi32_si128(*(uint32_t *)(pic_data + y * stride));

	mov	ecx, eax

; 364  : 
; 365  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 1) * stride), 1);

	lea	edx, DWORD PTR [rbx-1]

; 366  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 2) * stride), 2);

	mov	r8d, ebx
	imul	ecx, edi

; 367  :     a = _mm_insert_epi32(a, *(const uint32_t *)(pic_data + (y + 3) * stride), 3);

	lea	r9d, DWORD PTR [rbx+1]
	imul	edx, edi
	imul	r8d, edi
	add	eax, 4
	imul	r9d, edi
	add	ebx, 4
	movd	xmm0, DWORD PTR [rcx+r10]
	pinsrd	xmm0, DWORD PTR [rdx+r10], 1
	pinsrd	xmm0, DWORD PTR [r8+r10], 2
	pinsrd	xmm0, DWORD PTR [r9+r10], 3

; 368  : 
; 369  :     __m128i curr_sads = _mm_sad_epu8(a, ref_row);

	psadbw	xmm0, xmm2

; 370  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm0, xmm1
	movdqa	xmm1, xmm0
	cmp	eax, esi
	jl	SHORT $LL12@ver_sad_ss
$LN11@ver_sad_ss:
	mov	rsi, QWORD PTR [rsp+80]

; 371  :   }
; 372  :   if (height_residual_lines) {

	test	ebp, ebp
	mov	rbp, QWORD PTR [rsp+72]
	je	$LN37@ver_sad_ss

; 373  :     // Only pick the last dword, because we're comparing single dwords (lines)
; 374  :     ref_row = _mm_bsrli_si128(ref_row, 12);

	psrldq	xmm2, 12

; 375  : 
; 376  :     for (; y < height; y++) {

	cmp	eax, r11d
	jge	$LN37@ver_sad_ss
	npad	2
$LL15@ver_sad_ss:

; 377  :       __m128i a = _mm_cvtsi32_si128(*(const uint32_t *)(pic_data + y * stride));

	mov	ecx, eax
	inc	eax
	imul	ecx, edi
	movd	xmm0, DWORD PTR [rcx+r10]

; 378  : 
; 379  :       __m128i curr_sads = _mm_sad_epu8(a, ref_row);

	psadbw	xmm0, xmm2

; 380  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm0, xmm1
	movdqa	xmm1, xmm0
	cmp	eax, r11d
	jl	SHORT $LL15@ver_sad_ss
	mov	rbx, QWORD PTR [rsp+64]
	mov	rdi, QWORD PTR [rsp+48]

; 381  :     }
; 382  :   }
; 383  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	pshufd	xmm0, xmm0, 78				; 0000004eH

; 384  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	paddq	xmm0, xmm1

; 385  : 
; 386  :   return _mm_cvtsi128_si32(sad);

	movd	eax, xmm0
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c

; 99   : }

	add	rsp, 56					; 00000038H
	ret	0
$LN3@ver_sad_ss:

; 90   :     return ver_sad_w4(pic_data, ref_data, height, stride);
; 91   :   if (width == 8)

	cmp	r8d, 8
	jne	$LN4@ver_sad_ss
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 392  :   const __m128i ref_row = _mm_set1_epi64x(*(const uint64_t *)ref_data);

	movq	xmm3, QWORD PTR [rdx]

; 393  :   __m128i sse_inc = _mm_setzero_si128();
; 394  :   int32_t y;
; 395  : 
; 396  :   const int32_t height_fourline_groups = height & ~3;
; 397  :   const int32_t height_residual_lines  = height &  3;

	mov	edi, r11d

; 398  : 
; 399  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	r8d, DWORD PTR stride$[rsp]
	and	edi, 3
	and	r9d, -4
	punpcklqdq xmm3, xmm3
	xor	eax, eax
	xorps	xmm2, xmm2
	test	r9d, r9d
	jle	SHORT $LN20@ver_sad_ss
	xorps	xmm4, xmm4

; 392  :   const __m128i ref_row = _mm_set1_epi64x(*(const uint64_t *)ref_data);

	lea	ebx, QWORD PTR [rax+2]
	npad	5
$LL21@ver_sad_ss:

; 400  :     __m128d a_d = _mm_setzero_pd();
; 401  :     __m128d c_d = _mm_setzero_pd();
; 402  : 
; 403  :     a_d = _mm_loadl_pd(a_d, (const double *)(pic_data + (y + 0) * stride));

	mov	ecx, eax
	movaps	xmm1, xmm4
	imul	ecx, r8d

; 404  :     a_d = _mm_loadh_pd(a_d, (const double *)(pic_data + (y + 1) * stride));
; 405  : 
; 406  :     c_d = _mm_loadl_pd(c_d, (const double *)(pic_data + (y + 2) * stride));

	movaps	xmm0, xmm4
	add	eax, 4
	movlpd	xmm1, QWORD PTR [rcx+r10]
	lea	ecx, DWORD PTR [rbx-1]
	imul	ecx, r8d
	movhpd	xmm1, QWORD PTR [rcx+r10]
	mov	ecx, ebx
	imul	ecx, r8d

; 407  :     c_d = _mm_loadh_pd(c_d, (const double *)(pic_data + (y + 3) * stride));
; 408  : 
; 409  :     __m128i a = _mm_castpd_si128(a_d);
; 410  :     __m128i c = _mm_castpd_si128(c_d);
; 411  : 
; 412  :     __m128i curr_sads_ab = _mm_sad_epu8(a, ref_row);

	psadbw	xmm1, xmm3

; 413  :     __m128i curr_sads_cd = _mm_sad_epu8(c, ref_row);
; 414  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm1, xmm2
	movdqa	xmm2, xmm1
	movlpd	xmm0, QWORD PTR [rcx+r10]
	lea	ecx, DWORD PTR [rbx+1]
	imul	ecx, r8d
	add	ebx, 4
	movhpd	xmm0, QWORD PTR [rcx+r10]
	psadbw	xmm0, xmm3

; 415  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	paddq	xmm2, xmm0
	cmp	eax, r9d
	jl	SHORT $LL21@ver_sad_ss
$LN20@ver_sad_ss:

; 416  :   }
; 417  :   if (height_residual_lines) {

	test	edi, edi
	je	SHORT $LN23@ver_sad_ss

; 418  :     __m128i b = _mm_move_epi64(ref_row);

	movq	xmm1, xmm3

; 419  : 
; 420  :     for (; y < height; y++) {

	cmp	eax, r11d
	jge	SHORT $LN23@ver_sad_ss
	npad	12
$LL24@ver_sad_ss:

; 421  :       __m128i a = _mm_loadl_epi64((__m128i *)(pic_data + y * stride));

	mov	ecx, eax
	inc	eax
	imul	ecx, r8d
	movq	xmm0, QWORD PTR [rcx+r10]

; 422  : 
; 423  :       __m128i curr_sads_ab = _mm_sad_epu8(a, b);

	psadbw	xmm0, xmm1

; 424  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm0, xmm2
	movdqa	xmm2, xmm0
	cmp	eax, r11d
	jl	SHORT $LL24@ver_sad_ss
$LN23@ver_sad_ss:
	mov	rbx, QWORD PTR [rsp+64]
	mov	rdi, QWORD PTR [rsp+48]

; 425  :     }
; 426  :   }
; 427  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	pshufd	xmm0, xmm2, 78				; 0000004eH

; 428  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	paddq	xmm0, xmm2

; 429  : 
; 430  :   return _mm_cvtsi128_si32(sad);

	movd	eax, xmm0
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c

; 99   : }

	add	rsp, 56					; 00000038H
	ret	0
$LN4@ver_sad_ss:

; 92   :     return ver_sad_w8(pic_data, ref_data, height, stride);
; 93   :   if (width == 12)

	cmp	r8d, 12
	jne	SHORT $LN5@ver_sad_ss
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 436  :   const __m128i ref_row = _mm_loadu_si128((__m128i *)ref_data);

	movdqu	xmm4, XMMWORD PTR [rdx]
	xor	eax, eax
	xorps	xmm3, xmm3

; 437  :   __m128i sse_inc = _mm_setzero_si128();
; 438  :   int32_t y;
; 439  : 
; 440  :   for (y = 0; y < height; y++) {

	test	r11d, r11d
	jle	SHORT $LN29@ver_sad_ss
	mov	edx, DWORD PTR stride$[rsp]
	npad	13
$LL30@ver_sad_ss:

; 441  :     __m128i a = _mm_loadu_si128((const __m128i *)(pic_data + y * stride));

	mov	ecx, eax

; 442  : 
; 443  :     __m128i a_masked  = _mm_blend_epi16(ref_row, a, 0x3f);

	movdqa	xmm1, xmm4
	imul	ecx, edx

; 444  :     __m128i curr_sads = _mm_sad_epu8   (ref_row, a_masked);

	movdqa	xmm2, xmm4
	inc	eax
	movdqu	xmm0, XMMWORD PTR [rcx+r10]
	pblendw	xmm1, xmm0, 63				; 0000003fH
	psadbw	xmm2, xmm1

; 445  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm2, xmm3
	movdqa	xmm3, xmm2
	cmp	eax, r11d
	jl	SHORT $LL30@ver_sad_ss
$LN29@ver_sad_ss:
	mov	rbx, QWORD PTR [rsp+64]
	mov	rdi, QWORD PTR [rsp+48]

; 446  :   }
; 447  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	pshufd	xmm0, xmm3, 78				; 0000004eH

; 448  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	paddq	xmm0, xmm3

; 449  :   return _mm_cvtsi128_si32(sad);

	movd	eax, xmm0
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c

; 99   : }

	add	rsp, 56					; 00000038H
	ret	0
$LN5@ver_sad_ss:

; 94   :     return ver_sad_w12(pic_data, ref_data, height, stride);
; 95   :   if (width == 16)

	cmp	r8d, 16
	jne	$LN6@ver_sad_ss
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 455  :   const __m128i ref_row = _mm_loadu_si128((__m128i *)ref_data);

	movdqu	xmm2, XMMWORD PTR [rdx]

; 456  :   __m128i sse_inc       = _mm_setzero_si128();
; 457  :   int32_t y;
; 458  : 
; 459  :   const int32_t height_fourline_groups = height & ~3;
; 460  :   const int32_t height_residual_lines  = height &  3;
; 461  : 
; 462  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	edx, DWORD PTR stride$[rsp]
	mov	r8d, r11d
	and	r8d, -4
	and	r9d, 3
	xor	eax, eax
	xorps	xmm1, xmm1
	test	r8d, r8d
	jle	SHORT $LN34@ver_sad_ss

; 455  :   const __m128i ref_row = _mm_loadu_si128((__m128i *)ref_data);

	lea	ebx, QWORD PTR [rax+2]
$LL35@ver_sad_ss:

; 463  :     __m128i pic_row_1   = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * stride));

	mov	ecx, eax
	add	eax, 4
	imul	ecx, edx

; 464  :     __m128i pic_row_2   = _mm_loadu_si128((__m128i *)(pic_data + (y + 1) * stride));
; 465  :     __m128i pic_row_3   = _mm_loadu_si128((__m128i *)(pic_data + (y + 2) * stride));
; 466  :     __m128i pic_row_4   = _mm_loadu_si128((__m128i *)(pic_data + (y + 3) * stride));
; 467  : 
; 468  :     __m128i curr_sads_1 = _mm_sad_epu8   (pic_row_1, ref_row);

	movdqu	xmm0, XMMWORD PTR [rcx+r10]
	lea	ecx, DWORD PTR [rbx-1]
	imul	ecx, edx
	psadbw	xmm0, xmm2

; 469  :     __m128i curr_sads_2 = _mm_sad_epu8   (pic_row_2, ref_row);
; 470  :     __m128i curr_sads_3 = _mm_sad_epu8   (pic_row_3, ref_row);
; 471  :     __m128i curr_sads_4 = _mm_sad_epu8   (pic_row_4, ref_row);
; 472  : 
; 473  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_1);

	paddq	xmm0, xmm1
	movdqa	xmm1, xmm0
	movdqu	xmm0, XMMWORD PTR [rcx+r10]
	mov	ecx, ebx
	imul	ecx, edx
	psadbw	xmm0, xmm2

; 474  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_2);

	paddq	xmm1, xmm0
	movdqu	xmm0, XMMWORD PTR [rcx+r10]
	lea	ecx, DWORD PTR [rbx+1]
	add	ebx, 4
	imul	ecx, edx
	psadbw	xmm0, xmm2

; 475  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_3);

	paddq	xmm1, xmm0
	movdqu	xmm0, XMMWORD PTR [rcx+r10]
	psadbw	xmm0, xmm2

; 476  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_4);

	paddq	xmm1, xmm0
	cmp	eax, r8d
	jl	SHORT $LL35@ver_sad_ss
$LN34@ver_sad_ss:

; 477  :   }
; 478  :   if (height_residual_lines) {

	test	r9d, r9d
	je	SHORT $LN37@ver_sad_ss

; 479  :     for (; y < height; y++) {

	cmp	eax, r11d
	jge	SHORT $LN37@ver_sad_ss
	npad	7
$LL38@ver_sad_ss:

; 480  :       __m128i pic_row   = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * stride));

	mov	ecx, eax
	inc	eax
	imul	ecx, edx

; 481  :       __m128i curr_sads = _mm_sad_epu8   (pic_row, ref_row);

	movdqu	xmm0, XMMWORD PTR [rcx+r10]
	psadbw	xmm0, xmm2

; 482  : 
; 483  :       sse_inc = _mm_add_epi64(sse_inc, curr_sads);

	paddq	xmm0, xmm1
	movdqa	xmm1, xmm0
	cmp	eax, r11d
	jl	SHORT $LL38@ver_sad_ss
$LN37@ver_sad_ss:
	mov	rbx, QWORD PTR [rsp+64]
	mov	rdi, QWORD PTR [rsp+48]

; 484  :     }
; 485  :   }
; 486  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	pshufd	xmm0, xmm1, 78				; 0000004eH

; 487  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	paddq	xmm0, xmm1

; 488  : 
; 489  :   return _mm_cvtsi128_si32(sad);

	movd	eax, xmm0
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c

; 99   : }

	add	rsp, 56					; 00000038H
	ret	0
$LN6@ver_sad_ss:

; 96   :     return ver_sad_w16(pic_data, ref_data, height, stride);
; 97   :   else
; 98   :     return ver_sad_arbitrary(pic_data, ref_data, width, height, stride);

	mov	rbx, QWORD PTR [rsp+64]
	mov	rdi, QWORD PTR [rsp+48]

; 99   : }

	add	rsp, 56					; 00000038H

; 96   :     return ver_sad_w16(pic_data, ref_data, height, stride);
; 97   :   else
; 98   :     return ver_sad_arbitrary(pic_data, ref_data, width, height, stride);

	jmp	ver_sad_arbitrary
ver_sad_sse41 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
_TEXT	SEGMENT
pic_data$ = 112
ref_data$ = 120
height$ = 128
pic_stride$ = 136
ref_stride$ = 144
left$ = 152
right$ = 160
hor_sad_sse41_w32 PROC

; 104  : {

	mov	rax, rsp
	mov	QWORD PTR [rax+8], rbx
	mov	QWORD PTR [rax+16], rbp
	mov	QWORD PTR [rax+24], rsi
	mov	QWORD PTR [rax+32], rdi
	push	r14
	sub	rsp, 96					; 00000060H

; 105  :   const size_t vec_width       = 16;
; 106  :   const uint32_t blkwidth_log2 = 5;
; 107  :   const uint32_t left_eq_wid   = left  >> blkwidth_log2;

	mov	r11d, DWORD PTR left$[rsp]
	mov	r14, rcx

; 108  :   const uint32_t right_eq_wid  = right >> blkwidth_log2;
; 109  :   const int32_t  left_clamped  = left  - left_eq_wid;
; 110  :   const int32_t  right_clamped = right - right_eq_wid;
; 111  : 
; 112  :   const int32_t height_twoline_groups = height & ~1;
; 113  :   const int32_t height_residual_lines = height &  1;
; 114  : 
; 115  :   const __m128i zero       = _mm_setzero_si128();
; 116  :   const __m128i vec_widths = _mm_set1_epi8((uint8_t)vec_width);
; 117  :   const __m128i lefts      = _mm_set1_epi8((uint8_t)left_clamped);
; 118  :   const __m128i rights     = _mm_set1_epi8((uint8_t)right_clamped);
; 119  :   const __m128i nslo       = _mm_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);

	movdqa	xmm3, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	mov	r10d, r11d
	movaps	XMMWORD PTR [rax-24], xmm6
	mov	rbp, rdx
	mov	edx, DWORD PTR right$[rsp]
	mov	ebx, r8d
	movaps	XMMWORD PTR [rax-40], xmm7
	mov	ecx, edx
	movaps	XMMWORD PTR [rax-56], xmm8
	mov	edi, r8d
	movaps	XMMWORD PTR [rax-72], xmm9

; 120  :   const __m128i nshi       = _mm_add_epi8 (nslo, vec_widths);
; 121  : 
; 122  :   const __m128i rightmost_good_idx = _mm_set1_epi8((uint8_t)((vec_width << 1) - right - 1));
; 123  : 
; 124  :   const __m128i epol_mask_right_lo = _mm_min_epi8  (nslo,            rightmost_good_idx);

	movdqa	xmm0, xmm3
	movdqa	xmm9, XMMWORD PTR __xmm@10101010101010101010101010101010
	and	edi, -2
	movaps	XMMWORD PTR [rax-88], xmm10
	and	ebx, 1
	movzx	eax, r11b
	shr	r10d, 5
	sub	al, r10b
	shr	ecx, 5

; 125  :   const __m128i epol_mask_right_hi = _mm_min_epi8  (nshi,            rightmost_good_idx);
; 126  :   const __m128i epol_mask_lo       = _mm_max_epi8  (lefts,           epol_mask_right_lo);
; 127  :   const __m128i epol_mask_hi       = _mm_max_epi8  (lefts,           epol_mask_right_hi);
; 128  : 
; 129  :   const __m128i is_left            = _mm_cmpeq_epi8(rights,          zero);
; 130  :   const __m128i vecwid_for_left    = _mm_and_si128 (is_left,         vec_widths);
; 131  :   const __m128i ns_for_shufmask    = _mm_or_si128  (nslo,            vecwid_for_left);
; 132  : 
; 133  :   const __m128i shufmask1_right    = _mm_add_epi8  (ns_for_shufmask, rights);
; 134  :   const __m128i shufmask1          = _mm_sub_epi8  (shufmask1_right, lefts);
; 135  : 
; 136  :   const __m128i md2bimask          = _mm_cmpgt_epi8(vec_widths,      shufmask1);
; 137  :   const __m128i move_d_to_b_imask  = _mm_or_si128  (is_left,         md2bimask);
; 138  :   const __m128i move_b_to_d_mask   = _mm_cmpgt_epi8(lefts,           nslo);
; 139  : 
; 140  :   // If we're straddling the left border, start from the left border instead,
; 141  :   // and if right border, end on the border
; 142  :   const int32_t ld_offset = left - right;
; 143  : 
; 144  :   int32_t y;
; 145  :   __m128i sse_inc = _mm_setzero_si128();
; 146  :   for (y = 0; y < height_twoline_groups; y += 2) {

	mov	r10d, DWORD PTR ref_stride$[rsp]
	sub	r11d, edx
	movaps	XMMWORD PTR [rsp], xmm11
	xor	r8d, r8d
	movsx	eax, al
	mov	esi, r9d
	xorps	xmm6, xmm6
	movd	xmm8, eax
	movzx	eax, dl
	sub	al, cl
	punpcklbw xmm8, xmm8
	movsx	eax, al
	punpcklwd xmm8, xmm8
	pshufd	xmm8, xmm8, 0
	movdqa	xmm10, xmm8
	movdqa	xmm11, xmm8
	movd	xmm2, eax
	mov	eax, 31
	sub	al, dl
	punpcklbw xmm2, xmm2
	movsx	eax, al
	punpcklwd xmm2, xmm2
	pshufd	xmm2, xmm2, 0
	movd	xmm1, eax
	punpcklbw xmm1, xmm1
	punpcklwd xmm1, xmm1
	pshufd	xmm1, xmm1, 0
	pminsb	xmm0, xmm1
	pmaxsb	xmm10, xmm0
	movdqa	xmm0, XMMWORD PTR __xmm@1f1e1d1c1b1a19181716151413121110
	pminsb	xmm0, xmm1
	pmaxsb	xmm11, xmm0
	xorps	xmm0, xmm0
	pcmpeqb	xmm0, xmm2
	movdqa	xmm7, xmm0
	pand	xmm7, xmm9
	por	xmm7, xmm3
	paddb	xmm7, xmm2
	psubb	xmm7, xmm8
	pcmpgtb	xmm8, xmm3
	pcmpgtb	xmm9, xmm7
	por	xmm9, xmm0
	test	edi, edi
	jle	$LN3@hor_sad_ss
	movsxd	r9, r11d
	npad	3
$LL4@hor_sad_ss:

; 147  :     __m128i a = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * pic_stride + 0));
; 148  :     __m128i b = _mm_loadu_si128((__m128i *)(ref_data + (y + 0) * ref_stride + 0  + ld_offset));
; 149  :     __m128i c = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * pic_stride + 16));
; 150  :     __m128i d = _mm_loadu_si128((__m128i *)(ref_data + (y + 0) * ref_stride + 16 + ld_offset));
; 151  :     __m128i e = _mm_loadu_si128((__m128i *)(pic_data + (y + 1) * pic_stride + 0));
; 152  :     __m128i f = _mm_loadu_si128((__m128i *)(ref_data + (y + 1) * ref_stride + 0  + ld_offset));

	lea	edx, DWORD PTR [r8+1]
	mov	eax, r8d
	mov	ecx, r8d
	imul	eax, esi
	imul	ecx, r10d

; 153  :     __m128i g = _mm_loadu_si128((__m128i *)(pic_data + (y + 1) * pic_stride + 16));
; 154  :     __m128i h = _mm_loadu_si128((__m128i *)(ref_data + (y + 1) * ref_stride + 16 + ld_offset));
; 155  : 
; 156  :     __m128i b_shifted         = _mm_shuffle_epi8(b, shufmask1);
; 157  :     __m128i d_shifted         = _mm_shuffle_epi8(d, shufmask1);
; 158  :     __m128i f_shifted         = _mm_shuffle_epi8(f, shufmask1);
; 159  :     __m128i h_shifted         = _mm_shuffle_epi8(h, shufmask1);
; 160  : 
; 161  :     // TODO: could these be optimized for two-operand efficiency? Only one of
; 162  :     // these ever does useful work, the other should leave the vector untouched,
; 163  :     // so could the first result be used in the second calculation or something?
; 164  :     __m128i b_with_d_data     = _mm_blendv_epi8(d_shifted, b_shifted, move_d_to_b_imask);

	movdqa	xmm0, xmm9
	add	r8d, 2
	add	rcx, r9
	movdqu	xmm3, XMMWORD PTR [rcx+rbp+16]
	movdqu	xmm2, XMMWORD PTR [rcx+rbp]
	mov	ecx, edx
	pshufb	xmm3, xmm7
	pshufb	xmm2, xmm7
	movdqa	xmm1, xmm3
	imul	edx, esi
	pblendvb xmm1, xmm2, xmm0

; 165  :     __m128i d_with_b_data     = _mm_blendv_epi8(d_shifted, b_shifted, move_b_to_d_mask);
; 166  :     __m128i f_with_h_data     = _mm_blendv_epi8(h_shifted, f_shifted, move_d_to_b_imask);
; 167  :     __m128i h_with_f_data     = _mm_blendv_epi8(h_shifted, f_shifted, move_b_to_d_mask);
; 168  : 
; 169  :     __m128i b_final           = _mm_shuffle_epi8(b_with_d_data, epol_mask_lo);
; 170  :     __m128i d_final           = _mm_shuffle_epi8(d_with_b_data, epol_mask_hi);
; 171  :     __m128i f_final           = _mm_shuffle_epi8(f_with_h_data, epol_mask_lo);
; 172  :     __m128i h_final           = _mm_shuffle_epi8(h_with_f_data, epol_mask_hi);
; 173  : 
; 174  :     __m128i curr_sads_ab      = _mm_sad_epu8    (a, b_final);

	movdqu	xmm0, XMMWORD PTR [rax+r14]
	pshufb	xmm1, xmm10
	imul	ecx, r10d
	psadbw	xmm0, xmm1
	add	rcx, r9

; 175  :     __m128i curr_sads_cd      = _mm_sad_epu8    (c, d_final);
; 176  :     __m128i curr_sads_ef      = _mm_sad_epu8    (e, f_final);
; 177  :     __m128i curr_sads_gh      = _mm_sad_epu8    (g, h_final);
; 178  : 
; 179  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm0, xmm6
	movdqa	xmm6, xmm0
	movdqa	xmm0, xmm8
	pblendvb xmm3, xmm2, xmm0
	movdqu	xmm0, XMMWORD PTR [rax+r14+16]
	movdqu	xmm5, XMMWORD PTR [rcx+rbp+16]
	movdqu	xmm4, XMMWORD PTR [rcx+rbp]
	pshufb	xmm5, xmm7
	pshufb	xmm4, xmm7
	movdqa	xmm1, xmm5
	pshufb	xmm3, xmm11
	psadbw	xmm0, xmm3

; 180  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	paddq	xmm6, xmm0
	movdqa	xmm0, xmm9
	pblendvb xmm1, xmm4, xmm0
	movdqu	xmm0, XMMWORD PTR [rdx+r14]
	pshufb	xmm1, xmm10
	psadbw	xmm0, xmm1

; 181  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ef);

	paddq	xmm6, xmm0
	movdqa	xmm0, xmm8
	pblendvb xmm5, xmm4, xmm0
	movdqu	xmm0, XMMWORD PTR [rdx+r14+16]
	pshufb	xmm5, xmm11
	psadbw	xmm0, xmm5

; 182  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_gh);

	paddq	xmm6, xmm0
	cmp	r8d, edi
	jl	$LL4@hor_sad_ss
$LN3@hor_sad_ss:

; 183  :   }
; 184  :   if (height_residual_lines) {

	test	ebx, ebx
	je	SHORT $LN5@hor_sad_ss

; 185  :     __m128i a = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * pic_stride + 0));
; 186  :     __m128i b = _mm_loadu_si128((__m128i *)(ref_data + (y + 0) * ref_stride + 0  + ld_offset));

	mov	ecx, r8d
	movsxd	rdx, r11d
	imul	ecx, r10d

; 187  :     __m128i c = _mm_loadu_si128((__m128i *)(pic_data + (y + 0) * pic_stride + 16));
; 188  :     __m128i d = _mm_loadu_si128((__m128i *)(ref_data + (y + 0) * ref_stride + 16 + ld_offset));
; 189  : 
; 190  :     __m128i b_shifted         = _mm_shuffle_epi8(b, shufmask1);
; 191  :     __m128i d_shifted         = _mm_shuffle_epi8(d, shufmask1);
; 192  : 
; 193  :     __m128i b_with_d_data     = _mm_blendv_epi8(d_shifted, b_shifted, move_d_to_b_imask);

	movdqa	xmm0, xmm9
	add	rcx, rdx
	imul	r8d, esi
	movdqu	xmm4, XMMWORD PTR [rcx+rbp+16]
	movdqu	xmm3, XMMWORD PTR [rcx+rbp]
	pshufb	xmm4, xmm7

; 194  :     __m128i d_with_b_data     = _mm_blendv_epi8(d_shifted, b_shifted, move_b_to_d_mask);
; 195  : 
; 196  :     __m128i b_final           = _mm_shuffle_epi8(b_with_d_data, epol_mask_lo);
; 197  :     __m128i d_final           = _mm_shuffle_epi8(d_with_b_data, epol_mask_hi);
; 198  : 
; 199  :     __m128i curr_sads_ab      = _mm_sad_epu8    (a, b_final);

	movdqu	xmm1, XMMWORD PTR [r8+r14]
	movdqa	xmm2, xmm4
	pshufb	xmm3, xmm7
	pblendvb xmm2, xmm3, xmm0
	movdqa	xmm0, xmm8
	pblendvb xmm4, xmm3, xmm0

; 200  :     __m128i curr_sads_cd      = _mm_sad_epu8    (c, d_final);

	movdqu	xmm0, XMMWORD PTR [r8+r14+16]
	pshufb	xmm2, xmm10
	pshufb	xmm4, xmm11
	psadbw	xmm1, xmm2
	psadbw	xmm0, xmm4

; 201  : 
; 202  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_ab);

	paddq	xmm1, xmm6
	movdqa	xmm6, xmm1

; 203  :     sse_inc = _mm_add_epi64(sse_inc, curr_sads_cd);

	paddq	xmm6, xmm0
$LN5@hor_sad_ss:

; 204  :   }
; 205  :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));
; 206  :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);
; 207  :   return _mm_cvtsi128_si32(sad);
; 208  : }

	movaps	xmm7, XMMWORD PTR [rsp+64]
	lea	r11, QWORD PTR [rsp+96]
	mov	rbx, QWORD PTR [r11+16]
	mov	rbp, QWORD PTR [r11+24]
	mov	rsi, QWORD PTR [r11+32]
	mov	rdi, QWORD PTR [r11+40]
	movaps	xmm8, XMMWORD PTR [r11-48]
	movaps	xmm9, XMMWORD PTR [r11-64]
	movaps	xmm10, XMMWORD PTR [r11-80]
	movaps	xmm11, XMMWORD PTR [r11-96]
	pshufd	xmm0, xmm6, 78				; 0000004eH
	paddq	xmm0, xmm6
	movaps	xmm6, XMMWORD PTR [rsp+80]
	movd	eax, xmm0
	mov	rsp, r11
	pop	r14
	ret	0
hor_sad_sse41_w32 ENDP
_TEXT	ENDS
; Function compile flags: /Ogtpy
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c
_TEXT	SEGMENT
a_off$1$ = 64
is_left_bm$1$ = 68
invec_lend$1$ = 72
invec_lstart$1$ = 76
invec_linc$1$ = 80
left_offset$1$ = 84
x$2$ = 88
tv1869 = 88
a_off$1$ = 92
outside_vecs$1$ = 96
y$1$ = 100
tv1851 = 104
height_fourline_groups$1$ = 108
tv1844 = 112
tv1870 = 116
tv1860 = 120
tv1842 = 120
tv1859 = 128
tv1843 = 128
tv1867 = 136
tv1849 = 136
tv1856 = 144
tv1841 = 144
old_d$1$ = 160
tv1855 = 160
tv2041 = 176
tv1864 = 184
move_old_to_b_imask$1$ = 192
shufmask1$1$ = 208
tv1848 = 224
tv1840 = 232
tv1839 = 240
tv1847 = 248
old_f$1$ = 256
old_h$1$ = 272
is_left$1$ = 288
pic_data$ = 528
ref_data$ = 536
border_off$1$ = 544
width$ = 544
height$ = 552
pic_stride$ = 560
ref_stride$ = 568
left$ = 576
right$ = 584
hor_sad_sse41 PROC

; 213  : {

	mov	DWORD PTR [rsp+32], r9d
	mov	QWORD PTR [rsp+16], rdx
	push	rbx
	sub	rsp, 512				; 00000200H
	mov	ebx, r9d
	mov	r11, rdx
	mov	r10, rcx

; 214  :   if (width == 4)

	cmp	r8d, 4
	jne	SHORT $LN2@hor_sad_ss

; 215  :     return hor_sad_sse41_w4(pic_data, ref_data, height,

	mov	eax, DWORD PTR right$[rsp]
	mov	r8d, ebx
	mov	r9d, DWORD PTR pic_stride$[rsp]
	mov	DWORD PTR [rsp+48], eax
	mov	eax, DWORD PTR left$[rsp]
	mov	DWORD PTR [rsp+40], eax
	mov	eax, DWORD PTR ref_stride$[rsp]
	mov	DWORD PTR [rsp+32], eax
	call	hor_sad_sse41_w4

; 229  : }

	add	rsp, 512				; 00000200H
	pop	rbx
	ret	0
$LN2@hor_sad_ss:

; 216  :                             pic_stride, ref_stride, left, right);
; 217  :   if (width == 8)

	cmp	r8d, 8
	jne	SHORT $LN3@hor_sad_ss

; 218  :     return hor_sad_sse41_w8(pic_data, ref_data, height,

	mov	eax, DWORD PTR right$[rsp]
	mov	r8d, ebx
	mov	r9d, DWORD PTR pic_stride$[rsp]
	mov	DWORD PTR [rsp+48], eax
	mov	eax, DWORD PTR left$[rsp]
	mov	DWORD PTR [rsp+40], eax
	mov	eax, DWORD PTR ref_stride$[rsp]
	mov	DWORD PTR [rsp+32], eax
	call	hor_sad_sse41_w8

; 229  : }

	add	rsp, 512				; 00000200H
	pop	rbx
	ret	0
$LN3@hor_sad_ss:

; 219  :                             pic_stride, ref_stride, left, right);
; 220  :   if (width == 16)

	cmp	r8d, 16
	jne	SHORT $LN4@hor_sad_ss

; 221  :     return hor_sad_sse41_w16(pic_data, ref_data, height,

	mov	eax, DWORD PTR right$[rsp]
	mov	r8d, ebx
	mov	r9d, DWORD PTR pic_stride$[rsp]
	mov	DWORD PTR [rsp+48], eax
	mov	eax, DWORD PTR left$[rsp]
	mov	DWORD PTR [rsp+40], eax
	mov	eax, DWORD PTR ref_stride$[rsp]
	mov	DWORD PTR [rsp+32], eax
	call	hor_sad_sse41_w16

; 229  : }

	add	rsp, 512				; 00000200H
	pop	rbx
	ret	0
$LN4@hor_sad_ss:

; 222  :                              pic_stride, ref_stride, left, right);
; 223  :   if (width == 32)

	cmp	r8d, 32					; 00000020H
	jne	SHORT $LN5@hor_sad_ss

; 224  :     return hor_sad_sse41_w32(pic_data, ref_data, height,

	mov	eax, DWORD PTR right$[rsp]
	mov	r8d, ebx
	mov	r9d, DWORD PTR pic_stride$[rsp]
	mov	DWORD PTR [rsp+48], eax
	mov	eax, DWORD PTR left$[rsp]
	mov	DWORD PTR [rsp+40], eax
	mov	eax, DWORD PTR ref_stride$[rsp]
	mov	DWORD PTR [rsp+32], eax
	call	hor_sad_sse41_w32

; 229  : }

	add	rsp, 512				; 00000200H
	pop	rbx
	ret	0
$LN5@hor_sad_ss:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 852  :   const __m128i rights     = _mm_set1_epi8((uint8_t)right);

	mov	r9d, DWORD PTR right$[rsp]
	mov	eax, ebx

; 854  :   const __m128i vec_widths = _mm_set1_epi8((uint8_t)vec_width);

	movdqa	xmm4, XMMWORD PTR __xmm@10101010101010101010101010101010
	and	eax, -4
	mov	QWORD PTR [rsp+528], rbp
	mov	QWORD PTR [rsp+504], rsi
	mov	QWORD PTR [rsp+496], rdi
	mov	QWORD PTR [rsp+488], r12
	mov	QWORD PTR [rsp+480], r13

; 856  : 
; 857  :   uint32_t outside_vecs,  inside_vecs,  left_offset, is_left_bm;
; 858  :   int32_t  outside_width, inside_width, border_off,  invec_lstart,
; 859  :            invec_lend,    invec_linc;
; 860  :   if (left) {

	mov	r13d, DWORD PTR left$[rsp]
	mov	QWORD PTR [rsp+472], r14
	mov	QWORD PTR [rsp+464], r15
	movaps	XMMWORD PTR [rsp+384], xmm10
	xorps	xmm10, xmm10

; 849  :   const int32_t height_fourline_groups = height & ~3;

	mov	DWORD PTR height_fourline_groups$1$[rsp], eax

; 850  :   const int32_t height_residual_lines  = height &  3;

	mov	eax, ebx
	and	eax, 3
	movaps	XMMWORD PTR [rsp+368], xmm11

; 855  :   const __m128i nslo       = _mm_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);

	movdqa	xmm11, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	mov	DWORD PTR tv1870[rsp], eax
	movsx	eax, r9b
	movaps	XMMWORD PTR [rsp+352], xmm12
	movaps	XMMWORD PTR [rsp+336], xmm13
	xorps	xmm13, xmm13
	movaps	XMMWORD PTR [rsp+320], xmm14

; 852  :   const __m128i rights     = _mm_set1_epi8((uint8_t)right);

	movd	xmm0, eax
	punpcklbw xmm0, xmm0

; 853  :   const __m128i blk_widths = _mm_set1_epi8((uint8_t)width);

	movsx	eax, r8b

; 855  :   const __m128i nslo       = _mm_setr_epi8(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);

	movsxd	rdx, r8d
	add	rdx, 15
	punpcklwd xmm0, xmm0
	shr	rdx, 4
	movd	xmm14, eax
	punpcklbw xmm14, xmm14
	punpcklwd xmm14, xmm14
	pshufd	xmm14, xmm14, 0
	pshufd	xmm0, xmm0, 0

; 856  : 
; 857  :   uint32_t outside_vecs,  inside_vecs,  left_offset, is_left_bm;
; 858  :   int32_t  outside_width, inside_width, border_off,  invec_lstart,
; 859  :            invec_lend,    invec_linc;
; 860  :   if (left) {

	test	r13d, r13d
	je	SHORT $LN27@hor_sad_ss

; 861  :     outside_vecs  =    left                              >> vec_width_log2;

	mov	ebx, r13d

; 862  :     inside_vecs   = (( width           + vecwid_bitmask) >> vec_width_log2) - outside_vecs;
; 863  :     outside_width =    outside_vecs * vec_width;
; 864  :     inside_width  =    inside_vecs  * vec_width;
; 865  :     left_offset   =    left;
; 866  :     border_off    =    left;

	mov	DWORD PTR border_off$1$[rsp], r13d
	shr	ebx, 4
	mov	r14d, r13d
	sub	edx, ebx

; 867  :     invec_lstart  =    0;
; 868  :     invec_lend    =    inside_vecs;
; 869  :     invec_linc    =    1;

	mov	DWORD PTR invec_linc$1$[rsp], 1
	mov	ebp, edx
	xor	eax, eax
	mov	r15d, edx

; 870  :     is_left_bm    =    -1;

	mov	r12d, -1				; ffffffffH

; 871  :   } else {

	jmp	SHORT $LN61@hor_sad_ss
$LN27@hor_sad_ss:

; 872  :     inside_vecs   =  ((width - right) + vecwid_bitmask)  >> vec_width_log2;

	mov	eax, r8d

; 873  :     outside_vecs  = (( width          + vecwid_bitmask)  >> vec_width_log2) - inside_vecs;

	mov	ebx, edx
	sub	eax, r9d

; 874  :     outside_width =    outside_vecs * vec_width;
; 875  :     inside_width  =    inside_vecs  * vec_width;
; 876  :     left_offset   =    right - width;

	mov	r13d, r9d
	sub	r13d, r8d

; 877  :     border_off    =    width - 1 - right;
; 878  :     invec_lstart  =    inside_vecs - 1;
; 879  :     invec_lend    =    -1;

	mov	r15d, -1

; 880  :     invec_linc    =    -1;

	mov	DWORD PTR invec_linc$1$[rsp], r15d
	lea	rbp, QWORD PTR [rax+15]
	shr	rbp, 4
	lea	r14d, DWORD PTR [rax-1]
	sub	ebx, ebp
	mov	DWORD PTR border_off$1$[rsp], r14d

; 881  :     is_left_bm    =    0;

	xor	r12d, r12d
	lea	eax, DWORD PTR [rbp-1]
$LN61@hor_sad_ss:

; 903  :   int32_t x, y;
; 904  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	edx, DWORD PTR ref_stride$[rsp]
	and	r13d, 15
	mov	edi, DWORD PTR pic_stride$[rsp]
	xor	r8d, r8d
	mov	DWORD PTR invec_lstart$1$[rsp], eax
	xorps	xmm12, xmm12

; 884  : 
; 885  :   const __m128i left_offsets = _mm_set1_epi8 ((uint8_t)left_offset);
; 886  :   const __m128i is_left      = _mm_cmpeq_epi8(rights, _mm_setzero_si128());

	pcmpeqb	xmm12, xmm0
	movsx	eax, r13b

; 888  : 
; 889  :   // -x == (x ^ 0xff) + 1 = (x ^ 0xff) - 0xff. Also x == (x ^ 0x00) - 0x00.
; 890  :   // in other words, calculate inverse of left_offsets if is_left is true.
; 891  :   const __m128i offs_neg            = _mm_xor_si128 (left_offsets, is_left);

	movdqa	xmm3, xmm12
	movaps	XMMWORD PTR [rsp+448], xmm6

; 887  :   const __m128i vw_for_left  = _mm_and_si128 (is_left, vec_widths);

	movdqa	xmm0, xmm12
	mov	DWORD PTR is_left_bm$1$[rsp], r12d
	pand	xmm0, xmm4
	mov	DWORD PTR invec_lend$1$[rsp], r15d
	movd	xmm1, eax

; 892  :   const __m128i offs_for_sm1        = _mm_sub_epi8  (offs_neg,     is_left);
; 893  : 
; 894  :   const __m128i ns_for_sm1          = _mm_or_si128  (vw_for_left,  nslo);

	por	xmm0, xmm11
	punpcklbw xmm1, xmm1
	mov	eax, ebp
	shl	eax, 4
	mov	esi, ebx

; 896  : 
; 897  :   const __m128i mo2bmask_l          = _mm_cmpgt_epi8(left_offsets, nslo);
; 898  :   const __m128i mo2bimask_l         = _mm_cmpeq_epi8(mo2bmask_l,   _mm_setzero_si128());
; 899  :   const __m128i mo2bimask_r         = _mm_cmpgt_epi8(vec_widths,   shufmask1);
; 900  :   const __m128i move_old_to_b_imask = _mm_blendv_epi8(mo2bimask_r, mo2bimask_l, is_left);
; 901  : 
; 902  :   const int32_t outvec_offset = (~is_left_bm) & inside_width;

	movsxd	r9, eax
	mov	eax, r12d
	punpcklwd xmm1, xmm1
	not	eax
	pshufd	xmm1, xmm1, 0
	pxor	xmm3, xmm1
	shl	esi, 4
	psubb	xmm3, xmm12
	cdqe
	paddb	xmm3, xmm0
	mov	DWORD PTR outside_vecs$1$[rsp], ebx
	and	r9, rax
	mov	QWORD PTR tv2041[rsp], rbp
	pcmpgtb	xmm1, xmm11
	pcmpgtb	xmm4, xmm3
	pcmpeqb	xmm1, xmm10
	movdqa	xmm0, xmm12
	pblendvb xmm4, xmm1, xmm0
	movdqa	XMMWORD PTR move_old_to_b_imask$1$[rsp], xmm4
	movaps	XMMWORD PTR [rsp+432], xmm7

; 882  :   }
; 883  :   left_offset &= vecwid_bitmask;

	mov	DWORD PTR a_off$1$[rsp], esi
	mov	DWORD PTR left_offset$1$[rsp], r13d

; 884  : 
; 885  :   const __m128i left_offsets = _mm_set1_epi8 ((uint8_t)left_offset);
; 886  :   const __m128i is_left      = _mm_cmpeq_epi8(rights, _mm_setzero_si128());

	movdqa	XMMWORD PTR is_left$1$[rsp], xmm12

; 895  :   const __m128i shufmask1           = _mm_add_epi8  (ns_for_sm1,   offs_for_sm1);

	movdqa	XMMWORD PTR shufmask1$1$[rsp], xmm3

; 903  :   int32_t x, y;
; 904  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	DWORD PTR y$1$[rsp], r8d
	mov	QWORD PTR tv1864[rsp], r9
	cmp	DWORD PTR height_fourline_groups$1$[rsp], r8d
	jle	$LN10@hor_sad_ss
	mov	eax, esi

; 882  :   }
; 883  :   left_offset &= vecwid_bitmask;

	movaps	XMMWORD PTR [rsp+416], xmm8
	and	eax, r12d
	movaps	XMMWORD PTR [rsp+400], xmm9
	mov	esi, 2
	mov	DWORD PTR a_off$1$[rsp], eax
	movaps	XMMWORD PTR [rsp+304], xmm15
	mov	DWORD PTR tv1851[rsp], esi

; 903  :   int32_t x, y;
; 904  :   for (y = 0; y < height_fourline_groups; y += 4) {

	jmp	SHORT $LN11@hor_sad_ss
$LL60@hor_sad_ss:
	mov	r14d, DWORD PTR border_off$1$[rsp]
$LN11@hor_sad_ss:

; 905  :     __m128i borderpx_vec_b = _mm_set1_epi8(ref_data[(int32_t)((y + 0) * ref_stride + border_off)]);

	mov	r12d, r8d

; 906  :     __m128i borderpx_vec_d = _mm_set1_epi8(ref_data[(int32_t)((y + 1) * ref_stride + border_off)]);
; 907  :     __m128i borderpx_vec_f = _mm_set1_epi8(ref_data[(int32_t)((y + 2) * ref_stride + border_off)]);

	mov	r13d, esi
	imul	r12d, edx
	imul	r13d, edx
	mov	DWORD PTR tv1869[rsp], r12d
	lea	eax, DWORD PTR [r12+r14]
	movsxd	rcx, eax
	movsx	eax, BYTE PTR [rcx+r11]
	movd	xmm15, eax
	lea	eax, DWORD PTR [rsi-1]
	imul	eax, edx
	punpcklbw xmm15, xmm15
	punpcklwd xmm15, xmm15
	pshufd	xmm15, xmm15, 0
	add	eax, r14d
	cdqe
	movsx	ecx, BYTE PTR [rax+r11]
	lea	eax, DWORD PTR [r14+r13]

; 908  :     __m128i borderpx_vec_h = _mm_set1_epi8(ref_data[(int32_t)((y + 3) * ref_stride + border_off)]);

	lea	r14d, DWORD PTR [rsi+1]
	movd	xmm2, ecx
	movsxd	rcx, eax
	punpcklbw xmm2, xmm2
	punpcklwd xmm2, xmm2
	pshufd	xmm2, xmm2, 0
	movsx	eax, BYTE PTR [rcx+r11]
	movdqa	XMMWORD PTR old_d$1$[rsp], xmm2
	movd	xmm8, eax
	mov	eax, r14d
	imul	eax, edx

; 909  : 
; 910  :     for (x = 0; x < outside_vecs; x++) {

	xor	edx, edx
	punpcklbw xmm8, xmm8
	punpcklwd xmm8, xmm8
	pshufd	xmm8, xmm8, 0
	movdqa	XMMWORD PTR old_f$1$[rsp], xmm8
	mov	DWORD PTR tv1844[rsp], eax
	add	eax, DWORD PTR border_off$1$[rsp]
	movsxd	rcx, eax
	movsx	eax, BYTE PTR [rcx+r11]
	movd	xmm9, eax
	punpcklbw xmm9, xmm9
	punpcklwd xmm9, xmm9
	pshufd	xmm9, xmm9, 0
	movdqa	XMMWORD PTR old_h$1$[rsp], xmm9
	test	ebx, ebx
	je	$LN50@hor_sad_ss

; 911  :       __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + outvec_offset));

	mov	eax, r8d
	movdqa	xmm7, xmm2
	imul	eax, edi
	mov	QWORD PTR tv1860[rsp], rax
	lea	eax, DWORD PTR [rsi-1]
	imul	eax, edi
	mov	QWORD PTR tv1843[rsp], rax
	mov	eax, esi
	mov	r11, QWORD PTR tv1843[rsp]
	imul	eax, edi
	mov	QWORD PTR tv1849[rsp], rax
	mov	eax, r14d
	mov	r15, QWORD PTR tv1849[rsp]
	imul	eax, edi
	mov	rdi, QWORD PTR tv1860[rsp]
	mov	QWORD PTR tv1841[rsp], rax
	mov	r12d, eax
	npad	9
$LL14@hor_sad_ss:
	movsxd	rcx, edx

; 912  :       __m128i c = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 1) * pic_stride + outvec_offset));
; 913  :       __m128i e = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 2) * pic_stride + outvec_offset));
; 914  :       __m128i g = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 3) * pic_stride + outvec_offset));
; 915  : 
; 916  :       __m128i startoffs  = _mm_set1_epi8  ((x + inside_vecs) << vec_width_log2);
; 917  :       __m128i ns         = _mm_add_epi8   (startoffs, nslo);
; 918  : 
; 919  :       // Unread imask is (is_left NOR unrd_imask_for_right), do the maths etc
; 920  :       __m128i unrd_imask = _mm_cmpgt_epi8 (blk_widths, ns);

	movdqa	xmm2, xmm14
	add	rcx, rcx

; 921  :               unrd_imask = _mm_or_si128   (unrd_imask, is_left);
; 922  :       __m128i unrd_mask  = _mm_cmpeq_epi8 (unrd_imask, _mm_setzero_si128());
; 923  : 
; 924  :       __m128i b_unread   = _mm_blendv_epi8(borderpx_vec_b, a, unrd_mask);

	movdqa	xmm1, xmm15
	lea	rax, QWORD PTR [rdi+rcx*8]
	add	rax, r9
	movdqu	xmm3, XMMWORD PTR [rax+r10]
	lea	rax, QWORD PTR [r11+rcx*8]
	add	rax, r9
	movdqu	xmm4, XMMWORD PTR [rax+r10]
	lea	rax, QWORD PTR [r15+rcx*8]
	add	rax, r9
	movdqu	xmm5, XMMWORD PTR [rax+r10]
	lea	rax, QWORD PTR [r12+rcx*8]
	add	rax, r9
	movdqu	xmm6, XMMWORD PTR [rax+r10]
	lea	eax, DWORD PTR [rdx+rbp]
	inc	edx
	shl	al, 4
	movsx	eax, al
	movd	xmm0, eax
	punpcklbw xmm0, xmm0
	punpcklwd xmm0, xmm0
	pshufd	xmm0, xmm0, 0
	paddb	xmm0, xmm11
	pcmpgtb	xmm2, xmm0
	por	xmm2, xmm12
	pcmpeqb	xmm2, xmm10
	movdqa	xmm0, xmm2
	pblendvb xmm1, xmm3, xmm0

; 925  :       __m128i d_unread   = _mm_blendv_epi8(borderpx_vec_d, c, unrd_mask);
; 926  :       __m128i f_unread   = _mm_blendv_epi8(borderpx_vec_f, e, unrd_mask);
; 927  :       __m128i h_unread   = _mm_blendv_epi8(borderpx_vec_h, g, unrd_mask);
; 928  : 
; 929  :       __m128i sad_ab     = _mm_sad_epu8   (a, b_unread);

	psadbw	xmm3, xmm1
	movdqa	xmm1, xmm7
	pblendvb xmm1, xmm4, xmm0

; 930  :       __m128i sad_cd     = _mm_sad_epu8   (c, d_unread);

	psadbw	xmm4, xmm1
	movdqa	xmm1, xmm8
	pblendvb xmm1, xmm5, xmm0

; 931  :       __m128i sad_ef     = _mm_sad_epu8   (e, f_unread);

	psadbw	xmm5, xmm1
	movdqa	xmm1, xmm9

; 932  :       __m128i sad_gh     = _mm_sad_epu8   (g, h_unread);
; 933  : 
; 934  :       sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	paddq	xmm3, xmm13
	movdqa	xmm13, xmm3

; 935  :       sse_inc = _mm_add_epi64(sse_inc, sad_cd);

	paddq	xmm13, xmm4
	pblendvb xmm1, xmm6, xmm0
	psadbw	xmm6, xmm1

; 936  :       sse_inc = _mm_add_epi64(sse_inc, sad_ef);

	paddq	xmm13, xmm5

; 937  :       sse_inc = _mm_add_epi64(sse_inc, sad_gh);

	paddq	xmm13, xmm6
	cmp	edx, ebx
	jb	$LL14@hor_sad_ss
	movdqa	xmm2, XMMWORD PTR old_d$1$[rsp]
	movdqa	xmm3, XMMWORD PTR shufmask1$1$[rsp]
	movdqa	xmm4, XMMWORD PTR move_old_to_b_imask$1$[rsp]
	mov	r11, QWORD PTR ref_data$[rsp]
	mov	edi, DWORD PTR pic_stride$[rsp]
	mov	r15d, DWORD PTR invec_lend$1$[rsp]
	mov	r12d, DWORD PTR tv1869[rsp]
$LN50@hor_sad_ss:

; 941  : 
; 942  :     __m128i old_b = borderpx_vec_b;
; 943  :     __m128i old_d = borderpx_vec_d;
; 944  :     __m128i old_f = borderpx_vec_f;
; 945  :     __m128i old_h = borderpx_vec_h;
; 946  : 
; 947  :     for (x = invec_lstart; x != invec_lend; x += invec_linc) {

	mov	eax, DWORD PTR invec_lstart$1$[rsp]
	mov	DWORD PTR x$2$[rsp], eax
	cmp	eax, r15d
	je	$LN59@hor_sad_ss

; 938  :     }
; 939  :     int32_t a_off = outside_width & is_left_bm;
; 940  :     int32_t leftoff_with_sign_neg = (left_offset ^ is_left_bm) - is_left_bm;

	mov	eax, DWORD PTR is_left_bm$1$[rsp]
	xor	eax, DWORD PTR left_offset$1$[rsp]
	sub	eax, DWORD PTR is_left_bm$1$[rsp]
	mov	r9d, DWORD PTR invec_linc$1$[rsp]

; 949  :       __m128i c = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 1) * pic_stride + a_off));
; 950  :       __m128i e = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 2) * pic_stride + a_off));
; 951  :       __m128i g = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 3) * pic_stride + a_off));
; 952  :       __m128i b = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 0) * ref_stride + a_off - leftoff_with_sign_neg));

	movsxd	rcx, eax
	mov	eax, r8d
	imul	eax, edi
	imul	r14d, edi
	mov	QWORD PTR tv1867[rsp], rcx
	lea	ecx, DWORD PTR [rsi-1]
	mov	QWORD PTR tv1859[rsp], rax
	movsxd	rax, DWORD PTR a_off$1$[rsp]
	mov	rdx, QWORD PTR tv1859[rsp]
	mov	QWORD PTR tv1856[rsp], rax
	mov	eax, ecx
	imul	eax, edi
	mov	QWORD PTR tv1842[rsp], rax
	mov	eax, esi
	mov	rbp, QWORD PTR tv1842[rsp]
	imul	eax, edi
	mov	edi, DWORD PTR a_off$1$[rsp]
	mov	QWORD PTR tv1848[rsp], rax
	mov	rbx, QWORD PTR tv1848[rsp]
	mov	eax, r14d
	mov	r14, QWORD PTR tv1856[rsp]
	mov	QWORD PTR tv1840[rsp], rax
	mov	r8, QWORD PTR tv1840[rsp]
	mov	eax, r12d
	mov	r12d, DWORD PTR x$2$[rsp]
	mov	QWORD PTR tv1855[rsp], rax

; 948  :       __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + a_off));

	mov	eax, DWORD PTR ref_stride$[rsp]
	mov	rsi, QWORD PTR tv1855[rsp]
	imul	eax, ecx
	mov	QWORD PTR tv1839[rsp], rax
	mov	eax, r13d
	mov	r13, QWORD PTR tv1867[rsp]
	mov	QWORD PTR tv1847[rsp], rax
	npad	11
$LL17@hor_sad_ss:

; 953  :       __m128i d = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 1) * ref_stride + a_off - leftoff_with_sign_neg));
; 954  :       __m128i f = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 2) * ref_stride + a_off - leftoff_with_sign_neg));
; 955  :       __m128i h = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 3) * ref_stride + a_off - leftoff_with_sign_neg));
; 956  : 
; 957  :       __m128i b_shifted    = _mm_shuffle_epi8(b,     shufmask1);
; 958  :       __m128i d_shifted    = _mm_shuffle_epi8(d,     shufmask1);

	mov	r15, QWORD PTR tv1839[rsp]
	lea	rax, QWORD PTR [r14+rdx]
	movsxd	rcx, r12d

; 959  :       __m128i f_shifted    = _mm_shuffle_epi8(f,     shufmask1);
; 960  :       __m128i h_shifted    = _mm_shuffle_epi8(h,     shufmask1);
; 961  : 
; 962  :       __m128i b_with_old   = _mm_blendv_epi8 (old_b, b_shifted, move_old_to_b_imask);
; 963  :       __m128i d_with_old   = _mm_blendv_epi8 (old_d, d_shifted, move_old_to_b_imask);
; 964  :       __m128i f_with_old   = _mm_blendv_epi8 (old_f, f_shifted, move_old_to_b_imask);
; 965  :       __m128i h_with_old   = _mm_blendv_epi8 (old_h, h_shifted, move_old_to_b_imask);
; 966  : 
; 967  :       uint8_t startoff     = (x << vec_width_log2) + a_off;
; 968  :       __m128i startoffs    = _mm_set1_epi8   (startoff);
; 969  :       __m128i curr_ns      = _mm_add_epi8    (startoffs,    nslo);
; 970  :       __m128i unrd_imask   = _mm_cmpgt_epi8  (blk_widths,   curr_ns);

	movdqa	xmm1, xmm14
	shl	rcx, 4
	add	rax, rcx
	movdqu	xmm9, XMMWORD PTR [rax+r10]
	lea	rax, QWORD PTR [r14+rbp]
	add	rax, rcx
	movdqu	xmm10, XMMWORD PTR [rax+r10]
	lea	rax, QWORD PTR [rbx+r14]
	add	rax, rcx
	movdqu	xmm11, XMMWORD PTR [rax+r10]
	lea	rax, QWORD PTR [r8+r14]
	add	rax, rcx
	movdqu	xmm12, XMMWORD PTR [rax+r10]
	mov	rax, rsi
	sub	rax, r13
	add	rax, r14
	add	rax, rcx
	movdqu	xmm8, XMMWORD PTR [rax+r11]
	mov	rax, r14
	sub	rax, r13
	add	rax, rcx
	add	rax, r11
	pshufb	xmm8, xmm3
	movdqu	xmm7, XMMWORD PTR [rax+r15]
	mov	rax, QWORD PTR tv1847[rsp]
	sub	rax, r13
	add	rax, r14
	add	rax, rcx
	pshufb	xmm7, xmm3
	movdqu	xmm6, XMMWORD PTR [rax+r11]
	mov	rax, r14
	sub	rax, r13

; 974  :       __m128i d_unread     = _mm_blendv_epi8 (d_with_old,   c, unrd_mask);
; 975  :       __m128i f_unread     = _mm_blendv_epi8 (f_with_old,   e, unrd_mask);
; 976  :       __m128i h_unread     = _mm_blendv_epi8 (h_with_old,   g, unrd_mask);
; 977  : 
; 978  :       old_b = b_shifted;
; 979  :       old_d = d_shifted;

	movdqa	XMMWORD PTR old_d$1$[rsp], xmm7
	add	rax, rcx
	mov	ecx, DWORD PTR tv1844[rsp]
	add	rax, rcx
	pshufb	xmm6, xmm3
	movdqu	xmm5, XMMWORD PTR [rax+r11]
	movzx	eax, r12b
	shl	al, 4
	pshufb	xmm5, xmm3
	add	al, dil
	movsx	eax, al
	movd	xmm0, eax
	punpcklbw xmm0, xmm0
	punpcklwd xmm0, xmm0
	pshufd	xmm0, xmm0, 0
	paddb	xmm0, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	pcmpgtb	xmm1, xmm0
	xorps	xmm0, xmm0

; 971  :       __m128i unrd_mask    = _mm_cmpeq_epi8  (unrd_imask,   _mm_setzero_si128());

	pcmpeqb	xmm1, xmm0
	movdqa	xmm0, xmm4
	pblendvb xmm15, xmm8, xmm0

; 972  : 
; 973  :       __m128i b_unread     = _mm_blendv_epi8 (b_with_old,   a, unrd_mask);

	movdqa	xmm0, xmm1
	pblendvb xmm15, xmm9, xmm0
	movdqa	xmm4, xmm15
	movdqa	xmm15, XMMWORD PTR move_old_to_b_imask$1$[rsp]
	movdqa	xmm0, xmm15

; 980  :       old_f = f_shifted;
; 981  :       old_h = h_shifted;
; 982  : 
; 983  :       __m128i sad_ab     = _mm_sad_epu8(a, b_unread);

	psadbw	xmm9, xmm4
	pblendvb xmm2, xmm7, xmm0
	movdqa	xmm0, xmm1
	pblendvb xmm2, xmm10, xmm0
	movdqa	xmm0, xmm15
	movdqa	xmm3, xmm2
	movdqa	xmm2, XMMWORD PTR old_f$1$[rsp]

; 984  :       __m128i sad_cd     = _mm_sad_epu8(c, d_unread);
; 985  :       __m128i sad_ef     = _mm_sad_epu8(e, f_unread);
; 986  :       __m128i sad_gh     = _mm_sad_epu8(g, h_unread);
; 987  : 
; 988  :       sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	paddq	xmm9, xmm13
	pblendvb xmm2, xmm6, xmm0
	movdqa	xmm0, xmm1
	movdqa	XMMWORD PTR old_f$1$[rsp], xmm6
	pblendvb xmm2, xmm11, xmm0
	movdqa	xmm0, xmm15
	movdqa	xmm15, XMMWORD PTR old_h$1$[rsp]
	pblendvb xmm15, xmm5, xmm0
	movdqa	xmm0, xmm1
	movdqa	XMMWORD PTR old_h$1$[rsp], xmm5
	pblendvb xmm15, xmm12, xmm0
	movdqa	xmm1, xmm15
	movdqa	xmm15, xmm8
	mov	r15d, DWORD PTR invec_lend$1$[rsp]
	movdqa	xmm13, xmm9
	movdqa	xmm4, XMMWORD PTR move_old_to_b_imask$1$[rsp]
	add	r12d, r9d
	psadbw	xmm10, xmm3
	movdqa	xmm3, XMMWORD PTR shufmask1$1$[rsp]
	psadbw	xmm11, xmm2
	movdqa	xmm2, xmm7
	psadbw	xmm12, xmm1

; 989  :       sse_inc = _mm_add_epi64(sse_inc, sad_cd);

	paddq	xmm13, xmm10

; 990  :       sse_inc = _mm_add_epi64(sse_inc, sad_ef);

	paddq	xmm13, xmm11

; 991  :       sse_inc = _mm_add_epi64(sse_inc, sad_gh);

	paddq	xmm13, xmm12
	cmp	r12d, r15d
	jne	$LL17@hor_sad_ss
	movdqa	xmm11, XMMWORD PTR __xmm@0f0e0d0c0b0a09080706050403020100
	xorps	xmm10, xmm10
	movdqa	xmm12, XMMWORD PTR is_left$1$[rsp]
	mov	r8d, DWORD PTR y$1$[rsp]
	mov	r9, QWORD PTR tv1864[rsp]
	mov	ebx, DWORD PTR outside_vecs$1$[rsp]
	mov	edi, DWORD PTR pic_stride$[rsp]
	mov	esi, DWORD PTR tv1851[rsp]
	mov	rbp, QWORD PTR tv2041[rsp]
$LN59@hor_sad_ss:

; 903  :   int32_t x, y;
; 904  :   for (y = 0; y < height_fourline_groups; y += 4) {

	mov	edx, DWORD PTR ref_stride$[rsp]
	add	r8d, 4
	movdqa	xmm3, XMMWORD PTR shufmask1$1$[rsp]
	add	esi, 4
	movdqa	xmm4, XMMWORD PTR move_old_to_b_imask$1$[rsp]
	mov	DWORD PTR y$1$[rsp], r8d
	mov	DWORD PTR tv1851[rsp], esi
	cmp	r8d, DWORD PTR height_fourline_groups$1$[rsp]
	jl	$LL60@hor_sad_ss
	mov	esi, DWORD PTR a_off$1$[rsp]
	mov	r12d, DWORD PTR is_left_bm$1$[rsp]
	mov	r13d, DWORD PTR left_offset$1$[rsp]
	movaps	xmm15, XMMWORD PTR [rsp+304]
	movaps	xmm9, XMMWORD PTR [rsp+400]
	movaps	xmm8, XMMWORD PTR [rsp+416]
$LN10@hor_sad_ss:

; 992  :     }
; 993  :   }
; 994  :   if (height_residual_lines) {

	cmp	DWORD PTR tv1870[rsp], 0
	je	$LN19@hor_sad_ss

; 995  :     for (; y < height; y++) {

	cmp	r8d, DWORD PTR height$[rsp]
	jge	$LN19@hor_sad_ss
	movdqa	xmm6, XMMWORD PTR shufmask1$1$[rsp]
	and	esi, r12d
	movdqa	xmm7, XMMWORD PTR move_old_to_b_imask$1$[rsp]
	mov	DWORD PTR a_off$1$[rsp], esi
	npad	7
$LL20@hor_sad_ss:

; 996  :       __m128i borderpx_vec = _mm_set1_epi8(ref_data[(int32_t)((y + 0) * ref_stride + border_off)]);

	mov	eax, DWORD PTR border_off$1$[rsp]
	mov	r14d, r8d
	imul	r14d, edx
	add	eax, r14d
	movsxd	rcx, eax
	movsx	eax, BYTE PTR [rcx+r11]
	movd	xmm5, eax

; 997  :       for (x = 0; x < outside_vecs; x++) {

	xor	eax, eax
	punpcklbw xmm5, xmm5
	punpcklwd xmm5, xmm5
	pshufd	xmm5, xmm5, 0
	test	ebx, ebx
	je	SHORT $LN52@hor_sad_ss

; 998  :         __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + outvec_offset));

	mov	edx, r8d
	imul	edx, edi
	npad	10
$LL23@hor_sad_ss:
	movsxd	rcx, eax

; 999  : 
; 1000 :         __m128i startoffs  = _mm_set1_epi8  ((x + inside_vecs) << vec_width_log2);
; 1001 :         __m128i ns         = _mm_add_epi8   (startoffs, nslo);
; 1002 : 
; 1003 :         // Unread imask is (is_left NOR unrd_imask_for_right), do the maths etc
; 1004 :         __m128i unrd_imask = _mm_cmpgt_epi8 (blk_widths, ns);

	movdqa	xmm0, xmm14
	shl	rcx, 4
	add	rcx, rdx
	add	rcx, r9
	movdqu	xmm2, XMMWORD PTR [rcx+r10]
	lea	ecx, DWORD PTR [rax+rbp]
	inc	eax
	shl	cl, 4
	movsx	ecx, cl
	movd	xmm1, ecx
	punpcklbw xmm1, xmm1
	punpcklwd xmm1, xmm1
	pshufd	xmm1, xmm1, 0
	paddb	xmm1, xmm11
	pcmpgtb	xmm0, xmm1

; 1005 :                 unrd_imask = _mm_or_si128   (unrd_imask, is_left);
; 1006 :         __m128i unrd_mask  = _mm_cmpeq_epi8 (unrd_imask, _mm_setzero_si128());
; 1007 :         __m128i b_unread   = _mm_blendv_epi8(borderpx_vec, a, unrd_mask);

	movdqa	xmm1, xmm5
	por	xmm0, xmm12
	pcmpeqb	xmm0, xmm10
	pblendvb xmm1, xmm2, xmm0

; 1008 : 
; 1009 :         __m128i sad_ab     = _mm_sad_epu8   (a, b_unread);

	psadbw	xmm2, xmm1

; 1010 :         sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	paddq	xmm2, xmm13
	movdqa	xmm13, xmm2
	cmp	eax, ebx
	jb	SHORT $LL23@hor_sad_ss
$LN52@hor_sad_ss:

; 1014 : 
; 1015 :       __m128i old_b = borderpx_vec;
; 1016 :       for (x = invec_lstart; x != invec_lend; x += invec_linc) {

	mov	edx, DWORD PTR invec_lstart$1$[rsp]
	cmp	edx, r15d
	je	$LN18@hor_sad_ss
	mov	r9d, DWORD PTR invec_linc$1$[rsp]

; 1011 :       }
; 1012 :       int32_t a_off = outside_width & is_left_bm;
; 1013 :       int32_t leftoff_with_sign_neg = (left_offset ^ is_left_bm) - is_left_bm;

	mov	eax, r12d
	xor	eax, r13d
	movsxd	rsi, esi
	sub	eax, r12d

; 1017 :         __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + a_off));

	mov	r13d, r8d
	imul	r13d, edi
	mov	edi, DWORD PTR a_off$1$[rsp]

; 1018 :         __m128i b = _mm_loadu_si128((__m128i *)(ref_data + x * vec_width + (y + 0) * ref_stride + a_off - leftoff_with_sign_neg));

	movsxd	r12, eax
	npad	2
$LL26@hor_sad_ss:

; 1017 :         __m128i a = _mm_loadu_si128((__m128i *)(pic_data + x * vec_width + (y + 0) * pic_stride + a_off));

	movsxd	rcx, edx
	lea	rax, QWORD PTR [rsi+r13]
	shl	rcx, 4

; 1019 : 
; 1020 :         __m128i b_shifted    = _mm_shuffle_epi8(b,     shufmask1);
; 1021 :         __m128i b_with_old   = _mm_blendv_epi8 (old_b, b_shifted, move_old_to_b_imask);

	movdqa	xmm0, xmm7
	add	rax, rcx

; 1022 : 
; 1023 :         uint8_t startoff     = (x << vec_width_log2) + a_off;
; 1024 :         __m128i startoffs    = _mm_set1_epi8   (startoff);
; 1025 :         __m128i curr_ns      = _mm_add_epi8    (startoffs,    nslo);
; 1026 :         __m128i unrd_imask   = _mm_cmpgt_epi8  (blk_widths,   curr_ns);

	movdqa	xmm2, xmm14
	movdqu	xmm4, XMMWORD PTR [rax+r10]
	mov	rax, rsi
	sub	rax, r12
	add	rax, r14
	add	rax, rcx
	movdqu	xmm3, XMMWORD PTR [rax+r11]
	movzx	eax, dl
	add	edx, r9d
	shl	al, 4
	add	al, dil
	movsx	eax, al
	pshufb	xmm3, xmm6
	movd	xmm1, eax
	punpcklbw xmm1, xmm1
	punpcklwd xmm1, xmm1
	pshufd	xmm1, xmm1, 0
	paddb	xmm1, xmm11
	pcmpgtb	xmm2, xmm1

; 1027 :         __m128i unrd_mask    = _mm_cmpeq_epi8  (unrd_imask,   _mm_setzero_si128());

	pcmpeqb	xmm2, xmm10
	pblendvb xmm5, xmm3, xmm0

; 1028 :         __m128i b_unread     = _mm_blendv_epi8 (b_with_old,   a, unrd_mask);

	movdqa	xmm0, xmm2
	pblendvb xmm5, xmm4, xmm0
	movdqa	xmm1, xmm5

; 1029 : 
; 1030 :         old_b = b_shifted;

	movdqa	xmm5, xmm3

; 1031 : 
; 1032 :         __m128i sad_ab     = _mm_sad_epu8(a, b_unread);

	psadbw	xmm4, xmm1

; 1033 :         sse_inc = _mm_add_epi64(sse_inc, sad_ab);

	paddq	xmm4, xmm13
	movdqa	xmm13, xmm4
	cmp	edx, r15d
	jne	$LL26@hor_sad_ss
	mov	r9, QWORD PTR tv1864[rsp]
	mov	edi, DWORD PTR pic_stride$[rsp]
	mov	esi, DWORD PTR a_off$1$[rsp]
	mov	r12d, DWORD PTR is_left_bm$1$[rsp]
$LN18@hor_sad_ss:

; 995  :     for (; y < height; y++) {

	mov	edx, DWORD PTR ref_stride$[rsp]
	inc	r8d
	mov	r13d, DWORD PTR left_offset$1$[rsp]
	cmp	r8d, DWORD PTR height$[rsp]
	jl	$LL20@hor_sad_ss
$LN19@hor_sad_ss:
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c

; 227  :     return hor_sad_sse41_arbitrary(pic_data, ref_data, width, height,

	movaps	xmm14, XMMWORD PTR [rsp+320]
	movaps	xmm12, XMMWORD PTR [rsp+352]
	movaps	xmm11, XMMWORD PTR [rsp+368]
	movaps	xmm10, XMMWORD PTR [rsp+384]
	movaps	xmm7, XMMWORD PTR [rsp+432]
	movaps	xmm6, XMMWORD PTR [rsp+448]
	mov	r15, QWORD PTR [rsp+464]
	mov	r14, QWORD PTR [rsp+472]
	mov	r13, QWORD PTR [rsp+480]
	mov	r12, QWORD PTR [rsp+488]
	mov	rdi, QWORD PTR [rsp+496]
	mov	rsi, QWORD PTR [rsp+504]
	mov	rbp, QWORD PTR [rsp+528]
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 1037 :   __m128i sse_inc_2 = _mm_shuffle_epi32(sse_inc, _MM_SHUFFLE(1, 0, 3, 2));

	pshufd	xmm0, xmm13, 78				; 0000004eH

; 1038 :   __m128i sad       = _mm_add_epi64    (sse_inc, sse_inc_2);

	paddq	xmm0, xmm13
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c

; 227  :     return hor_sad_sse41_arbitrary(pic_data, ref_data, width, height,

	movaps	xmm13, XMMWORD PTR [rsp+336]
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\reg_sad_pow2_widths-sse41.h

; 1039 :   return _mm_cvtsi128_si32(sad);

	movd	eax, xmm0
; File F:\open_codec_learn_2021\kvazaar-master\src\strategies\sse41\picture-sse41.c

; 229  : }

	add	rsp, 512				; 00000200H
	pop	rbx
	ret	0
hor_sad_sse41 ENDP
_TEXT	ENDS
END
